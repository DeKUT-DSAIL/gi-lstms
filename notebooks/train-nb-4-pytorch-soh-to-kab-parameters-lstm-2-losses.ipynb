{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12908579,"sourceType":"datasetVersion","datasetId":5945289},{"sourceId":287936110,"sourceType":"kernelVersion"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook covers three model implementations:\n\n1. A physics based model based on the Gompertz function\n2. To train an LSTM model (data-driven-model) to predict a sequence (e.g., battery capacity over time)\n3. To train an LSTM model to predict a sequence (e.g., battery capacity over time) while ensuring it respects physical behavior modeled by the Gompertz function.\n\n\n## FIXES \n\n### Fix 1 : Added Sanity Check for val loss and val rmse\n\nExplanation:\nIssue\tFix\tWhy\ntorch.Tensor()\t‚Üí torch.tensor()\tThe lowercase version is the recommended constructor for creating a single-value tensor.\nMissing closing parenthesis\tAdded\tFixes syntax error.\navg_val_loss type\tEnsure it‚Äôs a scalar (float or int)\tIf it‚Äôs already a tensor, remove the outer torch.tensor() call.\n\n### Fix 2: Use log loss in calculation of metric charts\n### Fix 3: Update reproducibility\n### Fix 4: time GPU run\n### Fix 5: LSTMs that predict RUL given initial x SoH values.","metadata":{"_uuid":"16c6b475-8271-444d-85dd-c0c90ba51a4e","_cell_guid":"5fbc7d50-bc9d-4e73-aba7-d7ef5fd55ae5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\nprint(\"# A: Import Libraries and set reproducibility\")\n# !git clone https://github.com/Yuri-Njathi/battery-lstm-ML.git\n# import sys\n# sys.path.append(\"battery-lstm-ML/\")\n\n\nimport torch\nimport numpy as np\nimport random\nimport os\nimport pandas as pd\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error , root_mean_squared_error\nfrom typing import Callable, Optional\n\ndef set_seed(seed=42):\n    \"\"\"\n    Set all relevant random seeds to ensure full reproducibility.\n    \"\"\"\n    # 1. Set basic seeds\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if multiple GPUs\n    \n    # 2. Force deterministic behavior in cudnn\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False  # turn off auto-tuning\n    \n    # 3. Optional: make dataloaders deterministic\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'  # deterministic cublas (for CUDA >= 10.2)\n\n    print(f\"‚úÖ Reproducibility environment set with seed = {seed}\")\n\n# Call this once at the start, pull from assess\nset_seed(42)\n\n'''\nset mode i.e.\n0 == physics based\n1 == lstm (data driven) #SoH only \n2 == lstm (physics constrained)\n'''\nmode = 2\n\nif mode == 1:\n    model_columns = ['SoH']#,'rul','Cycle number']\nif mode == 2:\n    model_columns = ['SoH']\n\n\n\nprint(\"# B: Setup variables and functions\")\n# # Set variables\n# WINDOW_SIZE = 35\nmodel_type = ['lstm','seq2seq-lstm','pinn'][2]\n\ncutoff_soh = 0.70\n# Set Computing Environment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nINPUT_SIZE = len(model_columns)\nOUTPUT_SIZE =3 #RUL #len(model_columns)#1 #controls how many values the lstm outputs\nnum_epochs = 1000 #60\nbatch_size = 5120 #32\nnormalize_soh = False\nif normalize_soh:\n    soh_normalization_constant = 115.0 #115.0 may be better as it allows bounding between 0 and 1\n    cutoff_soh = cutoff_soh/soh_normalization_constant #set cutoff soh wrt to normalizer\nelse:\n    soh_normalization_constant = 1.0\n    cutoff_soh = cutoff_soh/soh_normalization_constant #set cutoff soh wrt to normalizer\n\nprint(\"Cutoff SoH : \",cutoff_soh)\n\ndef df_to_X_y_tensor(df, window_size=5,output_size=5):\n    '''\n    Converts a time series into (X, y) tensors for LSTM training.\n    \n    X shape: (num_samples, window_size, 1)\n    y shape: (num_samples, 1)\n    '''\n    # if isinstance(df, (pd.DataFrame, pd.Series)):\n    #     df_as_np = df.to_numpy()\n    # else:\n    #     df_as_np = df  # Assume already numpy\n\n    X, y = [], []\n    #for i in range(len(df_as_np) - window_size):\n    X.append(list(df['SoH'])[:window_size+1])\n    #y.append([df_as_np[i + window_size:i + window_size+output_size]])\n    y.append([list(df['k'])[-1],list(df['a'])[-1],list(df['b'])[-1]])\n    #append([[val] for val in df_as_np[i + window_size:i + window_size+1]]) #next cycle\n    X,y = np.array(X),np.array(y)\n    X_tensor = torch.tensor(X, dtype=torch.float32)#.squeeze()\n    y_tensor = torch.tensor(y, dtype=torch.float32)#.squeeze()\n    return X_tensor, y_tensor\n\ndef get_x_y_lists(paths):\n    X_list,y_list = [],[]\n    for path in paths:\n        print(path)\n        df = pd.read_csv(path)\n        df['Cycle number'] = df['Cycle number']/10000\n        df['rul'] = df['rul']/10000\n        #normalize SoH\n        df['SoH'] =  df['SoH']/soh_normalization_constant\n        df.index = df['Cycle number']\n        #SoH = df[model_columns]\n        X, y = df_to_X_y_tensor(df, window_size=WINDOW_SIZE,output_size=OUTPUT_SIZE)\n        X_list.append(X)\n        y_list.append(y)\n    return X_list,y_list\n\ndef give_paths_get_loaders(paths,data_type,shuffle=False):\n    X_list, y_list = get_x_y_lists(paths)\n\n    if INPUT_SIZE == 1:\n        # Concatenate all X and y\n        X_1,y_1 = torch.cat(X_list, dim=0).squeeze(-1),torch.cat(y_list, dim=0).view(-1,OUTPUT_SIZE).squeeze(-1)\n    else:\n        X_1,y_1 = torch.cat(X_list, dim=0), torch.cat(y_list, dim=0)#.view(-1, OUTPUT_SIZE)\n    \n    print(f\"X_{data_type} , y_{data_type} shapes : \",X_1.shape, y_1.shape)\n    \n    #DataLoader\n    print(\"load : \")\n    loader = DataLoader(TensorDataset(X_1, y_1), batch_size=32, shuffle=shuffle)\n    print(f\"{data_type}loader lengths : \",loader.__len__())\n    return loader,X_1,y_1\n\n\n\nprint(\"## üß† Model Architecture\")\n\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size=5, hidden_size=64, output_size=5):\n        super(LSTMModel, self).__init__()\n        \n        # LSTM: input_size=5 match your features\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        \n        # Linear Layer: Maps hidden_size (64) -> output_size (5)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # x shape: [32, 10, 5]\n        \n        # Run LSTM\n        # lstm_out shape: [32, 10, 64]\n        lstm_out, _ = self.lstm(x)\n        \n        # Take the last time step only\n        last_time_step = lstm_out[:, -1, :] \n        # last_time_step shape: [32, 64]\n        \n        # Project to 5 output features\n        prediction = self.fc(last_time_step)\n        # prediction shape: [32, 5]\n        \n        return prediction\n\nclass PhysicsInformedLSTM(nn.Module):\n    def __init__(self, input_size=5, hidden_size=64):\n        super(PhysicsInformedLSTM, self).__init__()\n        \n        # 1. LSTM Core\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        \n        # 2. Split Heads (One for each parameter)\n        # We split them so we can apply different activations to each\n        self.fc_k = nn.Linear(hidden_size, 1)\n        self.fc_a = nn.Linear(hidden_size, 1)\n        self.fc_b = nn.Linear(hidden_size, 1)\n        \n        # Activation to force k > 0 (Capacity cannot be negative)\n        self.softplus = nn.Softplus()\n\n    def forward(self, x):\n        # x shape: [Batch, Window, Feats]\n        \n        lstm_out, _ = self.lstm(x)\n        \n        # Take the last time step\n        last_time_step = lstm_out[:, -1, :] \n        \n        # --- PREDICT PARAMETERS SEPARATELY ---\n        \n        # 1. Predict k (Enforce Positive)\n        # Softplus ensures output is always > 0\n        k_pred = self.softplus(self.fc_k(last_time_step))\n        \n        # 2. Predict a (Can be negative or positive)\n        a_pred = self.fc_a(last_time_step)\n        \n        # 3. Predict b (Usually small positive, but let's keep it linear for now)\n        b_pred = self.fc_b(last_time_step)\n        \n        # Concatenate them back into shape [Batch, 3]\n        prediction = torch.cat([k_pred, a_pred, b_pred], dim=1)\n        \n        return prediction\n\n\nclass Seq2SeqLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, mid_size, output_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(hidden_size, mid_size)\n        self.fc2 = nn.Linear(mid_size, output_size)\n\n    def forward(self, x):\n        # out: [batch_size, seq_len, hidden_size]\n        out, _ = self.lstm(x)\n\n        # Apply the linear layers to each timestep\n        mid = self.relu(self.fc1(out))   # shape: [batch, seq_len, mid_size]\n        out_seq = self.fc2(mid)          # shape: [batch, seq_len, output_size]\n\n        return out_seq\n\n\n    #on initial tensorflow experiments I used 1,64,1,1 for those values.\n\n### TEST ON SEQUENTIAL MODEL ###\n# model(torch.Tensor([[86.4707],[86.4150],[86.3590],[86.3035],[86.2506],[86.2512],[86.1954],[86.1403],[86.1427],[86.0904],[86.0373],[85.9772],[85.9743],[85.9198],[85.8654],[85.8090],[85.8077],[85.7524],[85.6986],[85.6407],[85.5883],[85.5882],[85.6112],[85.4756],[85.4753],[85.4187],[85.3639],[85.3086],[85.3098],[85.3628],[85.1723],[85.1430],[85.1444],[85.0896],[85.0364]]))","metadata":{"_uuid":"d905f68b-0092-4d8e-980e-bd9d93d36ec8","_cell_guid":"bc0a48fc-f795-4b2c-a672-5942fa073d9a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-23T11:23:25.576557Z","iopub.execute_input":"2026-01-23T11:23:25.577308Z","iopub.status.idle":"2026-01-23T11:23:25.601049Z","shell.execute_reply.started":"2026-01-23T11:23:25.577278Z","shell.execute_reply":"2026-01-23T11:23:25.600453Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"# A: Import Libraries and set reproducibility\n‚úÖ Reproducibility environment set with seed = 42\n# B: Setup variables and functions\nCutoff SoH :  0.7\n## üß† Model Architecture\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"#### ‚ÄúNormalization‚Äù ‚â† ‚Äúscaling to [0,1]‚Äù.\n\n#### It simply means rescaling values to a stable, comparable numerical range.","metadata":{"_uuid":"5a5cf86c-22c4-4f30-a2ea-27e58384e19a","_cell_guid":"bfed866a-86b9-4ad2-9f29-45f1fac22912","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# C: Setup Train, Val and Test Loaders\n# 0. Data\n\nEach `X_train` is of shape `(num_samples, window_size)`\n\nEach `y_train` is of shape `(num_samples,)` (usually next value prediction)","metadata":{"_uuid":"e5c11278-599b-4fcf-a3a3-4b5cffc4aab6","_cell_guid":"ae57eb62-c436-4826-96f2-cc55c36376ee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"##  üß∞ Convert to Tensors for LSTM\nLSTM expects input shape: (batch_size, sequence_length, num_features)\n\nLet‚Äôs reshape the data and convert it:","metadata":{"_uuid":"e55a3991-26a1-4a5a-9aa1-a4095a055a67","_cell_guid":"e87496f0-d330-454c-82f9-86f0f16e746d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T11:23:25.602431Z","iopub.execute_input":"2026-01-23T11:23:25.602685Z","iopub.status.idle":"2026-01-23T11:23:25.613578Z","shell.execute_reply.started":"2026-01-23T11:23:25.602643Z","shell.execute_reply":"2026-01-23T11:23:25.613062Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"WINDOW_SIZES = [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100,500,1000] #[i for i in range (5,100,5)]\nprint(\"WINDOW SIZES TO TEST : \",WINDOW_SIZES,len(WINDOW_SIZES))\n\n\n# Make list of CSV paths\nmain_files_path = '/kaggle/input/generate-hust-data-gompertz-k-a-b/'\n#/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\ncsv_files = os.listdir(main_files_path)\ncsv_files = [f for f in csv_files if re.match(r'^\\d', f) and f.endswith('-hust_gompertz_params.csv')]\n\n#BatteryML like train-val-test split\ncsv_files = [f.removesuffix('-hust_gompertz_params.csv') for f in csv_files]\nprint(csv_files)\n\ntrain_ids = [\n    '1-3',  '1-4',  '1-5',  '1-6',  '1-7',  '1-8',  '2-2',  '2-3',\n    '2-4',  '2-6',  '2-7',  '2-8',  '3-2',  '3-3',  '3-4',  '3-5',\n    '3-6',  '3-7',  '3-8',  '4-1',  '4-2',  '4-3',  '4-4',  '4-6',\n    '4-7',  '4-8',  '5-1',  '5-2',  '5-4',  '5-5',  '5-6',  '5-7',\n    '6-3',  '6-4',  '6-5',  '7-1',  '7-2',  '7-3',  '7-4',  '7-7',\n    '7-8',  '8-2',  '8-3',  '8-4',  '8-7',  '9-1',  '9-2',  '9-3',\n    '9-5',  '9-7',  '9-8',  '10-2', '10-3', '10-5', '10-8']\n\ntest_ids = [f for f in csv_files if f not in train_ids]\n\nprint(test_ids,len(test_ids))\n\n#csv_paths = [os.path.join(main_files_path, file) for file in csv_files]\n#separate according to train, val and test\ntrain_paths = [os.path.join(main_files_path, file+'-hust_gompertz_params.csv') for file in train_ids]\n\ntesting_paths = [os.path.join(main_files_path, file+'-hust_gompertz_params.csv') for file in test_ids]\n\nval_paths = testing_paths[:int(len(testing_paths)*0.5)]\ntest_paths = testing_paths[int(len(testing_paths)*0.5):]\n\nprint(len(train_paths), len(val_paths), len(test_paths))","metadata":{"_uuid":"764511b2-155c-4f27-9abb-b9020665168e","_cell_guid":"da093279-8ac7-4672-b25f-7a87931a0801","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-23T11:23:25.614497Z","iopub.execute_input":"2026-01-23T11:23:25.614771Z","iopub.status.idle":"2026-01-23T11:23:25.634286Z","shell.execute_reply.started":"2026-01-23T11:23:25.614741Z","shell.execute_reply":"2026-01-23T11:23:25.633735Z"}},"outputs":[{"name":"stdout","text":"WINDOW SIZES TO TEST :  [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100, 500, 1000] 16\n['6-6', '8-7', '8-6', '9-1', '10-1', '6-8', '8-8', '10-7', '3-5', '5-1', '5-5', '7-1', '2-7', '10-6', '4-7', '7-7', '7-6', '4-5', '9-2', '10-4', '3-1', '9-7', '8-1', '10-8', '8-4', '4-6', '4-4', '3-8', '5-4', '9-6', '10-5', '7-8', '5-2', '9-8', '1-2', '5-6', '10-2', '2-6', '6-1', '2-4', '1-4', '4-1', '1-6', '6-2', '8-5', '5-7', '1-5', '1-8', '5-3', '6-5', '9-5', '4-8', '7-2', '2-5', '7-3', '9-3', '9-4', '8-2', '10-3', '6-3', '3-2', '7-5', '3-7', '2-3', '1-3', '8-3', '2-8', '7-4', '4-2', '6-4', '1-1', '3-3', '4-3', '3-4', '2-2', '1-7', '3-6']\n['6-6', '8-6', '10-1', '6-8', '8-8', '10-7', '10-6', '7-6', '4-5', '10-4', '3-1', '8-1', '9-6', '1-2', '6-1', '6-2', '8-5', '5-3', '2-5', '9-4', '7-5', '1-1'] 22\n55 11 11\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"for WINDOW_SIZE in WINDOW_SIZES:\n    #OUTPUT_SIZE = WINDOW_SIZE\n    print(\"## üß† Model\")\n    if model_type == 'lstm':\n        model = LSTMModel(input_size=INPUT_SIZE, output_size=OUTPUT_SIZE).to(device) # values for multioutput model\n        last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n        print(\"Last model window : \",last_model_path)\n    if model_type == 'seq2seq-lstm':\n        model = Seq2SeqLSTM(INPUT_SIZE,64,32,OUTPUT_SIZE).to(device) # values from previously working tensorflow model # 1, 64, 8, 1 \n        last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n        print(\"Last model window : \",last_model_path)\n    if model_type == 'pinn':\n        model = PhysicsInformedLSTM(input_size=INPUT_SIZE).to(device) # values for multioutput model\n        last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n        print(\"Last model window : \",last_model_path)\n    #### PREPARE ALL DATA ####\n    ### DONT RUN AS LOOP\n    def give_paths_get_loaders(paths,data_type,shuffle=False):\n        X_list, y_list = get_x_y_lists(paths)\n    \n        if INPUT_SIZE == 1:\n            # Concatenate all X and y\n            X_1,y_1 = torch.cat(X_list, dim=0).unsqueeze(-1),torch.cat(y_list, dim=0).view(-1,OUTPUT_SIZE)\n        else:\n            X_1,y_1 = torch.cat(X_list, dim=0).squeeze(2),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE)\n        \n        print(f\"X_{data_type} , y_{data_type} shapes : \",X_1.shape, y_1.shape)\n        \n        #DataLoader\n        print(\"load : \")\n        loader = DataLoader(TensorDataset(X_1, y_1), batch_size=32, shuffle=shuffle)\n        print(f\"{data_type}loader lengths : \",loader.__len__())\n        return loader,X_1,y_1\n    \n    data_use = {\n        0:[\"train\"],1:[\"val\"],2:[\"test\"]\n    }\n    train_loader,X_train,y_train = give_paths_get_loaders(train_paths,data_use[0],shuffle=True)\n    val_loader,X_val,y_val= give_paths_get_loaders(val_paths,data_use[1])\n    test_loader,X_test,y_test = give_paths_get_loaders(test_paths,data_use[2])\n    #break\n    print('''##\n    ### üìà Gompertz Function (Physics Law)\n    \n    * `x`: Time (or cycle number)\n    \n    * `k`: Max value (e.g., max capacity)\n    \n    * `a`, `b`: Shape parameters''')\n    \n    def gompertz_func(x, k, a, b):\n        return k * torch.exp(-a * torch.exp(-b * x))\n    \n    print(\"## üß† Loss Functions\\n\")\n    \n    print('''## ‚öôÔ∏è 1. Data-Informed Loss Function\n    a data loss (what the LSTM learns from data)\n    \n    * Mean Squared Error for Training\n    * RMSE for autoregressive approximation of compound error\n    \n    ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n    You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n    \n    * `alpha`: controls how strongly physics is enforced.''')\n        \n    def pinn_loss(prediction, target, x, k, a, b, alpha=0.5):\n        data_loss = F.mse_loss(prediction, target)\n        physics_pred = gompertz_func(x, k, a, b) #this needs to be refined !!!\n        physics_loss = F.mse_loss(prediction, physics_pred)\n        return data_loss + alpha * physics_loss, data_loss.item(), physics_loss.item()\n    \n    def data_loss_func(prediction, target, x, alpha=1.0):\n        data_loss = F.mse_loss(prediction, target)\n        return data_loss, data_loss.item() , None\n\n    def inverse_time_loss(prediction, y_true, x_true, lambda_time=0.1):\n        \"\"\"\n        Calculates the error between the actual time (x_true) and the \n        time derived from the predicted parameters (x_calc).\n        \n        Formula: x = (a - ln(ln(k/y))) / b\n        \"\"\"\n        # 1. Extract Parameters\n        k_pred = prediction[:, 0:1] # Ensure this is activated (e.g. Softplus) so it's positive\n        a_pred = prediction[:, 1:2]\n        b_pred = prediction[:, 2:3]\n        \n        # 2. Safety: Ensure k is slightly larger than y to avoid log(negative)\n        # We create a 'safe' ratio that is always >= 1.0001\n        epsilon = 1e-6\n        # If model predicts k < y, we clamp the ratio to 1.0001 to avoid crash, \n        # but the gradients from the other losses will eventually fix k.\n        ratio = k_pred / (y_true + epsilon)\n        safe_ratio = torch.clamp(ratio, min=1.0001)\n    \n        # 3. Calculate the Inverse (Time)\n        # Inner log: ln(k/y)\n        inner_term = torch.log(safe_ratio)\n        \n        # Double log: ln(ln(k/y))\n        # We clamp inner_term to avoid log(0) if ratio was exactly 1\n        double_log = torch.log(torch.clamp(inner_term, min=epsilon))\n        \n        # Calculate x\n        # Add epsilon to b to avoid division by zero\n        x_calc = (a_pred - double_log) / (b_pred + epsilon)\n    \n        # 4. Calculate MSE between Real Time and Calculated Time\n        time_loss = F.mse_loss(x_calc, x_true)\n        \n        return lambda_time * time_loss\n    \n    def physics_informed_loss(prediction, target, x, alpha=1.0, lambda_ode=0.1):\n        \"\"\"\n        prediction: (Batch, 3) -> [32, 3] (One set of k,a,b per sequence)\n        target:     (Batch, 3) -> [32, 3]\n        x:          (Batch, Window, 1) -> [32, 5, 1] (Time steps)\n        \"\"\"\n        \n        # 1. Standard Data Loss (MSE on parameters k, a, b)\n        # prediction and target are both [32, 3], so this works directly.\n        data_loss = F.mse_loss(prediction, target)\n    \n        # 2. Prepare Physics Inputs\n        if not x.requires_grad:\n            x.requires_grad_(True)\n    \n        # --- Slicing & Reshaping (THE FIX) ---\n        # Prediction is [32, 3]. We need [32, 1, 1] to broadcast over the Window dimension of x (5).\n        \n        # Take column 0, keep dims -> [32, 1], then add extra dim -> [32, 1, 1]\n        k_pred = prediction[:, 0:1].unsqueeze(1) \n        a_pred = prediction[:, 1:2].unsqueeze(1) \n        b_pred = prediction[:, 2:3].unsqueeze(1) \n\n        # Inside physics_informed_loss, before math operations:\n        k_pred = torch.clamp(k_pred, min=0, max=2)\n        a_pred = torch.clamp(a_pred, min=-5, max=-1)\n        b_pred = torch.clamp(b_pred, min=-30, max=-3)\n        # Now:\n        # b_pred: [32, 1, 1]\n        # x:      [32, 5, 1]\n        # Result: [32, 5, 1] (Broadcasting works!)\n        exponent_term = a_pred - (b_pred * x)\n        exponent_term = torch.clamp(exponent_term, min=-10.0, max=10.0)\n\n        y_pred_curve = k_pred * torch.exp(-torch.exp(exponent_term))\n    \n        # --- Physics Gradients ---\n        \n        # Calculate dy/dx (autograd)\n        dydx_autograd = torch.autograd.grad(\n            outputs=y_pred_curve,\n            inputs=x,\n            grad_outputs=torch.ones_like(y_pred_curve),\n            create_graph=True, \n            retain_graph=True,\n            only_inputs=True\n        )[0]\n    \n        # Calculate dy/dx (ODE Equation)\n        # dy/dx = b * y * e^(a-bx)\n        dydx_ode = b_pred * y_pred_curve * torch.exp(exponent_term)\n    \n        # Residual\n        ode_residual = dydx_autograd - dydx_ode\n        ode_loss = torch.mean(ode_residual ** 2)\n\n        #add RUL loss\n        #loss_rul = inverse_time_loss(prediction, target_y, x_seq)\n        \n        total_loss = (alpha * data_loss) + (lambda_ode * ode_loss) #++ loss_rul\n    \n        return total_loss, data_loss.item(), ode_loss.item()\n        \n    epoch = 0\n    avg_train_loss = 0.0\n    avg_val_loss = 0.0\n    data_loss = 0.0\n    phys_loss = None  # Only set if you're using physics loss\n    \n    if mode == 0:\n        criterion = physics_loss #??\n        best_model_path = f'best_pysics_model-window-{WINDOW_SIZE}.pth' #??\n        loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n    if mode == 1:\n        criterion = data_loss_func\n        best_model_path = f'best_lstm_model-window-{WINDOW_SIZE}.pth'\n        loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f}\"\n    if mode == 2:\n        # phys_loss = 0.0 # Only set if you're using physics loss\n        # criterion = pinn_loss\n        # best_model_path = f'best_pinn_lstm_model-window-{WINDOW_SIZE}.pth'\n        # loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n        criterion = physics_informed_loss\n        best_model_path = f'best_lstm_model-window-{WINDOW_SIZE}.pth'\n        loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f}\"\n    \n    print(\"## üõ†Ô∏è Parameter Strategy\")\n    \n    # k = nn.Parameter(torch.tensor(1.0))\n    # a = nn.Parameter(torch.tensor(0.1))\n    # b = nn.Parameter(torch.tensor(0.1))\n    # Include in optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=100)\n    \n    print(\"## üîÅ Training Loop\")\n    def compute_rmse(pred, target):\n        return torch.sqrt(F.mse_loss(pred, target))\n        \n    train_losses = []\n    val_losses = []\n    val_rmses = []\n    data_losses = []\n    phys_losses = []\n    \n    best_val_loss = float('inf')\n    best_epoch = 0\n    \n    # # 2. Create the sequence from 1 to WINDOW_SIZE\n    # # torch.arange(start, end) excludes the end, so we use WINDOW_SIZE + 1\n    # x_raw = torch.arange(1, WINDOW_SIZE + 1, dtype=torch.float32, device=device)\n    \n    # # 3. Scale it (divide by 10000)\n    # x_scaled = x_raw / 10000.0\n    \n    # # 4. Reshape to (N, 1) and enable gradients\n    # # .view(-1, 1) makes it a column vector. \n    # # .requires_grad_(True) is vital for the physics loss (dydx calculation).\n    # x_seq = x_scaled.view(-1, 1).requires_grad_(True)\n\n    # Provide as a train function\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        total_data_loss = 0\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            #Set computing environment\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            #Predict\n            y_pred = model(X_batch)\n            \n\n            # Create x_seq for physics loss\n            #x_seq = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_batch.size(0), 1).to(X_batch.device)\n            x_seq = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_batch.size(0), 1).unsqueeze(-1).to(X_batch.device)\n            \n            loss, data_loss, phys_loss = criterion(y_pred, y_batch, x_seq, alpha=0.3)\n            # loss.backward()\n            # optimizer.step()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # <--- ADD THIS\n            optimizer.step()\n            total_data_loss += data_loss#.item() #TRY\n            total_loss += loss.item()\n        \n        # Adjust learning rate\n        scheduler.step()\n        avg_data_loss = total_data_loss / len(train_loader)\n        avg_train_loss = total_loss / len(train_loader)\n    \n        # ---- Validation Pass ----\n        model.eval()\n        val_loss_total = 0.0\n        val_rmse = 0.0\n        #with torch.no_grad():\n        for X_val_batch, y_val_batch in val_loader:\n            #Set computing environment\n            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n            #Predict\n            y_val_pred = model(X_val_batch)\n\n            #ADD A METRIC\n            val_rmse += compute_rmse(y_val_pred, y_val_batch)\n\n            x_seq_val = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_val_batch.size(0), 1).unsqueeze(-1).to(X_val_batch.device)\n            # = x_seq#torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_val_batch.size(0), 1).to(X_val_batch.device)\n            val_loss, _, _ = criterion(y_val_pred, y_val_batch, x_seq_val, alpha=0.3)\n\n            val_loss_total += val_loss.item()\n    \n        avg_val_loss = val_loss_total / len(val_loader)\n        avg_val_rmse = val_rmse / len(val_loader)\n        # Save model if validation improves\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            best_epoch = epoch\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"‚úÖ Saved best model at epoch {epoch+1} (Val Loss = {best_val_loss:.8f})\")\n        if epoch+1 == num_epochs:\n            torch.save(model.state_dict(),last_model_path)\n            print(f\"‚úÖ Saved last model at epoch {epoch+1} \")\n    \n    \n        if mode == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f}\")\n        if mode == 1:\n            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f} | ‚àö(Val Loss) = {torch.sqrt(torch.tensor(avg_val_loss)):.8f}\")\n        if mode == 2:\n            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f} | ‚àö(Val Loss) = {torch.sqrt(torch.tensor(avg_val_loss)):.8f}\")\n            #print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f}\")\n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        val_rmses.append(avg_val_rmse.item())  # assuming avg_val_rmse is a tensor\n        data_losses.append(avg_data_loss)   # assuming this is the last batch's data loss\n        phys_losses.append(phys_loss)   # assuming this is the last batch's physics loss\n    \n    model.load_state_dict(torch.load(best_model_path))\n    \n    np.savez(f\"training_metrics__window_{WINDOW_SIZE}_model_{model_type}.npz\",\n             train_losses=train_losses,\n             val_losses=val_losses,\n             val_rmses=val_rmses,\n             data_losses=data_losses,\n             phys_losses=phys_losses)\n    print(\"Plot losses after training 3:\")\n    plt.figure(figsize=(12, 6))\n    plt.plot(train_losses[3:], label=\"Train Loss\")\n    plt.plot(val_losses[3:], label=\"Val Loss\")\n    plt.plot(val_rmses[3:], label=\"Val RMSE\")\n    #plt.plot(data_losses[3:], label=\"Data Loss\")\n    plt.plot(phys_losses[3:], label=\"Physics Loss\")\n    x_line = best_epoch \n    plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n    plt.text(x_line + 0.05, best_val_loss, 'Saved Model', rotation=45, color='red')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Training & Validation Metrics\")\n    plt.grid(True)\n    plt.savefig(fname = f\"history-from-3-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n    plt.show()\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Val Loss\")\n    plt.plot(val_rmses, label=\"Val RMSE\")\n    #plt.plot(data_losses, label=\"Data Loss\")\n    plt.plot(phys_losses, label=\"Physics Loss\")\n    plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n    plt.text(x_line + 0.05, best_val_loss, 'Saved Model', rotation=45, color='red')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Training & Validation Metrics\")\n    plt.grid(True)\n    plt.savefig(fname = f\"history-full-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n    plt.show()\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Val Loss\")\n    plt.plot(val_rmses, label=\"Val RMSE\")\n    #plt.plot(data_losses, label=\"Data Loss\")\n    #plt.plot(phys_losses, label=\"Physics Loss\")\n    plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n    plt.text(x_line, best_val_loss*10, 'Saved Model', rotation=30, color='red')\n    plt.yscale('log')  # visualize on log scale\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Training & Validation Metrics\")\n    plt.grid(True)\n    plt.savefig(fname = f\"history-full-log-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n    plt.show()\n\n    test_rmse = 0 \n    \n    for X_batch, y_batch in test_loader:\n        # Calculate RMSE directly\n        test_rmse += root_mean_squared_error(y_batch, model(X_batch.to(device)).cpu().detach().numpy())\n    print('Test Error : ',test_rmse*10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T11:23:25.707636Z","iopub.execute_input":"2026-01-23T11:23:25.708213Z","execution_failed":"2026-01-23T11:26:55.804Z"}},"outputs":[{"name":"stdout","text":"## üß† Model\nLast model window :  last_model_window_5_model_pinn.pth\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-8-hust_gompertz_params.csv\nX_['train'] , y_['train'] shapes :  torch.Size([55, 6, 1]) torch.Size([55, 3])\nload : \n['train']loader lengths :  2\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\nX_['val'] , y_['val'] shapes :  torch.Size([11, 6, 1]) torch.Size([11, 3])\nload : \n['val']loader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\nX_['test'] , y_['test'] shapes :  torch.Size([11, 6, 1]) torch.Size([11, 3])\nload : \n['test']loader lengths :  1\n##\n    ### üìà Gompertz Function (Physics Law)\n    \n    * `x`: Time (or cycle number)\n    \n    * `k`: Max value (e.g., max capacity)\n    \n    * `a`, `b`: Shape parameters\n## üß† Loss Functions\n\n## ‚öôÔ∏è 1. Data-Informed Loss Function\n    a data loss (what the LSTM learns from data)\n    \n    * Mean Squared Error for Training\n    * RMSE for autoregressive approximation of compound error\n    \n    ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n    You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n    \n    * `alpha`: controls how strongly physics is enforced.\n## üõ†Ô∏è Parameter Strategy\n## üîÅ Training Loop\n‚úÖ Saved best model at epoch 1 (Val Loss = 23.69785309)\nEpoch 1/1000 | Train Loss=23.93281841 | Val Loss=23.69785309 | Data=79.77605820 | Val RMSE: 8.88779163 | ‚àö(Val Loss) = 4.86804390\n‚úÖ Saved best model at epoch 2 (Val Loss = 23.60608864)\nEpoch 2/1000 | Train Loss=23.78827667 | Val Loss=23.60608864 | Data=79.29425049 | Val RMSE: 8.87056732 | ‚àö(Val Loss) = 4.85860968\n‚úÖ Saved best model at epoch 3 (Val Loss = 23.51069450)\nEpoch 3/1000 | Train Loss=22.99764729 | Val Loss=23.51069450 | Data=76.65882111 | Val RMSE: 8.85262585 | ‚àö(Val Loss) = 4.84878302\n‚úÖ Saved best model at epoch 4 (Val Loss = 23.40917015)\nEpoch 4/1000 | Train Loss=23.92515564 | Val Loss=23.40917015 | Data=79.75051498 | Val RMSE: 8.83349133 | ‚àö(Val Loss) = 4.83830261\n‚úÖ Saved best model at epoch 5 (Val Loss = 23.29839134)\nEpoch 5/1000 | Train Loss=23.52482414 | Val Loss=23.29839134 | Data=78.41607666 | Val RMSE: 8.81256485 | ‚àö(Val Loss) = 4.82684088\n‚úÖ Saved best model at epoch 6 (Val Loss = 23.17461586)\nEpoch 6/1000 | Train Loss=23.49326134 | Val Loss=23.17461586 | Data=78.31086731 | Val RMSE: 8.78912544 | ‚àö(Val Loss) = 4.81400204\n‚úÖ Saved best model at epoch 7 (Val Loss = 23.03315544)\nEpoch 7/1000 | Train Loss=23.37749004 | Val Loss=23.03315544 | Data=77.92496109 | Val RMSE: 8.76225948 | ‚àö(Val Loss) = 4.79928684\n‚úÖ Saved best model at epoch 8 (Val Loss = 22.86793518)\nEpoch 8/1000 | Train Loss=22.73592758 | Val Loss=22.86793518 | Data=75.78642273 | Val RMSE: 8.73077583 | ‚àö(Val Loss) = 4.78204298\n‚úÖ Saved best model at epoch 9 (Val Loss = 22.67092514)\nEpoch 9/1000 | Train Loss=22.62322044 | Val Loss=22.67092514 | Data=75.41073227 | Val RMSE: 8.69308662 | ‚àö(Val Loss) = 4.76139927\n‚úÖ Saved best model at epoch 10 (Val Loss = 22.43147278)\nEpoch 10/1000 | Train Loss=22.46401691 | Val Loss=22.43147278 | Data=74.88005447 | Val RMSE: 8.64705563 | ‚àö(Val Loss) = 4.73618746\n‚úÖ Saved best model at epoch 11 (Val Loss = 22.13552094)\nEpoch 11/1000 | Train Loss=22.73560238 | Val Loss=22.13552094 | Data=75.78533554 | Val RMSE: 8.58982372 | ‚àö(Val Loss) = 4.70484018\n‚úÖ Saved best model at epoch 12 (Val Loss = 21.76485062)\nEpoch 12/1000 | Train Loss=21.97898769 | Val Loss=21.76485062 | Data=73.26329041 | Val RMSE: 8.51759911 | ‚àö(Val Loss) = 4.66528130\n‚úÖ Saved best model at epoch 13 (Val Loss = 21.29688644)\nEpoch 13/1000 | Train Loss=21.39242554 | Val Loss=21.29688644 | Data=71.30808258 | Val RMSE: 8.42553329 | ‚àö(Val Loss) = 4.61485481\n‚úÖ Saved best model at epoch 14 (Val Loss = 20.70630836)\nEpoch 14/1000 | Train Loss=20.78804874 | Val Loss=20.70630836 | Data=69.29349136 | Val RMSE: 8.30788898 | ‚àö(Val Loss) = 4.55041838\n‚úÖ Saved best model at epoch 15 (Val Loss = 19.97029877)\nEpoch 15/1000 | Train Loss=20.08842659 | Val Loss=19.97029877 | Data=66.96141815 | Val RMSE: 8.15890026 | ‚àö(Val Loss) = 4.46881390\n‚úÖ Saved best model at epoch 16 (Val Loss = 19.07863808)\nEpoch 16/1000 | Train Loss=19.75863171 | Val Loss=19.07863808 | Data=65.86210251 | Val RMSE: 7.97467566 | ‚àö(Val Loss) = 4.36790991\n‚úÖ Saved best model at epoch 17 (Val Loss = 18.04684067)\nEpoch 17/1000 | Train Loss=18.98403740 | Val Loss=18.04684067 | Data=63.28012085 | Val RMSE: 7.75603819 | ‚àö(Val Loss) = 4.24815750\n‚úÖ Saved best model at epoch 18 (Val Loss = 16.92300606)\nEpoch 18/1000 | Train Loss=17.85394764 | Val Loss=16.92300606 | Data=59.51315498 | Val RMSE: 7.51066017 | ‚àö(Val Loss) = 4.11375809\n‚úÖ Saved best model at epoch 19 (Val Loss = 15.77782726)\nEpoch 19/1000 | Train Loss=16.52831507 | Val Loss=15.77782726 | Data=55.09438133 | Val RMSE: 7.25208616 | ‚àö(Val Loss) = 3.97213125\n‚úÖ Saved best model at epoch 20 (Val Loss = 14.67993450)\nEpoch 20/1000 | Train Loss=15.45046854 | Val Loss=14.67993450 | Data=51.50156021 | Val RMSE: 6.99522066 | ‚àö(Val Loss) = 3.83144021\n‚úÖ Saved best model at epoch 21 (Val Loss = 13.67485237)\nEpoch 21/1000 | Train Loss=14.28366184 | Val Loss=13.67485237 | Data=47.61220360 | Val RMSE: 6.75150633 | ‚àö(Val Loss) = 3.69795251\n‚úÖ Saved best model at epoch 22 (Val Loss = 12.77987003)\nEpoch 22/1000 | Train Loss=13.65857792 | Val Loss=12.77987003 | Data=45.52859116 | Val RMSE: 6.52683401 | ‚àö(Val Loss) = 3.57489443\n‚úÖ Saved best model at epoch 23 (Val Loss = 11.99148273)\nEpoch 23/1000 | Train Loss=12.40855932 | Val Loss=11.99148273 | Data=41.36186218 | Val RMSE: 6.32231045 | ‚àö(Val Loss) = 3.46287203\n‚úÖ Saved best model at epoch 24 (Val Loss = 11.29535198)\nEpoch 24/1000 | Train Loss=11.89186573 | Val Loss=11.29535198 | Data=39.63955116 | Val RMSE: 6.13605499 | ‚àö(Val Loss) = 3.36085582\n‚úÖ Saved best model at epoch 25 (Val Loss = 10.67420387)\nEpoch 25/1000 | Train Loss=10.91444778 | Val Loss=10.67420387 | Data=36.38149071 | Val RMSE: 5.96495390 | ‚àö(Val Loss) = 3.26713991\n‚úÖ Saved best model at epoch 26 (Val Loss = 10.11203671)\nEpoch 26/1000 | Train Loss=10.71757841 | Val Loss=10.11203671 | Data=35.72525978 | Val RMSE: 5.80575466 | ‚àö(Val Loss) = 3.17994285\n‚úÖ Saved best model at epoch 27 (Val Loss = 9.59653664)\nEpoch 27/1000 | Train Loss=9.91071320 | Val Loss=9.59653664 | Data=33.03570843 | Val RMSE: 5.65583372 | ‚àö(Val Loss) = 3.09782767\n‚úÖ Saved best model at epoch 28 (Val Loss = 9.11929989)\nEpoch 28/1000 | Train Loss=9.61833572 | Val Loss=9.11929989 | Data=32.06111717 | Val RMSE: 5.51340771 | ‚àö(Val Loss) = 3.01981783\n‚úÖ Saved best model at epoch 29 (Val Loss = 8.67560005)\nEpoch 29/1000 | Train Loss=8.54545784 | Val Loss=8.67560005 | Data=28.48485851 | Val RMSE: 5.37760782 | ‚àö(Val Loss) = 2.94543719\n‚úÖ Saved best model at epoch 30 (Val Loss = 8.26268578)\nEpoch 30/1000 | Train Loss=8.69229484 | Val Loss=8.26268578 | Data=28.97431469 | Val RMSE: 5.24807453 | ‚àö(Val Loss) = 2.87448883\n‚úÖ Saved best model at epoch 31 (Val Loss = 7.87870455)\nEpoch 31/1000 | Train Loss=7.90653777 | Val Loss=7.87870455 | Data=26.35512543 | Val RMSE: 5.12468052 | ‚àö(Val Loss) = 2.80690312\n‚úÖ Saved best model at epoch 32 (Val Loss = 7.52152205)\nEpoch 32/1000 | Train Loss=7.74927092 | Val Loss=7.52152205 | Data=25.83090210 | Val RMSE: 5.00716877 | ‚àö(Val Loss) = 2.74253941\n‚úÖ Saved best model at epoch 33 (Val Loss = 7.18887901)\nEpoch 33/1000 | Train Loss=7.18976188 | Val Loss=7.18887901 | Data=23.96587276 | Val RMSE: 4.89519453 | ‚àö(Val Loss) = 2.68120861\n‚úÖ Saved best model at epoch 34 (Val Loss = 6.87839174)\nEpoch 34/1000 | Train Loss=7.11003351 | Val Loss=6.87839174 | Data=23.70011139 | Val RMSE: 4.78831625 | ‚àö(Val Loss) = 2.62266874\n‚úÖ Saved best model at epoch 35 (Val Loss = 6.58796978)\nEpoch 35/1000 | Train Loss=6.77194238 | Val Loss=6.58796978 | Data=22.57314014 | Val RMSE: 4.68613911 | ‚àö(Val Loss) = 2.56670403\n‚úÖ Saved best model at epoch 36 (Val Loss = 6.31585026)\nEpoch 36/1000 | Train Loss=6.27574110 | Val Loss=6.31585026 | Data=20.91913605 | Val RMSE: 4.58833647 | ‚àö(Val Loss) = 2.51313543\n‚úÖ Saved best model at epoch 37 (Val Loss = 6.06045675)\nEpoch 37/1000 | Train Loss=6.11692810 | Val Loss=6.06045675 | Data=20.38975906 | Val RMSE: 4.49461031 | ‚àö(Val Loss) = 2.46179938\n‚úÖ Saved best model at epoch 38 (Val Loss = 5.82031536)\nEpoch 38/1000 | Train Loss=6.17555308 | Val Loss=5.82031536 | Data=20.58517647 | Val RMSE: 4.40466213 | ‚àö(Val Loss) = 2.41253304\n‚úÖ Saved best model at epoch 39 (Val Loss = 5.59393167)\nEpoch 39/1000 | Train Loss=5.55798960 | Val Loss=5.59393167 | Data=18.52663088 | Val RMSE: 4.31815243 | ‚àö(Val Loss) = 2.36514950\n‚úÖ Saved best model at epoch 40 (Val Loss = 5.37981462)\nEpoch 40/1000 | Train Loss=5.37194729 | Val Loss=5.37981462 | Data=17.90649033 | Val RMSE: 4.23470354 | ‚àö(Val Loss) = 2.31944275\n‚úÖ Saved best model at epoch 41 (Val Loss = 5.17664480)\nEpoch 41/1000 | Train Loss=5.40162873 | Val Loss=5.17664480 | Data=18.00542831 | Val RMSE: 4.15397167 | ‚àö(Val Loss) = 2.27522421\n‚úÖ Saved best model at epoch 42 (Val Loss = 4.98336124)\nEpoch 42/1000 | Train Loss=4.92651033 | Val Loss=4.98336124 | Data=16.42170048 | Val RMSE: 4.07568455 | ‚àö(Val Loss) = 2.23234439\n‚úÖ Saved best model at epoch 43 (Val Loss = 4.79909706)\nEpoch 43/1000 | Train Loss=4.92517614 | Val Loss=4.79909706 | Data=16.41725349 | Val RMSE: 3.99962354 | ‚àö(Val Loss) = 2.19068408\n‚úÖ Saved best model at epoch 44 (Val Loss = 4.62309504)\nEpoch 44/1000 | Train Loss=4.64978838 | Val Loss=4.62309504 | Data=15.49929428 | Val RMSE: 3.92559743 | ‚àö(Val Loss) = 2.15013838\n‚úÖ Saved best model at epoch 45 (Val Loss = 4.45468235)\nEpoch 45/1000 | Train Loss=4.70503139 | Val Loss=4.45468235 | Data=15.68343782 | Val RMSE: 3.85343218 | ‚àö(Val Loss) = 2.11061192\n‚úÖ Saved best model at epoch 46 (Val Loss = 4.29334688)\nEpoch 46/1000 | Train Loss=4.47933114 | Val Loss=4.29334688 | Data=14.93110371 | Val RMSE: 3.78300881 | ‚àö(Val Loss) = 2.07203937\n‚úÖ Saved best model at epoch 47 (Val Loss = 4.13873529)\nEpoch 47/1000 | Train Loss=4.07669795 | Val Loss=4.13873529 | Data=13.58899260 | Val RMSE: 3.71426773 | ‚àö(Val Loss) = 2.03438830\n‚úÖ Saved best model at epoch 48 (Val Loss = 3.99054670)\nEpoch 48/1000 | Train Loss=4.19828856 | Val Loss=3.99054670 | Data=13.99429417 | Val RMSE: 3.64716625 | ‚àö(Val Loss) = 1.99763525\n‚úÖ Saved best model at epoch 49 (Val Loss = 3.84853578)\nEpoch 49/1000 | Train Loss=4.01872146 | Val Loss=3.84853578 | Data=13.39573765 | Val RMSE: 3.58168292 | ‚àö(Val Loss) = 1.96176851\n‚úÖ Saved best model at epoch 50 (Val Loss = 3.71249962)\nEpoch 50/1000 | Train Loss=3.67151332 | Val Loss=3.71249962 | Data=12.23837757 | Val RMSE: 3.51781154 | ‚àö(Val Loss) = 1.92678475\n‚úÖ Saved best model at epoch 51 (Val Loss = 3.58229184)\nEpoch 51/1000 | Train Loss=3.57706845 | Val Loss=3.58229184 | Data=11.92356062 | Val RMSE: 3.45557117 | ‚àö(Val Loss) = 1.89269435\n‚úÖ Saved best model at epoch 52 (Val Loss = 3.45780778)\nEpoch 52/1000 | Train Loss=3.50505579 | Val Loss=3.45780778 | Data=11.68351889 | Val RMSE: 3.39500022 | ‚àö(Val Loss) = 1.85951817\n‚úÖ Saved best model at epoch 53 (Val Loss = 3.33891773)\nEpoch 53/1000 | Train Loss=3.54545665 | Val Loss=3.33891773 | Data=11.81818819 | Val RMSE: 3.33612442 | ‚àö(Val Loss) = 1.82727063\n‚úÖ Saved best model at epoch 54 (Val Loss = 3.22548127)\nEpoch 54/1000 | Train Loss=3.39275002 | Val Loss=3.22548127 | Data=11.30916643 | Val RMSE: 3.27896380 | ‚àö(Val Loss) = 1.79596245\n‚úÖ Saved best model at epoch 55 (Val Loss = 3.11737537)\nEpoch 55/1000 | Train Loss=3.24608636 | Val Loss=3.11737537 | Data=10.82028770 | Val RMSE: 3.22354627 | ‚àö(Val Loss) = 1.76560903\n‚úÖ Saved best model at epoch 56 (Val Loss = 3.01450038)\nEpoch 56/1000 | Train Loss=3.16559136 | Val Loss=3.01450038 | Data=10.55197096 | Val RMSE: 3.16991067 | ‚àö(Val Loss) = 1.73623168\n‚úÖ Saved best model at epoch 57 (Val Loss = 2.91671085)\nEpoch 57/1000 | Train Loss=3.16587168 | Val Loss=2.91671085 | Data=10.55290556 | Val RMSE: 3.11807132 | ‚àö(Val Loss) = 1.70783806\n‚úÖ Saved best model at epoch 58 (Val Loss = 2.82385254)\nEpoch 58/1000 | Train Loss=2.97759378 | Val Loss=2.82385254 | Data=9.92531204 | Val RMSE: 3.06803560 | ‚àö(Val Loss) = 1.68043220\n‚úÖ Saved best model at epoch 59 (Val Loss = 2.73588943)\nEpoch 59/1000 | Train Loss=2.78269744 | Val Loss=2.73588943 | Data=9.27565765 | Val RMSE: 3.01987267 | ‚àö(Val Loss) = 1.65405238\n‚úÖ Saved best model at epoch 60 (Val Loss = 2.65274620)\nEpoch 60/1000 | Train Loss=2.73392928 | Val Loss=2.65274620 | Data=9.11309719 | Val RMSE: 2.97363210 | ‚àö(Val Loss) = 1.62872529\n‚úÖ Saved best model at epoch 61 (Val Loss = 2.57432413)\nEpoch 61/1000 | Train Loss=2.61395025 | Val Loss=2.57432413 | Data=8.71316719 | Val RMSE: 2.92934823 | ‚àö(Val Loss) = 1.60447001\n‚úÖ Saved best model at epoch 62 (Val Loss = 2.50053668)\nEpoch 62/1000 | Train Loss=2.51596355 | Val Loss=2.50053668 | Data=8.38654470 | Val RMSE: 2.88706112 | ‚àö(Val Loss) = 1.58130848\n‚úÖ Saved best model at epoch 63 (Val Loss = 2.43141031)\nEpoch 63/1000 | Train Loss=2.57083189 | Val Loss=2.43141031 | Data=8.56943941 | Val RMSE: 2.84687567 | ‚àö(Val Loss) = 1.55929804\n‚úÖ Saved best model at epoch 64 (Val Loss = 2.36686087)\nEpoch 64/1000 | Train Loss=2.47974962 | Val Loss=2.36686087 | Data=8.26583195 | Val RMSE: 2.80883169 | ‚àö(Val Loss) = 1.53846049\n‚úÖ Saved best model at epoch 65 (Val Loss = 2.30685139)\nEpoch 65/1000 | Train Loss=2.25314647 | Val Loss=2.30685139 | Data=7.51048779 | Val RMSE: 2.77299571 | ‚àö(Val Loss) = 1.51883221\n‚úÖ Saved best model at epoch 66 (Val Loss = 2.25135732)\nEpoch 66/1000 | Train Loss=2.20622098 | Val Loss=2.25135732 | Data=7.35406971 | Val RMSE: 2.73943877 | ‚àö(Val Loss) = 1.50045240\n‚úÖ Saved best model at epoch 67 (Val Loss = 2.20037460)\nEpoch 67/1000 | Train Loss=2.17796916 | Val Loss=2.20037460 | Data=7.25989699 | Val RMSE: 2.70824337 | ‚àö(Val Loss) = 1.48336601\n‚úÖ Saved best model at epoch 68 (Val Loss = 2.15658975)\nEpoch 68/1000 | Train Loss=2.04489917 | Val Loss=2.15658975 | Data=6.81633019 | Val RMSE: 2.68116236 | ‚àö(Val Loss) = 1.46853316\n‚úÖ Saved best model at epoch 69 (Val Loss = 2.12161350)\nEpoch 69/1000 | Train Loss=1.96035954 | Val Loss=2.12161350 | Data=6.53453135 | Val RMSE: 2.65933156 | ‚àö(Val Loss) = 1.45657599\n‚úÖ Saved best model at epoch 70 (Val Loss = 2.09097719)\nEpoch 70/1000 | Train Loss=2.03042740 | Val Loss=2.09097719 | Data=6.76809120 | Val RMSE: 2.64006138 | ‚àö(Val Loss) = 1.44602120\n‚úÖ Saved best model at epoch 71 (Val Loss = 2.06141686)\nEpoch 71/1000 | Train Loss=2.19466799 | Val Loss=2.06141686 | Data=7.31555986 | Val RMSE: 2.62133360 | ‚àö(Val Loss) = 1.43576348\n‚úÖ Saved best model at epoch 72 (Val Loss = 2.03353882)\nEpoch 72/1000 | Train Loss=1.97601014 | Val Loss=2.03353882 | Data=6.58670020 | Val RMSE: 2.60354805 | ‚àö(Val Loss) = 1.42602205\n‚úÖ Saved best model at epoch 73 (Val Loss = 2.00791502)\nEpoch 73/1000 | Train Loss=2.09189159 | Val Loss=2.00791502 | Data=6.97297144 | Val RMSE: 2.58709288 | ‚àö(Val Loss) = 1.41700923\n‚úÖ Saved best model at epoch 74 (Val Loss = 1.98673713)\nEpoch 74/1000 | Train Loss=1.97550839 | Val Loss=1.98673713 | Data=6.58502769 | Val RMSE: 2.57341361 | ‚àö(Val Loss) = 1.40951657\n‚úÖ Saved best model at epoch 75 (Val Loss = 1.97016573)\nEpoch 75/1000 | Train Loss=1.96697557 | Val Loss=1.97016573 | Data=6.55658507 | Val RMSE: 2.56265855 | ‚àö(Val Loss) = 1.40362597\n‚úÖ Saved best model at epoch 76 (Val Loss = 1.95640731)\nEpoch 76/1000 | Train Loss=2.09306037 | Val Loss=1.95640731 | Data=6.97686744 | Val RMSE: 2.55369496 | ‚àö(Val Loss) = 1.39871633\n‚úÖ Saved best model at epoch 77 (Val Loss = 1.94577003)\nEpoch 77/1000 | Train Loss=1.83277369 | Val Loss=1.94577003 | Data=6.10924554 | Val RMSE: 2.54674292 | ‚àö(Val Loss) = 1.39490867\n‚úÖ Saved best model at epoch 78 (Val Loss = 1.93829107)\nEpoch 78/1000 | Train Loss=2.01713133 | Val Loss=1.93829107 | Data=6.72377110 | Val RMSE: 2.54184389 | ‚àö(Val Loss) = 1.39222527\n‚úÖ Saved best model at epoch 79 (Val Loss = 1.93220305)\nEpoch 79/1000 | Train Loss=1.90391248 | Val Loss=1.93220305 | Data=6.34637475 | Val RMSE: 2.53784895 | ‚àö(Val Loss) = 1.39003706\n‚úÖ Saved best model at epoch 80 (Val Loss = 1.92842579)\nEpoch 80/1000 | Train Loss=1.96951371 | Val Loss=1.92842579 | Data=6.56504560 | Val RMSE: 2.53536701 | ‚àö(Val Loss) = 1.38867772\n‚úÖ Saved best model at epoch 81 (Val Loss = 1.92609465)\nEpoch 81/1000 | Train Loss=1.96011418 | Val Loss=1.92609465 | Data=6.53371358 | Val RMSE: 2.53383422 | ‚àö(Val Loss) = 1.38783813\n‚úÖ Saved best model at epoch 82 (Val Loss = 1.92483580)\nEpoch 82/1000 | Train Loss=1.83442336 | Val Loss=1.92483580 | Data=6.11474442 | Val RMSE: 2.53300595 | ‚àö(Val Loss) = 1.38738453\n‚úÖ Saved best model at epoch 83 (Val Loss = 1.92451131)\nEpoch 83/1000 | Train Loss=1.82479906 | Val Loss=1.92451131 | Data=6.08266354 | Val RMSE: 2.53279257 | ‚àö(Val Loss) = 1.38726759\nEpoch 84/1000 | Train Loss=1.78120685 | Val Loss=1.92479086 | Data=5.93735600 | Val RMSE: 2.53297639 | ‚àö(Val Loss) = 1.38736832\nEpoch 85/1000 | Train Loss=1.94223273 | Val Loss=1.92517245 | Data=6.47410893 | Val RMSE: 2.53322744 | ‚àö(Val Loss) = 1.38750589\nEpoch 86/1000 | Train Loss=1.93379062 | Val Loss=1.92553091 | Data=6.44596839 | Val RMSE: 2.53346324 | ‚àö(Val Loss) = 1.38763499\nEpoch 87/1000 | Train Loss=1.92520779 | Val Loss=1.92596328 | Data=6.41735888 | Val RMSE: 2.53374767 | ‚àö(Val Loss) = 1.38779080\nEpoch 88/1000 | Train Loss=1.96554929 | Val Loss=1.92619967 | Data=6.55183077 | Val RMSE: 2.53390312 | ‚àö(Val Loss) = 1.38787591\nEpoch 89/1000 | Train Loss=1.91765392 | Val Loss=1.92656517 | Data=6.39217949 | Val RMSE: 2.53414345 | ‚àö(Val Loss) = 1.38800764\nEpoch 90/1000 | Train Loss=1.94607818 | Val Loss=1.92645311 | Data=6.48692679 | Val RMSE: 2.53406978 | ‚àö(Val Loss) = 1.38796723\nEpoch 91/1000 | Train Loss=1.97127849 | Val Loss=1.92625320 | Data=6.57092810 | Val RMSE: 2.53393841 | ‚àö(Val Loss) = 1.38789523\nEpoch 92/1000 | Train Loss=1.93113202 | Val Loss=1.92590809 | Data=6.43710637 | Val RMSE: 2.53371143 | ‚àö(Val Loss) = 1.38777089\nEpoch 93/1000 | Train Loss=1.91635817 | Val Loss=1.92570972 | Data=6.38786030 | Val RMSE: 2.53358078 | ‚àö(Val Loss) = 1.38769948\nEpoch 94/1000 | Train Loss=1.96098262 | Val Loss=1.92555082 | Data=6.53660822 | Val RMSE: 2.53347635 | ‚àö(Val Loss) = 1.38764215\nEpoch 95/1000 | Train Loss=1.94293892 | Val Loss=1.92530215 | Data=6.47646260 | Val RMSE: 2.53331280 | ‚àö(Val Loss) = 1.38755262\nEpoch 96/1000 | Train Loss=1.99646693 | Val Loss=1.92517149 | Data=6.65488958 | Val RMSE: 2.53322673 | ‚àö(Val Loss) = 1.38750553\nEpoch 97/1000 | Train Loss=1.92707807 | Val Loss=1.92498863 | Data=6.42359328 | Val RMSE: 2.53310657 | ‚àö(Val Loss) = 1.38743961\nEpoch 98/1000 | Train Loss=2.04364586 | Val Loss=1.92475843 | Data=6.81215262 | Val RMSE: 2.53295493 | ‚àö(Val Loss) = 1.38735664\nEpoch 99/1000 | Train Loss=1.86977267 | Val Loss=1.92464864 | Data=6.23257518 | Val RMSE: 2.53288293 | ‚àö(Val Loss) = 1.38731706\nEpoch 100/1000 | Train Loss=1.86341780 | Val Loss=1.92463422 | Data=6.21139264 | Val RMSE: 2.53287339 | ‚àö(Val Loss) = 1.38731182\nEpoch 101/1000 | Train Loss=1.94897211 | Val Loss=1.92463493 | Data=6.49657345 | Val RMSE: 2.53287387 | ‚àö(Val Loss) = 1.38731217\nEpoch 102/1000 | Train Loss=2.00536209 | Val Loss=1.92462790 | Data=6.68454027 | Val RMSE: 2.53286910 | ‚àö(Val Loss) = 1.38730955\nEpoch 103/1000 | Train Loss=1.85067654 | Val Loss=1.92462575 | Data=6.16892171 | Val RMSE: 2.53286767 | ‚àö(Val Loss) = 1.38730884\nEpoch 104/1000 | Train Loss=1.86906451 | Val Loss=1.92462099 | Data=6.23021483 | Val RMSE: 2.53286457 | ‚àö(Val Loss) = 1.38730705\nEpoch 105/1000 | Train Loss=1.84343189 | Val Loss=1.92462170 | Data=6.14477277 | Val RMSE: 2.53286505 | ‚àö(Val Loss) = 1.38730741\nEpoch 106/1000 | Train Loss=1.83508676 | Val Loss=1.92461419 | Data=6.11695552 | Val RMSE: 2.53286004 | ‚àö(Val Loss) = 1.38730466\nEpoch 107/1000 | Train Loss=1.86678386 | Val Loss=1.92461038 | Data=6.22261262 | Val RMSE: 2.53285766 | ‚àö(Val Loss) = 1.38730323\nEpoch 108/1000 | Train Loss=1.78195485 | Val Loss=1.92460477 | Data=5.93984938 | Val RMSE: 2.53285384 | ‚àö(Val Loss) = 1.38730121\nEpoch 109/1000 | Train Loss=1.80450833 | Val Loss=1.92459643 | Data=6.01502752 | Val RMSE: 2.53284836 | ‚àö(Val Loss) = 1.38729823\nEpoch 110/1000 | Train Loss=1.94082779 | Val Loss=1.92459095 | Data=6.46942568 | Val RMSE: 2.53284478 | ‚àö(Val Loss) = 1.38729632\nEpoch 111/1000 | Train Loss=1.92320293 | Val Loss=1.92458463 | Data=6.41067624 | Val RMSE: 2.53284073 | ‚àö(Val Loss) = 1.38729393\nEpoch 112/1000 | Train Loss=1.85161734 | Val Loss=1.92458093 | Data=6.17205763 | Val RMSE: 2.53283811 | ‚àö(Val Loss) = 1.38729262\nEpoch 113/1000 | Train Loss=1.82405716 | Val Loss=1.92458129 | Data=6.08019042 | Val RMSE: 2.53283858 | ‚àö(Val Loss) = 1.38729274\nEpoch 114/1000 | Train Loss=1.89430583 | Val Loss=1.92458189 | Data=6.31435251 | Val RMSE: 2.53283882 | ‚àö(Val Loss) = 1.38729298\nEpoch 115/1000 | Train Loss=1.83126509 | Val Loss=1.92458260 | Data=6.10421681 | Val RMSE: 2.53283930 | ‚àö(Val Loss) = 1.38729322\nEpoch 116/1000 | Train Loss=2.06818062 | Val Loss=1.92457783 | Data=6.89393508 | Val RMSE: 2.53283620 | ‚àö(Val Loss) = 1.38729155\nEpoch 117/1000 | Train Loss=2.00440139 | Val Loss=1.92458069 | Data=6.68133759 | Val RMSE: 2.53283811 | ‚àö(Val Loss) = 1.38729262\nEpoch 118/1000 | Train Loss=2.07544589 | Val Loss=1.92457736 | Data=6.91815269 | Val RMSE: 2.53283596 | ‚àö(Val Loss) = 1.38729143\nEpoch 119/1000 | Train Loss=1.86790574 | Val Loss=1.92457473 | Data=6.22635198 | Val RMSE: 2.53283405 | ‚àö(Val Loss) = 1.38729048\nEpoch 120/1000 | Train Loss=1.88294262 | Val Loss=1.92457676 | Data=6.27647519 | Val RMSE: 2.53283548 | ‚àö(Val Loss) = 1.38729119\nEpoch 121/1000 | Train Loss=1.94361681 | Val Loss=1.92457569 | Data=6.47872233 | Val RMSE: 2.53283477 | ‚àö(Val Loss) = 1.38729072\nEpoch 122/1000 | Train Loss=1.91154689 | Val Loss=1.92457783 | Data=6.37182260 | Val RMSE: 2.53283620 | ‚àö(Val Loss) = 1.38729155\nEpoch 123/1000 | Train Loss=1.84308028 | Val Loss=1.92457855 | Data=6.14360070 | Val RMSE: 2.53283668 | ‚àö(Val Loss) = 1.38729179\nEpoch 124/1000 | Train Loss=1.97607487 | Val Loss=1.92458475 | Data=6.58691597 | Val RMSE: 2.53284073 | ‚àö(Val Loss) = 1.38729405\nEpoch 125/1000 | Train Loss=1.89442068 | Val Loss=1.92458403 | Data=6.31473541 | Val RMSE: 2.53284025 | ‚àö(Val Loss) = 1.38729382\nEpoch 126/1000 | Train Loss=1.88293952 | Val Loss=1.92458951 | Data=6.27646494 | Val RMSE: 2.53284383 | ‚àö(Val Loss) = 1.38729572\nEpoch 127/1000 | Train Loss=1.84318036 | Val Loss=1.92459619 | Data=6.14393425 | Val RMSE: 2.53284836 | ‚àö(Val Loss) = 1.38729811\nEpoch 128/1000 | Train Loss=1.95548892 | Val Loss=1.92459536 | Data=6.51829624 | Val RMSE: 2.53284764 | ‚àö(Val Loss) = 1.38729787\nEpoch 129/1000 | Train Loss=1.79339486 | Val Loss=1.92460334 | Data=5.97798264 | Val RMSE: 2.53285289 | ‚àö(Val Loss) = 1.38730073\nEpoch 130/1000 | Train Loss=1.87079114 | Val Loss=1.92461002 | Data=6.23597002 | Val RMSE: 2.53285742 | ‚àö(Val Loss) = 1.38730311\nEpoch 131/1000 | Train Loss=1.91798639 | Val Loss=1.92460763 | Data=6.39328766 | Val RMSE: 2.53285575 | ‚àö(Val Loss) = 1.38730228\nEpoch 132/1000 | Train Loss=1.90787923 | Val Loss=1.92460907 | Data=6.35959721 | Val RMSE: 2.53285670 | ‚àö(Val Loss) = 1.38730276\nEpoch 133/1000 | Train Loss=1.86767668 | Val Loss=1.92460978 | Data=6.22558856 | Val RMSE: 2.53285718 | ‚àö(Val Loss) = 1.38730311\nEpoch 134/1000 | Train Loss=1.91808087 | Val Loss=1.92461097 | Data=6.39360261 | Val RMSE: 2.53285789 | ‚àö(Val Loss) = 1.38730347\nEpoch 135/1000 | Train Loss=1.83466661 | Val Loss=1.92461252 | Data=6.11555529 | Val RMSE: 2.53285909 | ‚àö(Val Loss) = 1.38730407\nEpoch 136/1000 | Train Loss=1.95032281 | Val Loss=1.92462134 | Data=6.50107574 | Val RMSE: 2.53286481 | ‚àö(Val Loss) = 1.38730729\nEpoch 137/1000 | Train Loss=1.90133876 | Val Loss=1.92462635 | Data=6.33779550 | Val RMSE: 2.53286815 | ‚àö(Val Loss) = 1.38730907\nEpoch 138/1000 | Train Loss=1.92571276 | Val Loss=1.92462420 | Data=6.41904235 | Val RMSE: 2.53286672 | ‚àö(Val Loss) = 1.38730824\nEpoch 139/1000 | Train Loss=1.86982441 | Val Loss=1.92462277 | Data=6.23274779 | Val RMSE: 2.53286576 | ‚àö(Val Loss) = 1.38730776\nEpoch 140/1000 | Train Loss=1.83007419 | Val Loss=1.92462420 | Data=6.10024691 | Val RMSE: 2.53286672 | ‚àö(Val Loss) = 1.38730824\nEpoch 141/1000 | Train Loss=1.89038676 | Val Loss=1.92462409 | Data=6.30128908 | Val RMSE: 2.53286672 | ‚àö(Val Loss) = 1.38730824\nEpoch 142/1000 | Train Loss=1.83056808 | Val Loss=1.92462111 | Data=6.10189319 | Val RMSE: 2.53286457 | ‚àö(Val Loss) = 1.38730717\nEpoch 143/1000 | Train Loss=1.94033688 | Val Loss=1.92461634 | Data=6.46778941 | Val RMSE: 2.53286147 | ‚àö(Val Loss) = 1.38730538\nEpoch 144/1000 | Train Loss=1.89474154 | Val Loss=1.92461312 | Data=6.31580496 | Val RMSE: 2.53285933 | ‚àö(Val Loss) = 1.38730431\nEpoch 145/1000 | Train Loss=1.86038566 | Val Loss=1.92461145 | Data=6.20128536 | Val RMSE: 2.53285837 | ‚àö(Val Loss) = 1.38730371\nEpoch 146/1000 | Train Loss=1.95035154 | Val Loss=1.92460978 | Data=6.50117159 | Val RMSE: 2.53285718 | ‚àö(Val Loss) = 1.38730311\nEpoch 147/1000 | Train Loss=1.86794990 | Val Loss=1.92461097 | Data=6.22649932 | Val RMSE: 2.53285789 | ‚àö(Val Loss) = 1.38730347\nEpoch 148/1000 | Train Loss=1.99310839 | Val Loss=1.92461467 | Data=6.64369440 | Val RMSE: 2.53286052 | ‚àö(Val Loss) = 1.38730478\nEpoch 149/1000 | Train Loss=2.00667471 | Val Loss=1.92461956 | Data=6.68891549 | Val RMSE: 2.53286362 | ‚àö(Val Loss) = 1.38730657\nEpoch 150/1000 | Train Loss=1.82689941 | Val Loss=1.92463219 | Data=6.08966446 | Val RMSE: 2.53287196 | ‚àö(Val Loss) = 1.38731110\nEpoch 151/1000 | Train Loss=1.97367936 | Val Loss=1.92464578 | Data=6.57893109 | Val RMSE: 2.53288102 | ‚àö(Val Loss) = 1.38731599\nEpoch 152/1000 | Train Loss=1.93960232 | Val Loss=1.92464888 | Data=6.46534085 | Val RMSE: 2.53288293 | ‚àö(Val Loss) = 1.38731718\nEpoch 153/1000 | Train Loss=1.95059001 | Val Loss=1.92465508 | Data=6.50196624 | Val RMSE: 2.53288698 | ‚àö(Val Loss) = 1.38731933\nEpoch 154/1000 | Train Loss=1.86596406 | Val Loss=1.92465043 | Data=6.21988010 | Val RMSE: 2.53288388 | ‚àö(Val Loss) = 1.38731766\nEpoch 155/1000 | Train Loss=1.75553969 | Val Loss=1.92464793 | Data=5.85179865 | Val RMSE: 2.53288245 | ‚àö(Val Loss) = 1.38731682\nEpoch 156/1000 | Train Loss=1.98628801 | Val Loss=1.92463219 | Data=6.62095976 | Val RMSE: 2.53287196 | ‚àö(Val Loss) = 1.38731110\nEpoch 157/1000 | Train Loss=1.84150118 | Val Loss=1.92462885 | Data=6.13833690 | Val RMSE: 2.53286982 | ‚àö(Val Loss) = 1.38730991\nEpoch 158/1000 | Train Loss=1.90428263 | Val Loss=1.92462039 | Data=6.34760857 | Val RMSE: 2.53286409 | ‚àö(Val Loss) = 1.38730693\nEpoch 159/1000 | Train Loss=1.90625042 | Val Loss=1.92461073 | Data=6.35416770 | Val RMSE: 2.53285789 | ‚àö(Val Loss) = 1.38730335\nEpoch 160/1000 | Train Loss=1.93341696 | Val Loss=1.92461181 | Data=6.44472289 | Val RMSE: 2.53285861 | ‚àö(Val Loss) = 1.38730383\nEpoch 161/1000 | Train Loss=1.74364957 | Val Loss=1.92460763 | Data=5.81216502 | Val RMSE: 2.53285575 | ‚àö(Val Loss) = 1.38730228\nEpoch 162/1000 | Train Loss=1.83134806 | Val Loss=1.92460752 | Data=6.10449338 | Val RMSE: 2.53285575 | ‚àö(Val Loss) = 1.38730228\nEpoch 163/1000 | Train Loss=1.89847791 | Val Loss=1.92459834 | Data=6.32825947 | Val RMSE: 2.53284979 | ‚àö(Val Loss) = 1.38729894\nEpoch 164/1000 | Train Loss=2.04088718 | Val Loss=1.92459130 | Data=6.80295706 | Val RMSE: 2.53284502 | ‚àö(Val Loss) = 1.38729644\nEpoch 165/1000 | Train Loss=1.91899240 | Val Loss=1.92457616 | Data=6.39664125 | Val RMSE: 2.53283501 | ‚àö(Val Loss) = 1.38729095\nEpoch 166/1000 | Train Loss=1.82609344 | Val Loss=1.92457235 | Data=6.08697796 | Val RMSE: 2.53283262 | ‚àö(Val Loss) = 1.38728952\nEpoch 167/1000 | Train Loss=1.93407863 | Val Loss=1.92456567 | Data=6.44692850 | Val RMSE: 2.53282833 | ‚àö(Val Loss) = 1.38728714\nEpoch 168/1000 | Train Loss=2.00024700 | Val Loss=1.92456162 | Data=6.66748977 | Val RMSE: 2.53282547 | ‚àö(Val Loss) = 1.38728571\nEpoch 169/1000 | Train Loss=1.99071795 | Val Loss=1.92456639 | Data=6.63572598 | Val RMSE: 2.53282881 | ‚àö(Val Loss) = 1.38728738\nEpoch 170/1000 | Train Loss=1.91539562 | Val Loss=1.92457843 | Data=6.38465190 | Val RMSE: 2.53283668 | ‚àö(Val Loss) = 1.38729179\nEpoch 171/1000 | Train Loss=1.82761323 | Val Loss=1.92457688 | Data=6.09204388 | Val RMSE: 2.53283548 | ‚àö(Val Loss) = 1.38729119\nEpoch 172/1000 | Train Loss=1.97782731 | Val Loss=1.92458105 | Data=6.59275746 | Val RMSE: 2.53283834 | ‚àö(Val Loss) = 1.38729274\nEpoch 173/1000 | Train Loss=1.88387698 | Val Loss=1.92457592 | Data=6.27958965 | Val RMSE: 2.53283501 | ‚àö(Val Loss) = 1.38729084\nEpoch 174/1000 | Train Loss=1.90705425 | Val Loss=1.92457998 | Data=6.35684729 | Val RMSE: 2.53283763 | ‚àö(Val Loss) = 1.38729227\nEpoch 175/1000 | Train Loss=1.95690554 | Val Loss=1.92458701 | Data=6.52301812 | Val RMSE: 2.53284216 | ‚àö(Val Loss) = 1.38729489\nEpoch 176/1000 | Train Loss=1.85956258 | Val Loss=1.92459548 | Data=6.19854164 | Val RMSE: 2.53284788 | ‚àö(Val Loss) = 1.38729787\nEpoch 177/1000 | Train Loss=1.94220537 | Val Loss=1.92459249 | Data=6.47401762 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 178/1000 | Train Loss=1.80336201 | Val Loss=1.92458916 | Data=6.01120651 | Val RMSE: 2.53284359 | ‚àö(Val Loss) = 1.38729560\nEpoch 179/1000 | Train Loss=1.80054039 | Val Loss=1.92459178 | Data=6.00180113 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729656\nEpoch 180/1000 | Train Loss=1.80459225 | Val Loss=1.92458260 | Data=6.01530719 | Val RMSE: 2.53283930 | ‚àö(Val Loss) = 1.38729322\nEpoch 181/1000 | Train Loss=2.05064225 | Val Loss=1.92458141 | Data=6.83547378 | Val RMSE: 2.53283858 | ‚àö(Val Loss) = 1.38729286\nEpoch 182/1000 | Train Loss=1.87891394 | Val Loss=1.92457747 | Data=6.26304603 | Val RMSE: 2.53283596 | ‚àö(Val Loss) = 1.38729143\nEpoch 183/1000 | Train Loss=1.91803908 | Val Loss=1.92457473 | Data=6.39346337 | Val RMSE: 2.53283405 | ‚àö(Val Loss) = 1.38729048\nEpoch 184/1000 | Train Loss=1.85581398 | Val Loss=1.92457545 | Data=6.18604636 | Val RMSE: 2.53283453 | ‚àö(Val Loss) = 1.38729072\nEpoch 185/1000 | Train Loss=1.92659682 | Val Loss=1.92457819 | Data=6.42198896 | Val RMSE: 2.53283644 | ‚àö(Val Loss) = 1.38729167\nEpoch 186/1000 | Train Loss=1.95601839 | Val Loss=1.92458200 | Data=6.52006102 | Val RMSE: 2.53283906 | ‚àö(Val Loss) = 1.38729310\nEpoch 187/1000 | Train Loss=1.95976126 | Val Loss=1.92458415 | Data=6.53253746 | Val RMSE: 2.53284025 | ‚àö(Val Loss) = 1.38729382\nEpoch 188/1000 | Train Loss=1.87077558 | Val Loss=1.92458951 | Data=6.23591828 | Val RMSE: 2.53284383 | ‚àö(Val Loss) = 1.38729572\nEpoch 189/1000 | Train Loss=1.94965869 | Val Loss=1.92459822 | Data=6.49886203 | Val RMSE: 2.53284955 | ‚àö(Val Loss) = 1.38729894\nEpoch 190/1000 | Train Loss=1.90651351 | Val Loss=1.92460406 | Data=6.35504484 | Val RMSE: 2.53285336 | ‚àö(Val Loss) = 1.38730097\nEpoch 191/1000 | Train Loss=1.81453097 | Val Loss=1.92461181 | Data=6.04843616 | Val RMSE: 2.53285861 | ‚àö(Val Loss) = 1.38730383\nEpoch 192/1000 | Train Loss=2.07794219 | Val Loss=1.92461538 | Data=6.92647362 | Val RMSE: 2.53286099 | ‚àö(Val Loss) = 1.38730502\nEpoch 193/1000 | Train Loss=1.87383550 | Val Loss=1.92461431 | Data=6.24611807 | Val RMSE: 2.53286028 | ‚àö(Val Loss) = 1.38730466\nEpoch 194/1000 | Train Loss=1.85727978 | Val Loss=1.92460895 | Data=6.19093227 | Val RMSE: 2.53285670 | ‚àö(Val Loss) = 1.38730276\nEpoch 195/1000 | Train Loss=1.98542422 | Val Loss=1.92460418 | Data=6.61808062 | Val RMSE: 2.53285360 | ‚àö(Val Loss) = 1.38730109\nEpoch 196/1000 | Train Loss=1.85495025 | Val Loss=1.92459738 | Data=6.18316722 | Val RMSE: 2.53284907 | ‚àö(Val Loss) = 1.38729858\nEpoch 197/1000 | Train Loss=1.96190470 | Val Loss=1.92459619 | Data=6.53968215 | Val RMSE: 2.53284836 | ‚àö(Val Loss) = 1.38729811\nEpoch 198/1000 | Train Loss=1.86293453 | Val Loss=1.92459571 | Data=6.20978141 | Val RMSE: 2.53284788 | ‚àö(Val Loss) = 1.38729799\nEpoch 199/1000 | Train Loss=1.98705333 | Val Loss=1.92459702 | Data=6.62351084 | Val RMSE: 2.53284883 | ‚àö(Val Loss) = 1.38729846\nEpoch 200/1000 | Train Loss=1.92118120 | Val Loss=1.92459404 | Data=6.40393710 | Val RMSE: 2.53284693 | ‚àö(Val Loss) = 1.38729739\nEpoch 201/1000 | Train Loss=1.93702137 | Val Loss=1.92459345 | Data=6.45673776 | Val RMSE: 2.53284645 | ‚àö(Val Loss) = 1.38729715\nEpoch 202/1000 | Train Loss=1.98218971 | Val Loss=1.92459321 | Data=6.60729885 | Val RMSE: 2.53284621 | ‚àö(Val Loss) = 1.38729703\nEpoch 203/1000 | Train Loss=1.88996160 | Val Loss=1.92459095 | Data=6.29987168 | Val RMSE: 2.53284478 | ‚àö(Val Loss) = 1.38729632\nEpoch 204/1000 | Train Loss=1.95801437 | Val Loss=1.92458951 | Data=6.52671432 | Val RMSE: 2.53284383 | ‚àö(Val Loss) = 1.38729572\nEpoch 205/1000 | Train Loss=1.96752799 | Val Loss=1.92458916 | Data=6.55842638 | Val RMSE: 2.53284359 | ‚àö(Val Loss) = 1.38729560\nEpoch 206/1000 | Train Loss=1.91416740 | Val Loss=1.92458904 | Data=6.38055778 | Val RMSE: 2.53284359 | ‚àö(Val Loss) = 1.38729560\nEpoch 207/1000 | Train Loss=1.92195123 | Val Loss=1.92458820 | Data=6.40650392 | Val RMSE: 2.53284311 | ‚àö(Val Loss) = 1.38729525\nEpoch 208/1000 | Train Loss=1.87951523 | Val Loss=1.92458820 | Data=6.26505041 | Val RMSE: 2.53284311 | ‚àö(Val Loss) = 1.38729525\nEpoch 209/1000 | Train Loss=1.95258486 | Val Loss=1.92458856 | Data=6.50861573 | Val RMSE: 2.53284335 | ‚àö(Val Loss) = 1.38729537\nEpoch 210/1000 | Train Loss=1.93935102 | Val Loss=1.92458904 | Data=6.46450305 | Val RMSE: 2.53284359 | ‚àö(Val Loss) = 1.38729560\nEpoch 211/1000 | Train Loss=1.88334292 | Val Loss=1.92458916 | Data=6.27780962 | Val RMSE: 2.53284359 | ‚àö(Val Loss) = 1.38729560\nEpoch 212/1000 | Train Loss=1.82397038 | Val Loss=1.92458975 | Data=6.07990098 | Val RMSE: 2.53284407 | ‚àö(Val Loss) = 1.38729584\nEpoch 213/1000 | Train Loss=1.85601890 | Val Loss=1.92458987 | Data=6.18672919 | Val RMSE: 2.53284407 | ‚àö(Val Loss) = 1.38729584\nEpoch 214/1000 | Train Loss=2.01430303 | Val Loss=1.92458916 | Data=6.71434307 | Val RMSE: 2.53284359 | ‚àö(Val Loss) = 1.38729560\nEpoch 215/1000 | Train Loss=1.81481975 | Val Loss=1.92458844 | Data=6.04939890 | Val RMSE: 2.53284311 | ‚àö(Val Loss) = 1.38729537\nEpoch 216/1000 | Train Loss=1.91727275 | Val Loss=1.92458844 | Data=6.39090896 | Val RMSE: 2.53284311 | ‚àö(Val Loss) = 1.38729537\nEpoch 217/1000 | Train Loss=2.03902483 | Val Loss=1.92458773 | Data=6.79674911 | Val RMSE: 2.53284264 | ‚àö(Val Loss) = 1.38729513\nEpoch 218/1000 | Train Loss=1.87666059 | Val Loss=1.92458808 | Data=6.25553513 | Val RMSE: 2.53284287 | ‚àö(Val Loss) = 1.38729525\nEpoch 219/1000 | Train Loss=1.98664564 | Val Loss=1.92458785 | Data=6.62215161 | Val RMSE: 2.53284287 | ‚àö(Val Loss) = 1.38729513\nEpoch 220/1000 | Train Loss=1.78714049 | Val Loss=1.92458844 | Data=5.95713472 | Val RMSE: 2.53284311 | ‚àö(Val Loss) = 1.38729537\nEpoch 221/1000 | Train Loss=2.03676587 | Val Loss=1.92458832 | Data=6.78921914 | Val RMSE: 2.53284311 | ‚àö(Val Loss) = 1.38729537\nEpoch 222/1000 | Train Loss=2.02951795 | Val Loss=1.92458951 | Data=6.76505971 | Val RMSE: 2.53284383 | ‚àö(Val Loss) = 1.38729572\nEpoch 223/1000 | Train Loss=1.87123787 | Val Loss=1.92458999 | Data=6.23745942 | Val RMSE: 2.53284431 | ‚àö(Val Loss) = 1.38729596\nEpoch 224/1000 | Train Loss=1.80072170 | Val Loss=1.92459071 | Data=6.00240564 | Val RMSE: 2.53284478 | ‚àö(Val Loss) = 1.38729620\nEpoch 225/1000 | Train Loss=2.01621777 | Val Loss=1.92459106 | Data=6.72072554 | Val RMSE: 2.53284478 | ‚àö(Val Loss) = 1.38729632\nEpoch 226/1000 | Train Loss=1.97897351 | Val Loss=1.92459142 | Data=6.59657788 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729644\nEpoch 227/1000 | Train Loss=1.86738414 | Val Loss=1.92459178 | Data=6.22461367 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729656\nEpoch 228/1000 | Train Loss=1.81705546 | Val Loss=1.92459166 | Data=6.05685139 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729656\nEpoch 229/1000 | Train Loss=1.90716803 | Val Loss=1.92459178 | Data=6.35722637 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729656\nEpoch 230/1000 | Train Loss=1.92380553 | Val Loss=1.92459142 | Data=6.41268468 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729644\nEpoch 231/1000 | Train Loss=1.77632931 | Val Loss=1.92459071 | Data=5.92109752 | Val RMSE: 2.53284478 | ‚àö(Val Loss) = 1.38729620\nEpoch 232/1000 | Train Loss=1.97605574 | Val Loss=1.92458916 | Data=6.58685231 | Val RMSE: 2.53284359 | ‚àö(Val Loss) = 1.38729560\nEpoch 233/1000 | Train Loss=1.97464281 | Val Loss=1.92458832 | Data=6.58214235 | Val RMSE: 2.53284311 | ‚àö(Val Loss) = 1.38729537\nEpoch 234/1000 | Train Loss=1.86162591 | Val Loss=1.92458832 | Data=6.20541930 | Val RMSE: 2.53284311 | ‚àö(Val Loss) = 1.38729537\nEpoch 235/1000 | Train Loss=1.90155762 | Val Loss=1.92458761 | Data=6.33852506 | Val RMSE: 2.53284264 | ‚àö(Val Loss) = 1.38729513\nEpoch 236/1000 | Train Loss=1.90407622 | Val Loss=1.92458677 | Data=6.34692049 | Val RMSE: 2.53284216 | ‚àö(Val Loss) = 1.38729477\nEpoch 237/1000 | Train Loss=1.86357343 | Val Loss=1.92458618 | Data=6.21191120 | Val RMSE: 2.53284168 | ‚àö(Val Loss) = 1.38729453\nEpoch 238/1000 | Train Loss=1.90939879 | Val Loss=1.92458487 | Data=6.36466241 | Val RMSE: 2.53284073 | ‚àö(Val Loss) = 1.38729405\nEpoch 239/1000 | Train Loss=1.96886390 | Val Loss=1.92458546 | Data=6.56287932 | Val RMSE: 2.53284121 | ‚àö(Val Loss) = 1.38729429\nEpoch 240/1000 | Train Loss=1.85328019 | Val Loss=1.92458558 | Data=6.17760038 | Val RMSE: 2.53284121 | ‚àö(Val Loss) = 1.38729429\nEpoch 241/1000 | Train Loss=2.02386999 | Val Loss=1.92458677 | Data=6.74623299 | Val RMSE: 2.53284216 | ‚àö(Val Loss) = 1.38729477\nEpoch 242/1000 | Train Loss=1.78940552 | Val Loss=1.92458642 | Data=5.96468484 | Val RMSE: 2.53284192 | ‚àö(Val Loss) = 1.38729465\nEpoch 243/1000 | Train Loss=1.80091536 | Val Loss=1.92458677 | Data=6.00305092 | Val RMSE: 2.53284216 | ‚àö(Val Loss) = 1.38729477\nEpoch 244/1000 | Train Loss=1.94211942 | Val Loss=1.92458701 | Data=6.47373104 | Val RMSE: 2.53284216 | ‚àö(Val Loss) = 1.38729489\nEpoch 245/1000 | Train Loss=2.07838947 | Val Loss=1.92458665 | Data=6.92796457 | Val RMSE: 2.53284192 | ‚àö(Val Loss) = 1.38729477\nEpoch 246/1000 | Train Loss=2.09077531 | Val Loss=1.92458737 | Data=6.96925068 | Val RMSE: 2.53284240 | ‚àö(Val Loss) = 1.38729501\nEpoch 247/1000 | Train Loss=1.90331632 | Val Loss=1.92458820 | Data=6.34438753 | Val RMSE: 2.53284311 | ‚àö(Val Loss) = 1.38729525\nEpoch 248/1000 | Train Loss=1.97689158 | Val Loss=1.92458963 | Data=6.58963823 | Val RMSE: 2.53284383 | ‚àö(Val Loss) = 1.38729584\nEpoch 249/1000 | Train Loss=2.08463013 | Val Loss=1.92459023 | Data=6.94876671 | Val RMSE: 2.53284431 | ‚àö(Val Loss) = 1.38729596\nEpoch 250/1000 | Train Loss=2.01682454 | Val Loss=1.92459095 | Data=6.72274804 | Val RMSE: 2.53284478 | ‚àö(Val Loss) = 1.38729632\nEpoch 251/1000 | Train Loss=1.78696704 | Val Loss=1.92459190 | Data=5.95655644 | Val RMSE: 2.53284550 | ‚àö(Val Loss) = 1.38729656\nEpoch 252/1000 | Train Loss=1.98251319 | Val Loss=1.92459238 | Data=6.60837698 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 253/1000 | Train Loss=1.82935488 | Val Loss=1.92459202 | Data=6.09784937 | Val RMSE: 2.53284550 | ‚àö(Val Loss) = 1.38729668\nEpoch 254/1000 | Train Loss=1.93957216 | Val Loss=1.92459261 | Data=6.46524000 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 255/1000 | Train Loss=1.78167510 | Val Loss=1.92459381 | Data=5.93891692 | Val RMSE: 2.53284669 | ‚àö(Val Loss) = 1.38729727\nEpoch 256/1000 | Train Loss=1.90666592 | Val Loss=1.92459381 | Data=6.35555267 | Val RMSE: 2.53284669 | ‚àö(Val Loss) = 1.38729727\nEpoch 257/1000 | Train Loss=1.94499856 | Val Loss=1.92459416 | Data=6.48332810 | Val RMSE: 2.53284693 | ‚àö(Val Loss) = 1.38729739\nEpoch 258/1000 | Train Loss=1.88527542 | Val Loss=1.92459452 | Data=6.28425097 | Val RMSE: 2.53284717 | ‚àö(Val Loss) = 1.38729751\nEpoch 259/1000 | Train Loss=1.94267583 | Val Loss=1.92459571 | Data=6.47558594 | Val RMSE: 2.53284788 | ‚àö(Val Loss) = 1.38729799\nEpoch 260/1000 | Train Loss=1.86788315 | Val Loss=1.92459619 | Data=6.22627711 | Val RMSE: 2.53284836 | ‚àö(Val Loss) = 1.38729811\nEpoch 261/1000 | Train Loss=1.98178577 | Val Loss=1.92459679 | Data=6.60595250 | Val RMSE: 2.53284860 | ‚àö(Val Loss) = 1.38729835\nEpoch 262/1000 | Train Loss=1.77733460 | Val Loss=1.92459702 | Data=5.92444825 | Val RMSE: 2.53284883 | ‚àö(Val Loss) = 1.38729846\nEpoch 263/1000 | Train Loss=1.80895692 | Val Loss=1.92459667 | Data=6.02985620 | Val RMSE: 2.53284860 | ‚àö(Val Loss) = 1.38729835\nEpoch 264/1000 | Train Loss=1.86983353 | Val Loss=1.92459679 | Data=6.23277807 | Val RMSE: 2.53284860 | ‚àö(Val Loss) = 1.38729835\nEpoch 265/1000 | Train Loss=2.01509786 | Val Loss=1.92459738 | Data=6.71699286 | Val RMSE: 2.53284907 | ‚àö(Val Loss) = 1.38729858\nEpoch 266/1000 | Train Loss=1.96850270 | Val Loss=1.92459679 | Data=6.56167555 | Val RMSE: 2.53284860 | ‚àö(Val Loss) = 1.38729835\nEpoch 267/1000 | Train Loss=1.89166129 | Val Loss=1.92459714 | Data=6.30553746 | Val RMSE: 2.53284883 | ‚àö(Val Loss) = 1.38729846\nEpoch 268/1000 | Train Loss=1.80924183 | Val Loss=1.92459762 | Data=6.03080595 | Val RMSE: 2.53284931 | ‚àö(Val Loss) = 1.38729870\nEpoch 269/1000 | Train Loss=1.89562643 | Val Loss=1.92459762 | Data=6.31875443 | Val RMSE: 2.53284931 | ‚àö(Val Loss) = 1.38729870\nEpoch 270/1000 | Train Loss=1.92017448 | Val Loss=1.92459702 | Data=6.40058136 | Val RMSE: 2.53284883 | ‚àö(Val Loss) = 1.38729846\nEpoch 271/1000 | Train Loss=1.96203578 | Val Loss=1.92459714 | Data=6.54011893 | Val RMSE: 2.53284883 | ‚àö(Val Loss) = 1.38729846\nEpoch 272/1000 | Train Loss=1.80814296 | Val Loss=1.92459679 | Data=6.02714300 | Val RMSE: 2.53284860 | ‚àö(Val Loss) = 1.38729835\nEpoch 273/1000 | Train Loss=2.01714569 | Val Loss=1.92459667 | Data=6.72381878 | Val RMSE: 2.53284860 | ‚àö(Val Loss) = 1.38729835\nEpoch 274/1000 | Train Loss=1.88159674 | Val Loss=1.92459643 | Data=6.27198863 | Val RMSE: 2.53284836 | ‚àö(Val Loss) = 1.38729823\nEpoch 275/1000 | Train Loss=1.96965492 | Val Loss=1.92459476 | Data=6.56551623 | Val RMSE: 2.53284740 | ‚àö(Val Loss) = 1.38729763\nEpoch 276/1000 | Train Loss=1.95275176 | Val Loss=1.92459524 | Data=6.50917220 | Val RMSE: 2.53284764 | ‚àö(Val Loss) = 1.38729787\nEpoch 277/1000 | Train Loss=1.83263201 | Val Loss=1.92459381 | Data=6.10877323 | Val RMSE: 2.53284669 | ‚àö(Val Loss) = 1.38729727\nEpoch 278/1000 | Train Loss=1.77861461 | Val Loss=1.92459345 | Data=5.92871535 | Val RMSE: 2.53284645 | ‚àö(Val Loss) = 1.38729715\nEpoch 279/1000 | Train Loss=1.96422786 | Val Loss=1.92459214 | Data=6.54742599 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729668\nEpoch 280/1000 | Train Loss=1.86523384 | Val Loss=1.92459261 | Data=6.21744585 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 281/1000 | Train Loss=1.96667534 | Val Loss=1.92459178 | Data=6.55558443 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729656\nEpoch 282/1000 | Train Loss=1.89366668 | Val Loss=1.92459202 | Data=6.31222200 | Val RMSE: 2.53284550 | ‚àö(Val Loss) = 1.38729668\nEpoch 283/1000 | Train Loss=1.94032377 | Val Loss=1.92459095 | Data=6.46774554 | Val RMSE: 2.53284478 | ‚àö(Val Loss) = 1.38729632\nEpoch 284/1000 | Train Loss=1.89538771 | Val Loss=1.92459059 | Data=6.31795883 | Val RMSE: 2.53284454 | ‚àö(Val Loss) = 1.38729620\nEpoch 285/1000 | Train Loss=1.93657553 | Val Loss=1.92458987 | Data=6.45525169 | Val RMSE: 2.53284407 | ‚àö(Val Loss) = 1.38729584\nEpoch 286/1000 | Train Loss=1.88072795 | Val Loss=1.92459047 | Data=6.26909280 | Val RMSE: 2.53284454 | ‚àö(Val Loss) = 1.38729608\nEpoch 287/1000 | Train Loss=1.94735593 | Val Loss=1.92459047 | Data=6.49118614 | Val RMSE: 2.53284454 | ‚àö(Val Loss) = 1.38729608\nEpoch 288/1000 | Train Loss=1.87845725 | Val Loss=1.92459095 | Data=6.26152372 | Val RMSE: 2.53284478 | ‚àö(Val Loss) = 1.38729632\nEpoch 289/1000 | Train Loss=1.84004217 | Val Loss=1.92458928 | Data=6.13347387 | Val RMSE: 2.53284383 | ‚àö(Val Loss) = 1.38729572\nEpoch 290/1000 | Train Loss=1.86962450 | Val Loss=1.92458916 | Data=6.23208141 | Val RMSE: 2.53284359 | ‚àö(Val Loss) = 1.38729560\nEpoch 291/1000 | Train Loss=2.03991938 | Val Loss=1.92459023 | Data=6.79973102 | Val RMSE: 2.53284431 | ‚àö(Val Loss) = 1.38729596\nEpoch 292/1000 | Train Loss=1.78172532 | Val Loss=1.92458951 | Data=5.93908429 | Val RMSE: 2.53284383 | ‚àö(Val Loss) = 1.38729572\nEpoch 293/1000 | Train Loss=1.93896633 | Val Loss=1.92459047 | Data=6.46322083 | Val RMSE: 2.53284454 | ‚àö(Val Loss) = 1.38729608\nEpoch 294/1000 | Train Loss=1.95898694 | Val Loss=1.92458951 | Data=6.52995610 | Val RMSE: 2.53284383 | ‚àö(Val Loss) = 1.38729572\nEpoch 295/1000 | Train Loss=1.90721029 | Val Loss=1.92458916 | Data=6.35736752 | Val RMSE: 2.53284359 | ‚àö(Val Loss) = 1.38729560\nEpoch 296/1000 | Train Loss=1.97017682 | Val Loss=1.92458975 | Data=6.56725597 | Val RMSE: 2.53284407 | ‚àö(Val Loss) = 1.38729584\nEpoch 297/1000 | Train Loss=1.76658067 | Val Loss=1.92459035 | Data=5.88860214 | Val RMSE: 2.53284431 | ‚àö(Val Loss) = 1.38729608\nEpoch 298/1000 | Train Loss=1.81421137 | Val Loss=1.92459130 | Data=6.04737091 | Val RMSE: 2.53284502 | ‚àö(Val Loss) = 1.38729644\nEpoch 299/1000 | Train Loss=1.84184283 | Val Loss=1.92459106 | Data=6.13947606 | Val RMSE: 2.53284478 | ‚àö(Val Loss) = 1.38729632\nEpoch 300/1000 | Train Loss=1.83764148 | Val Loss=1.92459130 | Data=6.12547112 | Val RMSE: 2.53284502 | ‚àö(Val Loss) = 1.38729644\nEpoch 301/1000 | Train Loss=1.88650244 | Val Loss=1.92459130 | Data=6.28834105 | Val RMSE: 2.53284502 | ‚àö(Val Loss) = 1.38729644\nEpoch 302/1000 | Train Loss=1.98061323 | Val Loss=1.92459190 | Data=6.60204387 | Val RMSE: 2.53284550 | ‚àö(Val Loss) = 1.38729656\nEpoch 303/1000 | Train Loss=1.87341195 | Val Loss=1.92459118 | Data=6.24470639 | Val RMSE: 2.53284502 | ‚àö(Val Loss) = 1.38729632\nEpoch 304/1000 | Train Loss=1.86308450 | Val Loss=1.92459166 | Data=6.21028137 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729656\nEpoch 305/1000 | Train Loss=1.89107728 | Val Loss=1.92459202 | Data=6.30359077 | Val RMSE: 2.53284550 | ‚àö(Val Loss) = 1.38729668\nEpoch 306/1000 | Train Loss=1.93235296 | Val Loss=1.92459142 | Data=6.44117618 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729644\nEpoch 307/1000 | Train Loss=1.92377716 | Val Loss=1.92459238 | Data=6.41259003 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 308/1000 | Train Loss=1.94584310 | Val Loss=1.92459178 | Data=6.48614335 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729656\nEpoch 309/1000 | Train Loss=2.05671549 | Val Loss=1.92459142 | Data=6.85571814 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729644\nEpoch 310/1000 | Train Loss=1.91124243 | Val Loss=1.92459238 | Data=6.37080789 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 311/1000 | Train Loss=1.93464202 | Val Loss=1.92459238 | Data=6.44880652 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 312/1000 | Train Loss=1.87335831 | Val Loss=1.92459249 | Data=6.24452734 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 313/1000 | Train Loss=1.84485286 | Val Loss=1.92459261 | Data=6.14950919 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 314/1000 | Train Loss=1.90868837 | Val Loss=1.92459238 | Data=6.36229444 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 315/1000 | Train Loss=1.80396903 | Val Loss=1.92459249 | Data=6.01322973 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 316/1000 | Train Loss=2.08635920 | Val Loss=1.92459273 | Data=6.95453048 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 317/1000 | Train Loss=1.93694144 | Val Loss=1.92459273 | Data=6.45647120 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 318/1000 | Train Loss=1.87897986 | Val Loss=1.92459261 | Data=6.26326609 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 319/1000 | Train Loss=1.85400540 | Val Loss=1.92459261 | Data=6.18001771 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 320/1000 | Train Loss=1.93415219 | Val Loss=1.92459202 | Data=6.44717360 | Val RMSE: 2.53284550 | ‚àö(Val Loss) = 1.38729668\nEpoch 321/1000 | Train Loss=1.87554294 | Val Loss=1.92459214 | Data=6.25180960 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729668\nEpoch 322/1000 | Train Loss=1.86272281 | Val Loss=1.92459202 | Data=6.20907593 | Val RMSE: 2.53284550 | ‚àö(Val Loss) = 1.38729668\nEpoch 323/1000 | Train Loss=1.96057791 | Val Loss=1.92459202 | Data=6.53525949 | Val RMSE: 2.53284550 | ‚àö(Val Loss) = 1.38729668\nEpoch 324/1000 | Train Loss=1.80058903 | Val Loss=1.92459261 | Data=6.00196302 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 325/1000 | Train Loss=1.88043684 | Val Loss=1.92459214 | Data=6.26812243 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729668\nEpoch 326/1000 | Train Loss=1.88367581 | Val Loss=1.92459261 | Data=6.27891898 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 327/1000 | Train Loss=1.87292159 | Val Loss=1.92459202 | Data=6.24307179 | Val RMSE: 2.53284550 | ‚àö(Val Loss) = 1.38729668\nEpoch 328/1000 | Train Loss=1.97578931 | Val Loss=1.92459273 | Data=6.58596396 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 329/1000 | Train Loss=1.90991300 | Val Loss=1.92459238 | Data=6.36637640 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 330/1000 | Train Loss=1.85257900 | Val Loss=1.92459249 | Data=6.17526317 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 331/1000 | Train Loss=1.99814653 | Val Loss=1.92459214 | Data=6.66048837 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729668\nEpoch 332/1000 | Train Loss=1.90075684 | Val Loss=1.92459178 | Data=6.33585596 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729656\nEpoch 333/1000 | Train Loss=1.86952460 | Val Loss=1.92459238 | Data=6.23174834 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 334/1000 | Train Loss=1.87208837 | Val Loss=1.92459261 | Data=6.24029446 | Val RMSE: 2.53284597 | ‚àö(Val Loss) = 1.38729692\nEpoch 335/1000 | Train Loss=1.81634820 | Val Loss=1.92459178 | Data=6.05449367 | Val RMSE: 2.53284526 | ‚àö(Val Loss) = 1.38729656\nEpoch 336/1000 | Train Loss=1.77379575 | Val Loss=1.92459214 | Data=5.91265237 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729668\nEpoch 337/1000 | Train Loss=1.77279565 | Val Loss=1.92459249 | Data=5.90931845 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 338/1000 | Train Loss=1.88978261 | Val Loss=1.92459238 | Data=6.29927516 | Val RMSE: 2.53284574 | ‚àö(Val Loss) = 1.38729680\nEpoch 339/1000 | Train Loss=1.88783187 | Val Loss=1.92459202 | Data=6.29277253 | Val RMSE: 2.53284550 | ‚àö(Val Loss) = 1.38729668\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# # Use X_batch.shape[1] to dynamically get the correct window size (e.g., 6)\n# current_batch_size = X_batch.size(0)\n# current_window_size = X_batch.size(1) \n\n# x_seq = torch.arange(current_window_size, dtype=torch.float32, device=X_batch.device)\n# x_seq = x_seq.view(1, -1, 1).repeat(current_batch_size, 1, 1) # Shape: [32, 6, 1]\n\n# loss, data_loss, phys_loss = criterion(y_pred, y_batch, x_seq, alpha=0.5)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for sample in train_loader:\n    print(sample)\n    break","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample[0]","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# help(model)\nfor X_batch, y_batch in train_loader:\n    #optimizer.zero_grad()\n    #Set computing environment\n    X_batch, y_batch = X_batch.to(device), y_batch.to(device)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model(X_batch)","metadata":{"_uuid":"ab16f4e0-1acc-499b-8d8e-43e88dc79030","_cell_guid":"35554e79-62f0-483e-95f8-aab488cb1af6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2026-01-23T11:26:55.804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_batch.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_batch.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_batch","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for X_batch, y_batch in test_loader:\n    print(model(X_batch.to(device)))\n    print(y_batch)\n    break","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_rmse = 0 \n\nfor X_batch, y_batch in test_loader:\n    # Calculate RMSE directly\n    y_pred = model(X_batch.to(device)).cpu().detach().numpy()\n    test_rmse += root_mean_squared_error(y_batch, y_pred)\nprint('Error : ',test_rmse*10000)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#['6-6', '8-6', '10-1', '6-8', '8-8', '10-7', '10-6', '7-6', '4-5', '10-4', '3-1', '8-1', '9-6', '1-2', '6-1', '6-2', '8-5', '5-3', '2-5', '9-4', '7-5', '1-1']\ny_batch, y_pred","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the function\ndef gompertz_func(x,k,a,b):\n    return k*np.exp(-np.exp(a-(b*x)))\n\ndef gompertz_exponent_func(x,k,a,b):\n    return a-(b*x)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if k,a and b are the correct k, a and b\npath = \"../input/generate-hust-data-gompertz-k-a-b/\"\nfiles = os.listdir(path)\nk_s, a_s, b_s ,e_s = [], [],[],[]\n#print(files)\nfiles = [f for f in files if re.match(r'^\\d', f) and f.endswith('-hust_gompertz_params.csv')]\nfor file in files:\n    df = pd.read_csv(path+file)\n    #print(df.head())\n    df['exponent'] =  gompertz_exponent_func(x=df['rul']/10000,k=df['k'],a=df['a'],b=df['b'])\n    \n    answers = file[:3],list(df['k'])[-1], list(df['a'])[-1], list(df['b'])[-1], list(df['exponent'])[-1]\n    k_s.append(answers[1]), a_s.append(answers[2]), b_s.append(answers[3]) , e_s.append(answers[4])\n    print(answers)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min(k_s), max(k_s), min(a_s), max(a_s), min(b_s), max(b_s), min(e_s), max(e_s)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('../input/generate-hust-data-gompertz-k-a-b/hust_gompertz_params.csv')\ndf['exponent'] = df['a'] - df['b']\nprint(df[['file','k','a','b','rul']].head(50))\nprint(df[['file','k','a','b','rul']].tail(27))","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-23T11:26:55.805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}