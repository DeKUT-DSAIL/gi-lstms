{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12908579,"sourceType":"datasetVersion","datasetId":5945289},{"sourceId":287936110,"sourceType":"kernelVersion"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook covers three model implementations:\n\n1. A physics based model based on the Gompertz function\n2. To train an LSTM model (data-driven-model) to predict a sequence (e.g., battery capacity over time)\n3. To train an LSTM model to predict a sequence (e.g., battery capacity over time) while ensuring it respects physical behavior modeled by the Gompertz function.\n\n\n## FIXES \n\n### Fix 1 : Added Sanity Check for val loss and val rmse\n\nExplanation:\nIssue\tFix\tWhy\ntorch.Tensor()\t‚Üí torch.tensor()\tThe lowercase version is the recommended constructor for creating a single-value tensor.\nMissing closing parenthesis\tAdded\tFixes syntax error.\navg_val_loss type\tEnsure it‚Äôs a scalar (float or int)\tIf it‚Äôs already a tensor, remove the outer torch.tensor() call.\n\n### Fix 2: Use log loss in calculation of metric charts\n### Fix 3: Update reproducibility\n### Fix 4: time GPU run\n### Fix 5: LSTMs that predict RUL given initial x SoH values.","metadata":{"_uuid":"16c6b475-8271-444d-85dd-c0c90ba51a4e","_cell_guid":"5fbc7d50-bc9d-4e73-aba7-d7ef5fd55ae5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\nprint(\"# A: Import Libraries and set reproducibility\")\n# !git clone https://github.com/Yuri-Njathi/battery-lstm-ML.git\n# import sys\n# sys.path.append(\"battery-lstm-ML/\")\n\n\nimport torch\nimport numpy as np\nimport random\nimport os\nimport pandas as pd\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error , root_mean_squared_error\nfrom typing import Callable, Optional\n\ndef set_seed(seed=42):\n    \"\"\"\n    Set all relevant random seeds to ensure full reproducibility.\n    \"\"\"\n    # 1. Set basic seeds\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if multiple GPUs\n    \n    # 2. Force deterministic behavior in cudnn\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False  # turn off auto-tuning\n    \n    # 3. Optional: make dataloaders deterministic\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'  # deterministic cublas (for CUDA >= 10.2)\n\n    print(f\"‚úÖ Reproducibility environment set with seed = {seed}\")\n\n# Call this once at the start, pull from assess\nset_seed(42)\n\n'''\nset mode i.e.\n0 == physics based\n1 == lstm (data driven) #SoH only \n2 == lstm (physics constrained)\n'''\nmode = 2\n\nif mode == 1:\n    model_columns = ['SoH', 'Cycle number']\nif mode == 2:\n    model_columns = ['SoH']\n\n\n\nprint(\"# B: Setup variables and functions\")\n# # Set variables\n# WINDOW_SIZE = 35\nmodel_type = ['lstm','seq2seq-lstm','pinn'][2]\n\ncutoff_soh = 0.70\n# Set Computing Environment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nINPUT_SIZE = len(model_columns)\nOUTPUT_SIZE =3 #RUL #len(model_columns)#1 #controls how many values the lstm outputs\nnum_epochs = 1000 #60\nbatch_size = 4 #32 #because the data is little a small batch size is better when training\nnormalize_soh = False\nif normalize_soh:\n    soh_normalization_constant = 115.0 #115.0 may be better as it allows bounding between 0 and 1\n    cutoff_soh = cutoff_soh/soh_normalization_constant #set cutoff soh wrt to normalizer\nelse:\n    soh_normalization_constant = 1.0\n    cutoff_soh = cutoff_soh/soh_normalization_constant #set cutoff soh wrt to normalizer\n\n\n\n# def give_paths_get_loaders(paths,data_type,shuffle=False):\n#     X_list, y_list, y_target = get_x_y_lists(paths)\n\n#     if INPUT_SIZE == 1:\n#         # Concatenate all X and y\n#         X_1,y_1,y_2 = torch.cat(X_list, dim=0).squeeze(-1),torch.cat(y_list, dim=0).view(-1,OUTPUT_SIZE).squeeze(-1)\n#     else:\n#         X_1,y_1, y_2 = torch.cat(X_list, dim=0), torch.cat(y_list, dim=0), torch.cat(y_target, dim=0).view(-1,OUTPUT_SIZE).squeeze(-1)#.view(-1, OUTPUT_SIZE)\n    \n#     print(f\"X_{data_type} , y_{data_type} shapes : \",X_1.shape, y_1.shape)\n    \n#     #DataLoader\n#     print(\"load : \")\n#     loader = DataLoader(TensorDataset(X_1, y_1), batch_size=32, shuffle=shuffle)\n#     print(f\"{data_type}loader lengths : \",loader.__len__())\n#     return loader,X_1,y_1,y2\n\n\n\nprint(\"## üß† Model Architecture\")\n\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size=5, hidden_size=64, output_size=5):\n        super(LSTMModel, self).__init__()\n        \n        # LSTM: input_size=5 match your features\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        \n        # Linear Layer: Maps hidden_size (64) -> output_size (5)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # x shape: [32, 10, 5]\n        \n        # Run LSTM\n        # lstm_out shape: [32, 10, 64]\n        lstm_out, _ = self.lstm(x)\n        \n        # Take the last time step only\n        last_time_step = lstm_out[:, -1, :] \n        # last_time_step shape: [32, 64]\n        \n        # Project to 5 output features\n        prediction = self.fc(last_time_step)\n        # prediction shape: [32, 5]\n        \n        return prediction\n\n# class PhysicsInformedLSTM(nn.Module):\n#     def __init__(self, input_size=5, hidden_size=128):\n#         super(PhysicsInformedLSTM, self).__init__()\n        \n#         # 1. LSTM Core\n#         self.lstm = nn.LSTM(input_size,  hidden_size, num_layers=4, batch_first=True, dropout=0.1)\n#         self.lstm1 = nn.LSTM(input_size,  hidden_size, num_layers=4, batch_first=True, dropout=0.1)\n#         self.lstm2 = nn.LSTM(input_size,  hidden_size, num_layers=4, batch_first=True, dropout=0.1)\n#         self.lstm3 = nn.LSTM(input_size,  hidden_size, num_layers=4, batch_first=True, dropout=0.1)\n#         # 2. Split Heads (One for each parameter)\n#         # We split them so we can apply different activations to each\n#         self.fc_k = nn.Linear(hidden_size, 1)\n#         self.fc_a = nn.Linear(hidden_size, 1)\n#         self.fc_b = nn.Linear(hidden_size, 1)\n        \n#         # Activation to force k > 0 (Capacity cannot be negative)\n#         self.softplus = nn.Softplus()\n\n#     def forward(self, x):\n#         # x shape: [Batch, Window, Feats]\n        \n#         lstm_out, _ = self.lstm1(x)\n        \n#         # Take the last time step\n#         last_time_step1 = lstm_out[:, -1, :] \n        \n#         lstm_out, _ = self.lstm2(x)\n        \n#         # Take the last time step\n#         last_time_step2 = lstm_out[:, -1, :] \n\n#         lstm_out, _ = self.lstm3(x)\n        \n#         # Take the last time step\n#         last_time_step3 = lstm_out[:, -1, :] \n\n\n#         lstm_out, _ = self.lstm(last_time_step1,last_time_step2,last_time_step3)\n        \n#         # Take the last time step\n#         last_time_step = lstm_out[:, -1, :] \n#         # --- PREDICT PARAMETERS SEPARATELY ---\n        \n#         # 1. Predict k (Enforce Positive)\n#         # Softplus ensures output is always > 0\n#         k_pred = self.softplus(self.fc_k(last_time_step))\n        \n\n#         # 2. Predict a (Can be negative or positive)\n#         a_pred = self.fc_a(last_time_step)\n        \n        \n#         # 3. Predict b (Usually small positive, but let's keep it linear for now)\n#         b_pred = self.fc_b(last_time_step)\n        \n#         # Concatenate them back into shape [Batch, 3]\n#         prediction = torch.cat([k_pred, a_pred, b_pred], dim=1)\n        \n#         return prediction\n\n\nclass PhysicsInformedLSTM(nn.Module):\n    def __init__(self, input_size=5, hidden_size=128):\n        super().__init__()\n        # Direct mapping: Input -> LSTM -> FC -> Output\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(128, 3) \n        \n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return self.fc(out[:, -1, :])\n\n    # def __init__(self, input_size=5, hidden_size=128):\n    #     super(PhysicsInformedLSTM, self).__init__()\n        \n    #     # --- BRANCHES (Parallel Feature Extractors) ---\n    #     # Note: These process the exact same input. \n    #     self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True, dropout=0.5)\n    #     self.lstm2 = nn.LSTM(input_size, hidden_size, num_layers=4, batch_first=True, dropout=0.2)\n    #     self.lstm3 = nn.LSTM(input_size, hidden_size, num_layers=8, batch_first=True, dropout=0.1)\n        \n    #     # --- FUSION LAYER (The \"Manager\") ---\n    #     # FIX 1: The input_size here must be 'hidden_size' (128), not 'input_size' (5).\n    #     # Reason: This layer receives the outputs of lstm1, lstm2, lstm3.\n    #     self.fusion_lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=2, batch_first=True, dropout=0.1)\n\n    #     # 2. Split Heads\n    #     self.fc_k = nn.Linear(hidden_size, 1)\n    #     self.fc_a = nn.Linear(hidden_size, 1)\n    #     self.fc_b = nn.Linear(hidden_size, 1)\n        \n    #     self.softplus = nn.Softplus()\n\n    # def forward(self, x):\n    #     # x shape: [Batch, Window, Feats]\n        \n    #     # 1. Run Parallel Branches\n    #     out1, _ = self.lstm1(x)\n    #     last_step1 = out1[:, -1, :] # Shape: [Batch, 128]\n        \n    #     out2, _ = self.lstm2(x)\n    #     last_step2 = out2[:, -1, :] # Shape: [Batch, 128]\n        \n    #     out3, _ = self.lstm3(x)\n    #     last_step3 = out3[:, -1, :] # Shape: [Batch, 128]\n\n    #     # 2. Create a \"Meta-Sequence\" from the branches\n    #     # FIX 2: Stack them to form a sequence of length 3\n    #     # Shape becomes: [Batch, 3, 128]\n    #     fusion_input = torch.stack([last_step1, last_step2, last_step3], dim=1)\n        \n    #     # 3. Feed into Fusion LSTM\n    #     fusion_out, _ = self.fusion_lstm(fusion_input)\n        \n    #     # Take the final result from the fusion layer\n    #     final_embedding = fusion_out[:, -1, :] \n        \n    #     # --- PREDICT PARAMETERS ---\n    #     # We use torch.abs() or Softplus for k depending on preference\n    #     k_pred = self.softplus(self.fc_k(final_embedding)) \n    #     a_pred = self.fc_a(final_embedding)\n    #     b_pred = self.fc_b(final_embedding)\n        \n    #     prediction = torch.cat([k_pred, a_pred, b_pred], dim=1)\n    #     return prediction\n\nclass Seq2SeqLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, mid_size, output_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(hidden_size, mid_size)\n        self.fc2 = nn.Linear(mid_size, output_size)\n\n    def forward(self, x):\n        # out: [batch_size, seq_len, hidden_size]\n        out, _ = self.lstm(x)\n\n        # Apply the linear layers to each timestep\n        mid = self.relu(self.fc1(out))   # shape: [batch, seq_len, mid_size]\n        out_seq = self.fc2(mid)          # shape: [batch, seq_len, output_size]\n\n        return out_seq\n\n\n    #on initial tensorflow experiments I used 1,64,1,1 for those values.\n\n### TEST ON SEQUENTIAL MODEL ###\n# model(torch.Tensor([[86.4707],[86.4150],[86.3590],[86.3035],[86.2506],[86.2512],[86.1954],[86.1403],[86.1427],[86.0904],[86.0373],[85.9772],[85.9743],[85.9198],[85.8654],[85.8090],[85.8077],[85.7524],[85.6986],[85.6407],[85.5883],[85.5882],[85.6112],[85.4756],[85.4753],[85.4187],[85.3639],[85.3086],[85.3098],[85.3628],[85.1723],[85.1430],[85.1444],[85.0896],[85.0364]]))","metadata":{"_uuid":"d905f68b-0092-4d8e-980e-bd9d93d36ec8","_cell_guid":"bc0a48fc-f795-4b2c-a672-5942fa073d9a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-03T21:06:19.049690Z","iopub.execute_input":"2026-02-03T21:06:19.050477Z","iopub.status.idle":"2026-02-03T21:06:21.755031Z","shell.execute_reply.started":"2026-02-03T21:06:19.050441Z","shell.execute_reply":"2026-02-03T21:06:21.754158Z"}},"outputs":[{"name":"stdout","text":"# A: Import Libraries and set reproducibility\n‚úÖ Reproducibility environment set with seed = 42\n# B: Setup variables and functions\n## üß† Model Architecture\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"#### ‚ÄúNormalization‚Äù ‚â† ‚Äúscaling to [0,1]‚Äù.\n\n#### It simply means rescaling values to a stable, comparable numerical range.","metadata":{"_uuid":"5a5cf86c-22c4-4f30-a2ea-27e58384e19a","_cell_guid":"bfed866a-86b9-4ad2-9f29-45f1fac22912","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# C: Setup Train, Val and Test Loaders\n# 0. Data\n\nEach `X_train` is of shape `(num_samples, window_size)`\n\nEach `y_train` is of shape `(num_samples,)` (usually next value prediction)","metadata":{"_uuid":"e5c11278-599b-4fcf-a3a3-4b5cffc4aab6","_cell_guid":"ae57eb62-c436-4826-96f2-cc55c36376ee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"##  üß∞ Convert to Tensors for LSTM\nLSTM expects input shape: (batch_size, sequence_length, num_features)\n\nLet‚Äôs reshape the data and convert it:","metadata":{"_uuid":"e55a3991-26a1-4a5a-9aa1-a4095a055a67","_cell_guid":"e87496f0-d330-454c-82f9-86f0f16e746d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:06:21.756459Z","iopub.execute_input":"2026-02-03T21:06:21.756859Z","iopub.status.idle":"2026-02-03T21:06:21.761778Z","shell.execute_reply.started":"2026-02-03T21:06:21.756834Z","shell.execute_reply":"2026-02-03T21:06:21.760979Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"WINDOW_SIZES = [100] #[i for i in range (5,100,5)]\nprint(\"WINDOW SIZES TO TEST : \",WINDOW_SIZES,len(WINDOW_SIZES))\n\n\n# Make list of CSV paths\nmain_files_path = '/kaggle/input/generate-hust-data-gompertz-k-a-b/'\n#/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\ncsv_files = os.listdir(main_files_path)\ncsv_files = [f for f in csv_files if re.match(r'^\\d', f) and f.endswith('-hust_gompertz_params.csv')]\n\n#BatteryML like train-val-test split\ncsv_files = [f.removesuffix('-hust_gompertz_params.csv') for f in csv_files]\nprint(csv_files)\n\ntrain_ids = [\n    '1-3',  '1-4',  '1-5',  '1-6',  '1-7',  '1-8',  '2-2',  '2-3',\n    '2-4',  '2-6',  '2-7',  '2-8',  '3-2',  '3-3',  '3-4',  '3-5',\n    '3-6',  '3-7',  '3-8',  '4-1',  '4-2',  '4-3',  '4-4',  '4-6',\n    '4-7',  '4-8',  '5-1',  '5-2',  '5-4',  '5-5',  '5-6',  '5-7',\n    '6-3',  '6-4',  '6-5',  '7-1',  '7-2',  '7-3',  '7-4',  '7-7',\n    '7-8',  '8-2',  '8-3',  '8-4',  '8-7',  '9-1',  '9-2',  '9-3',\n    '9-5',  '9-7',  '9-8',  '10-2', '10-3', '10-5', '10-8']\n\ntest_ids = [f for f in csv_files if f not in train_ids]\n\nprint(test_ids,len(test_ids))\n\n#csv_paths = [os.path.join(main_files_path, file) for file in csv_files]\n#separate according to train, val and test\ntrain_paths = [os.path.join(main_files_path, file+'-hust_gompertz_params.csv') for file in train_ids]\n\ntesting_paths = [os.path.join(main_files_path, file+'-hust_gompertz_params.csv') for file in test_ids]\n\nval_paths = testing_paths[:int(len(testing_paths)*0.5)]\ntest_paths = testing_paths[int(len(testing_paths)*0.5):]\n\nprint(len(train_paths), len(val_paths), len(test_paths))\n\n# 1. Helper function to detach any GPU tensors in your lists\ndef to_cpu_numpy(data_list):\n    clean_list = []\n    for x in data_list:\n        if torch.is_tensor(x):\n            clean_list.append(x.detach().cpu().item()) # Move to CPU, get scalar\n        else:\n            clean_list.append(x) # Already a float\n    return np.array(clean_list)","metadata":{"_uuid":"764511b2-155c-4f27-9abb-b9020665168e","_cell_guid":"da093279-8ac7-4672-b25f-7a87931a0801","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-03T21:06:21.762608Z","iopub.execute_input":"2026-02-03T21:06:21.762856Z","iopub.status.idle":"2026-02-03T21:06:21.788702Z","shell.execute_reply.started":"2026-02-03T21:06:21.762821Z","shell.execute_reply":"2026-02-03T21:06:21.787701Z"}},"outputs":[{"name":"stdout","text":"WINDOW SIZES TO TEST :  [100] 1\n['6-6', '8-7', '8-6', '9-1', '10-1', '6-8', '8-8', '10-7', '3-5', '5-1', '5-5', '7-1', '2-7', '10-6', '4-7', '7-7', '7-6', '4-5', '9-2', '10-4', '3-1', '9-7', '8-1', '10-8', '8-4', '4-6', '4-4', '3-8', '5-4', '9-6', '10-5', '7-8', '5-2', '9-8', '1-2', '5-6', '10-2', '2-6', '6-1', '2-4', '1-4', '4-1', '1-6', '6-2', '8-5', '5-7', '1-5', '1-8', '5-3', '6-5', '9-5', '4-8', '7-2', '2-5', '7-3', '9-3', '9-4', '8-2', '10-3', '6-3', '3-2', '7-5', '3-7', '2-3', '1-3', '8-3', '2-8', '7-4', '4-2', '6-4', '1-1', '3-3', '4-3', '3-4', '2-2', '1-7', '3-6']\n['6-6', '8-6', '10-1', '6-8', '8-8', '10-7', '10-6', '7-6', '4-5', '10-4', '3-1', '8-1', '9-6', '1-2', '6-1', '6-2', '8-5', '5-3', '2-5', '9-4', '7-5', '1-1'] 22\n55 11 11\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# id_cycle_length = {'6-6': 2468, '7-8': 1938, '5-3': 2689, '6-8': 2450, '5-5': 1583, '2-4': 1499, '10-3': 1848, '1-4': 1500, '6-3': 1804, '5-6': 2460, '8-3': 2290, '3-6': 2491, '10-4': 1811, '9-6': 1742, '3-2': 2283, '9-2': 2143, '9-7': 2012, '6-5': 2178, '4-8': 1706, '5-7': 1448, '1-5': 1971, '1-7': 1678, '1-8': 2285, '8-7': 2047, '6-2': 1908, '3-3': 1649, '6-1': 1609, '2-7': 2202, '1-2': 2678, '10-7': 1783, '10-1': 1702, '9-1': 2057, '8-5': 1348, '9-4': 1975, '4-4': 1491, '3-4': 1766, '10-6': 2285, '5-2': 1926, '7-5': 1875, '4-3': 1142, '5-4': 1962, '3-5': 2657, '7-2': 2030, '3-7': 2479, '1-3': 1858, '9-5': 2168, '3-1': 1938, '4-1': 2217, '9-3': 1905, '7-1': 1690, '8-8': 1679, '6-4': 1717, '4-2': 1782, '5-1': 2507, '2-3': 1751, '4-5': 1561, '4-6': 1380, '2-8': 1481, '8-1': 1308, '10-8': 1400, '9-8': 2308, '3-8': 2342, '7-3': 1295, '8-2': 2041, '8-6': 2365, '7-6': 1419, '10-5': 2030, '7-7': 1685, '7-4': 1393, '4-7': 2216, '2-6': 1572, '10-2': 1697, '1-6': 1143, '8-4': 1885, '2-5': 1386, '1-1': 1504, '2-2': 2651}\n\n# #3 dictionaries\n# group_low = {}   # 1142 - 1658\n# group_mid = {}   # 1659 - 2173\n# group_high = {}  # 2174 - 2689\n\n# for key, value in id_cycle_length.items():\n#     if 1142 <= value <= 1658:\n#         group_low[key] = value\n#     elif 1659 <= value <= 2173:\n#         group_mid[key] = value\n#     elif 2174 <= value <= 2689:\n#         group_high[key] = value\n\n# print(f\"Low Cycle Life ({len(group_low)} items):\", group_low)\n# print(\"-\" * 20)\n# print(f\"Mid Cycle Life ({len(group_mid)} items):\", group_mid)\n# print(\"-\" * 20)\n# print(f\"High Cycle Life ({len(group_high)} items):\", group_high)\n\n\nimport math\n\nid_cycle_length = {'6-6': 2468, '7-8': 1938, '5-3': 2689, '6-8': 2450, '5-5': 1583, '2-4': 1499, '10-3': 1848, '1-4': 1500, '6-3': 1804, '5-6': 2460, '8-3': 2290, '3-6': 2491, '10-4': 1811, '9-6': 1742, '3-2': 2283, '9-2': 2143, '9-7': 2012, '6-5': 2178, '4-8': 1706, '5-7': 1448, '1-5': 1971, '1-7': 1678, '1-8': 2285, '8-7': 2047, '6-2': 1908, '3-3': 1649, '6-1': 1609, '2-7': 2202, '1-2': 2678, '10-7': 1783, '10-1': 1702, '9-1': 2057, '8-5': 1348, '9-4': 1975, '4-4': 1491, '3-4': 1766, '10-6': 2285, '5-2': 1926, '7-5': 1875, '4-3': 1142, '5-4': 1962, '3-5': 2657, '7-2': 2030, '3-7': 2479, '1-3': 1858, '9-5': 2168, '3-1': 1938, '4-1': 2217, '9-3': 1905, '7-1': 1690, '8-8': 1679, '6-4': 1717, '4-2': 1782, '5-1': 2507, '2-3': 1751, '4-5': 1561, '4-6': 1380, '2-8': 1481, '8-1': 1308, '10-8': 1400, '9-8': 2308, '3-8': 2342, '7-3': 1295, '8-2': 2041, '8-6': 2365, '7-6': 1419, '10-5': 2030, '7-7': 1685, '7-4': 1393, '4-7': 2216, '2-6': 1572, '10-2': 1697, '1-6': 1143, '8-4': 1885, '2-5': 1386, '1-1': 1504, '2-2': 2651}\n\n# 1. Sort the dictionary items by value (Cycle Life)\nsorted_items = sorted(id_cycle_length.items(), key=lambda x: x[1])\n\n# 2. Calculate slice indices\ntotal_len = len(sorted_items)\nsplit_1 = total_len // 3          # First cut point (approx 33%)\nsplit_2 = (total_len * 2) // 3    # Second cut point (approx 66%)\n\n# 3. Create the new dictionaries using slicing\ngroup_low = dict(sorted_items[:split_1])\ngroup_mid = dict(sorted_items[split_1:split_2])\ngroup_high = dict(sorted_items[split_2:])\n\n# --- Output Results ---\nprint(f\"Low Cycle Life ({len(group_low)} items): Range {min(group_low.values())} - {max(group_low.values())}\")\nprint(list(group_low.values()))\nprint(\"-\" * 20)\n\nprint(f\"Mid Cycle Life ({len(group_mid)} items): Range {min(group_mid.values())} - {max(group_mid.values())}\")\nprint(list(group_mid.values()))\nprint(\"-\" * 20)\n\nprint(f\"High Cycle Life ({len(group_high)} items): Range {min(group_high.values())} - {max(group_high.values())}\")\nprint(list(group_high.values()))\n\n# New Dynamic Ranges for reference:\n# Low:  1142 - 1702\n# Mid:  1706 - 2178\n# High: 2202 - 2689","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:06:21.789531Z","iopub.execute_input":"2026-02-03T21:06:21.789752Z","iopub.status.idle":"2026-02-03T21:06:21.802386Z","shell.execute_reply.started":"2026-02-03T21:06:21.789721Z","shell.execute_reply":"2026-02-03T21:06:21.801582Z"}},"outputs":[{"name":"stdout","text":"Low Cycle Life (25 items): Range 1142 - 1690\n[1142, 1143, 1295, 1308, 1348, 1380, 1386, 1393, 1400, 1419, 1448, 1481, 1491, 1499, 1500, 1504, 1561, 1572, 1583, 1609, 1649, 1678, 1679, 1685, 1690]\n--------------------\nMid Cycle Life (26 items): Range 1697 - 2030\n[1697, 1702, 1706, 1717, 1742, 1751, 1766, 1782, 1783, 1804, 1811, 1848, 1858, 1875, 1885, 1905, 1908, 1926, 1938, 1938, 1962, 1971, 1975, 2012, 2030, 2030]\n--------------------\nHigh Cycle Life (26 items): Range 2041 - 2689\n[2041, 2047, 2057, 2143, 2168, 2178, 2202, 2216, 2217, 2283, 2285, 2285, 2290, 2308, 2342, 2365, 2450, 2460, 2468, 2479, 2491, 2507, 2651, 2657, 2678, 2689]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def train_model(WINDOW_SIZES,train_paths,val_paths,test_paths,data_range):\n    print(\"Cutoff SoH : \",cutoff_soh)\n    \n    def df_to_X_y_tensor(df, window_size=5,output_size=5):\n        '''\n        Converts a time series into (X, y) tensors for LSTM training.\n        \n        X shape: (num_samples, window_size, 1)\n        y shape: (num_samples, 1)\n        '''\n        # if isinstance(df, (pd.DataFrame, pd.Series)):\n        #     df_as_np = df.to_numpy()\n        # else:\n        #     df_as_np = df  # Assume already numpy\n    \n        X, y , y2 = [], [], []\n        #for i in range(len(df_as_np) - window_size):\n        X.append(list(df['SoH'])[:window_size+1])\n        #y.append([df_as_np[i + window_size:i + window_size+output_size]])\n        y.append([list(df['k'])[-1],list(df['a'])[-1],list(df['b'])[-1]])\n        #append([[val] for val in df_as_np[i + window_size:i + window_size+1]]) #next cycle\n        y2.append(list(df['rul'])[:1])\n        X,y,y2 = np.array(X),np.array(y), np.array(y2)\n        X_tensor = torch.tensor(X, dtype=torch.float32)#.squeeze()\n        y_tensor = torch.tensor(y, dtype=torch.float32)#.squeeze()\n        y_2_tensor = torch.tensor(y2, dtype=torch.float32)  #bug was here written y instead of y2\n        return X_tensor, y_tensor, y_2_tensor\n    \n    def get_x_y_lists(paths):\n        X_list,y_list,y_target = [],[],[]\n        for path in paths:\n            print(path)\n            df = pd.read_csv(path)\n            df['Cycle number'] = df['Cycle number']/10000\n            df['rul'] = df['rul']/10000\n            #normalize SoH\n            df['SoH'] =  df['SoH']/soh_normalization_constant\n            df.index = df['Cycle number']\n            #SoH = df[model_columns]\n            X, y , y1 = df_to_X_y_tensor(df, window_size=WINDOW_SIZE,output_size=OUTPUT_SIZE)\n            X_list.append(X)\n            y_list.append(y)\n            y_target.append(y1) #RUL\n        return X_list,y_list, y_target\n    \n        \n    for WINDOW_SIZE in WINDOW_SIZES:\n        #### PREPARE ALL DATA ####\n        ### DONT RUN AS LOOP\n        WINDOW_SIZE = WINDOW_SIZE -1\n        def give_paths_get_loaders(paths,data_type,shuffle=False):\n            X_list, y_list, y_target = get_x_y_lists(paths)\n    \n            batch_size = torch.cat(X_list, dim=0).shape[0]\n            \n            if INPUT_SIZE == 1:\n                # Concatenate all X and y\n                X_1,y_1,y_2 = torch.cat(X_list, dim=0).unsqueeze(-1),torch.cat(y_list, dim=0).view(batch_size,-1),torch.cat(y_target, dim=0).view(batch_size,-1)\n            else:\n                X_1,y_1,y_2 = torch.cat(X_list, dim=0).squeeze(2),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE),torch.cat(y_target, dim=0).view(-1,INPUT_SIZE)\n            \n            print(f\" X_{data_type} shape : {X_1.shape} , y_{data_type} shape : {y_1.shape} Ôºåy_2{data_type} shape: {y_2.shape}\" )\n            \n            #DataLoader\n            print(\"load : \")\n            loader = DataLoader(TensorDataset(X_1, y_1, y_2), batch_size=32, shuffle=shuffle)\n            print(f\"{data_type}loader lengths : \",loader.__len__())\n            return loader,X_1,y_1, y_2\n        \n        data_use = {\n            0:[\"train\"],1:[\"val\"],2:[\"test\"]\n        }\n        #normalize\n        train_loader,X_train,y_train,y_train_target= give_paths_get_loaders(train_paths,data_use[0],shuffle=True)\n        val_loader,X_val,y_val,y_val_target= give_paths_get_loaders(val_paths,data_use[1])\n        test_loader,X_test,y_test,y_test_target = give_paths_get_loaders(test_paths,data_use[2])\n        #OUTPUT_SIZE = WINDOW_SIZE\n        print(\"## üß† Model\")\n        if model_type == 'lstm':\n            model = LSTMModel(input_size=INPUT_SIZE, output_size=OUTPUT_SIZE).to(device) # values for multioutput model\n            last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n            print(\"Last model window : \",last_model_path)\n        if model_type == 'seq2seq-lstm':\n            model = Seq2SeqLSTM(INPUT_SIZE,64,32,OUTPUT_SIZE).to(device) # values from previously working tensorflow model # 1, 64, 8, 1 \n            last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n            print(\"Last model window : \",last_model_path)\n        if model_type == 'pinn':\n            model = PhysicsInformedLSTM(input_size=INPUT_SIZE).to(device) # values for multioutput model\n            last_model_path = f'last_model_window_{WINDOW_SIZE+1}_model_{model_type}_data_{data_range}.pth'\n            print(\"Last model window : \",last_model_path)\n            # 2. Calculate the \"Mean\" targets from your training data\n            # Assuming y_train is your tensor of shape [N, 3] (k, a, b)\n            # We calculate the mean across the batch dimension (dim=0)\n            # 2. Define your dataset means (approximate values based on your earlier logs)\n            # You can also calculate these dynamically: k_mean = y_train[:, 0].mean()\n            k_mean = float(torch.mean(y_train, dim=0)[0])\n            a_mean = float(torch.mean(y_train, dim=0)[1])\n            b_mean = float(torch.mean(y_train, dim=0)[2])\n            \n            print(f\"üöÄ Initializing model output to: k={k_mean}, a={a_mean}, b={b_mean}\")\n            \n            # 3. Manually set the Biases and Weights\n            with torch.no_grad():\n                # A. Set the Bias (The \"Starting Value\")\n                # Since self.fc outputs 3 values [k, a, b], we access them by index\n                model.fc.bias[0] = k_mean\n                model.fc.bias[1] = a_mean\n                model.fc.bias[2] = b_mean\n                \n                # B. Shrink the Weights (The \"Noise\")\n                # We set weights to near-zero so the LSTM input doesn't disturb our nice means yet.\n                # The model will gradually increase these weights as it learns the patterns.\n                model.fc.weight.fill_(0.001)\n                    \n                print(\"‚úÖ Model Output Parameters Initialized!\")\n        \n\n       \n    \n        #break\n        print('''##\n        ### üìà Gompertz Function (Physics Law)\n        \n        * `x`: Time (or cycle number)\n        \n        * `k`: Max value (e.g., max capacity)\n        \n        * `a`, `b`: Shape parameters''')\n        \n        def gompertz_func(x, k, a, b):\n            return k * torch.exp(-a * torch.exp(-b * x))\n        \n        print(\"## üß† Loss Functions\\n\")\n        \n        print('''## ‚öôÔ∏è 1. Data-Informed Loss Function\n        a data loss (what the LSTM learns from data)\n        \n        * Mean Squared Error for Training\n        * RMSE for autoregressive approximation of compound error\n        \n        ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n        You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n        \n        * `alpha`: controls how strongly physics is enforced.''')\n            \n        def pinn_loss(prediction, target, x, k, a, b, alpha=0.5):\n            data_loss = F.mse_loss(prediction, target)\n            physics_pred = gompertz_func(x, k, a, b) #this needs to be refined !!!\n            physics_loss = F.mse_loss(prediction, physics_pred)\n            return data_loss + alpha * physics_loss, data_loss.item(), physics_loss.item()\n        \n        def data_loss_func(prediction, target, x, alpha=1.0):\n            data_loss = F.mse_loss(prediction, target)\n            return data_loss, data_loss.item() , None\n    \n        def rul_consistency_loss(prediction, true_rul_target, failure_threshold=0.7, lambda_rul=0.1):\n            \"\"\"\n            Calculates the error between the True RUL and the RUL calculated \n            from the predicted Gompertz parameters at the failure threshold.\n            \n            Formula: RUL_calc = (a - ln(ln(k / 0.7))) / b\n            \n            prediction:      [Batch, 3] (Predicted k, a, b)\n            true_rul_target: [Batch, 1] (The ground truth RUL for each sample)\n            failure_threshold: Float (e.g., 0.7 for 70% capacity)\n            \"\"\"\n            \n            # 1. Extract Parameters\n            # We do NOT unsqueeze to [Batch, 1, 1] because RUL is just one number per battery, not a sequence.\n            # Shape: [32, 1]\n            k_pred = prediction[:, 0:1] \n            a_pred = prediction[:, 1:2]\n            b_pred = prediction[:, 2:3]\n            \n            # 2. Safety Check: k must be > threshold\n            # If the max capacity (k) is predicted to be LOWER than 0.7, the battery never 'failed' \n            # (it started broken), which breaks the math. We clamp k to be at least 0.7001.\n            k_safe = torch.clamp(k_pred, min=failure_threshold + 0.0001)\n        \n            # 3. The Inverse Gompertz Formula for RUL\n            # y = 0.7 (fixed)\n            # x (RUL) = (a - ln(ln(k / 0.7))) / b\n            \n            ratio = k_safe / failure_threshold\n            \n            # We create a scalar tensor for the threshold to ensure device compatibility\n            inner_log = torch.log(ratio)\n            double_log = torch.log(inner_log + 1e-6)\n            \n            # Calculated RUL based on physics parameters\n            rul_calc = (a_pred - double_log) / (b_pred + 1e-6)\n        \n            # 4. Calculate MSE between Physics-RUL and Ground-Truth RUL\n            # true_rul_target must be shape [32, 1]\n            rul_loss = F.mse_loss(rul_calc, true_rul_target)\n            \n            return lambda_rul * rul_loss\n        def eol_boundary_loss(prediction, true_failure_cycle, failure_threshold=0.7):\n            \"\"\"\n            Enforces that the Gompertz curve passes through the failure_threshold (0.7)\n            at the exact cycle defined by true_failure_cycle.\n            \"\"\"\n            # 1. Extract Parameters\n            k_pred = prediction[:, 0:1]\n            a_pred = prediction[:, 1:2]\n            b_pred = prediction[:, 2:3]\n            \n            # 2. Compute Gompertz Value at the True Failure Cycle\n            # We use the formula: y = k * exp(-a * exp(-b * x))\n            # true_failure_cycle must be shape [Batch, 1]\n            \n            #print(true_failure_cycle)\n            exponent_term = a_pred-b_pred * true_failure_cycle #*10000\n            # Clamp exponent to prevent overflow/underflow\n            exponent_term = torch.clamp(exponent_term, min=-10.0, max=10.0)\n            \n            inner_exp = torch.exp(exponent_term)\n            gompertz_at_failure = k_pred * torch.exp(-inner_exp)\n            #print(gompertz_at_failure)\n            \n            # 3. Calculate Error\n            # value to be exactly 0.7 \n            target = torch.full_like(gompertz_at_failure, failure_threshold)\n            #print(target)\n            loss = F.mse_loss(gompertz_at_failure, target)\n            \n            return loss\n    \n        \n        def physics_informed_loss(prediction, target, target_y, x, alpha=1.0, lambda_ode=0.1, lambda_rul=0.05, weights=None, physics_weights=1.0):\n            \"\"\"\n            prediction: (Batch, 3) -> [32, 3] (One set of k,a,b per sequence)\n            target:     (Batch, 3) -> [32, 3]\n            x:          (Batch, Window, 1) -> [32, 5, 1] (Time steps)\n            \"\"\"\n            boundary_loss = eol_boundary_loss(\n                    prediction, \n                    target_y,\n                    failure_threshold=0.7\n                )\n            # # 1. Standard Data Loss (MSE on parameters k, a, b)\n            # # prediction and target are both [32, 3], so this works directly.\n            # data_loss = F.mse_loss(prediction, target)\n            \n            # 1. Weighted Data Loss\n            # Define weights: k needs HUGE focus, b needs to be quieted\n            # Move to same device as your data automatically\n            if weights is None:\n                weights = torch.tensor([10000.0, 100.0, 0.1]).to(prediction.device)\n                \n            weights =  weights #weig torch.tensor([10000.0, 100.0, 0.1]).to(prediction.device)\n            \n            # Calculate squared error manually (Batch, 3)\n            raw_mse = (prediction - target) ** 2\n            \n            # Multiply error by importance weights (Broadcasting [32, 3] * [3])\n            weighted_mse = raw_mse * weights\n            \n            # Now take the mean\n            data_loss = torch.mean(weighted_mse)\n        \n            # 2. Prepare Physics Inputs\n            if not x.requires_grad:\n                x.requires_grad_(True)\n        \n            # --- Slicing & Reshaping (THE FIX) ---\n            # Prediction is [32, 3]. We need [32, 1, 1] to broadcast over the Window dimension of x (5).\n            \n            # Take column 0, keep dims -> [32, 1], then add extra dim -> [32, 1, 1]\n            k_pred = prediction[:, 0:1].unsqueeze(1) \n            a_pred = prediction[:, 1:2].unsqueeze(1) \n            b_pred = prediction[:, 2:3].unsqueeze(1) \n    \n            # Inside physics_informed_loss, before math operations:\n            k_pred = torch.clamp(k_pred, min=0, max=2)\n            a_pred = torch.clamp(a_pred, min=-5, max=-1)\n            b_pred = torch.clamp(b_pred, min=-30, max=-3)\n            # Now:\n            # b_pred: [32, 1, 1]\n            # x:      [32, 5, 1]\n            # Result: [32, 5, 1] (Broadcasting works!)\n            exponent_term = a_pred - (b_pred * x)\n            exponent_term = torch.clamp(exponent_term, min=-10.0, max=10.0)\n    \n            y_pred_curve = k_pred * torch.exp(-torch.exp(exponent_term))\n        \n            # --- Physics Gradients ---\n            \n            # Calculate dy/dx (autograd)\n            dydx_autograd = torch.autograd.grad(\n                outputs=y_pred_curve,\n                inputs=x,\n                grad_outputs=torch.ones_like(y_pred_curve),\n                create_graph=True, \n                retain_graph=True,\n                only_inputs=True\n            )[0]\n        \n            # Calculate dy/dx (ODE Equation)\n            # dy/dx = b * y * e^(a-bx)\n            dydx_ode = b_pred * y_pred_curve * torch.exp(exponent_term)\n        \n            # Residual\n            ode_residual = dydx_autograd - dydx_ode\n            ode_loss = torch.mean(ode_residual ** 2)\n    \n            #add RUL loss\n            # loss_rul = inverse_time_loss(prediction, target_y, x_seq)\n            loss_rul = rul_consistency_loss(prediction, target_y, failure_threshold=0.7, lambda_rul=lambda_rul)\n            \n            total_loss = (alpha * data_loss) + ((lambda_ode * ode_loss) + loss_rul+boundary_loss)*physics_weights\n        \n            return total_loss, data_loss.item(), (ode_loss.item()+loss_rul.item()+ boundary_loss.item()) \n            \n        epoch = 0\n        avg_train_loss = 0.0\n        avg_val_loss = 0.0\n        val_rmse_total = 0.0\n        data_loss = 0.0\n        phys_loss = 0.0 # Only set if you're using physics loss\n        \n        if mode == 0:\n            criterion = physics_loss #??\n            best_model_path = f'best_pysics_model-window-{WINDOW_SIZE+1}.pth' #??\n            loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n        if mode == 1:\n            criterion = data_loss_func\n            best_model_path = f'best_lstm_model-window-{WINDOW_SIZE+1}.pth'\n            loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f}\"\n        if mode == 2:\n            # phys_loss = 0.0 # Only set if you're using physics loss\n            # criterion = pinn_loss\n            # best_model_path = f'best_pinn_lstm_model-window-{WINDOW_SIZE}.pth'\n            # loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n            criterion = physics_informed_loss\n            best_model_path = f'best_lstm_model-window-{WINDOW_SIZE+1}_model_{model_type}_data_{data_range}.pth'\n            loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n        \n        print(\"## üõ†Ô∏è Parameter Strategy\")\n        \n        # k = nn.Parameter(torch.tensor(1.0))\n        # a = nn.Parameter(torch.tensor(0.1))\n        # b = nn.Parameter(torch.tensor(0.1))\n        # Include in optimizer\n        optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=400)\n        # Function to set a new LR\n        def set_learning_rate(optimizer, new_lr):\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = new_lr\n            print(f\"‚úÖ Learning Rate updated to {new_lr}\")\n        \n        print(\"## üîÅ Training Loop\")\n        def compute_rmse(pred, target):\n            return torch.sqrt(F.mse_loss(pred, target))\n            \n        train_losses = []\n        val_losses = []\n        val_rmses = []\n        data_losses = []\n        phys_losses = []\n        \n        best_val_loss = float('inf')\n        best_epoch = 0\n        \n        # # 2. Create the sequence from 1 to WINDOW_SIZE\n        # # torch.arange(start, end) excludes the end, so we use WINDOW_SIZE + 1\n        # x_raw = torch.arange(1, WINDOW_SIZE + 1, dtype=torch.float32, device=device)\n        \n        # # 3. Scale it (divide by 10000)\n        # x_scaled = x_raw / 10000.0\n        \n        # # 4. Reshape to (N, 1) and enable gradients\n        # # .view(-1, 1) makes it a column vector. \n        # # .requires_grad_(True) is vital for the physics loss (dydx calculation).\n        # x_seq = x_scaled.view(-1, 1).requires_grad_(True)\n    \n        # Provide as a train function\n        for epoch in range(num_epochs):\n            model.train()\n            total_loss = 0\n            total_data_loss = 0\n            total_phys_loss = 0\n            for X_batch, y_batch, y_target in train_loader:\n                optimizer.zero_grad()\n                #Set computing environment\n                X_batch, y_batch, y_target = X_batch.to(device), y_batch.to(device), y_target.to(device)\n                #Predict\n                y_pred = model(X_batch)\n                \n    \n                # Create x_seq for physics loss\n                #x_seq = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_batch.size(0), 1).to(X_batch.device)\n                x_seq = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_batch.size(0), 1).unsqueeze(-1).to(X_batch.device)\n    \n    \n                # loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.4, lambda_ode=0.2, lambda_rul=0.4)\n    \n    \n                #CURRICULUM LEARNING\n                if epoch <= 50:\n                    current_weights = torch.tensor([0.0, 0.0, 100.0]).to(device)\n                    #loss_weights = torch.tensor([1.0, 0.0, 0.0]).to(device)\n                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n                elif epoch <= 100:\n                    current_weights = torch.tensor([0.0, 10.0, 100.0]).to(device)\n                    #loss_weights = torch.tensor([1.0, 0.0, 0.0]).to(device)\n                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n                    #loss = torch.mean(loss * loss_weights)\n                elif epoch <= 200:\n                    current_weights = torch.tensor([1.0, 10.0, 100.0]).to(device)\n                    #loss_weights = torch.tensor([1.0, 1.0, 0.0]).to(device)\n                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n                    #loss = torch.mean(loss * loss_weights)\n                elif epoch <= 300:\n                    current_weights = torch.tensor([1.0, 10.0, 100.0]).to(device)\n                    #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n                    #loss = torch.mean(loss * loss_weights)\n                elif epoch <= 400:\n                    current_weights = torch.tensor([1.0, 10.0, 100.0]).to(device)\n                    #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n                    #loss = torch.mean(loss * loss_weights)\n                elif epoch <= 600:\n                    current_weights = torch.tensor([10000.0, 100.0, 1.0]).to(device)\n                    #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n                    #loss = torch.mean(loss * loss_weights)\n    \n                elif epoch <= 1800:\n                    current_weights = torch.tensor([10000.0, 100.0, 10.0]).to(device)\n                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n                    \n                elif epoch <= 2600:\n                    current_weights = torch.tensor([0.0, 0.0, 100.0]).to(device)\n                    #loss_weights = torch.tensor([1.0, 1.0, 0.0]).to(device)\n                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=10000.0, lambda_ode=100.1, lambda_rul=10000.0,weights=current_weights)\n                    #loss = torch.mean(loss * loss_weights)\n                # elif epoch <= 1200:\n                #     current_weights = torch.tensor([10000.0, 100.0, 0.0]).to(device)\n                #     #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n                #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=1.0, lambda_ode=1.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n                #     #loss = torch.mean(loss * loss_weights)\n                # elif epoch <= 1400:\n                #     current_weights = torch.tensor([10000.0, 100.0, 1.0]).to(device)\n                #     #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n                #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.0, lambda_ode=1.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n                #     #loss = torch.mean(loss * loss_weights)\n                # elif epoch <= 1600:\n                #     current_weights = torch.tensor([10000.0, 100.0, 1.0]).to(device)\n                #     #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n                #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.0, lambda_ode=1.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n                # elif epoch <= 1800:\n                #     current_weights = torch.tensor([10000.0, 100.0, 1.0]).to(device)\n                #     #loss_weights = torch.tensor([10000.0, 100.0, 0.1]).to(device)\n                #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.0, lambda_ode=0.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n                # elif epoch <= 2000:\n                #     current_weights = torch.tensor([10000.0, 100.0, 1.0]).to(device)\n                #     #loss_weights = torch.tensor([10000.0, 100.0, 0.1]).to(device)\n                #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.0, lambda_ode=0.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n                else:\n                    current_weights = torch.tensor([0.0, 0.0, 10000.0]).to(device)\n                    #loss_weights = torch.tensor([10000.0, 100.0, 0.1]).to(device)\n                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=10000.0, lambda_ode=100.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n                #     #loss = torch.mean(loss * loss_weights)\n                # # elif epoch <= 300:\n                # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.4, lambda_ode=0.2, lambda_rul=0.4)\n                # # elif epoch <= 400:\n                # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.2, lambda_ode=0.2, lambda_rul=0.6)            \n                # # elif epoch <= 500:\n                # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.4, lambda_ode=0.4, lambda_rul=0.2)\n                # # elif epoch <= 600:\n                # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.3, lambda_ode=0.4, lambda_rul=0.3)\n                # # # elif epoch <= 700:\n                # # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.0, lambda_ode=0.4, lambda_rul=1.0)\n                # # # elif epoch <= 800:\n                # # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.2, lambda_ode=0.4, lambda_rul=0.6)\n                # # else:\n                # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.3, lambda_ode=0.4, lambda_rul=0.3)\n                # # loss.backward()\n                # # optimizer.step()\n                total_data_loss += data_loss\n                total_loss +=loss.item() if torch.is_tensor(loss) else loss\n                total_phys_loss += phys_loss.item() if torch.is_tensor(phys_loss) else phys_loss\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20.0)\n                optimizer.step()\n    \n            # Adjust learning rate\n            scheduler.step()\n            #Get current learning rate\n            current_lr = scheduler.get_last_lr()[0]\n            #consolidate losses\n            avg_data_loss = total_data_loss / len(train_loader)\n            avg_train_loss = total_loss / len(train_loader)\n            avg_phys_loss = total_phys_loss / len(train_loader)\n            \n            # ---- Validation Pass ----\n            model.eval()\n            val_loss_total = 0.0\n            val_rmse_total = 0.0\n    \n            # if epoch%20 == 0:\n                # test_rmse = 0 \n                # for X_batch, y_batch, y_target in test_loader:\n                #     # Calculate RMSE directly\n                #     y_pred = model(X_batch.to(device)).cpu().detach().numpy()\n                #     test_rmse += root_mean_squared_error(y_batch, y_pred)\n                #     print('\\n Test RMSE : ',test_rmse,'\\n Epoch : ', epoch,'\\n Target : ',y_batch,'\\n Prediction : ',y_pred,'\\n')\n                    \n            with torch.no_grad():\n                for X_val_batch, y_val_batch, y_val_target_batch in val_loader:\n                    #Set computing environment\n                    X_val_batch, y_val_batch, y_val_target_batch = X_val_batch.to(device), y_val_batch.to(device), y_val_target_batch.to(device)\n                    #Predict\n                    y_val_pred = model(X_val_batch)\n        \n                    #ADD A METRIC\n                    val_rmse_total += compute_rmse(y_val_pred, y_val_batch)\n                    #temporarily enable gradients just for pinns loss\n                    with torch.enable_grad():\n                        x_seq_val = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_val_batch.size(0), 1).unsqueeze(-1).to(X_val_batch.device)\n                        x_seq_val.requires_grad_(True) #due to PINN loss\n                        # = x_seq#torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_val_batch.size(0), 1).to(X_val_batch.device)\n                        val_loss, _, _ = criterion(y_val_pred, y_val_batch, y_val_target_batch, x_seq_val, alpha=0.3)\n        \n                    val_loss_total += val_loss.item()\n        \n            avg_val_loss = val_loss_total / len(val_loader)\n            avg_val_rmse = val_rmse_total / len(val_loader)\n            # Save model if validation improves\n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                best_epoch = epoch\n                torch.save(model.state_dict(), best_model_path)\n                print(f\"‚úÖ Saved best model at epoch {epoch+1} (Val Loss = {best_val_loss:.8f})\")\n            if epoch+1 == num_epochs:\n                torch.save(model.state_dict(),last_model_path)\n                print(f\"‚úÖ Saved last model at epoch {epoch+1} \")\n    \n            \n        \n            if mode == 0:\n                print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f}\")\n            if mode == 1:\n                print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Val RMSE: {avg_val_rmse:.8f} | ‚àö(Val Loss) = {torch.sqrt(torch.tensor(avg_val_loss)):.8f}\")\n            if mode == 2:\n                print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse:.8f} | ‚àö(Val Loss) = {torch.sqrt(torch.tensor(avg_val_loss)):.8f} | Current Learning Rate: {current_lr}\")\n                #print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f}\")\n            train_losses.append(avg_train_loss)\n            val_losses.append(avg_val_loss)\n            val_rmses.append(avg_val_rmse)  # assuming avg_val_rmse is a tensor\n            data_losses.append(avg_data_loss)   # assuming this is the last batch's data loss\n            phys_losses.append(avg_phys_loss)   # assuming this is the last batch's physics loss\n    \n            # Load best model for testing\n            model.load_state_dict(torch.load(best_model_path))\n            if epoch == 700 or epoch == 1800 or epoch == 2700:\n                #Reset LR \n                # Bump LR back up to 1e-3 to help it learn the new difficult task\n                set_learning_rate(optimizer, 1e-3)\n            if epoch%20 == 0:\n                model.eval()\n                test_rmse_total = 0 \n                with torch.no_grad():\n                    for X_batch, y_batch, y_target in test_loader:\n                        X_batch = X_batch.to(device)\n                        y_pred = model(X_batch).cpu().detach().numpy()\n                        # # Un-normalize to get real values\n                        # y_pred_real = scaler.inverse_transform(y_pred_norm.cpu().detach().numpy())\n                        # y_batch_real = scaler.inverse_transform(y_batch.cpu().detach().numpy())\n    \n                        # Accumulate\n                        test_rmse_total += root_mean_squared_error(y_batch, y_pred)\n                        print('\\n Epoch : ', epoch,'\\n Target : ',y_batch,'\\n Prediction : ',y_pred,'\\n')\n                \n                # FIX 2: Calculate Average\n                final_test_rmse = test_rmse_total / len(test_loader)\n                print('Final Test RMSE: ', final_test_rmse)\n            \n        model.load_state_dict(torch.load(best_model_path))\n        train_losses=to_cpu_numpy(train_losses)\n        val_losses=to_cpu_numpy(val_losses)\n        val_rmses=to_cpu_numpy(val_rmses)\n        data_losses=to_cpu_numpy(data_losses)\n        phys_losses=to_cpu_numpy(phys_losses)    \n        np.savez(f\"training_metrics__window_{WINDOW_SIZE}_model_{model_type}.npz\",\n                 train_losses=train_losses,\n                 val_losses=val_losses,\n                 val_rmses=val_rmses,\n                 data_losses=data_losses,\n                 phys_losses=phys_losses)\n        # # Save using the clean versions\n        # np.savez(f\"training_metrics__window_{WINDOW_SIZE}_model_{model_type}.npz\",\n        #          train_losses=to_cpu_numpy(train_losses),\n        #          val_losses=to_cpu_numpy(val_losses),\n        #          val_rmses=to_cpu_numpy(val_rmses),\n        #          data_losses=to_cpu_numpy(data_losses),\n        #          phys_losses=to_cpu_numpy(phys_losses))\n    \n        print(\"‚úÖ Metrics saved successfully!\")\n        print(\"Plot losses after training 3:\")\n        plt.figure(figsize=(12, 6))\n        plt.plot(train_losses[3:], label=\"Train Loss\")\n        plt.plot(val_losses[3:], label=\"Val Loss\")\n        plt.plot(val_rmses[3:], label=\"Val RMSE\")\n        plt.plot(data_losses[3:], label=\"Data Loss\")\n        plt.plot(phys_losses[3:], label=\"Physics Loss\")\n        x_line = best_epoch \n        plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n        plt.text(x_line + 0.05, best_val_loss, 'Saved Model', rotation=45, color='red')\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.title(f\"Training & Validation Metrics Window Size : {WINDOW_SIZE+1}\")\n        plt.grid(True)\n        plt.savefig(fname = f\"history-from-3-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n        plt.show()\n    \n        plt.figure(figsize=(12, 6))\n        plt.plot(train_losses, label=\"Train Loss\")\n        plt.plot(val_losses, label=\"Val Loss\")\n        plt.plot(val_rmses, label=\"Val RMSE\")\n        plt.plot(data_losses, label=\"Data Loss\")\n        plt.plot(phys_losses, label=\"Physics Loss\")\n        plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n        plt.text(x_line + 0.05, best_val_loss, 'Saved Model', rotation=45, color='red')\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.title(f\"Training & Validation Metrics Window Size {WINDOW_SIZE+1}\")\n        plt.grid(True)\n        plt.savefig(fname = f\"history-full-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n        plt.show()\n    \n        plt.figure(figsize=(12, 6))\n        plt.plot(train_losses, label=\"Train Loss\")\n        plt.plot(val_losses, label=\"Val Loss\")\n        plt.plot(val_rmses, label=\"Val RMSE\")\n        plt.plot(data_losses, label=\"Data Loss\")\n        plt.plot(phys_losses, label=\"Physics Loss\")\n        plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n        plt.text(x_line, best_val_loss*10, 'Saved Model', rotation=30, color='red')\n        plt.yscale('log')  # visualize on log scale\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.title(f\"Training & Validation Metrics Window Size {WINDOW_SIZE+1}\")\n        plt.grid(True)\n        plt.savefig(fname = f\"history-full-log-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n        plt.show()\n    \n        test_rmse = 0 \n        \n        for X_batch, y_batch, y_target in test_loader:\n            # Calculate RMSE directly\n            test_rmse += root_mean_squared_error(y_batch, model(X_batch.to(device)).cpu().detach().numpy())\n        print('Test RMSE : ',test_rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:06:21.804644Z","iopub.execute_input":"2026-02-03T21:06:21.804892Z","iopub.status.idle":"2026-02-03T21:06:21.866258Z","shell.execute_reply.started":"2026-02-03T21:06:21.804872Z","shell.execute_reply":"2026-02-03T21:06:21.865319Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 1. Setup the Cycle Life Dictionaries (from previous step)\nid_cycle_length = {\n    '6-6': 2468, '7-8': 1938, '5-3': 2689, '6-8': 2450, '5-5': 1583, '2-4': 1499, '10-3': 1848, '1-4': 1500, '6-3': 1804, '5-6': 2460, '8-3': 2290, '3-6': 2491, '10-4': 1811, '9-6': 1742, '3-2': 2283, '9-2': 2143, '9-7': 2012, '6-5': 2178, '4-8': 1706, '5-7': 1448, '1-5': 1971, '1-7': 1678, '1-8': 2285, '8-7': 2047, '6-2': 1908, '3-3': 1649, '6-1': 1609, '2-7': 2202, '1-2': 2678, '10-7': 1783, '10-1': 1702, '9-1': 2057, '8-5': 1348, '9-4': 1975, '4-4': 1491, '3-4': 1766, '10-6': 2285, '5-2': 1926, '7-5': 1875, '4-3': 1142, '5-4': 1962, '3-5': 2657, '7-2': 2030, '3-7': 2479, '1-3': 1858, '9-5': 2168, '3-1': 1938, '4-1': 2217, '9-3': 1905, '7-1': 1690, '8-8': 1679, '6-4': 1717, '4-2': 1782, '5-1': 2507, '2-3': 1751, '4-5': 1561, '4-6': 1380, '2-8': 1481, '8-1': 1308, '10-8': 1400, '9-8': 2308, '3-8': 2342, '7-3': 1295, '8-2': 2041, '8-6': 2365, '7-6': 1419, '10-5': 2030, '7-7': 1685, '7-4': 1393, '4-7': 2216, '2-6': 1572, '10-2': 1697, '1-6': 1143, '8-4': 1885, '2-5': 1386, '1-1': 1504, '2-2': 2651\n}\n\nlow_ids = {k for k, v in id_cycle_length.items() if 1142 <= v <= 1658}\nmid_ids = {k for k, v in id_cycle_length.items() if 1659 <= v <= 2173}\nhigh_ids = {k for k, v in id_cycle_length.items() if 2174 <= v <= 2689}\n\n# 2. Setup Base Paths (Your Original Code)\nmain_files_path = '/kaggle/input/generate-hust-data-gompertz-k-a-b/'\n\n# Using dictionary keys as the source of truth for all files since we can't ls directory here\nall_ids = list(id_cycle_length.keys()) \n\ntrain_ids = [\n    '1-3', '1-4', '1-5', '1-6', '1-7', '1-8', '2-2', '2-3',\n    '2-4', '2-6', '2-7', '2-8', '3-2', '3-3', '3-4', '3-5',\n    '3-6', '3-7', '3-8', '4-1', '4-2', '4-3', '4-4', '4-6',\n    '4-7', '4-8', '5-1', '5-2', '5-4', '5-5', '5-6', '5-7',\n    '6-3', '6-4', '6-5', '7-1', '7-2', '7-3', '7-4', '7-7',\n    '7-8', '8-2', '8-3', '8-4', '8-7', '9-1', '9-2', '9-3',\n    '9-5', '9-7', '9-8', '10-2', '10-3', '10-5', '10-8'\n]\n\ntest_ids_pool = [f for f in all_ids if f not in train_ids]\n\n# Generate Full Paths\ntrain_paths_all = [os.path.join(main_files_path, f\"{fid}-hust_gompertz_params.csv\") for fid in train_ids]\n\n# Split Test Pool into Val and Test\nmid_point = int(len(test_ids_pool) * 0.5)\nval_ids = test_ids_pool[:mid_point]\ntest_ids_final = test_ids_pool[mid_point:]\n\nval_paths_all = [os.path.join(main_files_path, f\"{fid}-hust_gompertz_params.csv\") for fid in val_ids]\ntest_paths_all = [os.path.join(main_files_path, f\"{fid}-hust_gompertz_params.csv\") for fid in test_ids_final]\n\n# 3. Helper Function to Filter Paths\ndef filter_paths(paths, allowed_ids):\n    \"\"\"Returns paths that contain an ID from the allowed set.\"\"\"\n    return [p for p in paths if any(f\"/{allowed_id}-\" in p for allowed_id in allowed_ids)]\n\n# 4. Create the 4 Splits\n# Dictionary to hold all datasets for easy access\ndatasets = {\n    \"all\":  (train_paths_all, val_paths_all, test_paths_all),\n    \"low\":  (filter_paths(train_paths_all, low_ids), filter_paths(val_paths_all, low_ids), filter_paths(test_paths_all, low_ids)),\n    \"mid\":  (filter_paths(train_paths_all, mid_ids), filter_paths(val_paths_all, mid_ids), filter_paths(test_paths_all, mid_ids)),\n    \"high\": (filter_paths(train_paths_all, high_ids), filter_paths(val_paths_all, high_ids), filter_paths(test_paths_all, high_ids)),\n}\n\n# 5. Verify Counts\nprint(f\"{'Dataset':<10} | {'Train':<5} | {'Val':<5} | {'Test':<5}\")\nprint(\"-\" * 35)\nfor name, (tr, va, te) in datasets.items():\n    print(f\"{name.upper():<10} | {len(tr):<5} | {len(va):<5} | {len(te):<5}\")\n\n# Example Usage: Accessing the 'Mid' dataset\n# train_mid, val_mid, test_mid = datasets['mid']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:06:21.867482Z","iopub.execute_input":"2026-02-03T21:06:21.867822Z","iopub.status.idle":"2026-02-03T21:06:21.889979Z","shell.execute_reply.started":"2026-02-03T21:06:21.867787Z","shell.execute_reply":"2026-02-03T21:06:21.889161Z"}},"outputs":[{"name":"stdout","text":"Dataset    | Train | Val   | Test \n-----------------------------------\nALL        | 55    | 11    | 11   \nLOW        | 14    | 2     | 5    \nMID        | 26    | 5     | 4    \nHIGH       | 15    | 4     | 2    \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"data_ranges = ['all','low','mid','high']\nfor data_range in data_ranges:\n    train_model(WINDOW_SIZES,*datasets[data_range],data_range)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:06:21.891127Z","iopub.execute_input":"2026-02-03T21:06:21.891465Z","iopub.status.idle":"2026-02-03T21:07:26.821045Z","shell.execute_reply.started":"2026-02-03T21:06:21.891428Z","shell.execute_reply":"2026-02-03T21:07:26.820449Z"}},"outputs":[{"name":"stdout","text":"Cutoff SoH :  0.7\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-8-hust_gompertz_params.csv\n X_['train'] shape : torch.Size([55, 100, 1]) , y_['train'] shape : torch.Size([55, 3]) Ôºåy_2['train'] shape: torch.Size([55, 1])\nload : \n['train']loader lengths :  2\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-5-hust_gompertz_params.csv\n X_['val'] shape : torch.Size([11, 100, 1]) , y_['val'] shape : torch.Size([11, 3]) Ôºåy_2['val'] shape: torch.Size([11, 1])\nload : \n['val']loader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\n X_['test'] shape : torch.Size([11, 100, 1]) , y_['test'] shape : torch.Size([11, 3]) Ôºåy_2['test'] shape: torch.Size([11, 1])\nload : \n['test']loader lengths :  1\n## üß† Model\nLast model window :  last_model_window_100_model_pinn_data_all.pth\nüöÄ Initializing model output to: k=0.9770737886428833, a=-3.6902196407318115, b=-14.176105499267578\n‚úÖ Model Output Parameters Initialized!\n##\n        ### üìà Gompertz Function (Physics Law)\n        \n        * `x`: Time (or cycle number)\n        \n        * `k`: Max value (e.g., max capacity)\n        \n        * `a`, `b`: Shape parameters\n## üß† Loss Functions\n\n## ‚öôÔ∏è 1. Data-Informed Loss Function\n        a data loss (what the LSTM learns from data)\n        \n        * Mean Squared Error for Training\n        * RMSE for autoregressive approximation of compound error\n        \n        ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n        You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n        \n        * `alpha`: controls how strongly physics is enforced.\n## üõ†Ô∏è Parameter Strategy\n## üîÅ Training Loop\n‚úÖ Saved best model at epoch 1 (Val Loss = 3.04396296)\nEpoch 1/1000 | Train Loss=62947.09179688 | Val Loss=3.04396296 | Data=629.32955933 | Physics=12.80792015 | Val RMSE: 2.71330595 | ‚àö(Val Loss) = 1.74469566 | Current Learning Rate: 0.002\n\n Epoch :  0 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.99716794  -3.713916   -14.187192  ]\n [  0.9972256   -3.7139897  -14.187211  ]\n [  0.99710023  -3.7138295  -14.187169  ]\n [  0.99714154  -3.7138822  -14.187182  ]\n [  0.99715173  -3.7138953  -14.187186  ]\n [  0.99704456  -3.7137582  -14.18715   ]\n [  0.99695194  -3.71364    -14.187119  ]\n [  0.99717003  -3.7139187  -14.187192  ]\n [  0.9969474   -3.713634   -14.187117  ]\n [  0.99704194  -3.713755   -14.187149  ]\n [  0.99693686  -3.7136204  -14.187113  ]] \n\nFinal Test RMSE:  1.6680244207382202\nEpoch 2/1000 | Train Loss=60566.79687500 | Val Loss=3.63496232 | Data=605.52761841 | Physics=14.77000398 | Val RMSE: 2.71368480 | ‚àö(Val Loss) = 1.90655768 | Current Learning Rate: 0.002\nEpoch 3/1000 | Train Loss=61502.81054688 | Val Loss=3.49160767 | Data=614.88650513 | Physics=14.94326439 | Val RMSE: 2.71283770 | ‚àö(Val Loss) = 1.86858439 | Current Learning Rate: 0.002\nEpoch 4/1000 | Train Loss=62967.58007812 | Val Loss=3.38284326 | Data=629.53308105 | Physics=15.23795114 | Val RMSE: 2.71296430 | ‚àö(Val Loss) = 1.83925068 | Current Learning Rate: 0.002\nEpoch 5/1000 | Train Loss=61172.18750000 | Val Loss=3.55832005 | Data=611.58439636 | Physics=12.22478364 | Val RMSE: 2.71295285 | ‚àö(Val Loss) = 1.88635099 | Current Learning Rate: 0.002\nEpoch 6/1000 | Train Loss=62187.06054688 | Val Loss=3.60367203 | Data=621.72772217 | Physics=15.99069464 | Val RMSE: 2.71294951 | ‚àö(Val Loss) = 1.89833403 | Current Learning Rate: 0.002\nEpoch 7/1000 | Train Loss=56941.57617188 | Val Loss=3.55139565 | Data=569.28510284 | Physics=7.45016203 | Val RMSE: 2.71336555 | ‚àö(Val Loss) = 1.88451469 | Current Learning Rate: 0.002\nEpoch 8/1000 | Train Loss=61283.47460938 | Val Loss=3.48483133 | Data=612.69653320 | Physics=12.93637437 | Val RMSE: 2.71333861 | ‚àö(Val Loss) = 1.86677027 | Current Learning Rate: 0.002\nEpoch 9/1000 | Train Loss=67318.18750000 | Val Loss=3.53044987 | Data=673.03118896 | Physics=21.51053263 | Val RMSE: 2.71273041 | ‚àö(Val Loss) = 1.87894917 | Current Learning Rate: 0.002\nEpoch 10/1000 | Train Loss=66263.29296875 | Val Loss=3.38758636 | Data=662.48776245 | Physics=17.89341385 | Val RMSE: 2.71248937 | ‚àö(Val Loss) = 1.84053969 | Current Learning Rate: 0.002\nEpoch 11/1000 | Train Loss=66466.21093750 | Val Loss=3.36577916 | Data=664.52037048 | Physics=14.77953763 | Val RMSE: 2.71264911 | ‚àö(Val Loss) = 1.83460605 | Current Learning Rate: 0.002\nEpoch 12/1000 | Train Loss=61372.93554688 | Val Loss=3.34576011 | Data=613.59336853 | Physics=11.13744725 | Val RMSE: 2.71302676 | ‚àö(Val Loss) = 1.82914186 | Current Learning Rate: 0.002\nEpoch 13/1000 | Train Loss=61426.66796875 | Val Loss=3.27125621 | Data=614.12664795 | Physics=14.08560474 | Val RMSE: 2.71276426 | ‚àö(Val Loss) = 1.80866146 | Current Learning Rate: 0.002\nEpoch 14/1000 | Train Loss=63007.48632812 | Val Loss=3.24016261 | Data=629.93539429 | Physics=13.50596927 | Val RMSE: 2.71285748 | ‚àö(Val Loss) = 1.80004513 | Current Learning Rate: 0.002\nEpoch 15/1000 | Train Loss=61893.34375000 | Val Loss=3.23548675 | Data=618.79122925 | Physics=15.65957854 | Val RMSE: 2.71321130 | ‚àö(Val Loss) = 1.79874587 | Current Learning Rate: 0.002\nEpoch 16/1000 | Train Loss=61878.78125000 | Val Loss=3.18370295 | Data=618.65029907 | Physics=11.90958151 | Val RMSE: 2.71293330 | ‚àö(Val Loss) = 1.78429341 | Current Learning Rate: 0.002\nEpoch 17/1000 | Train Loss=63544.85156250 | Val Loss=3.20396924 | Data=635.30676270 | Physics=15.07033678 | Val RMSE: 2.71295905 | ‚àö(Val Loss) = 1.78996348 | Current Learning Rate: 0.002\nEpoch 18/1000 | Train Loss=64363.10546875 | Val Loss=3.18812275 | Data=643.49331665 | Physics=12.36310287 | Val RMSE: 2.71323371 | ‚àö(Val Loss) = 1.78553152 | Current Learning Rate: 0.002\nEpoch 19/1000 | Train Loss=64473.30664062 | Val Loss=3.14890122 | Data=644.59136963 | Physics=14.91959960 | Val RMSE: 2.71294928 | ‚àö(Val Loss) = 1.77451432 | Current Learning Rate: 0.002\nEpoch 20/1000 | Train Loss=64523.18359375 | Val Loss=3.18439269 | Data=645.09481812 | Physics=11.85254750 | Val RMSE: 2.71326351 | ‚àö(Val Loss) = 1.78448665 | Current Learning Rate: 0.002\nEpoch 21/1000 | Train Loss=64866.15039062 | Val Loss=3.16702509 | Data=648.51660156 | Physics=17.61871046 | Val RMSE: 2.71325421 | ‚àö(Val Loss) = 1.77961373 | Current Learning Rate: 0.002\n\n Epoch :  20 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.99716794  -3.713916   -14.187192  ]\n [  0.9972256   -3.7139897  -14.187211  ]\n [  0.99710023  -3.7138295  -14.187169  ]\n [  0.99714154  -3.7138822  -14.187182  ]\n [  0.99715173  -3.7138953  -14.187186  ]\n [  0.99704456  -3.7137582  -14.18715   ]\n [  0.99695194  -3.71364    -14.187119  ]\n [  0.99717003  -3.7139187  -14.187192  ]\n [  0.9969474   -3.713634   -14.187117  ]\n [  0.99704194  -3.713755   -14.187149  ]\n [  0.99693686  -3.7136204  -14.187113  ]] \n\nFinal Test RMSE:  1.6680244207382202\nEpoch 22/1000 | Train Loss=65061.35156250 | Val Loss=3.16443729 | Data=650.47100830 | Physics=15.80537239 | Val RMSE: 2.71323466 | ‚àö(Val Loss) = 1.77888656 | Current Learning Rate: 0.002\nEpoch 23/1000 | Train Loss=62742.26953125 | Val Loss=3.17128658 | Data=627.28005981 | Physics=15.91969556 | Val RMSE: 2.71323419 | ‚àö(Val Loss) = 1.78081059 | Current Learning Rate: 0.002\nEpoch 24/1000 | Train Loss=62313.84375000 | Val Loss=3.13467288 | Data=622.99685669 | Physics=14.99178957 | Val RMSE: 2.71295238 | ‚àö(Val Loss) = 1.77050078 | Current Learning Rate: 0.002\nEpoch 25/1000 | Train Loss=62604.33203125 | Val Loss=3.11501765 | Data=625.90557861 | Physics=12.31586246 | Val RMSE: 2.71300220 | ‚àö(Val Loss) = 1.76494122 | Current Learning Rate: 0.002\nEpoch 26/1000 | Train Loss=61463.59375000 | Val Loss=3.12319469 | Data=614.49682617 | Physics=13.42872135 | Val RMSE: 2.71333432 | ‚àö(Val Loss) = 1.76725626 | Current Learning Rate: 0.002\nEpoch 27/1000 | Train Loss=63087.06445312 | Val Loss=3.21536684 | Data=630.72882080 | Physics=15.18559521 | Val RMSE: 2.71329808 | ‚àö(Val Loss) = 1.79314435 | Current Learning Rate: 0.002\nEpoch 28/1000 | Train Loss=61777.62109375 | Val Loss=3.15266752 | Data=617.63415527 | Physics=15.31414689 | Val RMSE: 2.71298623 | ‚àö(Val Loss) = 1.77557528 | Current Learning Rate: 0.002\nEpoch 29/1000 | Train Loss=68090.48632812 | Val Loss=3.12926388 | Data=680.76393127 | Physics=14.58570449 | Val RMSE: 2.71302962 | ‚àö(Val Loss) = 1.76897252 | Current Learning Rate: 0.002\nEpoch 30/1000 | Train Loss=61115.08398438 | Val Loss=3.15155435 | Data=611.01264954 | Physics=12.70410924 | Val RMSE: 2.71335268 | ‚àö(Val Loss) = 1.77526176 | Current Learning Rate: 0.002\nEpoch 31/1000 | Train Loss=64750.49218750 | Val Loss=3.12362027 | Data=647.36584473 | Physics=13.24863345 | Val RMSE: 2.71303892 | ‚àö(Val Loss) = 1.76737666 | Current Learning Rate: 0.002\nEpoch 32/1000 | Train Loss=60984.01953125 | Val Loss=3.11006236 | Data=609.69920349 | Physics=14.62781161 | Val RMSE: 2.71306849 | ‚àö(Val Loss) = 1.76353693 | Current Learning Rate: 0.002\nEpoch 33/1000 | Train Loss=63303.11328125 | Val Loss=3.09913421 | Data=632.89355469 | Physics=12.18064280 | Val RMSE: 2.71309757 | ‚àö(Val Loss) = 1.76043582 | Current Learning Rate: 0.002\nEpoch 34/1000 | Train Loss=60911.84570312 | Val Loss=3.11525393 | Data=608.98135376 | Physics=11.91562248 | Val RMSE: 2.71341610 | ‚àö(Val Loss) = 1.76500821 | Current Learning Rate: 0.002\nEpoch 35/1000 | Train Loss=62068.54492188 | Val Loss=3.40409684 | Data=620.54925537 | Physics=10.94103350 | Val RMSE: 2.71330428 | ‚àö(Val Loss) = 1.84501946 | Current Learning Rate: 0.002\nEpoch 36/1000 | Train Loss=60074.23437500 | Val Loss=3.26687264 | Data=600.60766602 | Physics=10.44569908 | Val RMSE: 2.71325541 | ‚àö(Val Loss) = 1.80744922 | Current Learning Rate: 0.002\nEpoch 37/1000 | Train Loss=59457.33593750 | Val Loss=3.25413203 | Data=594.44216919 | Physics=7.45081518 | Val RMSE: 2.71295881 | ‚àö(Val Loss) = 1.80392134 | Current Learning Rate: 0.002\nEpoch 38/1000 | Train Loss=65619.91992188 | Val Loss=3.17605710 | Data=656.05960083 | Physics=13.64798564 | Val RMSE: 2.71298790 | ‚àö(Val Loss) = 1.78214955 | Current Learning Rate: 0.002\nEpoch 39/1000 | Train Loss=63241.33984375 | Val Loss=3.17633772 | Data=632.27435303 | Physics=13.43733337 | Val RMSE: 2.71332073 | ‚àö(Val Loss) = 1.78222835 | Current Learning Rate: 0.002\nEpoch 40/1000 | Train Loss=65663.71289062 | Val Loss=3.15827537 | Data=656.49812317 | Physics=13.36843608 | Val RMSE: 2.71329641 | ‚àö(Val Loss) = 1.77715373 | Current Learning Rate: 0.002\nEpoch 41/1000 | Train Loss=66145.49609375 | Val Loss=3.13297367 | Data=661.31381226 | Physics=14.65812392 | Val RMSE: 2.71300268 | ‚àö(Val Loss) = 1.77002084 | Current Learning Rate: 0.002\n\n Epoch :  40 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.99716794  -3.713916   -14.187192  ]\n [  0.9972256   -3.7139897  -14.187211  ]\n [  0.99710023  -3.7138295  -14.187169  ]\n [  0.99714154  -3.7138822  -14.187182  ]\n [  0.99715173  -3.7138953  -14.187186  ]\n [  0.99704456  -3.7137582  -14.18715   ]\n [  0.99695194  -3.71364    -14.187119  ]\n [  0.99717003  -3.7139187  -14.187192  ]\n [  0.9969474   -3.713634   -14.187117  ]\n [  0.99704194  -3.713755   -14.187149  ]\n [  0.99693686  -3.7136204  -14.187113  ]] \n\nFinal Test RMSE:  1.6680244207382202\nEpoch 42/1000 | Train Loss=62341.60351562 | Val Loss=3.17563868 | Data=623.27847290 | Physics=12.22415454 | Val RMSE: 2.71301293 | ‚àö(Val Loss) = 1.78203213 | Current Learning Rate: 0.002\nEpoch 43/1000 | Train Loss=64098.96484375 | Val Loss=3.17731166 | Data=640.84768677 | Physics=15.26669267 | Val RMSE: 2.71304250 | ‚àö(Val Loss) = 1.78250146 | Current Learning Rate: 0.002\nEpoch 44/1000 | Train Loss=60548.18554688 | Val Loss=3.19173431 | Data=605.34754944 | Physics=9.93219512 | Val RMSE: 2.71336174 | ‚àö(Val Loss) = 1.78654253 | Current Learning Rate: 0.002\nEpoch 45/1000 | Train Loss=60491.01562500 | Val Loss=3.17040300 | Data=604.77621460 | Physics=9.48002663 | Val RMSE: 2.71304607 | ‚àö(Val Loss) = 1.78056252 | Current Learning Rate: 0.002\nEpoch 46/1000 | Train Loss=60651.35546875 | Val Loss=3.15338612 | Data=606.37409973 | Physics=13.65062378 | Val RMSE: 2.71335578 | ‚àö(Val Loss) = 1.77577758 | Current Learning Rate: 0.002\nEpoch 47/1000 | Train Loss=66340.49414062 | Val Loss=3.12626338 | Data=663.26756287 | Physics=11.88256563 | Val RMSE: 2.71304774 | ‚àö(Val Loss) = 1.76812422 | Current Learning Rate: 0.002\nEpoch 48/1000 | Train Loss=65661.75195312 | Val Loss=3.10946012 | Data=656.47485352 | Physics=15.81664481 | Val RMSE: 2.71307826 | ‚àö(Val Loss) = 1.76336610 | Current Learning Rate: 0.002\nEpoch 49/1000 | Train Loss=63252.22460938 | Val Loss=3.09972334 | Data=632.38574219 | Physics=11.37771033 | Val RMSE: 2.71310544 | ‚àö(Val Loss) = 1.76060307 | Current Learning Rate: 0.002\nEpoch 50/1000 | Train Loss=58830.13867188 | Val Loss=3.12119627 | Data=588.15992737 | Physics=15.05620117 | Val RMSE: 2.71341014 | ‚àö(Val Loss) = 1.76669073 | Current Learning Rate: 0.002\nEpoch 51/1000 | Train Loss=60182.11328125 | Val Loss=3.11852074 | Data=601.68275452 | Physics=12.92014151 | Val RMSE: 2.71338320 | ‚àö(Val Loss) = 1.76593339 | Current Learning Rate: 0.002\nEpoch 52/1000 | Train Loss=63009.02148438 | Val Loss=3.09136534 | Data=629.95086670 | Physics=13.39409415 | Val RMSE: 2.71304560 | ‚àö(Val Loss) = 1.75822794 | Current Learning Rate: 0.002\nEpoch 53/1000 | Train Loss=64321.62304688 | Val Loss=3.10455465 | Data=643.07757568 | Physics=13.00449182 | Val RMSE: 2.71335101 | ‚àö(Val Loss) = 1.76197469 | Current Learning Rate: 0.002\nEpoch 54/1000 | Train Loss=63953.30859375 | Val Loss=3.11445856 | Data=639.39077759 | Physics=15.51290242 | Val RMSE: 2.71295714 | ‚àö(Val Loss) = 1.76478291 | Current Learning Rate: 0.002\nEpoch 55/1000 | Train Loss=62273.48828125 | Val Loss=3.07472324 | Data=622.59191895 | Physics=16.07943327 | Val RMSE: 2.71288276 | ‚àö(Val Loss) = 1.75348890 | Current Learning Rate: 0.002\nEpoch 56/1000 | Train Loss=59685.05664062 | Val Loss=3.07159090 | Data=596.71453857 | Physics=11.20728915 | Val RMSE: 2.71292639 | ‚àö(Val Loss) = 1.75259542 | Current Learning Rate: 0.002\nEpoch 57/1000 | Train Loss=60762.67773438 | Val Loss=3.07954121 | Data=607.48582458 | Physics=14.56839365 | Val RMSE: 2.71325755 | ‚àö(Val Loss) = 1.75486219 | Current Learning Rate: 0.002\nEpoch 58/1000 | Train Loss=61529.57617188 | Val Loss=3.07448864 | Data=615.15740967 | Physics=12.76258642 | Val RMSE: 2.71290684 | ‚àö(Val Loss) = 1.75342202 | Current Learning Rate: 0.002\nEpoch 59/1000 | Train Loss=65287.93164062 | Val Loss=3.23401713 | Data=652.73251343 | Physics=18.23678000 | Val RMSE: 2.71273994 | ‚àö(Val Loss) = 1.79833734 | Current Learning Rate: 0.002\nEpoch 60/1000 | Train Loss=63573.53710938 | Val Loss=3.68055916 | Data=635.59603882 | Physics=13.90477810 | Val RMSE: 2.71313405 | ‚àö(Val Loss) = 1.91847837 | Current Learning Rate: 0.002\nEpoch 61/1000 | Train Loss=62136.23046875 | Val Loss=3.52343678 | Data=621.22097778 | Physics=14.92231460 | Val RMSE: 2.71281505 | ‚àö(Val Loss) = 1.87708199 | Current Learning Rate: 0.002\n\n Epoch :  60 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.99716794  -3.713916   -14.187192  ]\n [  0.9972256   -3.7139897  -14.187211  ]\n [  0.99710023  -3.7138295  -14.187169  ]\n [  0.99714154  -3.7138822  -14.187182  ]\n [  0.99715173  -3.7138953  -14.187186  ]\n [  0.99704456  -3.7137582  -14.18715   ]\n [  0.99695194  -3.71364    -14.187119  ]\n [  0.99717003  -3.7139187  -14.187192  ]\n [  0.9969474   -3.713634   -14.187117  ]\n [  0.99704194  -3.713755   -14.187149  ]\n [  0.99693686  -3.7136204  -14.187113  ]] \n\nFinal Test RMSE:  1.6680244207382202\nEpoch 62/1000 | Train Loss=65732.89648438 | Val Loss=3.45859194 | Data=657.18402100 | Physics=17.78938428 | Val RMSE: 2.71315694 | ‚àö(Val Loss) = 1.85972905 | Current Learning Rate: 0.002\nEpoch 63/1000 | Train Loss=66234.07421875 | Val Loss=3.34653568 | Data=662.19966125 | Physics=14.72922417 | Val RMSE: 2.71286178 | ‚àö(Val Loss) = 1.82935393 | Current Learning Rate: 0.002\nEpoch 64/1000 | Train Loss=64699.95117188 | Val Loss=3.28234434 | Data=646.86013794 | Physics=13.41042174 | Val RMSE: 2.71292281 | ‚àö(Val Loss) = 1.81172419 | Current Learning Rate: 0.002\nEpoch 65/1000 | Train Loss=64433.33593750 | Val Loss=3.22759318 | Data=644.19097900 | Physics=15.40317221 | Val RMSE: 2.71297359 | ‚àö(Val Loss) = 1.79655039 | Current Learning Rate: 0.002\nEpoch 66/1000 | Train Loss=64748.78125000 | Val Loss=3.21735907 | Data=647.34802246 | Physics=13.90936415 | Val RMSE: 2.71330643 | ‚àö(Val Loss) = 1.79369986 | Current Learning Rate: 0.002\nEpoch 67/1000 | Train Loss=60800.45312500 | Val Loss=3.16199589 | Data=607.86926270 | Physics=10.49496736 | Val RMSE: 2.71298313 | ‚àö(Val Loss) = 1.77820015 | Current Learning Rate: 0.002\nEpoch 68/1000 | Train Loss=60661.03515625 | Val Loss=3.16888332 | Data=606.46925354 | Physics=14.74249652 | Val RMSE: 2.71331072 | ‚àö(Val Loss) = 1.78013575 | Current Learning Rate: 0.002\nEpoch 69/1000 | Train Loss=66757.87500000 | Val Loss=3.15085697 | Data=667.43377686 | Physics=17.52872278 | Val RMSE: 2.71327162 | ‚àö(Val Loss) = 1.77506530 | Current Learning Rate: 0.002\nEpoch 70/1000 | Train Loss=62327.81445312 | Val Loss=3.11823964 | Data=623.14266968 | Physics=10.68113888 | Val RMSE: 2.71294951 | ‚àö(Val Loss) = 1.76585376 | Current Learning Rate: 0.002\nEpoch 71/1000 | Train Loss=64733.25000000 | Val Loss=3.10298038 | Data=647.19650269 | Physics=11.07101796 | Val RMSE: 2.71299338 | ‚àö(Val Loss) = 1.76152790 | Current Learning Rate: 0.002\nEpoch 72/1000 | Train Loss=64502.04492188 | Val Loss=3.08682442 | Data=644.88479614 | Physics=10.81772211 | Val RMSE: 2.71302748 | ‚àö(Val Loss) = 1.75693607 | Current Learning Rate: 0.002\nEpoch 73/1000 | Train Loss=60042.60351562 | Val Loss=3.08655787 | Data=600.29362488 | Physics=8.51893900 | Val RMSE: 2.71302247 | ‚àö(Val Loss) = 1.75686026 | Current Learning Rate: 0.002\nEpoch 74/1000 | Train Loss=61928.32031250 | Val Loss=3.15484977 | Data=619.14828491 | Physics=10.31304539 | Val RMSE: 2.71293950 | ‚àö(Val Loss) = 1.77618968 | Current Learning Rate: 0.002\nEpoch 75/1000 | Train Loss=68876.25000000 | Val Loss=3.16360211 | Data=688.61698914 | Physics=17.80034854 | Val RMSE: 2.71298814 | ‚àö(Val Loss) = 1.77865171 | Current Learning Rate: 0.002\nEpoch 76/1000 | Train Loss=66066.95898438 | Val Loss=3.28253436 | Data=660.53363037 | Physics=11.04127383 | Val RMSE: 2.71321034 | ‚àö(Val Loss) = 1.81177652 | Current Learning Rate: 0.002\nEpoch 77/1000 | Train Loss=62060.73437500 | Val Loss=3.24820089 | Data=620.46969604 | Physics=12.29648204 | Val RMSE: 2.71309829 | ‚àö(Val Loss) = 1.80227661 | Current Learning Rate: 0.002\nEpoch 78/1000 | Train Loss=60286.12695312 | Val Loss=3.25731516 | Data=602.72492981 | Physics=11.20773670 | Val RMSE: 2.71310401 | ‚àö(Val Loss) = 1.80480337 | Current Learning Rate: 0.002\nEpoch 79/1000 | Train Loss=66302.11718750 | Val Loss=3.40889192 | Data=662.87359619 | Physics=19.38253150 | Val RMSE: 2.71297526 | ‚àö(Val Loss) = 1.84631848 | Current Learning Rate: 0.002\nEpoch 80/1000 | Train Loss=60948.31054688 | Val Loss=3.34588337 | Data=609.34777832 | Physics=10.73021354 | Val RMSE: 2.71291757 | ‚àö(Val Loss) = 1.82917559 | Current Learning Rate: 0.002\nEpoch 81/1000 | Train Loss=64811.29687500 | Val Loss=3.25454426 | Data=647.96887207 | Physics=16.63135132 | Val RMSE: 2.71270037 | ‚àö(Val Loss) = 1.80403554 | Current Learning Rate: 0.002\n\n Epoch :  80 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.99716794  -3.713916   -14.187192  ]\n [  0.9972256   -3.7139897  -14.187211  ]\n [  0.99710023  -3.7138295  -14.187169  ]\n [  0.99714154  -3.7138822  -14.187182  ]\n [  0.99715173  -3.7138953  -14.187186  ]\n [  0.99704456  -3.7137582  -14.18715   ]\n [  0.99695194  -3.71364    -14.187119  ]\n [  0.99717003  -3.7139187  -14.187192  ]\n [  0.9969474   -3.713634   -14.187117  ]\n [  0.99704194  -3.713755   -14.187149  ]\n [  0.99693686  -3.7136204  -14.187113  ]] \n\nFinal Test RMSE:  1.6680244207382202\nEpoch 82/1000 | Train Loss=64412.41015625 | Val Loss=3.24417949 | Data=643.97994995 | Physics=16.96015991 | Val RMSE: 2.71306419 | ‚àö(Val Loss) = 1.80116057 | Current Learning Rate: 0.002\nEpoch 83/1000 | Train Loss=58725.67480469 | Val Loss=3.30763698 | Data=587.12016296 | Physics=11.44349988 | Val RMSE: 2.71299958 | ‚àö(Val Loss) = 1.81869102 | Current Learning Rate: 0.002\nEpoch 84/1000 | Train Loss=60777.76757812 | Val Loss=3.26294017 | Data=607.63681030 | Physics=14.58532891 | Val RMSE: 2.71291137 | ‚àö(Val Loss) = 1.80636108 | Current Learning Rate: 0.002\nEpoch 85/1000 | Train Loss=65586.44140625 | Val Loss=3.25593281 | Data=655.72477722 | Physics=13.74300777 | Val RMSE: 2.71295428 | ‚àö(Val Loss) = 1.80442035 | Current Learning Rate: 0.002\nEpoch 86/1000 | Train Loss=62384.07617188 | Val Loss=3.28017831 | Data=623.70663452 | Physics=9.74920505 | Val RMSE: 2.71293402 | ‚àö(Val Loss) = 1.81112623 | Current Learning Rate: 0.002\nEpoch 87/1000 | Train Loss=67673.55468750 | Val Loss=3.19307184 | Data=676.58897400 | Physics=18.57342197 | Val RMSE: 2.71269464 | ‚àö(Val Loss) = 1.78691685 | Current Learning Rate: 0.002\nEpoch 88/1000 | Train Loss=62562.23632812 | Val Loss=3.18087053 | Data=625.48272705 | Physics=13.61954697 | Val RMSE: 2.71278429 | ‚àö(Val Loss) = 1.78349948 | Current Learning Rate: 0.002\nEpoch 89/1000 | Train Loss=67453.06054688 | Val Loss=3.14512539 | Data=674.38813782 | Physics=15.64660658 | Val RMSE: 2.71285009 | ‚àö(Val Loss) = 1.77345014 | Current Learning Rate: 0.002\nEpoch 90/1000 | Train Loss=58466.43457031 | Val Loss=3.14325070 | Data=584.52703857 | Physics=12.06306741 | Val RMSE: 2.71319628 | ‚àö(Val Loss) = 1.77292156 | Current Learning Rate: 0.002\nEpoch 91/1000 | Train Loss=64757.90234375 | Val Loss=3.13369799 | Data=647.43325806 | Physics=18.05867279 | Val RMSE: 2.71319985 | ‚àö(Val Loss) = 1.77022541 | Current Learning Rate: 0.002\nEpoch 92/1000 | Train Loss=69819.78515625 | Val Loss=3.10009122 | Data=698.04757690 | Physics=21.25362160 | Val RMSE: 2.71290636 | ‚àö(Val Loss) = 1.76070762 | Current Learning Rate: 0.002\nEpoch 93/1000 | Train Loss=62357.46093750 | Val Loss=3.09366107 | Data=623.43585205 | Physics=13.03825404 | Val RMSE: 2.71295595 | ‚àö(Val Loss) = 1.75888062 | Current Learning Rate: 0.002\nEpoch 94/1000 | Train Loss=65646.00195312 | Val Loss=3.09647536 | Data=656.31954956 | Physics=14.26783557 | Val RMSE: 2.71295953 | ‚àö(Val Loss) = 1.75968051 | Current Learning Rate: 0.002\nEpoch 95/1000 | Train Loss=60978.80273438 | Val Loss=3.09633732 | Data=609.65046692 | Physics=12.20028754 | Val RMSE: 2.71299124 | ‚àö(Val Loss) = 1.75964129 | Current Learning Rate: 0.002\nEpoch 96/1000 | Train Loss=64453.87109375 | Val Loss=3.09496737 | Data=644.39544678 | Physics=16.25105976 | Val RMSE: 2.71331716 | ‚àö(Val Loss) = 1.75925195 | Current Learning Rate: 0.002\nEpoch 97/1000 | Train Loss=63031.61523438 | Val Loss=3.10348868 | Data=630.17715454 | Physics=13.17428139 | Val RMSE: 2.71298313 | ‚àö(Val Loss) = 1.76167214 | Current Learning Rate: 0.002\nEpoch 98/1000 | Train Loss=62616.66601562 | Val Loss=3.10229683 | Data=626.03051758 | Physics=11.18512047 | Val RMSE: 2.71326613 | ‚àö(Val Loss) = 1.76133382 | Current Learning Rate: 0.002\nEpoch 99/1000 | Train Loss=64331.67578125 | Val Loss=3.09978032 | Data=643.17239380 | Physics=17.04429740 | Val RMSE: 2.71325445 | ‚àö(Val Loss) = 1.76061928 | Current Learning Rate: 0.002\nEpoch 100/1000 | Train Loss=62129.84960938 | Val Loss=3.08174562 | Data=621.15911865 | Physics=13.50357013 | Val RMSE: 2.71294284 | ‚àö(Val Loss) = 1.75549018 | Current Learning Rate: 0.002\nEpoch 101/1000 | Train Loss=61279.09765625 | Val Loss=3.07416701 | Data=612.65132141 | Physics=13.67094276 | Val RMSE: 2.71298242 | ‚àö(Val Loss) = 1.75333023 | Current Learning Rate: 0.002\n\n Epoch :  100 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.99716794  -3.713916   -14.187192  ]\n [  0.9972256   -3.7139897  -14.187211  ]\n [  0.99710023  -3.7138295  -14.187169  ]\n [  0.99714154  -3.7138822  -14.187182  ]\n [  0.99715173  -3.7138953  -14.187186  ]\n [  0.99704456  -3.7137582  -14.18715   ]\n [  0.99695194  -3.71364    -14.187119  ]\n [  0.99717003  -3.7139187  -14.187192  ]\n [  0.9969474   -3.713634   -14.187117  ]\n [  0.99704194  -3.713755   -14.187149  ]\n [  0.99693686  -3.7136204  -14.187113  ]] \n\nFinal Test RMSE:  1.6680244207382202\nEpoch 102/1000 | Train Loss=65082.54492188 | Val Loss=3.31632733 | Data=650.68661499 | Physics=13.08970029 | Val RMSE: 2.71319747 | ‚àö(Val Loss) = 1.82107866 | Current Learning Rate: 0.002\nEpoch 103/1000 | Train Loss=60741.11523438 | Val Loss=3.23225117 | Data=607.27754211 | Physics=9.46425844 | Val RMSE: 2.71310258 | ‚àö(Val Loss) = 1.79784620 | Current Learning Rate: 0.002\nEpoch 104/1000 | Train Loss=61821.07226562 | Val Loss=3.20718074 | Data=618.07626343 | Physics=10.02601478 | Val RMSE: 2.71309996 | ‚àö(Val Loss) = 1.79086030 | Current Learning Rate: 0.002\nEpoch 105/1000 | Train Loss=62778.30078125 | Val Loss=3.18793583 | Data=627.64620972 | Physics=11.72894975 | Val RMSE: 2.71310234 | ‚àö(Val Loss) = 1.78547919 | Current Learning Rate: 0.002\nEpoch 106/1000 | Train Loss=61196.28320312 | Val Loss=3.21620917 | Data=611.82678223 | Physics=11.11556035 | Val RMSE: 2.71269298 | ‚àö(Val Loss) = 1.79337931 | Current Learning Rate: 0.002\nEpoch 107/1000 | Train Loss=67247.18554688 | Val Loss=3.27453876 | Data=672.33189392 | Physics=14.05767038 | Val RMSE: 2.71308446 | ‚àö(Val Loss) = 1.80956864 | Current Learning Rate: 0.002\nEpoch 108/1000 | Train Loss=62673.97265625 | Val Loss=3.25180507 | Data=626.60339355 | Physics=11.32459327 | Val RMSE: 2.71309590 | ‚àö(Val Loss) = 1.80327618 | Current Learning Rate: 0.002\nEpoch 109/1000 | Train Loss=66752.74609375 | Val Loss=3.18242908 | Data=667.38728333 | Physics=14.04461904 | Val RMSE: 2.71281028 | ‚àö(Val Loss) = 1.78393638 | Current Learning Rate: 0.002\nEpoch 110/1000 | Train Loss=61744.22265625 | Val Loss=3.18344212 | Data=617.30108643 | Physics=14.61341622 | Val RMSE: 2.71270990 | ‚àö(Val Loss) = 1.78422034 | Current Learning Rate: 0.002\nEpoch 111/1000 | Train Loss=59468.95507812 | Val Loss=3.04504704 | Data=594.55763245 | Physics=8.12058883 | Val RMSE: 2.71289921 | ‚àö(Val Loss) = 1.74500632 | Current Learning Rate: 0.002\nEpoch 112/1000 | Train Loss=60433.02929688 | Val Loss=3.09489155 | Data=604.19502258 | Physics=10.56275075 | Val RMSE: 2.71255803 | ‚àö(Val Loss) = 1.75923038 | Current Learning Rate: 0.002\nEpoch 113/1000 | Train Loss=62434.76757812 | Val Loss=3.11090231 | Data=624.20327759 | Physics=17.05211199 | Val RMSE: 2.71288872 | ‚àö(Val Loss) = 1.76377499 | Current Learning Rate: 0.002\nEpoch 114/1000 | Train Loss=61179.88867188 | Val Loss=3.08194065 | Data=611.66197205 | Physics=11.70168687 | Val RMSE: 2.71263504 | ‚àö(Val Loss) = 1.75554574 | Current Learning Rate: 0.002\nEpoch 115/1000 | Train Loss=63010.14648438 | Val Loss=3.20265555 | Data=629.96200562 | Physics=13.55655826 | Val RMSE: 2.71275878 | ‚àö(Val Loss) = 1.78959644 | Current Learning Rate: 0.002\nEpoch 116/1000 | Train Loss=63680.21484375 | Val Loss=3.16949153 | Data=636.66290283 | Physics=13.40434584 | Val RMSE: 2.71254563 | ‚àö(Val Loss) = 1.78030658 | Current Learning Rate: 0.002\nEpoch 117/1000 | Train Loss=64733.66796875 | Val Loss=3.26839948 | Data=647.19689941 | Physics=13.78580583 | Val RMSE: 2.71278453 | ‚àö(Val Loss) = 1.80787158 | Current Learning Rate: 0.002\nEpoch 118/1000 | Train Loss=64325.31445312 | Val Loss=3.20672965 | Data=643.11502075 | Physics=12.57717731 | Val RMSE: 2.71255565 | ‚àö(Val Loss) = 1.79073441 | Current Learning Rate: 0.002\nEpoch 119/1000 | Train Loss=64742.55273438 | Val Loss=3.16119552 | Data=647.28820801 | Physics=12.00629971 | Val RMSE: 2.71268535 | ‚àö(Val Loss) = 1.77797508 | Current Learning Rate: 0.002\nEpoch 120/1000 | Train Loss=66666.33398438 | Val Loss=3.14370012 | Data=666.52139282 | Physics=15.33075396 | Val RMSE: 2.71276498 | ‚àö(Val Loss) = 1.77304828 | Current Learning Rate: 0.002\nEpoch 121/1000 | Train Loss=62470.10937500 | Val Loss=3.12532449 | Data=624.56304932 | Physics=12.51500199 | Val RMSE: 2.71282005 | ‚àö(Val Loss) = 1.76785874 | Current Learning Rate: 0.002\n\n Epoch :  120 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.99716794  -3.713916   -14.187192  ]\n [  0.9972256   -3.7139897  -14.187211  ]\n [  0.99710023  -3.7138295  -14.187169  ]\n [  0.99714154  -3.7138822  -14.187182  ]\n [  0.99715173  -3.7138953  -14.187186  ]\n [  0.99704456  -3.7137582  -14.18715   ]\n [  0.99695194  -3.71364    -14.187119  ]\n [  0.99717003  -3.7139187  -14.187192  ]\n [  0.9969474   -3.713634   -14.187117  ]\n [  0.99704194  -3.713755   -14.187149  ]\n [  0.99693686  -3.7136204  -14.187113  ]] \n\nFinal Test RMSE:  1.6680244207382202\nEpoch 122/1000 | Train Loss=60845.38281250 | Val Loss=3.13691187 | Data=608.31445312 | Physics=13.55584189 | Val RMSE: 2.71317935 | ‚àö(Val Loss) = 1.77113295 | Current Learning Rate: 0.002\nEpoch 123/1000 | Train Loss=61914.10546875 | Val Loss=3.12529492 | Data=619.00509644 | Physics=11.07788618 | Val RMSE: 2.71317077 | ‚àö(Val Loss) = 1.76785040 | Current Learning Rate: 0.002\nEpoch 124/1000 | Train Loss=59557.18164062 | Val Loss=3.13140416 | Data=595.43487549 | Physics=11.76108743 | Val RMSE: 2.71317267 | ‚àö(Val Loss) = 1.76957738 | Current Learning Rate: 0.002\nEpoch 125/1000 | Train Loss=66489.43554688 | Val Loss=3.11442137 | Data=664.75541687 | Physics=13.21438969 | Val RMSE: 2.71317458 | ‚àö(Val Loss) = 1.76477230 | Current Learning Rate: 0.002\nEpoch 126/1000 | Train Loss=66467.19335938 | Val Loss=3.09048319 | Data=664.53378296 | Physics=12.60022520 | Val RMSE: 2.71288919 | ‚àö(Val Loss) = 1.75797701 | Current Learning Rate: 0.002\nEpoch 127/1000 | Train Loss=61949.79882812 | Val Loss=3.10799265 | Data=619.35369873 | Physics=16.98077062 | Val RMSE: 2.71323323 | ‚àö(Val Loss) = 1.76294994 | Current Learning Rate: 0.002\nEpoch 128/1000 | Train Loss=64369.65039062 | Val Loss=3.09046245 | Data=643.54843140 | Physics=19.71364879 | Val RMSE: 2.71322608 | ‚àö(Val Loss) = 1.75797117 | Current Learning Rate: 0.002\nEpoch 129/1000 | Train Loss=63175.60546875 | Val Loss=3.09121013 | Data=631.61874390 | Physics=12.02452030 | Val RMSE: 2.71289825 | ‚àö(Val Loss) = 1.75818372 | Current Learning Rate: 0.002\nEpoch 130/1000 | Train Loss=67055.27734375 | Val Loss=3.07071090 | Data=670.40917969 | Physics=16.49797495 | Val RMSE: 2.71294165 | ‚àö(Val Loss) = 1.75234437 | Current Learning Rate: 0.002\nEpoch 131/1000 | Train Loss=66494.44726562 | Val Loss=3.07518339 | Data=664.79891968 | Physics=17.85977361 | Val RMSE: 2.71296787 | ‚àö(Val Loss) = 1.75362003 | Current Learning Rate: 0.002\nEpoch 132/1000 | Train Loss=64612.75195312 | Val Loss=3.07479453 | Data=645.98281860 | Physics=17.26589133 | Val RMSE: 2.71298194 | ‚àö(Val Loss) = 1.75350916 | Current Learning Rate: 0.002\nEpoch 133/1000 | Train Loss=61917.34960938 | Val Loss=3.09737325 | Data=619.03091431 | Physics=15.77673492 | Val RMSE: 2.71330523 | ‚àö(Val Loss) = 1.75993562 | Current Learning Rate: 0.002\nEpoch 134/1000 | Train Loss=61248.97265625 | Val Loss=3.09925199 | Data=612.35238647 | Physics=12.02839801 | Val RMSE: 2.71324825 | ‚àö(Val Loss) = 1.76046920 | Current Learning Rate: 0.002\nEpoch 135/1000 | Train Loss=63924.99218750 | Val Loss=3.09720945 | Data=639.10665894 | Physics=16.29831008 | Val RMSE: 2.71324587 | ‚àö(Val Loss) = 1.75988901 | Current Learning Rate: 0.002\nEpoch 136/1000 | Train Loss=61636.20117188 | Val Loss=3.08999753 | Data=616.22076416 | Physics=14.82306217 | Val RMSE: 2.71324420 | ‚àö(Val Loss) = 1.75783885 | Current Learning Rate: 0.002\nEpoch 137/1000 | Train Loss=64610.99414062 | Val Loss=3.11979198 | Data=645.96804810 | Physics=15.28085402 | Val RMSE: 2.71321487 | ‚àö(Val Loss) = 1.76629329 | Current Learning Rate: 0.002\nEpoch 138/1000 | Train Loss=62044.82421875 | Val Loss=3.14535308 | Data=620.31158447 | Physics=11.55160693 | Val RMSE: 2.71315360 | ‚àö(Val Loss) = 1.77351427 | Current Learning Rate: 0.002\nEpoch 139/1000 | Train Loss=63531.92773438 | Val Loss=3.11857653 | Data=635.17337036 | Physics=18.18252481 | Val RMSE: 2.71314669 | ‚àö(Val Loss) = 1.76594913 | Current Learning Rate: 0.002\nEpoch 140/1000 | Train Loss=63442.06054688 | Val Loss=3.11616611 | Data=634.28018188 | Physics=14.26129589 | Val RMSE: 2.71314669 | ‚àö(Val Loss) = 1.76526654 | Current Learning Rate: 0.002\nEpoch 141/1000 | Train Loss=64429.16210938 | Val Loss=3.09950352 | Data=644.14987183 | Physics=15.17117510 | Val RMSE: 2.71286416 | ‚àö(Val Loss) = 1.76054072 | Current Learning Rate: 0.002\n\n Epoch :  140 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.99716794  -3.713916   -14.187192  ]\n [  0.9972256   -3.7139897  -14.187211  ]\n [  0.99710023  -3.7138295  -14.187169  ]\n [  0.99714154  -3.7138822  -14.187182  ]\n [  0.99715173  -3.7138953  -14.187186  ]\n [  0.99704456  -3.7137582  -14.18715   ]\n [  0.99695194  -3.71364    -14.187119  ]\n [  0.99717003  -3.7139187  -14.187192  ]\n [  0.9969474   -3.713634   -14.187117  ]\n [  0.99704194  -3.713755   -14.187149  ]\n [  0.99693686  -3.7136204  -14.187113  ]] \n\nFinal Test RMSE:  1.6680244207382202\nEpoch 142/1000 | Train Loss=60280.16601562 | Val Loss=3.09990096 | Data=602.66485596 | Physics=11.70852667 | Val RMSE: 2.71322203 | ‚àö(Val Loss) = 1.76065361 | Current Learning Rate: 0.002\nEpoch 143/1000 | Train Loss=62611.44531250 | Val Loss=3.11671686 | Data=625.97470093 | Physics=13.70233322 | Val RMSE: 2.71288395 | ‚àö(Val Loss) = 1.76542258 | Current Learning Rate: 0.002\nEpoch 144/1000 | Train Loss=61398.06054688 | Val Loss=3.08055234 | Data=613.84173584 | Physics=13.07900989 | Val RMSE: 2.71319199 | ‚àö(Val Loss) = 1.75515020 | Current Learning Rate: 0.002\nEpoch 145/1000 | Train Loss=66137.16992188 | Val Loss=3.09397912 | Data=661.23156738 | Physics=14.02373785 | Val RMSE: 2.71319294 | ‚àö(Val Loss) = 1.75897110 | Current Learning Rate: 0.002\nEpoch 146/1000 | Train Loss=65423.23437500 | Val Loss=3.08647442 | Data=654.09072876 | Physics=15.07455679 | Val RMSE: 2.71289229 | ‚àö(Val Loss) = 1.75683653 | Current Learning Rate: 0.002\nEpoch 147/1000 | Train Loss=67129.98437500 | Val Loss=3.07577753 | Data=671.15826416 | Physics=15.06170220 | Val RMSE: 2.71291494 | ‚àö(Val Loss) = 1.75378942 | Current Learning Rate: 0.002\nEpoch 148/1000 | Train Loss=66632.75781250 | Val Loss=3.08877587 | Data=666.18560791 | Physics=15.30997279 | Val RMSE: 2.71324110 | ‚àö(Val Loss) = 1.75749135 | Current Learning Rate: 0.002\nEpoch 149/1000 | Train Loss=66439.97460938 | Val Loss=3.07493234 | Data=664.26004028 | Physics=13.72573595 | Val RMSE: 2.71293378 | ‚àö(Val Loss) = 1.75354850 | Current Learning Rate: 0.002\nEpoch 150/1000 | Train Loss=64894.16601562 | Val Loss=3.06569076 | Data=648.79312134 | Physics=20.00760847 | Val RMSE: 2.71297693 | ‚àö(Val Loss) = 1.75091136 | Current Learning Rate: 0.002\nEpoch 151/1000 | Train Loss=64929.70117188 | Val Loss=3.06635141 | Data=649.15386963 | Physics=16.19602068 | Val RMSE: 2.71301770 | ‚àö(Val Loss) = 1.75110006 | Current Learning Rate: 0.002\nEpoch 152/1000 | Train Loss=59889.61914062 | Val Loss=3.07371640 | Data=598.76133728 | Physics=10.25690481 | Val RMSE: 2.71334839 | ‚àö(Val Loss) = 1.75320172 | Current Learning Rate: 0.002\nEpoch 153/1000 | Train Loss=63684.85937500 | Val Loss=3.05809522 | Data=636.70666504 | Physics=15.32327967 | Val RMSE: 2.71301603 | ‚àö(Val Loss) = 1.74874103 | Current Learning Rate: 0.002\nEpoch 154/1000 | Train Loss=64573.18164062 | Val Loss=3.09968662 | Data=645.59033203 | Physics=14.96709670 | Val RMSE: 2.71296501 | ‚àö(Val Loss) = 1.76059270 | Current Learning Rate: 0.002\nEpoch 155/1000 | Train Loss=69087.82421875 | Val Loss=3.08540010 | Data=690.73320007 | Physics=17.56520027 | Val RMSE: 2.71296310 | ‚àö(Val Loss) = 1.75653064 | Current Learning Rate: 0.002\nEpoch 156/1000 | Train Loss=63643.70703125 | Val Loss=3.08471966 | Data=636.28961182 | Physics=19.20469796 | Val RMSE: 2.71328402 | ‚àö(Val Loss) = 1.75633705 | Current Learning Rate: 0.002\nEpoch 157/1000 | Train Loss=60827.31445312 | Val Loss=3.07735038 | Data=608.13581848 | Physics=12.03581019 | Val RMSE: 2.71295762 | ‚àö(Val Loss) = 1.75423789 | Current Learning Rate: 0.002\nEpoch 158/1000 | Train Loss=61486.68945312 | Val Loss=3.08843160 | Data=614.73178101 | Physics=10.47636725 | Val RMSE: 2.71330476 | ‚àö(Val Loss) = 1.75739336 | Current Learning Rate: 0.002\nEpoch 159/1000 | Train Loss=61282.75390625 | Val Loss=3.09604120 | Data=612.69107056 | Physics=11.42759496 | Val RMSE: 2.71327233 | ‚àö(Val Loss) = 1.75955713 | Current Learning Rate: 0.002\nEpoch 160/1000 | Train Loss=68157.38085938 | Val Loss=3.07362175 | Data=681.43585205 | Physics=12.48614373 | Val RMSE: 2.71296048 | ‚àö(Val Loss) = 1.75317478 | Current Learning Rate: 0.002\nEpoch 161/1000 | Train Loss=59677.90039062 | Val Loss=3.08116746 | Data=596.64048767 | Physics=12.86837526 | Val RMSE: 2.71328497 | ‚àö(Val Loss) = 1.75532544 | Current Learning Rate: 0.002\n\n Epoch :  160 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.99716794  -3.713916   -14.187192  ]\n [  0.9972256   -3.7139897  -14.187211  ]\n [  0.99710023  -3.7138295  -14.187169  ]\n [  0.99714154  -3.7138822  -14.187182  ]\n [  0.99715173  -3.7138953  -14.187186  ]\n [  0.99704456  -3.7137582  -14.18715   ]\n [  0.99695194  -3.71364    -14.187119  ]\n [  0.99717003  -3.7139187  -14.187192  ]\n [  0.9969474   -3.713634   -14.187117  ]\n [  0.99704194  -3.713755   -14.187149  ]\n [  0.99693686  -3.7136204  -14.187113  ]] \n\nFinal Test RMSE:  1.6680244207382202\nEpoch 162/1000 | Train Loss=63799.60937500 | Val Loss=3.08850455 | Data=637.85379028 | Physics=15.59606498 | Val RMSE: 2.71327281 | ‚àö(Val Loss) = 1.75741422 | Current Learning Rate: 0.002\nEpoch 163/1000 | Train Loss=60042.95898438 | Val Loss=3.09061885 | Data=600.29269409 | Physics=11.67883512 | Val RMSE: 2.71287489 | ‚àö(Val Loss) = 1.75801563 | Current Learning Rate: 0.002\nEpoch 164/1000 | Train Loss=61633.98828125 | Val Loss=3.12869143 | Data=616.19729614 | Physics=15.71039572 | Val RMSE: 2.71296048 | ‚àö(Val Loss) = 1.76881075 | Current Learning Rate: 0.002\nEpoch 165/1000 | Train Loss=63090.06054688 | Val Loss=3.11863995 | Data=630.76089478 | Physics=13.72407863 | Val RMSE: 2.71297288 | ‚àö(Val Loss) = 1.76596713 | Current Learning Rate: 0.002\nEpoch 166/1000 | Train Loss=64634.13281250 | Val Loss=3.10591412 | Data=646.20339966 | Physics=12.42544005 | Val RMSE: 2.71300769 | ‚àö(Val Loss) = 1.76236033 | Current Learning Rate: 0.002\nEpoch 167/1000 | Train Loss=65469.51757812 | Val Loss=3.11621165 | Data=654.55191040 | Physics=16.29121755 | Val RMSE: 2.71333575 | ‚àö(Val Loss) = 1.76527953 | Current Learning Rate: 0.002\n‚úÖ Saved best model at epoch 168 (Val Loss = 2.91136456)\nEpoch 168/1000 | Train Loss=64311.22070312 | Val Loss=2.91136456 | Data=642.96881104 | Physics=15.74919940 | Val RMSE: 2.71283650 | ‚àö(Val Loss) = 1.70627213 | Current Learning Rate: 0.002\nEpoch 169/1000 | Train Loss=64977.64257812 | Val Loss=3.12894416 | Data=649.63619995 | Physics=13.42952554 | Val RMSE: 2.71218181 | ‚àö(Val Loss) = 1.76888216 | Current Learning Rate: 0.002\nEpoch 170/1000 | Train Loss=65307.14257812 | Val Loss=3.20272398 | Data=652.92633057 | Physics=16.70062329 | Val RMSE: 2.71230125 | ‚àö(Val Loss) = 1.78961563 | Current Learning Rate: 0.002\nEpoch 171/1000 | Train Loss=64044.93164062 | Val Loss=3.13453197 | Data=640.30621338 | Physics=15.54029361 | Val RMSE: 2.71243262 | ‚àö(Val Loss) = 1.77046096 | Current Learning Rate: 0.002\nEpoch 172/1000 | Train Loss=62949.55273438 | Val Loss=3.14237213 | Data=629.34704590 | Physics=19.47584251 | Val RMSE: 2.71278358 | ‚àö(Val Loss) = 1.77267373 | Current Learning Rate: 0.002\nEpoch 173/1000 | Train Loss=65327.10937500 | Val Loss=3.14521670 | Data=653.12905884 | Physics=14.60019369 | Val RMSE: 2.71246481 | ‚àö(Val Loss) = 1.77347589 | Current Learning Rate: 0.002\nEpoch 174/1000 | Train Loss=66268.53515625 | Val Loss=3.14871311 | Data=662.54577637 | Physics=12.99990523 | Val RMSE: 2.71252966 | ‚àö(Val Loss) = 1.77446139 | Current Learning Rate: 0.002\nEpoch 175/1000 | Train Loss=59988.15234375 | Val Loss=3.13163161 | Data=599.74481201 | Physics=10.89461995 | Val RMSE: 2.71257520 | ‚àö(Val Loss) = 1.76964164 | Current Learning Rate: 0.002\nEpoch 176/1000 | Train Loss=61675.71679688 | Val Loss=3.19430280 | Data=616.61691284 | Physics=13.49401013 | Val RMSE: 2.71290827 | ‚àö(Val Loss) = 1.78726125 | Current Learning Rate: 0.002\nEpoch 177/1000 | Train Loss=61213.51171875 | Val Loss=3.16687441 | Data=611.99478149 | Physics=13.72879974 | Val RMSE: 2.71282601 | ‚àö(Val Loss) = 1.77957141 | Current Learning Rate: 0.002\nEpoch 178/1000 | Train Loss=66278.80468750 | Val Loss=3.16659951 | Data=662.64086914 | Physics=18.56551540 | Val RMSE: 2.71280289 | ‚àö(Val Loss) = 1.77949417 | Current Learning Rate: 0.002\nEpoch 179/1000 | Train Loss=67269.03515625 | Val Loss=3.12907124 | Data=672.54956055 | Physics=13.84005291 | Val RMSE: 2.71249223 | ‚àö(Val Loss) = 1.76891804 | Current Learning Rate: 0.002\nEpoch 180/1000 | Train Loss=59792.80859375 | Val Loss=3.09053779 | Data=597.78868103 | Physics=13.23704748 | Val RMSE: 2.71284437 | ‚àö(Val Loss) = 1.75799251 | Current Learning Rate: 0.002\nEpoch 181/1000 | Train Loss=64225.60937500 | Val Loss=3.06923842 | Data=642.11172485 | Physics=16.30285031 | Val RMSE: 2.71254277 | ‚àö(Val Loss) = 1.75192416 | Current Learning Rate: 0.002\n\n Epoch :  180 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 182/1000 | Train Loss=65966.77929688 | Val Loss=3.08384871 | Data=659.52737427 | Physics=13.60196258 | Val RMSE: 2.71258569 | ‚àö(Val Loss) = 1.75608909 | Current Learning Rate: 0.002\nEpoch 183/1000 | Train Loss=64942.84375000 | Val Loss=3.07630873 | Data=649.28671265 | Physics=14.56382368 | Val RMSE: 2.71261358 | ‚àö(Val Loss) = 1.75394094 | Current Learning Rate: 0.002\nEpoch 184/1000 | Train Loss=60334.15429688 | Val Loss=3.06386638 | Data=603.20570374 | Physics=10.51517875 | Val RMSE: 2.71293187 | ‚àö(Val Loss) = 1.75039029 | Current Learning Rate: 0.002\nEpoch 185/1000 | Train Loss=57897.10351562 | Val Loss=3.05308604 | Data=578.83723450 | Physics=9.04924637 | Val RMSE: 2.71288848 | ‚àö(Val Loss) = 1.74730825 | Current Learning Rate: 0.002\nEpoch 186/1000 | Train Loss=63261.04101562 | Val Loss=3.05268621 | Data=632.46936035 | Physics=14.21300764 | Val RMSE: 2.71285868 | ‚àö(Val Loss) = 1.74719381 | Current Learning Rate: 0.002\nEpoch 187/1000 | Train Loss=65987.95117188 | Val Loss=3.04298520 | Data=659.73263550 | Physics=18.07953588 | Val RMSE: 2.71256113 | ‚àö(Val Loss) = 1.74441540 | Current Learning Rate: 0.002\nEpoch 188/1000 | Train Loss=63958.83203125 | Val Loss=3.03743958 | Data=639.44271851 | Physics=17.41566468 | Val RMSE: 2.71289182 | ‚àö(Val Loss) = 1.74282515 | Current Learning Rate: 0.002\nEpoch 189/1000 | Train Loss=63357.75781250 | Val Loss=3.02885079 | Data=633.43768311 | Physics=13.23393499 | Val RMSE: 2.71257830 | ‚àö(Val Loss) = 1.74035943 | Current Learning Rate: 0.002\nEpoch 190/1000 | Train Loss=60982.57617188 | Val Loss=3.04134607 | Data=609.68505859 | Physics=13.83205417 | Val RMSE: 2.71260953 | ‚àö(Val Loss) = 1.74394560 | Current Learning Rate: 0.002\nEpoch 191/1000 | Train Loss=60543.26562500 | Val Loss=3.03717017 | Data=605.29403687 | Physics=12.48411062 | Val RMSE: 2.71293187 | ‚àö(Val Loss) = 1.74274790 | Current Learning Rate: 0.002\nEpoch 192/1000 | Train Loss=65867.67187500 | Val Loss=3.02216458 | Data=658.53234863 | Physics=16.24132939 | Val RMSE: 2.71261954 | ‚àö(Val Loss) = 1.73843741 | Current Learning Rate: 0.002\nEpoch 193/1000 | Train Loss=63850.12109375 | Val Loss=3.01475954 | Data=638.35961914 | Physics=14.39479814 | Val RMSE: 2.71265006 | ‚àö(Val Loss) = 1.73630631 | Current Learning Rate: 0.002\nEpoch 194/1000 | Train Loss=62530.40625000 | Val Loss=3.01619029 | Data=625.16317749 | Physics=13.82142172 | Val RMSE: 2.71267557 | ‚àö(Val Loss) = 1.73671830 | Current Learning Rate: 0.002\nEpoch 195/1000 | Train Loss=60014.50195312 | Val Loss=3.00819254 | Data=600.00610352 | Physics=12.59070701 | Val RMSE: 2.71298218 | ‚àö(Val Loss) = 1.73441422 | Current Learning Rate: 0.002\nEpoch 196/1000 | Train Loss=63479.90429688 | Val Loss=3.03033018 | Data=634.66192627 | Physics=11.29393978 | Val RMSE: 2.71293807 | ‚àö(Val Loss) = 1.74078441 | Current Learning Rate: 0.002\nEpoch 197/1000 | Train Loss=62764.38281250 | Val Loss=3.02837610 | Data=627.50128174 | Physics=15.23831139 | Val RMSE: 2.71289611 | ‚àö(Val Loss) = 1.74022305 | Current Learning Rate: 0.002\nEpoch 198/1000 | Train Loss=59954.12500000 | Val Loss=3.03617287 | Data=599.40289307 | Physics=12.26535922 | Val RMSE: 2.71285391 | ‚àö(Val Loss) = 1.74246168 | Current Learning Rate: 0.002\nEpoch 199/1000 | Train Loss=63594.14843750 | Val Loss=3.04488826 | Data=635.80035400 | Physics=14.09836565 | Val RMSE: 2.71256113 | ‚àö(Val Loss) = 1.74496078 | Current Learning Rate: 0.002\nEpoch 200/1000 | Train Loss=62995.72656250 | Val Loss=3.05246854 | Data=629.81845093 | Physics=12.43553461 | Val RMSE: 2.71260071 | ‚àö(Val Loss) = 1.74713147 | Current Learning Rate: 0.002\nEpoch 201/1000 | Train Loss=60996.74804688 | Val Loss=3.33292222 | Data=609.82470703 | Physics=15.42895085 | Val RMSE: 2.71279597 | ‚àö(Val Loss) = 1.82562923 | Current Learning Rate: 0.002\n\n Epoch :  200 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 202/1000 | Train Loss=66431.46875000 | Val Loss=3.28063583 | Data=664.17051697 | Physics=15.88785323 | Val RMSE: 2.71249032 | ‚àö(Val Loss) = 1.81125259 | Current Learning Rate: 0.002\nEpoch 203/1000 | Train Loss=63920.98632812 | Val Loss=3.23851228 | Data=639.06985474 | Physics=13.35469383 | Val RMSE: 2.71255493 | ‚àö(Val Loss) = 1.79958665 | Current Learning Rate: 0.002\nEpoch 204/1000 | Train Loss=62388.29882812 | Val Loss=3.41387153 | Data=623.74493408 | Physics=11.86351445 | Val RMSE: 2.71287560 | ‚àö(Val Loss) = 1.84766650 | Current Learning Rate: 0.002\nEpoch 205/1000 | Train Loss=65566.13476562 | Val Loss=3.29390240 | Data=655.52334595 | Physics=11.84247968 | Val RMSE: 2.71252370 | ‚àö(Val Loss) = 1.81491113 | Current Learning Rate: 0.002\nEpoch 206/1000 | Train Loss=62330.10937500 | Val Loss=3.33016372 | Data=623.16275024 | Physics=12.26626311 | Val RMSE: 2.71254539 | ‚àö(Val Loss) = 1.82487357 | Current Learning Rate: 0.002\nEpoch 207/1000 | Train Loss=64969.27734375 | Val Loss=3.68576479 | Data=649.55725098 | Physics=10.13942592 | Val RMSE: 2.71281743 | ‚àö(Val Loss) = 1.91983461 | Current Learning Rate: 0.002\nEpoch 208/1000 | Train Loss=67319.32226562 | Val Loss=3.49763703 | Data=673.05078125 | Physics=15.83926770 | Val RMSE: 2.71272922 | ‚àö(Val Loss) = 1.87019706 | Current Learning Rate: 0.002\nEpoch 209/1000 | Train Loss=68126.85546875 | Val Loss=3.39204884 | Data=681.12818909 | Physics=13.50791602 | Val RMSE: 2.71243834 | ‚àö(Val Loss) = 1.84175158 | Current Learning Rate: 0.002\nEpoch 210/1000 | Train Loss=60752.20312500 | Val Loss=3.30594158 | Data=607.38145447 | Physics=13.53018574 | Val RMSE: 2.71252799 | ‚àö(Val Loss) = 1.81822479 | Current Learning Rate: 0.002\nEpoch 211/1000 | Train Loss=68080.34765625 | Val Loss=3.25154161 | Data=680.66122437 | Physics=14.85219950 | Val RMSE: 2.71259451 | ‚àö(Val Loss) = 1.80320311 | Current Learning Rate: 0.002\nEpoch 212/1000 | Train Loss=64150.43359375 | Val Loss=3.18409061 | Data=641.35787964 | Physics=17.89081165 | Val RMSE: 2.71263051 | ‚àö(Val Loss) = 1.78440201 | Current Learning Rate: 0.002\nEpoch 213/1000 | Train Loss=63924.70312500 | Val Loss=3.17005730 | Data=639.10733032 | Physics=12.90397564 | Val RMSE: 2.71267200 | ‚àö(Val Loss) = 1.78046548 | Current Learning Rate: 0.002\nEpoch 214/1000 | Train Loss=62997.41210938 | Val Loss=3.11269641 | Data=629.82946777 | Physics=16.49643279 | Val RMSE: 2.71269321 | ‚àö(Val Loss) = 1.76428354 | Current Learning Rate: 0.002\nEpoch 215/1000 | Train Loss=61921.03906250 | Val Loss=3.08346295 | Data=619.06765747 | Physics=15.52466160 | Val RMSE: 2.71299887 | ‚àö(Val Loss) = 1.75597918 | Current Learning Rate: 0.002\nEpoch 216/1000 | Train Loss=66887.64257812 | Val Loss=3.06544185 | Data=668.73118591 | Physics=16.97895795 | Val RMSE: 2.71266913 | ‚àö(Val Loss) = 1.75084031 | Current Learning Rate: 0.002\nEpoch 217/1000 | Train Loss=63292.99023438 | Val Loss=3.10886955 | Data=632.78738403 | Physics=15.15629244 | Val RMSE: 2.71267581 | ‚àö(Val Loss) = 1.76319861 | Current Learning Rate: 0.002\nEpoch 218/1000 | Train Loss=64406.70117188 | Val Loss=3.09234166 | Data=643.92807007 | Physics=12.30597805 | Val RMSE: 2.71269369 | ‚àö(Val Loss) = 1.75850546 | Current Learning Rate: 0.002\nEpoch 219/1000 | Train Loss=60548.77539062 | Val Loss=3.07929134 | Data=605.35035706 | Physics=11.59133657 | Val RMSE: 2.71299386 | ‚àö(Val Loss) = 1.75479102 | Current Learning Rate: 0.002\nEpoch 220/1000 | Train Loss=57837.29394531 | Val Loss=3.06374884 | Data=578.24342346 | Physics=6.01625010 | Val RMSE: 2.71294713 | ‚àö(Val Loss) = 1.75035679 | Current Learning Rate: 0.002\nEpoch 221/1000 | Train Loss=61564.75585938 | Val Loss=3.06804132 | Data=615.51431274 | Physics=8.58413351 | Val RMSE: 2.71290708 | ‚àö(Val Loss) = 1.75158250 | Current Learning Rate: 0.002\n\n Epoch :  220 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 222/1000 | Train Loss=62681.70312500 | Val Loss=3.04329896 | Data=626.67572021 | Physics=14.51521689 | Val RMSE: 2.71287394 | ‚àö(Val Loss) = 1.74450541 | Current Learning Rate: 0.002\nEpoch 223/1000 | Train Loss=65970.33203125 | Val Loss=3.04292059 | Data=659.55458069 | Physics=19.61972521 | Val RMSE: 2.71257114 | ‚àö(Val Loss) = 1.74439692 | Current Learning Rate: 0.002\nEpoch 224/1000 | Train Loss=62841.91210938 | Val Loss=3.04911280 | Data=628.28182983 | Physics=11.42248936 | Val RMSE: 2.71260667 | ‚àö(Val Loss) = 1.74617088 | Current Learning Rate: 0.002\nEpoch 225/1000 | Train Loss=62674.98828125 | Val Loss=3.03919983 | Data=626.60531616 | Physics=16.76317604 | Val RMSE: 2.71292925 | ‚àö(Val Loss) = 1.74333012 | Current Learning Rate: 0.002\nEpoch 226/1000 | Train Loss=61460.42773438 | Val Loss=3.06839013 | Data=614.46563721 | Physics=12.33632793 | Val RMSE: 2.71289921 | ‚àö(Val Loss) = 1.75168204 | Current Learning Rate: 0.002\nEpoch 227/1000 | Train Loss=62618.71093750 | Val Loss=3.11587572 | Data=626.05245972 | Physics=9.38686526 | Val RMSE: 2.71257234 | ‚àö(Val Loss) = 1.76518428 | Current Learning Rate: 0.002\nEpoch 228/1000 | Train Loss=63269.64453125 | Val Loss=3.15658236 | Data=632.55541992 | Physics=14.19250003 | Val RMSE: 2.71282649 | ‚àö(Val Loss) = 1.77667737 | Current Learning Rate: 0.002\nEpoch 229/1000 | Train Loss=59223.66210938 | Val Loss=3.16717839 | Data=592.09674072 | Physics=13.56314691 | Val RMSE: 2.71281052 | ‚àö(Val Loss) = 1.77965677 | Current Learning Rate: 0.002\nEpoch 230/1000 | Train Loss=62885.87695312 | Val Loss=3.19782281 | Data=628.71560669 | Physics=15.73163673 | Val RMSE: 2.71268082 | ‚àö(Val Loss) = 1.78824568 | Current Learning Rate: 0.002\nEpoch 231/1000 | Train Loss=62744.82812500 | Val Loss=3.23483109 | Data=627.30825806 | Physics=13.94143495 | Val RMSE: 2.71270227 | ‚àö(Val Loss) = 1.79856360 | Current Learning Rate: 0.002\nEpoch 232/1000 | Train Loss=63141.83984375 | Val Loss=3.17457223 | Data=631.27578735 | Physics=15.25536606 | Val RMSE: 2.71242642 | ‚àö(Val Loss) = 1.78173292 | Current Learning Rate: 0.002\nEpoch 233/1000 | Train Loss=61828.79492188 | Val Loss=3.15095234 | Data=618.15396118 | Physics=9.19503622 | Val RMSE: 2.71279073 | ‚àö(Val Loss) = 1.77509224 | Current Learning Rate: 0.002\nEpoch 234/1000 | Train Loss=59880.71679688 | Val Loss=3.21429825 | Data=598.66944885 | Physics=11.68398938 | Val RMSE: 2.71275663 | ‚àö(Val Loss) = 1.79284644 | Current Learning Rate: 0.002\nEpoch 235/1000 | Train Loss=61484.44531250 | Val Loss=3.17506957 | Data=614.70816040 | Physics=10.65796436 | Val RMSE: 2.71245790 | ‚àö(Val Loss) = 1.78187251 | Current Learning Rate: 0.002\nEpoch 236/1000 | Train Loss=64590.07421875 | Val Loss=3.13133383 | Data=645.75219727 | Physics=19.73967520 | Val RMSE: 2.71281743 | ‚àö(Val Loss) = 1.76955748 | Current Learning Rate: 0.002\nEpoch 237/1000 | Train Loss=64242.13867188 | Val Loss=3.12076473 | Data=642.27716064 | Physics=16.43623748 | Val RMSE: 2.71280479 | ‚àö(Val Loss) = 1.76656866 | Current Learning Rate: 0.002\nEpoch 238/1000 | Train Loss=62837.58007812 | Val Loss=3.09481621 | Data=628.23620605 | Physics=13.21929017 | Val RMSE: 2.71278906 | ‚àö(Val Loss) = 1.75920892 | Current Learning Rate: 0.002\nEpoch 239/1000 | Train Loss=64637.88476562 | Val Loss=3.11596680 | Data=646.23629761 | Physics=15.28373668 | Val RMSE: 2.71276593 | ‚àö(Val Loss) = 1.76521015 | Current Learning Rate: 0.002\nEpoch 240/1000 | Train Loss=62704.90234375 | Val Loss=3.15768218 | Data=626.90957642 | Physics=12.78342311 | Val RMSE: 2.71247077 | ‚àö(Val Loss) = 1.77698684 | Current Learning Rate: 0.002\nEpoch 241/1000 | Train Loss=60794.25000000 | Val Loss=3.12430763 | Data=607.79814148 | Physics=16.59283216 | Val RMSE: 2.71279812 | ‚àö(Val Loss) = 1.76757109 | Current Learning Rate: 0.002\n\n Epoch :  240 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 242/1000 | Train Loss=60253.68750000 | Val Loss=3.12133884 | Data=602.40405273 | Physics=8.32929314 | Val RMSE: 2.71278214 | ‚àö(Val Loss) = 1.76673114 | Current Learning Rate: 0.002\nEpoch 243/1000 | Train Loss=62489.28710938 | Val Loss=3.21342301 | Data=624.75357056 | Physics=12.87267721 | Val RMSE: 2.71275282 | ‚àö(Val Loss) = 1.79260230 | Current Learning Rate: 0.002\nEpoch 244/1000 | Train Loss=61822.99218750 | Val Loss=3.17399740 | Data=618.08355713 | Physics=18.12636412 | Val RMSE: 2.71272945 | ‚àö(Val Loss) = 1.78157163 | Current Learning Rate: 0.002\nEpoch 245/1000 | Train Loss=62077.54101562 | Val Loss=3.20762777 | Data=620.63604736 | Physics=12.99101198 | Val RMSE: 2.71270418 | ‚àö(Val Loss) = 1.79098511 | Current Learning Rate: 0.002\nEpoch 246/1000 | Train Loss=59131.44531250 | Val Loss=3.18562508 | Data=591.17630005 | Physics=12.15130876 | Val RMSE: 2.71269727 | ‚àö(Val Loss) = 1.78483200 | Current Learning Rate: 0.002\nEpoch 247/1000 | Train Loss=61123.83007812 | Val Loss=3.15019965 | Data=611.09785461 | Physics=13.48172604 | Val RMSE: 2.71242762 | ‚àö(Val Loss) = 1.77488017 | Current Learning Rate: 0.002\nEpoch 248/1000 | Train Loss=66883.50781250 | Val Loss=3.11024880 | Data=668.69378662 | Physics=14.18041339 | Val RMSE: 2.71251059 | ‚àö(Val Loss) = 1.76358974 | Current Learning Rate: 0.002\nEpoch 249/1000 | Train Loss=61007.28710938 | Val Loss=3.09041619 | Data=609.93264771 | Physics=13.65403718 | Val RMSE: 2.71284652 | ‚àö(Val Loss) = 1.75795794 | Current Learning Rate: 0.002\nEpoch 250/1000 | Train Loss=61856.50976562 | Val Loss=3.08675551 | Data=618.42050171 | Physics=16.75983023 | Val RMSE: 2.71280885 | ‚àö(Val Loss) = 1.75691652 | Current Learning Rate: 0.002\nEpoch 251/1000 | Train Loss=59653.53125000 | Val Loss=3.10232854 | Data=596.40159607 | Physics=8.77792725 | Val RMSE: 2.71251822 | ‚àö(Val Loss) = 1.76134276 | Current Learning Rate: 0.002\nEpoch 252/1000 | Train Loss=64765.83984375 | Val Loss=3.21648121 | Data=647.52182007 | Physics=11.01831099 | Val RMSE: 2.71280956 | ‚àö(Val Loss) = 1.79345512 | Current Learning Rate: 0.002\nEpoch 253/1000 | Train Loss=62469.15039062 | Val Loss=3.22142673 | Data=624.55679321 | Physics=9.72274860 | Val RMSE: 2.71277499 | ‚àö(Val Loss) = 1.79483330 | Current Learning Rate: 0.002\nEpoch 254/1000 | Train Loss=62477.52539062 | Val Loss=3.82216001 | Data=624.63189697 | Physics=15.94449188 | Val RMSE: 2.71211767 | ‚àö(Val Loss) = 1.95503449 | Current Learning Rate: 0.002\nEpoch 255/1000 | Train Loss=68982.23828125 | Val Loss=4.04223013 | Data=689.67193604 | Physics=19.75706343 | Val RMSE: 2.71210623 | ‚àö(Val Loss) = 2.01052976 | Current Learning Rate: 0.002\nEpoch 256/1000 | Train Loss=65970.48242188 | Val Loss=3.72927713 | Data=659.56375122 | Physics=13.42887282 | Val RMSE: 2.71233821 | ‚àö(Val Loss) = 1.93113363 | Current Learning Rate: 0.002\nEpoch 257/1000 | Train Loss=64875.59375000 | Val Loss=3.57895064 | Data=648.61492920 | Physics=14.39222947 | Val RMSE: 2.71245193 | ‚àö(Val Loss) = 1.89181149 | Current Learning Rate: 0.002\nEpoch 258/1000 | Train Loss=64276.46875000 | Val Loss=3.46169209 | Data=642.62078857 | Physics=16.14961107 | Val RMSE: 2.71254349 | ‚àö(Val Loss) = 1.86056232 | Current Learning Rate: 0.002\nEpoch 259/1000 | Train Loss=62753.07617188 | Val Loss=3.37081265 | Data=627.39135742 | Physics=13.40397924 | Val RMSE: 2.71287251 | ‚àö(Val Loss) = 1.83597732 | Current Learning Rate: 0.002\nEpoch 260/1000 | Train Loss=63904.74804688 | Val Loss=3.30699205 | Data=638.90908813 | Physics=12.32909899 | Val RMSE: 2.71254778 | ‚àö(Val Loss) = 1.81851375 | Current Learning Rate: 0.002\nEpoch 261/1000 | Train Loss=61946.68359375 | Val Loss=3.24671865 | Data=619.32058716 | Physics=18.18080910 | Val RMSE: 2.71288872 | ‚àö(Val Loss) = 1.80186534 | Current Learning Rate: 0.002\n\n Epoch :  260 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 262/1000 | Train Loss=66074.31835938 | Val Loss=3.18388462 | Data=660.60310364 | Physics=12.98567039 | Val RMSE: 2.71257734 | ‚àö(Val Loss) = 1.78434432 | Current Learning Rate: 0.002\nEpoch 263/1000 | Train Loss=61601.74023438 | Val Loss=3.13276744 | Data=615.87649536 | Physics=14.36354507 | Val RMSE: 2.71290112 | ‚àö(Val Loss) = 1.76996255 | Current Learning Rate: 0.002\nEpoch 264/1000 | Train Loss=62159.27343750 | Val Loss=3.12637377 | Data=621.45397949 | Physics=12.64865802 | Val RMSE: 2.71285319 | ‚àö(Val Loss) = 1.76815546 | Current Learning Rate: 0.002\nEpoch 265/1000 | Train Loss=61944.89257812 | Val Loss=3.12307835 | Data=619.30688477 | Physics=14.97051737 | Val RMSE: 2.71284199 | ‚àö(Val Loss) = 1.76722336 | Current Learning Rate: 0.002\nEpoch 266/1000 | Train Loss=66950.47265625 | Val Loss=3.09630513 | Data=669.35830688 | Physics=18.14546071 | Val RMSE: 2.71281767 | ‚àö(Val Loss) = 1.75963211 | Current Learning Rate: 0.002\nEpoch 267/1000 | Train Loss=60875.15625000 | Val Loss=3.11925507 | Data=608.61129761 | Physics=13.65088534 | Val RMSE: 2.71279311 | ‚àö(Val Loss) = 1.76614130 | Current Learning Rate: 0.002\nEpoch 268/1000 | Train Loss=59419.20703125 | Val Loss=3.11288452 | Data=594.05522156 | Physics=11.25444117 | Val RMSE: 2.71277595 | ‚àö(Val Loss) = 1.76433682 | Current Learning Rate: 0.002\nEpoch 269/1000 | Train Loss=61077.91992188 | Val Loss=3.08742952 | Data=610.64276123 | Physics=11.12395626 | Val RMSE: 2.71277428 | ‚àö(Val Loss) = 1.75710833 | Current Learning Rate: 0.002\nEpoch 270/1000 | Train Loss=65192.85156250 | Val Loss=3.10705996 | Data=651.78527832 | Physics=15.58722775 | Val RMSE: 2.71249318 | ‚àö(Val Loss) = 1.76268542 | Current Learning Rate: 0.002\nEpoch 271/1000 | Train Loss=62009.06445312 | Val Loss=3.08261967 | Data=619.94802856 | Physics=15.35101080 | Val RMSE: 2.71282530 | ‚àö(Val Loss) = 1.75573909 | Current Learning Rate: 0.002\nEpoch 272/1000 | Train Loss=62195.23046875 | Val Loss=3.09648895 | Data=621.81280518 | Physics=13.02848792 | Val RMSE: 2.71251321 | ‚àö(Val Loss) = 1.75968432 | Current Learning Rate: 0.002\nEpoch 273/1000 | Train Loss=59152.93164062 | Val Loss=3.11917996 | Data=591.39242554 | Physics=11.15932470 | Val RMSE: 2.71255326 | ‚àö(Val Loss) = 1.76612008 | Current Learning Rate: 0.002\nEpoch 274/1000 | Train Loss=60479.19921875 | Val Loss=3.13453531 | Data=604.65321350 | Physics=12.38398929 | Val RMSE: 2.71260738 | ‚àö(Val Loss) = 1.77046192 | Current Learning Rate: 0.002\nEpoch 275/1000 | Train Loss=60689.54687500 | Val Loss=3.09427285 | Data=606.75010681 | Physics=17.43136137 | Val RMSE: 2.71291685 | ‚àö(Val Loss) = 1.75905454 | Current Learning Rate: 0.002\nEpoch 276/1000 | Train Loss=61420.07031250 | Val Loss=3.08734608 | Data=614.06050110 | Physics=13.43109052 | Val RMSE: 2.71260762 | ‚àö(Val Loss) = 1.75708449 | Current Learning Rate: 0.002\nEpoch 277/1000 | Train Loss=65106.81445312 | Val Loss=3.18061161 | Data=650.92852783 | Physics=13.19603504 | Val RMSE: 2.71289802 | ‚àö(Val Loss) = 1.78342688 | Current Learning Rate: 0.002\nEpoch 278/1000 | Train Loss=65509.92187500 | Val Loss=3.16845036 | Data=654.95793152 | Physics=14.22487421 | Val RMSE: 2.71257544 | ‚àö(Val Loss) = 1.78001416 | Current Learning Rate: 0.002\nEpoch 279/1000 | Train Loss=63235.00390625 | Val Loss=3.13066506 | Data=632.20739746 | Physics=15.51565074 | Val RMSE: 2.71289253 | ‚àö(Val Loss) = 1.76936853 | Current Learning Rate: 0.002\nEpoch 280/1000 | Train Loss=63087.25195312 | Val Loss=3.11095548 | Data=630.73361206 | Physics=12.82308801 | Val RMSE: 2.71287060 | ‚àö(Val Loss) = 1.76379013 | Current Learning Rate: 0.002\nEpoch 281/1000 | Train Loss=63744.56835938 | Val Loss=3.10501504 | Data=637.29885864 | Physics=18.37014111 | Val RMSE: 2.71284127 | ‚àö(Val Loss) = 1.76210523 | Current Learning Rate: 0.002\n\n Epoch :  280 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 282/1000 | Train Loss=65185.52148438 | Val Loss=3.12161255 | Data=651.70928955 | Physics=17.50290472 | Val RMSE: 2.71254539 | ‚àö(Val Loss) = 1.76680863 | Current Learning Rate: 0.002\nEpoch 283/1000 | Train Loss=58389.43847656 | Val Loss=3.09139562 | Data=583.75752258 | Physics=11.32453943 | Val RMSE: 2.71285582 | ‚àö(Val Loss) = 1.75823653 | Current Learning Rate: 0.002\nEpoch 284/1000 | Train Loss=62950.27929688 | Val Loss=3.06928420 | Data=629.35833740 | Physics=16.92720420 | Val RMSE: 2.71284652 | ‚àö(Val Loss) = 1.75193727 | Current Learning Rate: 0.002\nEpoch 285/1000 | Train Loss=62450.72265625 | Val Loss=3.05089879 | Data=624.36883545 | Physics=12.14387468 | Val RMSE: 2.71255183 | ‚àö(Val Loss) = 1.74668217 | Current Learning Rate: 0.002\nEpoch 286/1000 | Train Loss=63357.85742188 | Val Loss=3.03881621 | Data=633.44012451 | Physics=11.93922362 | Val RMSE: 2.71260428 | ‚àö(Val Loss) = 1.74322009 | Current Learning Rate: 0.002\nEpoch 287/1000 | Train Loss=63834.10156250 | Val Loss=3.02503467 | Data=638.19296265 | Physics=19.08741650 | Val RMSE: 2.71263433 | ‚àö(Val Loss) = 1.73926270 | Current Learning Rate: 0.002\nEpoch 288/1000 | Train Loss=67283.22851562 | Val Loss=3.01478887 | Data=672.68731689 | Physics=16.80442409 | Val RMSE: 2.71266389 | ‚àö(Val Loss) = 1.73631477 | Current Learning Rate: 0.002\nEpoch 289/1000 | Train Loss=64115.61132812 | Val Loss=3.00424337 | Data=641.01336670 | Physics=15.16601057 | Val RMSE: 2.71269035 | ‚àö(Val Loss) = 1.73327529 | Current Learning Rate: 0.002\nEpoch 290/1000 | Train Loss=68277.38085938 | Val Loss=2.99230337 | Data=682.62950134 | Physics=16.19596299 | Val RMSE: 2.71271324 | ‚àö(Val Loss) = 1.72982752 | Current Learning Rate: 0.002\nEpoch 291/1000 | Train Loss=67653.73242188 | Val Loss=2.99776435 | Data=676.39471436 | Physics=15.25142780 | Val RMSE: 2.71300578 | ‚àö(Val Loss) = 1.73140526 | Current Learning Rate: 0.002\nEpoch 292/1000 | Train Loss=64372.32226562 | Val Loss=2.99605250 | Data=643.58319092 | Physics=13.24695135 | Val RMSE: 2.71267891 | ‚àö(Val Loss) = 1.73091090 | Current Learning Rate: 0.002\nEpoch 293/1000 | Train Loss=63311.02148438 | Val Loss=3.04234123 | Data=632.96914673 | Physics=14.12079415 | Val RMSE: 2.71298313 | ‚àö(Val Loss) = 1.74423087 | Current Learning Rate: 0.002\nEpoch 294/1000 | Train Loss=66811.20703125 | Val Loss=3.03780198 | Data=667.96707153 | Physics=17.01126265 | Val RMSE: 2.71293473 | ‚àö(Val Loss) = 1.74292910 | Current Learning Rate: 0.002\nEpoch 295/1000 | Train Loss=60366.53906250 | Val Loss=3.04939437 | Data=603.53163147 | Physics=8.98237520 | Val RMSE: 2.71288466 | ‚àö(Val Loss) = 1.74625146 | Current Learning Rate: 0.002\nEpoch 296/1000 | Train Loss=60932.62500000 | Val Loss=3.05163527 | Data=609.18992615 | Physics=10.76881586 | Val RMSE: 2.71287560 | ‚àö(Val Loss) = 1.74689305 | Current Learning Rate: 0.002\nEpoch 297/1000 | Train Loss=66354.96093750 | Val Loss=3.02485466 | Data=663.40997314 | Physics=12.82213390 | Val RMSE: 2.71257734 | ‚àö(Val Loss) = 1.73921096 | Current Learning Rate: 0.002\nEpoch 298/1000 | Train Loss=61778.85937500 | Val Loss=3.00888658 | Data=617.65002441 | Physics=12.47614889 | Val RMSE: 2.71289921 | ‚àö(Val Loss) = 1.73461425 | Current Learning Rate: 0.002\nEpoch 299/1000 | Train Loss=64376.10546875 | Val Loss=3.00793624 | Data=643.61972046 | Physics=14.39400268 | Val RMSE: 2.71287632 | ‚àö(Val Loss) = 1.73434031 | Current Learning Rate: 0.002\nEpoch 300/1000 | Train Loss=62141.28320312 | Val Loss=3.16112447 | Data=621.27020264 | Physics=15.29025019 | Val RMSE: 2.71280289 | ‚àö(Val Loss) = 1.77795517 | Current Learning Rate: 0.002\nEpoch 301/1000 | Train Loss=60786.14257812 | Val Loss=3.15347481 | Data=607.72932434 | Physics=7.71586193 | Val RMSE: 2.71250153 | ‚àö(Val Loss) = 1.77580261 | Current Learning Rate: 0.002\n\n Epoch :  300 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 302/1000 | Train Loss=64376.41015625 | Val Loss=3.13854194 | Data=643.62423706 | Physics=13.41389060 | Val RMSE: 2.71284533 | ‚àö(Val Loss) = 1.77159309 | Current Learning Rate: 0.002\nEpoch 303/1000 | Train Loss=62758.18554688 | Val Loss=3.11660814 | Data=627.43923950 | Physics=15.33033764 | Val RMSE: 2.71283102 | ‚àö(Val Loss) = 1.76539183 | Current Learning Rate: 0.002\nEpoch 304/1000 | Train Loss=65828.30664062 | Val Loss=3.29477310 | Data=658.13883972 | Physics=16.43763297 | Val RMSE: 2.71246481 | ‚àö(Val Loss) = 1.81515098 | Current Learning Rate: 0.002\nEpoch 305/1000 | Train Loss=65912.75390625 | Val Loss=3.35285711 | Data=658.98330688 | Physics=16.49662879 | Val RMSE: 2.71269989 | ‚àö(Val Loss) = 1.83108091 | Current Learning Rate: 0.002\nEpoch 306/1000 | Train Loss=67073.11718750 | Val Loss=3.29384375 | Data=670.58822632 | Physics=15.42419696 | Val RMSE: 2.71240950 | ‚àö(Val Loss) = 1.81489491 | Current Learning Rate: 0.002\nEpoch 307/1000 | Train Loss=63842.17968750 | Val Loss=3.24642420 | Data=638.28103638 | Physics=13.59133628 | Val RMSE: 2.71249437 | ‚àö(Val Loss) = 1.80178356 | Current Learning Rate: 0.002\nEpoch 308/1000 | Train Loss=66619.07812500 | Val Loss=3.22938633 | Data=666.05158997 | Physics=12.75643492 | Val RMSE: 2.71254706 | ‚àö(Val Loss) = 1.79704928 | Current Learning Rate: 0.002\nEpoch 309/1000 | Train Loss=65182.01562500 | Val Loss=3.21007776 | Data=651.68457031 | Physics=10.20240142 | Val RMSE: 2.71258330 | ‚àö(Val Loss) = 1.79166901 | Current Learning Rate: 0.002\nEpoch 310/1000 | Train Loss=62039.13867188 | Val Loss=3.52719116 | Data=620.25363159 | Physics=11.90440655 | Val RMSE: 2.71275163 | ‚àö(Val Loss) = 1.87808180 | Current Learning Rate: 0.002\nEpoch 311/1000 | Train Loss=65236.98632812 | Val Loss=3.43096423 | Data=652.22998047 | Physics=13.03837379 | Val RMSE: 2.71246171 | ‚àö(Val Loss) = 1.85228622 | Current Learning Rate: 0.002\nEpoch 312/1000 | Train Loss=59928.26757812 | Val Loss=3.39125061 | Data=599.14855957 | Physics=9.29141752 | Val RMSE: 2.71251559 | ‚àö(Val Loss) = 1.84153485 | Current Learning Rate: 0.002\nEpoch 313/1000 | Train Loss=62236.22851562 | Val Loss=3.33976054 | Data=622.22817993 | Physics=9.16027986 | Val RMSE: 2.71259594 | ‚àö(Val Loss) = 1.82750118 | Current Learning Rate: 0.002\nEpoch 314/1000 | Train Loss=62392.04882812 | Val Loss=3.27786946 | Data=623.78259277 | Physics=11.94014925 | Val RMSE: 2.71263623 | ‚àö(Val Loss) = 1.81048870 | Current Learning Rate: 0.002\nEpoch 315/1000 | Train Loss=61691.63671875 | Val Loss=3.30286837 | Data=616.77532959 | Physics=14.12646409 | Val RMSE: 2.71294880 | ‚àö(Val Loss) = 1.81737947 | Current Learning Rate: 0.002\nEpoch 316/1000 | Train Loss=63220.46289062 | Val Loss=3.22050953 | Data=632.06005859 | Physics=16.22014586 | Val RMSE: 2.71261811 | ‚àö(Val Loss) = 1.79457784 | Current Learning Rate: 0.002\nEpoch 317/1000 | Train Loss=57215.72558594 | Val Loss=3.35447621 | Data=572.02494049 | Physics=7.92089465 | Val RMSE: 2.71285176 | ‚àö(Val Loss) = 1.83152294 | Current Learning Rate: 0.002\nEpoch 318/1000 | Train Loss=64913.35937500 | Val Loss=3.28460646 | Data=648.99163818 | Physics=14.57721775 | Val RMSE: 2.71255994 | ‚àö(Val Loss) = 1.81234837 | Current Learning Rate: 0.002\nEpoch 319/1000 | Train Loss=61656.47070312 | Val Loss=3.21807075 | Data=616.42459106 | Physics=13.70923157 | Val RMSE: 2.71289325 | ‚àö(Val Loss) = 1.79389822 | Current Learning Rate: 0.002\nEpoch 320/1000 | Train Loss=63851.05078125 | Val Loss=3.16716719 | Data=638.36657715 | Physics=16.21908925 | Val RMSE: 2.71257615 | ‚àö(Val Loss) = 1.77965367 | Current Learning Rate: 0.002\nEpoch 321/1000 | Train Loss=61652.47656250 | Val Loss=3.19189119 | Data=616.38366699 | Physics=14.00498822 | Val RMSE: 2.71261954 | ‚àö(Val Loss) = 1.78658652 | Current Learning Rate: 0.002\n\n Epoch :  320 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 322/1000 | Train Loss=61426.96679688 | Val Loss=3.13135290 | Data=614.12548828 | Physics=16.70524898 | Val RMSE: 2.71292734 | ‚àö(Val Loss) = 1.76956296 | Current Learning Rate: 0.002\nEpoch 323/1000 | Train Loss=67130.86523438 | Val Loss=3.14004087 | Data=671.16714478 | Physics=14.23648396 | Val RMSE: 2.71261978 | ‚àö(Val Loss) = 1.77201605 | Current Learning Rate: 0.002\nEpoch 324/1000 | Train Loss=64868.68945312 | Val Loss=3.08765745 | Data=648.54483032 | Physics=14.80253144 | Val RMSE: 2.71263599 | ‚àö(Val Loss) = 1.75717318 | Current Learning Rate: 0.002\nEpoch 325/1000 | Train Loss=66076.31054688 | Val Loss=3.07097554 | Data=660.62368774 | Physics=12.90431165 | Val RMSE: 2.71266770 | ‚àö(Val Loss) = 1.75241995 | Current Learning Rate: 0.002\nEpoch 326/1000 | Train Loss=62436.34765625 | Val Loss=3.04859853 | Data=624.22113037 | Physics=15.28895212 | Val RMSE: 2.71297646 | ‚àö(Val Loss) = 1.74602365 | Current Learning Rate: 0.002\nEpoch 327/1000 | Train Loss=64653.36328125 | Val Loss=3.03935480 | Data=646.38970947 | Physics=16.29661504 | Val RMSE: 2.71293616 | ‚àö(Val Loss) = 1.74337459 | Current Learning Rate: 0.002\nEpoch 328/1000 | Train Loss=63369.66210938 | Val Loss=3.03362727 | Data=633.55419922 | Physics=14.97799088 | Val RMSE: 2.71262503 | ‚àö(Val Loss) = 1.74173117 | Current Learning Rate: 0.002\nEpoch 329/1000 | Train Loss=60018.03906250 | Val Loss=3.20276093 | Data=600.04209900 | Physics=12.17103895 | Val RMSE: 2.71287322 | ‚àö(Val Loss) = 1.78962588 | Current Learning Rate: 0.002\nEpoch 330/1000 | Train Loss=59781.96875000 | Val Loss=3.16729546 | Data=597.68331909 | Physics=11.09762970 | Val RMSE: 2.71285319 | ‚àö(Val Loss) = 1.77968967 | Current Learning Rate: 0.002\nEpoch 331/1000 | Train Loss=65503.62500000 | Val Loss=3.14517069 | Data=654.89212036 | Physics=16.32713771 | Val RMSE: 2.71284199 | ‚àö(Val Loss) = 1.77346289 | Current Learning Rate: 0.002\nEpoch 332/1000 | Train Loss=68239.36914062 | Val Loss=3.10016990 | Data=682.24691772 | Physics=17.94198418 | Val RMSE: 2.71254277 | ‚àö(Val Loss) = 1.76072991 | Current Learning Rate: 0.002\nEpoch 333/1000 | Train Loss=60258.69531250 | Val Loss=3.15243506 | Data=602.45130920 | Physics=10.03455821 | Val RMSE: 2.71258116 | ‚àö(Val Loss) = 1.77550983 | Current Learning Rate: 0.002\nEpoch 334/1000 | Train Loss=63744.46289062 | Val Loss=3.18715572 | Data=637.30599976 | Physics=12.23236010 | Val RMSE: 2.71260262 | ‚àö(Val Loss) = 1.78526068 | Current Learning Rate: 0.002\nEpoch 335/1000 | Train Loss=65631.07812500 | Val Loss=3.13244891 | Data=656.16348267 | Physics=18.48666305 | Val RMSE: 2.71261668 | ‚àö(Val Loss) = 1.76987255 | Current Learning Rate: 0.002\nEpoch 336/1000 | Train Loss=62826.55468750 | Val Loss=3.10675406 | Data=628.12704468 | Physics=12.45069587 | Val RMSE: 2.71293020 | ‚àö(Val Loss) = 1.76259863 | Current Learning Rate: 0.002\nEpoch 337/1000 | Train Loss=60204.67578125 | Val Loss=3.07832527 | Data=601.90548706 | Physics=14.48889353 | Val RMSE: 2.71289635 | ‚àö(Val Loss) = 1.75451565 | Current Learning Rate: 0.002\nEpoch 338/1000 | Train Loss=58549.75390625 | Val Loss=3.05546403 | Data=585.35968018 | Physics=12.04120451 | Val RMSE: 2.71287131 | ‚àö(Val Loss) = 1.74798858 | Current Learning Rate: 0.002\nEpoch 339/1000 | Train Loss=66468.07617188 | Val Loss=3.05207157 | Data=664.53350830 | Physics=18.51436275 | Val RMSE: 2.71257353 | ‚àö(Val Loss) = 1.74701786 | Current Learning Rate: 0.002\nEpoch 340/1000 | Train Loss=64374.00000000 | Val Loss=3.03871059 | Data=643.59857178 | Physics=14.58617494 | Val RMSE: 2.71289587 | ‚àö(Val Loss) = 1.74318981 | Current Learning Rate: 0.002\nEpoch 341/1000 | Train Loss=69349.07031250 | Val Loss=3.04026198 | Data=693.34393311 | Physics=18.18992233 | Val RMSE: 2.71259189 | ‚àö(Val Loss) = 1.74363470 | Current Learning Rate: 0.002\n\n Epoch :  340 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 342/1000 | Train Loss=64780.24804688 | Val Loss=3.04764485 | Data=647.66171265 | Physics=13.98337730 | Val RMSE: 2.71289825 | ‚àö(Val Loss) = 1.74575055 | Current Learning Rate: 0.002\nEpoch 343/1000 | Train Loss=61616.11914062 | Val Loss=3.03554749 | Data=616.01846313 | Physics=15.60987119 | Val RMSE: 2.71287823 | ‚àö(Val Loss) = 1.74228227 | Current Learning Rate: 0.002\nEpoch 344/1000 | Train Loss=64653.15625000 | Val Loss=3.08602142 | Data=646.39086914 | Physics=13.71814498 | Val RMSE: 2.71257830 | ‚àö(Val Loss) = 1.75670755 | Current Learning Rate: 0.002\nEpoch 345/1000 | Train Loss=62354.54687500 | Val Loss=3.14005280 | Data=623.40493774 | Physics=13.72253279 | Val RMSE: 2.71288085 | ‚àö(Val Loss) = 1.77201939 | Current Learning Rate: 0.002\nEpoch 346/1000 | Train Loss=66449.18750000 | Val Loss=3.25569153 | Data=664.34602356 | Physics=17.52571638 | Val RMSE: 2.71253443 | ‚àö(Val Loss) = 1.80435348 | Current Learning Rate: 0.002\nEpoch 347/1000 | Train Loss=63812.50195312 | Val Loss=3.38784599 | Data=637.98486328 | Physics=13.55995006 | Val RMSE: 2.71247315 | ‚àö(Val Loss) = 1.84061027 | Current Learning Rate: 0.002\nEpoch 348/1000 | Train Loss=61647.56250000 | Val Loss=3.38168001 | Data=616.33264160 | Physics=16.04610440 | Val RMSE: 2.71282673 | ‚àö(Val Loss) = 1.83893442 | Current Learning Rate: 0.002\nEpoch 349/1000 | Train Loss=61807.25390625 | Val Loss=3.29854226 | Data=617.93661499 | Physics=10.07177041 | Val RMSE: 2.71252656 | ‚àö(Val Loss) = 1.81618893 | Current Learning Rate: 0.002\nEpoch 350/1000 | Train Loss=63752.33203125 | Val Loss=3.28677964 | Data=637.38677979 | Physics=10.90357997 | Val RMSE: 2.71257520 | ‚àö(Val Loss) = 1.81294775 | Current Learning Rate: 0.002\nEpoch 351/1000 | Train Loss=63326.60937500 | Val Loss=3.23677993 | Data=633.12567139 | Physics=13.57538398 | Val RMSE: 2.71261477 | ‚àö(Val Loss) = 1.79910529 | Current Learning Rate: 0.002\nEpoch 352/1000 | Train Loss=64355.72070312 | Val Loss=3.18552446 | Data=643.42147827 | Physics=10.20416715 | Val RMSE: 2.71265554 | ‚àö(Val Loss) = 1.78480375 | Current Learning Rate: 0.002\nEpoch 353/1000 | Train Loss=65202.54492188 | Val Loss=3.14118671 | Data=651.88412476 | Physics=14.11129426 | Val RMSE: 2.71268749 | ‚àö(Val Loss) = 1.77233934 | Current Learning Rate: 0.002\nEpoch 354/1000 | Train Loss=61498.27343750 | Val Loss=3.09756064 | Data=614.84143066 | Physics=14.66009102 | Val RMSE: 2.71299028 | ‚àö(Val Loss) = 1.75998878 | Current Learning Rate: 0.002\nEpoch 355/1000 | Train Loss=62728.50585938 | Val Loss=3.07211781 | Data=627.13757324 | Physics=18.96642820 | Val RMSE: 2.71295071 | ‚àö(Val Loss) = 1.75274575 | Current Learning Rate: 0.002\nEpoch 356/1000 | Train Loss=63958.11523438 | Val Loss=3.06075430 | Data=639.43685913 | Physics=16.36473243 | Val RMSE: 2.71263194 | ‚àö(Val Loss) = 1.74950111 | Current Learning Rate: 0.002\nEpoch 357/1000 | Train Loss=58362.79785156 | Val Loss=3.04085970 | Data=583.48753357 | Physics=13.89328012 | Val RMSE: 2.71294045 | ‚àö(Val Loss) = 1.74380612 | Current Learning Rate: 0.002\nEpoch 358/1000 | Train Loss=64142.44335938 | Val Loss=3.08304238 | Data=641.28546143 | Physics=12.67694985 | Val RMSE: 2.71289921 | ‚àö(Val Loss) = 1.75585949 | Current Learning Rate: 0.002\nEpoch 359/1000 | Train Loss=60136.94335938 | Val Loss=3.08787465 | Data=601.22994995 | Physics=13.14693776 | Val RMSE: 2.71286297 | ‚àö(Val Loss) = 1.75723493 | Current Learning Rate: 0.002\nEpoch 360/1000 | Train Loss=62820.55859375 | Val Loss=3.09603238 | Data=628.06518555 | Physics=13.56174303 | Val RMSE: 2.71256590 | ‚àö(Val Loss) = 1.75955462 | Current Learning Rate: 0.002\nEpoch 361/1000 | Train Loss=66279.69921875 | Val Loss=3.07966709 | Data=662.65872192 | Physics=11.98564432 | Val RMSE: 2.71260929 | ‚àö(Val Loss) = 1.75489807 | Current Learning Rate: 0.002\n\n Epoch :  360 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 362/1000 | Train Loss=65481.04882812 | Val Loss=3.06876779 | Data=654.66784668 | Physics=15.36188670 | Val RMSE: 2.71292543 | ‚àö(Val Loss) = 1.75178993 | Current Learning Rate: 0.002\nEpoch 363/1000 | Train Loss=62960.51757812 | Val Loss=3.05633116 | Data=629.46664429 | Physics=12.12250903 | Val RMSE: 2.71261311 | ‚àö(Val Loss) = 1.74823654 | Current Learning Rate: 0.002\nEpoch 364/1000 | Train Loss=65613.76171875 | Val Loss=3.02895188 | Data=655.99801636 | Physics=12.90870585 | Val RMSE: 2.71264648 | ‚àö(Val Loss) = 1.74038839 | Current Learning Rate: 0.002\nEpoch 365/1000 | Train Loss=62965.81250000 | Val Loss=3.02523994 | Data=629.51379395 | Physics=16.35466191 | Val RMSE: 2.71267176 | ‚àö(Val Loss) = 1.73932171 | Current Learning Rate: 0.002\nEpoch 366/1000 | Train Loss=62093.81054688 | Val Loss=3.08914685 | Data=620.80081177 | Physics=11.48477867 | Val RMSE: 2.71294856 | ‚àö(Val Loss) = 1.75759685 | Current Learning Rate: 0.002\nEpoch 367/1000 | Train Loss=63634.22070312 | Val Loss=3.12376738 | Data=636.19897461 | Physics=15.60003047 | Val RMSE: 2.71263027 | ‚àö(Val Loss) = 1.76741827 | Current Learning Rate: 0.002\nEpoch 368/1000 | Train Loss=63712.78320312 | Val Loss=3.13721943 | Data=636.98400879 | Physics=16.12348424 | Val RMSE: 2.71294117 | ‚àö(Val Loss) = 1.77121973 | Current Learning Rate: 0.002\nEpoch 369/1000 | Train Loss=64397.72656250 | Val Loss=3.12512088 | Data=643.83471680 | Physics=15.13068329 | Val RMSE: 2.71261191 | ‚àö(Val Loss) = 1.76780117 | Current Learning Rate: 0.002\nEpoch 370/1000 | Train Loss=60941.88281250 | Val Loss=3.09377050 | Data=609.27787781 | Physics=14.33047976 | Val RMSE: 2.71292663 | ‚àö(Val Loss) = 1.75891173 | Current Learning Rate: 0.002\nEpoch 371/1000 | Train Loss=65220.82812500 | Val Loss=3.08851314 | Data=652.06802368 | Physics=13.56266506 | Val RMSE: 2.71260858 | ‚àö(Val Loss) = 1.75741661 | Current Learning Rate: 0.002\nEpoch 372/1000 | Train Loss=66053.49804688 | Val Loss=3.09261489 | Data=660.39080811 | Physics=16.47663965 | Val RMSE: 2.71292710 | ‚àö(Val Loss) = 1.75858319 | Current Learning Rate: 0.002\nEpoch 373/1000 | Train Loss=60509.74804688 | Val Loss=3.11599684 | Data=604.95953369 | Physics=12.02138795 | Val RMSE: 2.71285176 | ‚àö(Val Loss) = 1.76521862 | Current Learning Rate: 0.002\nEpoch 374/1000 | Train Loss=65079.28515625 | Val Loss=3.14741254 | Data=650.65484619 | Physics=11.86687179 | Val RMSE: 2.71256566 | ‚àö(Val Loss) = 1.77409482 | Current Learning Rate: 0.002\nEpoch 375/1000 | Train Loss=66295.79296875 | Val Loss=3.13438225 | Data=662.81253052 | Physics=17.28489292 | Val RMSE: 2.71289539 | ‚àö(Val Loss) = 1.77041864 | Current Learning Rate: 0.002\nEpoch 376/1000 | Train Loss=64052.54101562 | Val Loss=3.18777919 | Data=640.39190674 | Physics=8.58628951 | Val RMSE: 2.71257257 | ‚àö(Val Loss) = 1.78543532 | Current Learning Rate: 0.002\nEpoch 377/1000 | Train Loss=60762.18554688 | Val Loss=3.13578963 | Data=607.48832703 | Physics=8.84438323 | Val RMSE: 2.71287990 | ‚àö(Val Loss) = 1.77081609 | Current Learning Rate: 0.002\nEpoch 378/1000 | Train Loss=62117.83593750 | Val Loss=3.09887123 | Data=621.04312134 | Physics=9.66971985 | Val RMSE: 2.71257758 | ‚àö(Val Loss) = 1.76036108 | Current Learning Rate: 0.002\nEpoch 379/1000 | Train Loss=68580.75000000 | Val Loss=3.06844068 | Data=685.66119385 | Physics=17.90414329 | Val RMSE: 2.71261549 | ‚àö(Val Loss) = 1.75169647 | Current Learning Rate: 0.002\nEpoch 380/1000 | Train Loss=61981.15039062 | Val Loss=3.07258177 | Data=619.67208862 | Physics=12.70824232 | Val RMSE: 2.71265388 | ‚àö(Val Loss) = 1.75287819 | Current Learning Rate: 0.002\nEpoch 381/1000 | Train Loss=63983.03710938 | Val Loss=3.06162500 | Data=639.68881226 | Physics=14.53169449 | Val RMSE: 2.71296287 | ‚àö(Val Loss) = 1.74975002 | Current Learning Rate: 0.002\n\n Epoch :  380 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\nEpoch 382/1000 | Train Loss=60918.19921875 | Val Loss=3.06067061 | Data=609.03842163 | Physics=15.93250527 | Val RMSE: 2.71292567 | ‚àö(Val Loss) = 1.74947727 | Current Learning Rate: 0.002\nEpoch 383/1000 | Train Loss=64943.64257812 | Val Loss=3.04003763 | Data=649.29632568 | Physics=13.57140043 | Val RMSE: 2.71289611 | ‚àö(Val Loss) = 1.74357033 | Current Learning Rate: 0.002\nEpoch 384/1000 | Train Loss=63636.41601562 | Val Loss=3.02559352 | Data=636.22314453 | Physics=14.01778694 | Val RMSE: 2.71259165 | ‚àö(Val Loss) = 1.73942327 | Current Learning Rate: 0.002\nEpoch 385/1000 | Train Loss=67819.88671875 | Val Loss=3.02039528 | Data=678.04971313 | Physics=19.99401410 | Val RMSE: 2.71290684 | ‚àö(Val Loss) = 1.73792839 | Current Learning Rate: 0.002\nEpoch 386/1000 | Train Loss=66860.54687500 | Val Loss=3.03418636 | Data=668.45719910 | Physics=19.11639986 | Val RMSE: 2.71260285 | ‚àö(Val Loss) = 1.74189162 | Current Learning Rate: 0.002\nEpoch 387/1000 | Train Loss=62591.27929688 | Val Loss=3.03382683 | Data=625.77755737 | Physics=9.99516282 | Val RMSE: 2.71291804 | ‚àö(Val Loss) = 1.74178839 | Current Learning Rate: 0.002\nEpoch 388/1000 | Train Loss=59634.81640625 | Val Loss=3.08874750 | Data=596.21122742 | Physics=11.01650475 | Val RMSE: 2.71260333 | ‚àö(Val Loss) = 1.75748324 | Current Learning Rate: 0.002\nEpoch 389/1000 | Train Loss=64294.96093750 | Val Loss=3.52390409 | Data=642.80679321 | Physics=15.47858731 | Val RMSE: 2.71245098 | ‚àö(Val Loss) = 1.87720644 | Current Learning Rate: 0.002\nEpoch 390/1000 | Train Loss=63423.05859375 | Val Loss=4.18676090 | Data=634.08718872 | Physics=15.66054770 | Val RMSE: 2.71258044 | ‚àö(Val Loss) = 2.04615760 | Current Learning Rate: 0.002\nEpoch 391/1000 | Train Loss=62945.20507812 | Val Loss=3.82849216 | Data=629.31967163 | Physics=7.44643226 | Val RMSE: 2.71225858 | ‚àö(Val Loss) = 1.95665336 | Current Learning Rate: 0.002\nEpoch 392/1000 | Train Loss=62450.56835938 | Val Loss=3.63957405 | Data=624.36508179 | Physics=14.00486774 | Val RMSE: 2.71268082 | ‚àö(Val Loss) = 1.90776682 | Current Learning Rate: 0.002\nEpoch 393/1000 | Train Loss=60545.63476562 | Val Loss=3.50438452 | Data=605.31935120 | Physics=11.57884332 | Val RMSE: 2.71265721 | ‚àö(Val Loss) = 1.87200010 | Current Learning Rate: 0.002\nEpoch 394/1000 | Train Loss=61582.42382812 | Val Loss=3.43712330 | Data=615.68286133 | Physics=14.33813935 | Val RMSE: 2.71237612 | ‚àö(Val Loss) = 1.85394800 | Current Learning Rate: 0.002\nEpoch 395/1000 | Train Loss=59513.11132812 | Val Loss=3.35781598 | Data=594.99099731 | Physics=13.78983555 | Val RMSE: 2.71274114 | ‚àö(Val Loss) = 1.83243442 | Current Learning Rate: 0.002\nEpoch 396/1000 | Train Loss=66369.17382812 | Val Loss=3.28119874 | Data=663.54536438 | Physics=17.51880094 | Val RMSE: 2.71246743 | ‚àö(Val Loss) = 1.81140792 | Current Learning Rate: 0.002\nEpoch 397/1000 | Train Loss=64910.75781250 | Val Loss=3.20300841 | Data=648.96234131 | Physics=17.50765654 | Val RMSE: 2.71280384 | ‚àö(Val Loss) = 1.78969502 | Current Learning Rate: 0.002\nEpoch 398/1000 | Train Loss=62585.58007812 | Val Loss=3.20677423 | Data=625.71173096 | Physics=16.39478722 | Val RMSE: 2.71280146 | ‚àö(Val Loss) = 1.79074681 | Current Learning Rate: 0.002\nEpoch 399/1000 | Train Loss=63852.76562500 | Val Loss=3.16140389 | Data=638.38708496 | Physics=14.04055713 | Val RMSE: 2.71278572 | ‚àö(Val Loss) = 1.77803373 | Current Learning Rate: 0.002\nEpoch 400/1000 | Train Loss=67292.22460938 | Val Loss=3.15558910 | Data=672.77461243 | Physics=18.70991539 | Val RMSE: 2.71250176 | ‚àö(Val Loss) = 1.77639782 | Current Learning Rate: 0.0002\nEpoch 401/1000 | Train Loss=59599.74218750 | Val Loss=2.92984915 | Data=595.85929871 | Physics=11.99581501 | Val RMSE: 2.71283674 | ‚àö(Val Loss) = 1.71168017 | Current Learning Rate: 0.0002\n\n Epoch :  400 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9945713   -3.6943538  -14.185797  ]\n [  0.9946237   -3.6943629  -14.18578   ]\n [  0.99450994  -3.6943436  -14.185818  ]\n [  0.99454737  -3.6943498  -14.185805  ]\n [  0.99455655  -3.6943514  -14.1858015 ]\n [  0.9944594   -3.694335   -14.185835  ]\n [  0.99437517  -3.6943207  -14.1858635 ]\n [  0.99457335  -3.6943543  -14.185797  ]\n [  0.994371    -3.69432    -14.185864  ]\n [  0.9944568   -3.6943345  -14.185836  ]\n [  0.9943617   -3.6943183  -14.185868  ]] \n\nFinal Test RMSE:  1.6700115203857422\n‚úÖ Saved best model at epoch 402 (Val Loss = 2.77294493)\nEpoch 402/1000 | Train Loss=1660.10949707 | Val Loss=2.77294493 | Data=16.46449757 | Physics=10.96162133 | Val RMSE: 2.71280432 | ‚àö(Val Loss) = 1.66521621 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 403 (Val Loss = 2.66908741)\nEpoch 403/1000 | Train Loss=1570.54779053 | Val Loss=2.66908741 | Data=15.56656551 | Physics=11.87451976 | Val RMSE: 2.71277785 | ‚àö(Val Loss) = 1.63373423 | Current Learning Rate: 0.0002\nEpoch 404/1000 | Train Loss=1539.70220947 | Val Loss=2.69745159 | Data=15.24731445 | Physics=17.97181724 | Val RMSE: 2.71276164 | ‚àö(Val Loss) = 1.64239204 | Current Learning Rate: 0.0002\nEpoch 405/1000 | Train Loss=1724.74176025 | Val Loss=2.68581629 | Data=17.10145712 | Physics=15.23050333 | Val RMSE: 2.71275949 | ‚àö(Val Loss) = 1.63884604 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 406 (Val Loss = 2.66836238)\nEpoch 406/1000 | Train Loss=1727.71746826 | Val Loss=2.66836238 | Data=17.13084126 | Physics=15.70824113 | Val RMSE: 2.71275711 | ‚àö(Val Loss) = 1.63351226 | Current Learning Rate: 0.0002\nEpoch 407/1000 | Train Loss=1575.79144287 | Val Loss=2.69078898 | Data=15.61513376 | Physics=12.36029855 | Val RMSE: 2.71273828 | ‚àö(Val Loss) = 1.64036250 | Current Learning Rate: 0.0002\nEpoch 408/1000 | Train Loss=1527.27767944 | Val Loss=2.68844151 | Data=15.12683916 | Physics=14.60883783 | Val RMSE: 2.71278691 | ‚àö(Val Loss) = 1.63964677 | Current Learning Rate: 0.0002\nEpoch 409/1000 | Train Loss=1779.53796387 | Val Loss=2.68321943 | Data=17.65223837 | Physics=12.75300442 | Val RMSE: 2.71278620 | ‚àö(Val Loss) = 1.63805354 | Current Learning Rate: 0.0002\nEpoch 410/1000 | Train Loss=1546.63287354 | Val Loss=2.67147708 | Data=15.32449150 | Physics=11.85519035 | Val RMSE: 2.71277595 | ‚àö(Val Loss) = 1.63446534 | Current Learning Rate: 0.0002\nEpoch 411/1000 | Train Loss=1725.52233887 | Val Loss=2.67875671 | Data=17.10961676 | Physics=14.49008278 | Val RMSE: 2.71278286 | ‚àö(Val Loss) = 1.63669074 | Current Learning Rate: 0.0002\nEpoch 412/1000 | Train Loss=1739.01708984 | Val Loss=2.67613721 | Data=17.24957991 | Physics=10.94049313 | Val RMSE: 2.71277666 | ‚àö(Val Loss) = 1.63589036 | Current Learning Rate: 0.0002\nEpoch 413/1000 | Train Loss=1736.96746826 | Val Loss=2.67491436 | Data=17.22006702 | Physics=17.33390882 | Val RMSE: 2.71277738 | ‚àö(Val Loss) = 1.63551652 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 414 (Val Loss = 2.66760731)\nEpoch 414/1000 | Train Loss=1567.53900146 | Val Loss=2.66760731 | Data=15.52959490 | Physics=14.67365297 | Val RMSE: 2.71276784 | ‚àö(Val Loss) = 1.63328111 | Current Learning Rate: 0.0002\nEpoch 415/1000 | Train Loss=1573.01574707 | Val Loss=2.66770506 | Data=15.58980322 | Physics=10.97554545 | Val RMSE: 2.71277905 | ‚àö(Val Loss) = 1.63331103 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 416 (Val Loss = 2.66746473)\nEpoch 416/1000 | Train Loss=1515.19552612 | Val Loss=2.66746473 | Data=15.00755239 | Physics=13.94751694 | Val RMSE: 2.71277690 | ‚àö(Val Loss) = 1.63323748 | Current Learning Rate: 0.0002\nEpoch 417/1000 | Train Loss=1755.05419922 | Val Loss=2.67109442 | Data=17.40464020 | Physics=15.01805728 | Val RMSE: 2.71278691 | ‚àö(Val Loss) = 1.63434827 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 418 (Val Loss = 2.66634893)\nEpoch 418/1000 | Train Loss=1540.46267700 | Val Loss=2.66634893 | Data=15.25666142 | Physics=16.59739994 | Val RMSE: 2.71277833 | ‚àö(Val Loss) = 1.63289583 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 419 (Val Loss = 2.66626120)\nEpoch 419/1000 | Train Loss=1534.14248657 | Val Loss=2.66626120 | Data=15.19387484 | Physics=16.38559375 | Val RMSE: 2.71278429 | ‚àö(Val Loss) = 1.63286901 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 420 (Val Loss = 2.66533017)\nEpoch 420/1000 | Train Loss=1566.40814209 | Val Loss=2.66533017 | Data=15.52367020 | Physics=11.39539494 | Val RMSE: 2.71278119 | ‚àö(Val Loss) = 1.63258386 | Current Learning Rate: 0.0002\nEpoch 421/1000 | Train Loss=1740.80633545 | Val Loss=2.66644454 | Data=17.26323318 | Physics=14.55803000 | Val RMSE: 2.71278429 | ‚àö(Val Loss) = 1.63292515 | Current Learning Rate: 0.0002\n\n Epoch :  420 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.97797495  -3.6962962  -14.185288  ]\n [  0.97801864  -3.696295   -14.185266  ]\n [  0.9779235   -3.696298   -14.185315  ]\n [  0.97795486  -3.696297   -14.185299  ]\n [  0.9779627   -3.6962967  -14.185294  ]\n [  0.9778813   -3.696299   -14.185337  ]\n [  0.9778111   -3.696301   -14.185373  ]\n [  0.9779765   -3.6962965  -14.185287  ]\n [  0.97780764  -3.696301   -14.185375  ]\n [  0.97787935  -3.6962988  -14.185338  ]\n [  0.9777996   -3.6963015  -14.18538   ]] \n\nFinal Test RMSE:  1.6668376922607422\n‚úÖ Saved best model at epoch 422 (Val Loss = 2.66326690)\nEpoch 422/1000 | Train Loss=1499.90356445 | Val Loss=2.66326690 | Data=14.86091375 | Physics=9.82427734 | Val RMSE: 2.71276879 | ‚àö(Val Loss) = 1.63195181 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 423 (Val Loss = 2.66315651)\nEpoch 423/1000 | Train Loss=1702.31158447 | Val Loss=2.66315651 | Data=16.88014841 | Physics=13.24014195 | Val RMSE: 2.71276617 | ‚àö(Val Loss) = 1.63191807 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 424 (Val Loss = 2.66290474)\nEpoch 424/1000 | Train Loss=1760.63937378 | Val Loss=2.66290474 | Data=17.46237469 | Physics=13.91001680 | Val RMSE: 2.71276116 | ‚àö(Val Loss) = 1.63184094 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 425 (Val Loss = 2.66247034)\nEpoch 425/1000 | Train Loss=1695.46087646 | Val Loss=2.66247034 | Data=16.81344509 | Physics=11.79407259 | Val RMSE: 2.71275449 | ‚àö(Val Loss) = 1.63170779 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 426 (Val Loss = 2.66226435)\nEpoch 426/1000 | Train Loss=1524.78433228 | Val Loss=2.66226435 | Data=15.09897137 | Physics=17.17510129 | Val RMSE: 2.71275043 | ‚àö(Val Loss) = 1.63164473 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 427 (Val Loss = 2.66218615)\nEpoch 427/1000 | Train Loss=1678.31976318 | Val Loss=2.66218615 | Data=16.64311123 | Physics=10.91315404 | Val RMSE: 2.71274710 | ‚àö(Val Loss) = 1.63162076 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 428 (Val Loss = 2.65877533)\nEpoch 428/1000 | Train Loss=1587.66876221 | Val Loss=2.65877533 | Data=15.73157454 | Physics=14.50746331 | Val RMSE: 2.71273184 | ‚àö(Val Loss) = 1.63057518 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 429 (Val Loss = 2.65635419)\nEpoch 429/1000 | Train Loss=1521.81832886 | Val Loss=2.65635419 | Data=15.07591248 | Physics=12.54435094 | Val RMSE: 2.71271801 | ‚àö(Val Loss) = 1.62983251 | Current Learning Rate: 0.0002\nEpoch 430/1000 | Train Loss=1494.59881592 | Val Loss=2.65844727 | Data=14.80125809 | Physics=14.38538611 | Val RMSE: 2.71273780 | ‚àö(Val Loss) = 1.63047457 | Current Learning Rate: 0.0002\nEpoch 431/1000 | Train Loss=1679.77691650 | Val Loss=2.65963602 | Data=16.65639782 | Physics=11.90812856 | Val RMSE: 2.71274018 | ‚àö(Val Loss) = 1.63083911 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 432 (Val Loss = 2.65629864)\nEpoch 432/1000 | Train Loss=1568.91882324 | Val Loss=2.65629864 | Data=15.54292011 | Physics=15.48209531 | Val RMSE: 2.71272063 | ‚àö(Val Loss) = 1.62981558 | Current Learning Rate: 0.0002\nEpoch 433/1000 | Train Loss=1724.58721924 | Val Loss=2.65797520 | Data=17.09434986 | Physics=19.14095525 | Val RMSE: 2.71273255 | ‚àö(Val Loss) = 1.63032973 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 434 (Val Loss = 2.65569973)\nEpoch 434/1000 | Train Loss=1577.02520752 | Val Loss=2.65569973 | Data=15.62413931 | Physics=15.40582906 | Val RMSE: 2.71271706 | ‚àö(Val Loss) = 1.62963176 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 435 (Val Loss = 2.65380073)\nEpoch 435/1000 | Train Loss=1565.47430420 | Val Loss=2.65380073 | Data=15.51396942 | Physics=11.67092945 | Val RMSE: 2.71270442 | ‚àö(Val Loss) = 1.62904906 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 436 (Val Loss = 2.65306783)\nEpoch 436/1000 | Train Loss=1705.47607422 | Val Loss=2.65306783 | Data=16.91122150 | Physics=13.52895428 | Val RMSE: 2.71270323 | ‚àö(Val Loss) = 1.62882411 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 437 (Val Loss = 2.65057468)\nEpoch 437/1000 | Train Loss=1558.55163574 | Val Loss=2.65057468 | Data=15.43385363 | Physics=19.32502687 | Val RMSE: 2.71268749 | ‚àö(Val Loss) = 1.62805855 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 438 (Val Loss = 2.64946747)\nEpoch 438/1000 | Train Loss=1693.49530029 | Val Loss=2.64946747 | Data=16.78675842 | Physics=16.75270508 | Val RMSE: 2.71268344 | ‚àö(Val Loss) = 1.62771845 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 439 (Val Loss = 2.64675021)\nEpoch 439/1000 | Train Loss=1506.36929321 | Val Loss=2.64675021 | Data=14.92587090 | Physics=9.33314472 | Val RMSE: 2.71266413 | ‚àö(Val Loss) = 1.62688363 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 440 (Val Loss = 2.64580297)\nEpoch 440/1000 | Train Loss=1551.65478516 | Val Loss=2.64580297 | Data=15.37589407 | Physics=11.35581483 | Val RMSE: 2.71265435 | ‚àö(Val Loss) = 1.62659240 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 441 (Val Loss = 2.64500642)\nEpoch 441/1000 | Train Loss=1512.81121826 | Val Loss=2.64500642 | Data=14.98570561 | Physics=12.61064129 | Val RMSE: 2.71265197 | ‚àö(Val Loss) = 1.62634754 | Current Learning Rate: 0.0002\n\n Epoch :  440 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9786402   -3.6887782  -14.185052  ]\n [  0.97878     -3.6887026  -14.185023  ]\n [  0.9784753   -3.6888676  -14.185085  ]\n [  0.9785759   -3.688813   -14.185065  ]\n [  0.97860104  -3.6887994  -14.18506   ]\n [  0.9783402   -3.6889408  -14.185114  ]\n [  0.97811574  -3.689062   -14.18516   ]\n [  0.9786448   -3.6887758  -14.185051  ]\n [  0.97810477  -3.689068   -14.185162  ]\n [  0.97833437  -3.6889439  -14.185115  ]\n [  0.97807825  -3.6890824  -14.185167  ]] \n\nFinal Test RMSE:  1.667851448059082\nEpoch 442/1000 | Train Loss=1545.23590088 | Val Loss=2.64682531 | Data=15.30804539 | Physics=13.87466987 | Val RMSE: 2.71266437 | ‚àö(Val Loss) = 1.62690663 | Current Learning Rate: 0.0002\nEpoch 443/1000 | Train Loss=1595.83801270 | Val Loss=2.65237570 | Data=15.81275845 | Physics=14.79254914 | Val RMSE: 2.71268177 | ‚àö(Val Loss) = 1.62861156 | Current Learning Rate: 0.0002\nEpoch 444/1000 | Train Loss=1516.77191162 | Val Loss=2.64986944 | Data=15.02452660 | Physics=13.09154350 | Val RMSE: 2.71267748 | ‚àö(Val Loss) = 1.62784195 | Current Learning Rate: 0.0002\nEpoch 445/1000 | Train Loss=1742.64581299 | Val Loss=2.65184164 | Data=17.27778435 | Physics=16.78523082 | Val RMSE: 2.71268392 | ‚àö(Val Loss) = 1.62844765 | Current Learning Rate: 0.0002\nEpoch 446/1000 | Train Loss=1477.76870728 | Val Loss=2.64819527 | Data=14.63785958 | Physics=10.63753939 | Val RMSE: 2.71265960 | ‚àö(Val Loss) = 1.62732768 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 447 (Val Loss = 2.64287329)\nEpoch 447/1000 | Train Loss=1605.66076660 | Val Loss=2.64287329 | Data=15.90626335 | Physics=18.17047112 | Val RMSE: 2.71262956 | ‚àö(Val Loss) = 1.62569165 | Current Learning Rate: 0.0002\nEpoch 448/1000 | Train Loss=1803.24960327 | Val Loss=2.64584184 | Data=17.88179564 | Physics=18.09047308 | Val RMSE: 2.71262383 | ‚àö(Val Loss) = 1.62660444 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 449 (Val Loss = 2.64144158)\nEpoch 449/1000 | Train Loss=1535.16793823 | Val Loss=2.64144158 | Data=15.20835972 | Physics=12.95611390 | Val RMSE: 2.71261120 | ‚àö(Val Loss) = 1.62525129 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 450 (Val Loss = 2.64047194)\nEpoch 450/1000 | Train Loss=1560.55993652 | Val Loss=2.64047194 | Data=15.46152258 | Physics=13.36659993 | Val RMSE: 2.71259594 | ‚àö(Val Loss) = 1.62495291 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 451 (Val Loss = 2.63978052)\nEpoch 451/1000 | Train Loss=1541.77676392 | Val Loss=2.63978052 | Data=15.27090311 | Physics=15.24200172 | Val RMSE: 2.71258354 | ‚àö(Val Loss) = 1.62474012 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 452 (Val Loss = 2.63957596)\nEpoch 452/1000 | Train Loss=1587.55517578 | Val Loss=2.63957596 | Data=15.72279930 | Physics=19.34795575 | Val RMSE: 2.71257663 | ‚àö(Val Loss) = 1.62467718 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 453 (Val Loss = 2.63944101)\nEpoch 453/1000 | Train Loss=1521.34390259 | Val Loss=2.63944101 | Data=15.06878471 | Physics=13.54189191 | Val RMSE: 2.71257138 | ‚àö(Val Loss) = 1.62463570 | Current Learning Rate: 0.0002\nEpoch 454/1000 | Train Loss=1541.22021484 | Val Loss=2.63973117 | Data=15.26868916 | Physics=12.70334890 | Val RMSE: 2.71257043 | ‚àö(Val Loss) = 1.62472498 | Current Learning Rate: 0.0002\nEpoch 455/1000 | Train Loss=1592.02282715 | Val Loss=2.64003491 | Data=15.76855898 | Physics=18.49966162 | Val RMSE: 2.71257353 | ‚àö(Val Loss) = 1.62481844 | Current Learning Rate: 0.0002\nEpoch 456/1000 | Train Loss=1770.14816284 | Val Loss=2.64497209 | Data=17.55824757 | Physics=12.42097611 | Val RMSE: 2.71258950 | ‚àö(Val Loss) = 1.62633705 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 457 (Val Loss = 2.63864207)\nEpoch 457/1000 | Train Loss=1487.42181396 | Val Loss=2.63864207 | Data=14.72554159 | Physics=16.40721908 | Val RMSE: 2.71256852 | ‚àö(Val Loss) = 1.62438977 | Current Learning Rate: 0.0002\nEpoch 458/1000 | Train Loss=1664.45532227 | Val Loss=2.63932943 | Data=16.50839233 | Physics=7.52924840 | Val RMSE: 2.71256375 | ‚àö(Val Loss) = 1.62460136 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 459 (Val Loss = 2.63565826)\nEpoch 459/1000 | Train Loss=1581.97473145 | Val Loss=2.63565826 | Data=15.66946697 | Physics=17.57726320 | Val RMSE: 2.71255136 | ‚àö(Val Loss) = 1.62347102 | Current Learning Rate: 0.0002\nEpoch 460/1000 | Train Loss=1696.47558594 | Val Loss=2.63689232 | Data=16.81631088 | Physics=16.28358243 | Val RMSE: 2.71255469 | ‚àö(Val Loss) = 1.62385106 | Current Learning Rate: 0.0002\nEpoch 461/1000 | Train Loss=1696.40484619 | Val Loss=2.63680363 | Data=16.82057714 | Physics=12.77339326 | Val RMSE: 2.71255565 | ‚àö(Val Loss) = 1.62382376 | Current Learning Rate: 0.0002\n\n Epoch :  460 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9774804   -3.684147   -14.184716  ]\n [  0.97767615  -3.6840258  -14.184681  ]\n [  0.9772486   -3.68429    -14.184758  ]\n [  0.97739005  -3.6842027  -14.184732  ]\n [  0.9774256   -3.6841807  -14.184726  ]\n [  0.9770594   -3.6844068  -14.1847925 ]\n [  0.97674483  -3.684601   -14.184849  ]\n [  0.9774865   -3.6841428  -14.184715  ]\n [  0.9767295   -3.6846106  -14.184852  ]\n [  0.97705156  -3.6844118  -14.184793  ]\n [  0.9766918   -3.6846337  -14.184859  ]] \n\nFinal Test RMSE:  1.6683921813964844\nEpoch 462/1000 | Train Loss=1549.32934570 | Val Loss=2.63626432 | Data=15.35490513 | Physics=9.17657598 | Val RMSE: 2.71255517 | ‚àö(Val Loss) = 1.62365770 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 463 (Val Loss = 2.63361216)\nEpoch 463/1000 | Train Loss=1544.99340820 | Val Loss=2.63361216 | Data=15.31007481 | Physics=10.29759568 | Val RMSE: 2.71253943 | ‚àö(Val Loss) = 1.62284076 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 464 (Val Loss = 2.63253355)\nEpoch 464/1000 | Train Loss=1562.85986328 | Val Loss=2.63253355 | Data=15.48425484 | Physics=13.58358322 | Val RMSE: 2.71252847 | ‚àö(Val Loss) = 1.62250841 | Current Learning Rate: 0.0002\nEpoch 465/1000 | Train Loss=1767.11013794 | Val Loss=2.63316226 | Data=17.53036785 | Physics=11.16049084 | Val RMSE: 2.71253562 | ‚àö(Val Loss) = 1.62270212 | Current Learning Rate: 0.0002\nEpoch 466/1000 | Train Loss=1735.82421875 | Val Loss=2.63297343 | Data=17.20802879 | Physics=17.87241345 | Val RMSE: 2.71253395 | ‚àö(Val Loss) = 1.62264395 | Current Learning Rate: 0.0002\nEpoch 467/1000 | Train Loss=1682.92590332 | Val Loss=2.63270760 | Data=16.68359566 | Physics=14.63450520 | Val RMSE: 2.71253133 | ‚àö(Val Loss) = 1.62256205 | Current Learning Rate: 0.0002\nEpoch 468/1000 | Train Loss=1769.99114990 | Val Loss=2.63255119 | Data=17.55382109 | Physics=14.95097599 | Val RMSE: 2.71252942 | ‚àö(Val Loss) = 1.62251389 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 469 (Val Loss = 2.63145733)\nEpoch 469/1000 | Train Loss=1573.84564209 | Val Loss=2.63145733 | Data=15.58941412 | Physics=17.10193802 | Val RMSE: 2.71250916 | ‚àö(Val Loss) = 1.62217677 | Current Learning Rate: 0.0002\nEpoch 470/1000 | Train Loss=1505.01007080 | Val Loss=2.63155437 | Data=14.90402293 | Physics=15.04590301 | Val RMSE: 2.71252322 | ‚àö(Val Loss) = 1.62220669 | Current Learning Rate: 0.0002\nEpoch 471/1000 | Train Loss=1745.41748047 | Val Loss=2.63244438 | Data=17.30451107 | Physics=17.51359485 | Val RMSE: 2.71252990 | ‚àö(Val Loss) = 1.62248099 | Current Learning Rate: 0.0002\nEpoch 472/1000 | Train Loss=1747.04736328 | Val Loss=2.63215065 | Data=17.32257366 | Physics=16.37081332 | Val RMSE: 2.71253085 | ‚àö(Val Loss) = 1.62239039 | Current Learning Rate: 0.0002\nEpoch 473/1000 | Train Loss=1568.18109131 | Val Loss=2.63487482 | Data=15.53914976 | Physics=12.63482306 | Val RMSE: 2.71255231 | ‚àö(Val Loss) = 1.62322974 | Current Learning Rate: 0.0002\nEpoch 474/1000 | Train Loss=1497.41253662 | Val Loss=2.63269472 | Data=14.83517313 | Physics=9.98578943 | Val RMSE: 2.71253729 | ‚àö(Val Loss) = 1.62255812 | Current Learning Rate: 0.0002\nEpoch 475/1000 | Train Loss=1514.63247681 | Val Loss=2.63180423 | Data=15.00870657 | Physics=9.06451229 | Val RMSE: 2.71252942 | ‚àö(Val Loss) = 1.62228370 | Current Learning Rate: 0.0002\nEpoch 476/1000 | Train Loss=1527.25460815 | Val Loss=2.63462973 | Data=15.12580776 | Physics=15.47729637 | Val RMSE: 2.71254396 | ‚àö(Val Loss) = 1.62315428 | Current Learning Rate: 0.0002\nEpoch 477/1000 | Train Loss=1553.53472900 | Val Loss=2.64207196 | Data=15.38900900 | Physics=15.00707096 | Val RMSE: 2.71257830 | ‚àö(Val Loss) = 1.62544513 | Current Learning Rate: 0.0002\nEpoch 478/1000 | Train Loss=1549.30322266 | Val Loss=2.64171052 | Data=15.35044765 | Physics=12.37077111 | Val RMSE: 2.71256351 | ‚àö(Val Loss) = 1.62533402 | Current Learning Rate: 0.0002\nEpoch 479/1000 | Train Loss=1553.50708008 | Val Loss=2.63254333 | Data=15.38865280 | Physics=15.30126510 | Val RMSE: 2.71252298 | ‚àö(Val Loss) = 1.62251139 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 480 (Val Loss = 2.63108468)\nEpoch 480/1000 | Train Loss=1535.70367432 | Val Loss=2.63108468 | Data=15.21914530 | Physics=9.32695005 | Val RMSE: 2.71251750 | ‚àö(Val Loss) = 1.62206185 | Current Learning Rate: 0.0002\nEpoch 481/1000 | Train Loss=1565.42547607 | Val Loss=2.63680959 | Data=15.51125050 | Physics=12.46833987 | Val RMSE: 2.71253276 | ‚àö(Val Loss) = 1.62382555 | Current Learning Rate: 0.0002\n\n Epoch :  480 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9777474   -3.6829562  -14.184573  ]\n [  0.9779805   -3.682811   -14.184534  ]\n [  0.97747076  -3.6831284  -14.184621  ]\n [  0.9776396   -3.6830235  -14.184592  ]\n [  0.9776822   -3.682997   -14.184585  ]\n [  0.9772454   -3.6832688  -14.18466   ]\n [  0.9768706   -3.6835022  -14.184724  ]\n [  0.9777544   -3.682952   -14.184572  ]\n [  0.9768524   -3.6835134  -14.184727  ]\n [  0.9772362   -3.6832745  -14.184661  ]\n [  0.9768071   -3.6835413  -14.184734  ]] \n\nFinal Test RMSE:  1.668563723564148\nEpoch 482/1000 | Train Loss=1563.52410889 | Val Loss=2.63119197 | Data=15.48769617 | Physics=15.67345071 | Val RMSE: 2.71251011 | ‚àö(Val Loss) = 1.62209499 | Current Learning Rate: 0.0002\nEpoch 483/1000 | Train Loss=1528.53643799 | Val Loss=2.63110542 | Data=15.14447403 | Physics=10.98253711 | Val RMSE: 2.71251202 | ‚àö(Val Loss) = 1.62206829 | Current Learning Rate: 0.0002\nEpoch 484/1000 | Train Loss=1575.73022461 | Val Loss=2.63114381 | Data=15.60781574 | Physics=17.06861331 | Val RMSE: 2.71251440 | ‚àö(Val Loss) = 1.62208009 | Current Learning Rate: 0.0002\nEpoch 485/1000 | Train Loss=1703.39434814 | Val Loss=2.63513446 | Data=16.88762903 | Physics=14.76763245 | Val RMSE: 2.71253204 | ‚àö(Val Loss) = 1.62330973 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 486 (Val Loss = 2.63027549)\nEpoch 486/1000 | Train Loss=1541.45428467 | Val Loss=2.63027549 | Data=15.26765060 | Physics=15.24228327 | Val RMSE: 2.71251130 | ‚àö(Val Loss) = 1.62181246 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 487 (Val Loss = 2.62974715)\nEpoch 487/1000 | Train Loss=1515.42465210 | Val Loss=2.62974715 | Data=15.01203346 | Physics=11.86930793 | Val RMSE: 2.71250725 | ‚àö(Val Loss) = 1.62164950 | Current Learning Rate: 0.0002\nEpoch 488/1000 | Train Loss=1787.75070190 | Val Loss=2.63360119 | Data=17.72468090 | Physics=19.25155779 | Val RMSE: 2.71252275 | ‚àö(Val Loss) = 1.62283742 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 489 (Val Loss = 2.62880349)\nEpoch 489/1000 | Train Loss=1604.69537354 | Val Loss=2.62880349 | Data=15.89708900 | Physics=17.26580237 | Val RMSE: 2.71250176 | ‚àö(Val Loss) = 1.62135851 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 490 (Val Loss = 2.62794828)\nEpoch 490/1000 | Train Loss=1557.30847168 | Val Loss=2.62794828 | Data=15.42569160 | Physics=15.56700747 | Val RMSE: 2.71249700 | ‚àö(Val Loss) = 1.62109482 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 491 (Val Loss = 2.62734485)\nEpoch 491/1000 | Train Loss=1517.64096069 | Val Loss=2.62734485 | Data=15.02733231 | Physics=16.78413984 | Val RMSE: 2.71249413 | ‚àö(Val Loss) = 1.62090862 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 492 (Val Loss = 2.62673044)\nEpoch 492/1000 | Train Loss=1585.94982910 | Val Loss=2.62673044 | Data=15.71745348 | Physics=11.80834546 | Val RMSE: 2.71249104 | ‚àö(Val Loss) = 1.62071908 | Current Learning Rate: 0.0002\nEpoch 493/1000 | Train Loss=1660.63665771 | Val Loss=2.62966156 | Data=16.45933962 | Physics=15.28878791 | Val RMSE: 2.71250844 | ‚àö(Val Loss) = 1.62162316 | Current Learning Rate: 0.0002\nEpoch 494/1000 | Train Loss=1779.03930664 | Val Loss=2.62870026 | Data=17.64300919 | Physics=15.65722035 | Val RMSE: 2.71250319 | ‚àö(Val Loss) = 1.62132668 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 495 (Val Loss = 2.62591577)\nEpoch 495/1000 | Train Loss=1602.83886719 | Val Loss=2.62591577 | Data=15.87917995 | Physics=16.97241164 | Val RMSE: 2.71248674 | ‚àö(Val Loss) = 1.62046778 | Current Learning Rate: 0.0002\nEpoch 496/1000 | Train Loss=1723.31646729 | Val Loss=2.62769032 | Data=17.08272886 | Physics=17.86490454 | Val RMSE: 2.71250224 | ‚àö(Val Loss) = 1.62101519 | Current Learning Rate: 0.0002\nEpoch 497/1000 | Train Loss=1755.18411255 | Val Loss=2.62696004 | Data=17.41257572 | Physics=9.98865746 | Val RMSE: 2.71249747 | ‚àö(Val Loss) = 1.62078989 | Current Learning Rate: 0.0002\nEpoch 498/1000 | Train Loss=1731.07568359 | Val Loss=2.62680006 | Data=17.16793585 | Physics=12.49804930 | Val RMSE: 2.71249723 | ‚àö(Val Loss) = 1.62074053 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 499 (Val Loss = 2.62498593)\nEpoch 499/1000 | Train Loss=1615.96759033 | Val Loss=2.62498593 | Data=16.01175880 | Physics=16.15172270 | Val RMSE: 2.71247673 | ‚àö(Val Loss) = 1.62018085 | Current Learning Rate: 0.0002\nEpoch 500/1000 | Train Loss=1696.60217285 | Val Loss=2.62548923 | Data=16.82270432 | Physics=12.96105697 | Val RMSE: 2.71248770 | ‚àö(Val Loss) = 1.62033617 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 501 (Val Loss = 2.62389064)\nEpoch 501/1000 | Train Loss=1507.29458618 | Val Loss=2.62389064 | Data=14.93017507 | Physics=12.67066670 | Val RMSE: 2.71245742 | ‚àö(Val Loss) = 1.61984277 | Current Learning Rate: 0.0002\n\n Epoch :  500 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.97997135  -3.6819327  -14.184224  ]\n [  0.98029345  -3.6817393  -14.184174  ]\n [  0.97958714  -3.682163   -14.184285  ]\n [  0.9798218   -3.6820223  -14.184248  ]\n [  0.9798812   -3.6819868  -14.184238  ]\n [  0.97927576  -3.68235    -14.184334  ]\n [  0.9787572   -3.6826613  -14.184416  ]\n [  0.97998035  -3.6819272  -14.184223  ]\n [  0.97873205  -3.6826763  -14.18442   ]\n [  0.97926354  -3.6823575  -14.184336  ]\n [  0.9786684   -3.6827142  -14.18443   ]] \n\nFinal Test RMSE:  1.668878197669983\n‚úÖ Saved best model at epoch 502 (Val Loss = 2.62304401)\nEpoch 502/1000 | Train Loss=1650.90124512 | Val Loss=2.62304401 | Data=16.36768532 | Physics=11.64796503 | Val RMSE: 2.71246147 | ‚àö(Val Loss) = 1.61958146 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 503 (Val Loss = 2.62008333)\nEpoch 503/1000 | Train Loss=1529.45169067 | Val Loss=2.62008333 | Data=15.14588165 | Physics=16.89663583 | Val RMSE: 2.71242762 | ‚àö(Val Loss) = 1.61866713 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 504 (Val Loss = 2.61619353)\nEpoch 504/1000 | Train Loss=1489.90664673 | Val Loss=2.61619353 | Data=14.75656390 | Physics=12.45064625 | Val RMSE: 2.71243525 | ‚àö(Val Loss) = 1.61746514 | Current Learning Rate: 0.0002\nEpoch 505/1000 | Train Loss=1723.83587646 | Val Loss=2.62155724 | Data=17.09063196 | Physics=15.68117760 | Val RMSE: 2.71246910 | ‚àö(Val Loss) = 1.61912239 | Current Learning Rate: 0.0002\nEpoch 506/1000 | Train Loss=1660.37487793 | Val Loss=2.61927295 | Data=16.46534777 | Physics=9.27250108 | Val RMSE: 2.71246076 | ‚àö(Val Loss) = 1.61841679 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 507 (Val Loss = 2.61378026)\nEpoch 507/1000 | Train Loss=1580.44628906 | Val Loss=2.61378026 | Data=15.65473366 | Physics=17.33863471 | Val RMSE: 2.71242332 | ‚àö(Val Loss) = 1.61671901 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 508 (Val Loss = 2.61156964)\nEpoch 508/1000 | Train Loss=1594.40704346 | Val Loss=2.61156964 | Data=15.79463768 | Physics=17.03241328 | Val RMSE: 2.71241164 | ‚àö(Val Loss) = 1.61603510 | Current Learning Rate: 0.0002\nEpoch 509/1000 | Train Loss=1744.66558838 | Val Loss=2.61419272 | Data=17.29980230 | Physics=15.06530056 | Val RMSE: 2.71243119 | ‚àö(Val Loss) = 1.61684656 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 510 (Val Loss = 2.60957956)\nEpoch 510/1000 | Train Loss=1577.38214111 | Val Loss=2.60957956 | Data=15.63079405 | Physics=12.39822969 | Val RMSE: 2.71239829 | ‚àö(Val Loss) = 1.61541927 | Current Learning Rate: 0.0002\nEpoch 511/1000 | Train Loss=1543.39617920 | Val Loss=2.61772442 | Data=15.27861643 | Physics=21.03089904 | Val RMSE: 2.71244884 | ‚àö(Val Loss) = 1.61793828 | Current Learning Rate: 0.0002\nEpoch 512/1000 | Train Loss=1518.63690186 | Val Loss=2.61340022 | Data=15.04571533 | Physics=10.71643429 | Val RMSE: 2.71242857 | ‚àö(Val Loss) = 1.61660147 | Current Learning Rate: 0.0002\nEpoch 513/1000 | Train Loss=1745.16369629 | Val Loss=2.61920357 | Data=17.30573273 | Physics=14.36851590 | Val RMSE: 2.71245146 | ‚àö(Val Loss) = 1.61839533 | Current Learning Rate: 0.0002\nEpoch 514/1000 | Train Loss=1560.44476318 | Val Loss=2.61044574 | Data=15.46504641 | Physics=9.83681511 | Val RMSE: 2.71241546 | ‚àö(Val Loss) = 1.61568737 | Current Learning Rate: 0.0002\nEpoch 515/1000 | Train Loss=1587.32019043 | Val Loss=2.60981107 | Data=15.72601652 | Physics=15.37236651 | Val RMSE: 2.71240973 | ‚àö(Val Loss) = 1.61549091 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 516 (Val Loss = 2.60934782)\nEpoch 516/1000 | Train Loss=1565.49865723 | Val Loss=2.60934782 | Data=15.51556730 | Physics=9.84985631 | Val RMSE: 2.71240544 | ‚àö(Val Loss) = 1.61534762 | Current Learning Rate: 0.0002\nEpoch 517/1000 | Train Loss=1698.34130859 | Val Loss=2.61589646 | Data=16.83144808 | Physics=18.50898475 | Val RMSE: 2.71244359 | ‚àö(Val Loss) = 1.61737335 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 518 (Val Loss = 2.60803795)\nEpoch 518/1000 | Train Loss=1532.22393799 | Val Loss=2.60803795 | Data=15.17023659 | Physics=18.81906978 | Val RMSE: 2.71240950 | ‚àö(Val Loss) = 1.61494207 | Current Learning Rate: 0.0002\nEpoch 519/1000 | Train Loss=1718.55529785 | Val Loss=2.61272526 | Data=17.03129578 | Physics=20.30708248 | Val RMSE: 2.71244740 | ‚àö(Val Loss) = 1.61639261 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 520 (Val Loss = 2.60612941)\nEpoch 520/1000 | Train Loss=1579.01397705 | Val Loss=2.60612941 | Data=15.64147043 | Physics=16.49925985 | Val RMSE: 2.71240711 | ‚àö(Val Loss) = 1.61435103 | Current Learning Rate: 0.0002\nEpoch 521/1000 | Train Loss=1697.28942871 | Val Loss=2.60889649 | Data=16.82638073 | Physics=14.99750899 | Val RMSE: 2.71244168 | ‚àö(Val Loss) = 1.61520791 | Current Learning Rate: 0.0002\n\n Epoch :  520 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9791595   -3.677985   -14.184273  ]\n [  0.9796957   -3.677649   -14.184195  ]\n [  0.97851396  -3.6783884  -14.184368  ]\n [  0.97890884  -3.6781416  -14.18431   ]\n [  0.9790095   -3.678079   -14.184295  ]\n [  0.97799677  -3.6787121  -14.184444  ]\n [  0.9771329   -3.6792526  -14.184571  ]\n [  0.9791723   -3.6779766  -14.184272  ]\n [  0.97709095  -3.6792789  -14.184578  ]\n [  0.9779776   -3.6787245  -14.184447  ]\n [  0.97698224  -3.6793463  -14.184594  ]] \n\nFinal Test RMSE:  1.6691302061080933\nEpoch 522/1000 | Train Loss=1735.54791260 | Val Loss=2.60773373 | Data=17.21072006 | Physics=13.72891652 | Val RMSE: 2.71243310 | ‚àö(Val Loss) = 1.61484790 | Current Learning Rate: 0.0002\nEpoch 523/1000 | Train Loss=1712.59985352 | Val Loss=2.60678077 | Data=16.97644949 | Physics=17.11713207 | Val RMSE: 2.71242499 | ‚àö(Val Loss) = 1.61455286 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 524 (Val Loss = 2.60606241)\nEpoch 524/1000 | Train Loss=1765.37268066 | Val Loss=2.60606241 | Data=17.50587749 | Physics=15.93819048 | Val RMSE: 2.71241808 | ‚àö(Val Loss) = 1.61433029 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 525 (Val Loss = 2.60300207)\nEpoch 525/1000 | Train Loss=1533.23474121 | Val Loss=2.60300207 | Data=15.18458796 | Physics=16.14453823 | Val RMSE: 2.71238589 | ‚àö(Val Loss) = 1.61338222 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 526 (Val Loss = 2.60209417)\nEpoch 526/1000 | Train Loss=1726.26519775 | Val Loss=2.60209417 | Data=17.12267780 | Physics=10.63414200 | Val RMSE: 2.71239972 | ‚àö(Val Loss) = 1.61310077 | Current Learning Rate: 0.0002\nEpoch 527/1000 | Train Loss=1589.26324463 | Val Loss=2.60820389 | Data=15.74673367 | Physics=14.79082017 | Val RMSE: 2.71245384 | ‚àö(Val Loss) = 1.61499345 | Current Learning Rate: 0.0002\nEpoch 528/1000 | Train Loss=1619.99877930 | Val Loss=2.60235953 | Data=16.04758072 | Physics=19.47578096 | Val RMSE: 2.71242857 | ‚àö(Val Loss) = 1.61318302 | Current Learning Rate: 0.0002\nEpoch 529/1000 | Train Loss=1757.67471313 | Val Loss=2.61320686 | Data=17.43144512 | Physics=14.27024645 | Val RMSE: 2.71247029 | ‚àö(Val Loss) = 1.61654162 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 530 (Val Loss = 2.59909797)\nEpoch 530/1000 | Train Loss=1628.26885986 | Val Loss=2.59909797 | Data=16.13493156 | Physics=16.18573883 | Val RMSE: 2.71240044 | ‚àö(Val Loss) = 1.61217177 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 531 (Val Loss = 2.59580493)\nEpoch 531/1000 | Train Loss=1533.72463989 | Val Loss=2.59580493 | Data=15.19391298 | Physics=13.03293546 | Val RMSE: 2.71239805 | ‚àö(Val Loss) = 1.61115015 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 532 (Val Loss = 2.59178257)\nEpoch 532/1000 | Train Loss=1583.91070557 | Val Loss=2.59178257 | Data=15.69474220 | Physics=13.72111369 | Val RMSE: 2.71239281 | ‚àö(Val Loss) = 1.60990143 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 533 (Val Loss = 2.58570671)\nEpoch 533/1000 | Train Loss=1579.86444092 | Val Loss=2.58570671 | Data=15.65325260 | Physics=14.61312346 | Val RMSE: 2.71238375 | ‚àö(Val Loss) = 1.60801327 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 534 (Val Loss = 2.57558608)\nEpoch 534/1000 | Train Loss=1581.21820068 | Val Loss=2.57558608 | Data=15.66654396 | Physics=14.80170999 | Val RMSE: 2.71236253 | ‚àö(Val Loss) = 1.60486329 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 535 (Val Loss = 2.55282021)\nEpoch 535/1000 | Train Loss=1611.72619629 | Val Loss=2.55282021 | Data=15.96998882 | Physics=16.30000694 | Val RMSE: 2.71229720 | ‚àö(Val Loss) = 1.59775472 | Current Learning Rate: 0.0002\nEpoch 536/1000 | Train Loss=1706.21087646 | Val Loss=2.61196518 | Data=16.91862679 | Physics=13.66099231 | Val RMSE: 2.71245098 | ‚àö(Val Loss) = 1.61615753 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 537 (Val Loss = 2.53790760)\nEpoch 537/1000 | Train Loss=1606.37915039 | Val Loss=2.53790760 | Data=15.91954565 | Physics=14.65440702 | Val RMSE: 2.71213436 | ‚àö(Val Loss) = 1.59308112 | Current Learning Rate: 0.0002\nEpoch 538/1000 | Train Loss=1464.02697754 | Val Loss=2.92522502 | Data=14.49578023 | Physics=14.83696011 | Val RMSE: 2.71268749 | ‚àö(Val Loss) = 1.71032894 | Current Learning Rate: 0.0002\nEpoch 539/1000 | Train Loss=1765.32537842 | Val Loss=3.16281033 | Data=17.50259542 | Physics=16.93240689 | Val RMSE: 2.71286464 | ‚àö(Val Loss) = 1.77842915 | Current Learning Rate: 0.0002\nEpoch 540/1000 | Train Loss=1547.15289307 | Val Loss=2.80595684 | Data=15.32316875 | Physics=17.13421208 | Val RMSE: 2.71261907 | ‚àö(Val Loss) = 1.67509902 | Current Learning Rate: 0.0002\nEpoch 541/1000 | Train Loss=1666.96380615 | Val Loss=2.64100957 | Data=16.52931213 | Physics=11.98350116 | Val RMSE: 2.71246386 | ‚àö(Val Loss) = 1.62511837 | Current Learning Rate: 0.0002\n\n Epoch :  540 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9866886   -3.6750624  -14.18311   ]\n [  0.98910564  -3.6736248  -14.182774  ]\n [  0.9837666   -3.676797   -14.183518  ]\n [  0.9855103   -3.675762   -14.183274  ]\n [  0.9859695   -3.6754894  -14.18321   ]\n [  0.9815305   -3.6781244  -14.18383   ]\n [  0.97776985  -3.6803527  -14.184357  ]\n [  0.98672104  -3.6750424  -14.183105  ]\n [  0.9775982   -3.6804545  -14.184381  ]\n [  0.98140645  -3.6781979  -14.183847  ]\n [  0.97712487  -3.6807342  -14.184447  ]] \n\nFinal Test RMSE:  1.6688241958618164\nEpoch 542/1000 | Train Loss=1476.29754639 | Val Loss=2.55582976 | Data=14.61220574 | Physics=18.85876537 | Val RMSE: 2.71234560 | ‚àö(Val Loss) = 1.59869623 | Current Learning Rate: 0.0002\nEpoch 543/1000 | Train Loss=1507.99285889 | Val Loss=2.74260306 | Data=14.93968868 | Physics=12.65057624 | Val RMSE: 2.71256065 | ‚àö(Val Loss) = 1.65608060 | Current Learning Rate: 0.0002\nEpoch 544/1000 | Train Loss=1484.23632812 | Val Loss=2.98626685 | Data=14.70443869 | Physics=10.48118480 | Val RMSE: 2.71271634 | ‚àö(Val Loss) = 1.72808182 | Current Learning Rate: 0.0002\nEpoch 545/1000 | Train Loss=1753.07901001 | Val Loss=2.69555831 | Data=17.39366913 | Physics=10.84532806 | Val RMSE: 2.71250653 | ‚àö(Val Loss) = 1.64181554 | Current Learning Rate: 0.0002\nEpoch 546/1000 | Train Loss=1489.60806274 | Val Loss=2.95033669 | Data=14.75383139 | Physics=13.89555509 | Val RMSE: 2.71269751 | ‚àö(Val Loss) = 1.71765447 | Current Learning Rate: 0.0002\nEpoch 547/1000 | Train Loss=1716.09625244 | Val Loss=3.07695365 | Data=17.01305580 | Physics=16.02982665 | Val RMSE: 2.71280289 | ‚àö(Val Loss) = 1.75412476 | Current Learning Rate: 0.0002\nEpoch 548/1000 | Train Loss=1731.30511475 | Val Loss=2.76169538 | Data=17.17794704 | Physics=8.49061945 | Val RMSE: 2.71259093 | ‚àö(Val Loss) = 1.66183496 | Current Learning Rate: 0.0002\nEpoch 549/1000 | Train Loss=1639.54736328 | Val Loss=2.63265991 | Data=16.25641346 | Physics=10.89318759 | Val RMSE: 2.71246552 | ‚àö(Val Loss) = 1.62254739 | Current Learning Rate: 0.0002\nEpoch 550/1000 | Train Loss=1571.12402344 | Val Loss=2.56362915 | Data=15.56854343 | Physics=14.45963751 | Val RMSE: 2.71235657 | ‚àö(Val Loss) = 1.60113370 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 551 (Val Loss = 2.50710344)\nEpoch 551/1000 | Train Loss=1723.85101318 | Val Loss=2.50710344 | Data=17.09672165 | Physics=13.38368837 | Val RMSE: 2.71224022 | ‚àö(Val Loss) = 1.58338356 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 552 (Val Loss = 2.41200161)\nEpoch 552/1000 | Train Loss=1454.83197021 | Val Loss=2.41200161 | Data=14.40295935 | Physics=16.98418007 | Val RMSE: 2.71197462 | ‚àö(Val Loss) = 1.55306196 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 553 (Val Loss = 2.28930926)\nEpoch 553/1000 | Train Loss=1464.62283325 | Val Loss=2.28930926 | Data=14.50673652 | Physics=15.07555611 | Val RMSE: 2.71135974 | ‚àö(Val Loss) = 1.51304638 | Current Learning Rate: 0.0002\nEpoch 554/1000 | Train Loss=2049.31112671 | Val Loss=4.85446739 | Data=20.36088896 | Physics=13.10237545 | Val RMSE: 2.71272540 | ‚àö(Val Loss) = 2.20328569 | Current Learning Rate: 0.0002\nEpoch 555/1000 | Train Loss=1492.80184937 | Val Loss=2.35688210 | Data=14.79761124 | Physics=10.52850378 | Val RMSE: 2.71166587 | ‚àö(Val Loss) = 1.53521407 | Current Learning Rate: 0.0002\nEpoch 556/1000 | Train Loss=1502.21969604 | Val Loss=2.36225080 | Data=14.88411617 | Physics=16.69343052 | Val RMSE: 2.71171880 | ‚àö(Val Loss) = 1.53696156 | Current Learning Rate: 0.0002\nEpoch 557/1000 | Train Loss=1537.56604004 | Val Loss=2.36685109 | Data=15.23795938 | Physics=15.82932725 | Val RMSE: 2.71175385 | ‚àö(Val Loss) = 1.53845739 | Current Learning Rate: 0.0002\nEpoch 558/1000 | Train Loss=1509.91513062 | Val Loss=2.72596669 | Data=14.96455526 | Physics=12.38981902 | Val RMSE: 2.71237993 | ‚àö(Val Loss) = 1.65105021 | Current Learning Rate: 0.0002\nEpoch 559/1000 | Train Loss=1516.69210815 | Val Loss=2.40002275 | Data=15.03169680 | Physics=13.54443217 | Val RMSE: 2.71178317 | ‚àö(Val Loss) = 1.54920065 | Current Learning Rate: 0.0002\nEpoch 560/1000 | Train Loss=1781.74896240 | Val Loss=2.65034676 | Data=17.68343115 | Physics=13.15296167 | Val RMSE: 2.71230078 | ‚àö(Val Loss) = 1.62798858 | Current Learning Rate: 0.0002\nEpoch 561/1000 | Train Loss=1479.28247070 | Val Loss=2.39433026 | Data=14.66036224 | Physics=13.33772574 | Val RMSE: 2.71172976 | ‚àö(Val Loss) = 1.54736233 | Current Learning Rate: 0.0002\n\n Epoch :  560 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 562/1000 | Train Loss=1594.47534180 | Val Loss=2.40107608 | Data=15.81623936 | Physics=8.56727370 | Val RMSE: 2.71182537 | ‚àö(Val Loss) = 1.54954064 | Current Learning Rate: 0.0002\nEpoch 563/1000 | Train Loss=1565.06555176 | Val Loss=2.40498757 | Data=15.51700306 | Physics=13.21910836 | Val RMSE: 2.71187139 | ‚àö(Val Loss) = 1.55080223 | Current Learning Rate: 0.0002\nEpoch 564/1000 | Train Loss=1525.56457520 | Val Loss=2.44279885 | Data=15.12284040 | Physics=11.23384802 | Val RMSE: 2.71203780 | ‚àö(Val Loss) = 1.56294560 | Current Learning Rate: 0.0002\nEpoch 565/1000 | Train Loss=1706.71063232 | Val Loss=2.66020751 | Data=16.92863274 | Physics=13.36750815 | Val RMSE: 2.71237254 | ‚àö(Val Loss) = 1.63101423 | Current Learning Rate: 0.0002\nEpoch 566/1000 | Train Loss=1744.36694336 | Val Loss=2.60161138 | Data=17.30658197 | Physics=14.13352972 | Val RMSE: 2.71229696 | ‚àö(Val Loss) = 1.61295116 | Current Learning Rate: 0.0002\nEpoch 567/1000 | Train Loss=1599.86517334 | Val Loss=2.55083561 | Data=15.85722542 | Physics=16.87894805 | Val RMSE: 2.71224093 | ‚àö(Val Loss) = 1.59713352 | Current Learning Rate: 0.0002\nEpoch 568/1000 | Train Loss=1508.86361694 | Val Loss=2.42375922 | Data=14.95260048 | Physics=15.22017921 | Val RMSE: 2.71183634 | ‚àö(Val Loss) = 1.55684268 | Current Learning Rate: 0.0002\nEpoch 569/1000 | Train Loss=1507.40081787 | Val Loss=2.42907739 | Data=14.93913555 | Physics=13.02068533 | Val RMSE: 2.71193314 | ‚àö(Val Loss) = 1.55854976 | Current Learning Rate: 0.0002\nEpoch 570/1000 | Train Loss=1525.11566162 | Val Loss=2.42627621 | Data=15.11983871 | Physics=10.87009430 | Val RMSE: 2.71194696 | ‚àö(Val Loss) = 1.55765080 | Current Learning Rate: 0.0002\nEpoch 571/1000 | Train Loss=1702.43707275 | Val Loss=2.58477736 | Data=16.88783932 | Physics=13.30151282 | Val RMSE: 2.71227527 | ‚àö(Val Loss) = 1.60772431 | Current Learning Rate: 0.0002\nEpoch 572/1000 | Train Loss=1558.49066162 | Val Loss=2.52951264 | Data=15.45384550 | Physics=10.86281586 | Val RMSE: 2.71223736 | ‚àö(Val Loss) = 1.59044421 | Current Learning Rate: 0.0002\nEpoch 573/1000 | Train Loss=1574.96893311 | Val Loss=2.42480683 | Data=15.61197853 | Physics=14.55865418 | Val RMSE: 2.71181893 | ‚àö(Val Loss) = 1.55717909 | Current Learning Rate: 0.0002\nEpoch 574/1000 | Train Loss=1509.60501099 | Val Loss=2.41210151 | Data=14.95755196 | Physics=15.82438737 | Val RMSE: 2.71181631 | ‚àö(Val Loss) = 1.55309415 | Current Learning Rate: 0.0002\nEpoch 575/1000 | Train Loss=1687.96398926 | Val Loss=2.53058028 | Data=16.74221945 | Physics=14.58153369 | Val RMSE: 2.71218324 | ‚àö(Val Loss) = 1.59077978 | Current Learning Rate: 0.0002\nEpoch 576/1000 | Train Loss=1674.88165283 | Val Loss=2.49238014 | Data=16.60798597 | Physics=15.76575544 | Val RMSE: 2.71210599 | ‚àö(Val Loss) = 1.57872736 | Current Learning Rate: 0.0002\nEpoch 577/1000 | Train Loss=1478.35156250 | Val Loss=2.40724516 | Data=14.64912033 | Physics=14.07225216 | Val RMSE: 2.71159887 | ‚àö(Val Loss) = 1.55152988 | Current Learning Rate: 0.0002\nEpoch 578/1000 | Train Loss=1485.25494385 | Val Loss=2.38766289 | Data=14.72443390 | Physics=8.95741508 | Val RMSE: 2.71162248 | ‚àö(Val Loss) = 1.54520643 | Current Learning Rate: 0.0002\nEpoch 579/1000 | Train Loss=1733.24468994 | Val Loss=2.51173377 | Data=17.18980026 | Physics=17.08820205 | Val RMSE: 2.71208525 | ‚àö(Val Loss) = 1.58484507 | Current Learning Rate: 0.0002\nEpoch 580/1000 | Train Loss=1502.13839722 | Val Loss=2.38744283 | Data=14.89033270 | Physics=11.82972680 | Val RMSE: 2.71155858 | ‚àö(Val Loss) = 1.54513526 | Current Learning Rate: 0.0002\nEpoch 581/1000 | Train Loss=1567.81195068 | Val Loss=2.37070012 | Data=15.54172325 | Physics=15.03412812 | Val RMSE: 2.71156907 | ‚àö(Val Loss) = 1.53970778 | Current Learning Rate: 0.0002\n\n Epoch :  580 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 582/1000 | Train Loss=1512.35650635 | Val Loss=2.39687443 | Data=14.98898745 | Physics=13.88606931 | Val RMSE: 2.71186042 | ‚àö(Val Loss) = 1.54818428 | Current Learning Rate: 0.0002\nEpoch 583/1000 | Train Loss=1454.01916504 | Val Loss=2.38627958 | Data=14.40861106 | Physics=11.47037341 | Val RMSE: 2.71182871 | ‚àö(Val Loss) = 1.54475880 | Current Learning Rate: 0.0002\nEpoch 584/1000 | Train Loss=1566.30181885 | Val Loss=2.52402163 | Data=15.52759838 | Physics=13.09064233 | Val RMSE: 2.71222091 | ‚àö(Val Loss) = 1.58871698 | Current Learning Rate: 0.0002\nEpoch 585/1000 | Train Loss=1485.05456543 | Val Loss=2.41442156 | Data=14.71645498 | Physics=12.71694723 | Val RMSE: 2.71184111 | ‚àö(Val Loss) = 1.55384088 | Current Learning Rate: 0.0002\nEpoch 586/1000 | Train Loss=1531.66229248 | Val Loss=2.40187120 | Data=15.17462921 | Physics=18.43094299 | Val RMSE: 2.71181870 | ‚àö(Val Loss) = 1.54979718 | Current Learning Rate: 0.0002\nEpoch 587/1000 | Train Loss=1769.95184326 | Val Loss=2.51372409 | Data=17.56125450 | Physics=15.25132821 | Val RMSE: 2.71214414 | ‚àö(Val Loss) = 1.58547282 | Current Learning Rate: 0.0002\nEpoch 588/1000 | Train Loss=1646.18951416 | Val Loss=2.47656178 | Data=16.32296228 | Physics=14.72877295 | Val RMSE: 2.71206784 | ‚àö(Val Loss) = 1.57370961 | Current Learning Rate: 0.0002\nEpoch 589/1000 | Train Loss=1671.30004883 | Val Loss=2.44498420 | Data=16.57929516 | Physics=12.41398631 | Val RMSE: 2.71200013 | ‚àö(Val Loss) = 1.56364453 | Current Learning Rate: 0.0002\nEpoch 590/1000 | Train Loss=1682.19818115 | Val Loss=2.42270374 | Data=16.69064903 | Physics=10.58676775 | Val RMSE: 2.71195650 | ‚àö(Val Loss) = 1.55650365 | Current Learning Rate: 0.0002\nEpoch 591/1000 | Train Loss=1642.83966064 | Val Loss=2.40143204 | Data=16.28890276 | Physics=16.37631188 | Val RMSE: 2.71189952 | ‚àö(Val Loss) = 1.54965544 | Current Learning Rate: 0.0002\nEpoch 592/1000 | Train Loss=1675.95544434 | Val Loss=2.38713717 | Data=16.62595177 | Physics=12.14046005 | Val RMSE: 2.71186256 | ‚àö(Val Loss) = 1.54503632 | Current Learning Rate: 0.0002\nEpoch 593/1000 | Train Loss=1546.00891113 | Val Loss=2.41103458 | Data=15.33059549 | Physics=9.40443673 | Val RMSE: 2.71186566 | ‚àö(Val Loss) = 1.55275071 | Current Learning Rate: 0.0002\nEpoch 594/1000 | Train Loss=1706.71661377 | Val Loss=2.50369525 | Data=16.92731953 | Physics=16.39404441 | Val RMSE: 2.71214581 | ‚àö(Val Loss) = 1.58230698 | Current Learning Rate: 0.0002\nEpoch 595/1000 | Train Loss=1743.31216431 | Val Loss=2.46924853 | Data=17.28452539 | Physics=20.43284027 | Val RMSE: 2.71206546 | ‚àö(Val Loss) = 1.57138431 | Current Learning Rate: 0.0002\nEpoch 596/1000 | Train Loss=1672.36413574 | Val Loss=2.43963003 | Data=16.58915997 | Physics=13.05152877 | Val RMSE: 2.71199584 | ‚àö(Val Loss) = 1.56193149 | Current Learning Rate: 0.0002\nEpoch 597/1000 | Train Loss=1528.28192139 | Val Loss=2.39512539 | Data=15.15293646 | Physics=10.07919794 | Val RMSE: 2.71164370 | ‚àö(Val Loss) = 1.54761922 | Current Learning Rate: 0.0002\nEpoch 598/1000 | Train Loss=1525.66943359 | Val Loss=2.43697929 | Data=15.12134743 | Physics=13.24750914 | Val RMSE: 2.71202826 | ‚àö(Val Loss) = 1.56108272 | Current Learning Rate: 0.0002\nEpoch 599/1000 | Train Loss=1487.21557617 | Val Loss=2.57150745 | Data=14.73555803 | Physics=13.81419107 | Val RMSE: 2.71225858 | ‚àö(Val Loss) = 1.60359204 | Current Learning Rate: 0.0002\nEpoch 600/1000 | Train Loss=1751.97296143 | Val Loss=2.52962089 | Data=17.38449621 | Physics=12.96376441 | Val RMSE: 2.71218085 | ‚àö(Val Loss) = 1.59047818 | Current Learning Rate: 0.0002\nEpoch 601/1000 | Train Loss=1541.08831787 | Val Loss=2.38910604 | Data=15.27836370 | Physics=12.03895317 | Val RMSE: 2.71180820 | ‚àö(Val Loss) = 1.54567337 | Current Learning Rate: 0.0002\n\n Epoch :  600 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 602/1000 | Train Loss=7355.31298828 | Val Loss=2.50337219 | Data=73.41738129 | Physics=12.85954273 | Val RMSE: 2.71210384 | ‚àö(Val Loss) = 1.58220482 | Current Learning Rate: 0.0002\nEpoch 603/1000 | Train Loss=7588.03515625 | Val Loss=2.46575737 | Data=75.74636078 | Physics=12.06728937 | Val RMSE: 2.71203113 | ‚àö(Val Loss) = 1.57027304 | Current Learning Rate: 0.0002\nEpoch 604/1000 | Train Loss=7163.86938477 | Val Loss=2.44110990 | Data=71.50783157 | Physics=10.40110298 | Val RMSE: 2.71201444 | ‚àö(Val Loss) = 1.56240511 | Current Learning Rate: 0.0002\nEpoch 605/1000 | Train Loss=7672.28833008 | Val Loss=2.41757941 | Data=76.58614349 | Physics=14.00157787 | Val RMSE: 2.71194887 | ‚àö(Val Loss) = 1.55485666 | Current Learning Rate: 0.0002\nEpoch 606/1000 | Train Loss=6905.20751953 | Val Loss=2.39611125 | Data=68.92430687 | Physics=8.32548629 | Val RMSE: 2.71188283 | ‚àö(Val Loss) = 1.54793775 | Current Learning Rate: 0.0002\nEpoch 607/1000 | Train Loss=6881.90795898 | Val Loss=2.41133714 | Data=68.68861961 | Physics=11.70995599 | Val RMSE: 2.71143627 | ‚àö(Val Loss) = 1.55284810 | Current Learning Rate: 0.0002\nEpoch 608/1000 | Train Loss=7014.19506836 | Val Loss=2.38352346 | Data=70.00917244 | Physics=13.24246827 | Val RMSE: 2.71148181 | ‚àö(Val Loss) = 1.54386640 | Current Learning Rate: 0.0002\nEpoch 609/1000 | Train Loss=7265.10864258 | Val Loss=2.42330337 | Data=72.52114105 | Physics=9.71557930 | Val RMSE: 2.71191072 | ‚àö(Val Loss) = 1.55669630 | Current Learning Rate: 0.0002\nEpoch 610/1000 | Train Loss=7468.36425781 | Val Loss=2.39668179 | Data=74.54342842 | Physics=16.25013114 | Val RMSE: 2.71184492 | ‚àö(Val Loss) = 1.54812205 | Current Learning Rate: 0.0002\nEpoch 611/1000 | Train Loss=7208.28247070 | Val Loss=2.38525391 | Data=71.94478989 | Physics=15.45753913 | Val RMSE: 2.71182370 | ‚àö(Val Loss) = 1.54442668 | Current Learning Rate: 0.0002\nEpoch 612/1000 | Train Loss=6935.47729492 | Val Loss=2.37253046 | Data=69.21512032 | Physics=16.25080206 | Val RMSE: 2.71178794 | ‚àö(Val Loss) = 1.54030204 | Current Learning Rate: 0.0002\nEpoch 613/1000 | Train Loss=6673.37304688 | Val Loss=2.42128873 | Data=66.60556602 | Physics=9.24507807 | Val RMSE: 2.71130681 | ‚àö(Val Loss) = 1.55604911 | Current Learning Rate: 0.0002\nEpoch 614/1000 | Train Loss=6918.34716797 | Val Loss=2.37955117 | Data=69.05002022 | Physics=13.70251903 | Val RMSE: 2.71139026 | ‚àö(Val Loss) = 1.54257941 | Current Learning Rate: 0.0002\nEpoch 615/1000 | Train Loss=7510.61230469 | Val Loss=2.41049123 | Data=74.97201729 | Physics=12.56233552 | Val RMSE: 2.71184897 | ‚àö(Val Loss) = 1.55257571 | Current Learning Rate: 0.0002\nEpoch 616/1000 | Train Loss=7290.02221680 | Val Loss=2.38740087 | Data=72.76827621 | Physics=11.02167844 | Val RMSE: 2.71180224 | ‚àö(Val Loss) = 1.54512167 | Current Learning Rate: 0.0002\nEpoch 617/1000 | Train Loss=7112.14111328 | Val Loss=2.38051653 | Data=70.98736382 | Physics=13.52229682 | Val RMSE: 2.71148205 | ‚àö(Val Loss) = 1.54289222 | Current Learning Rate: 0.0002\nEpoch 618/1000 | Train Loss=7594.78784180 | Val Loss=2.41439128 | Data=75.80245209 | Physics=20.12617390 | Val RMSE: 2.71188116 | ‚àö(Val Loss) = 1.55383122 | Current Learning Rate: 0.0002\nEpoch 619/1000 | Train Loss=7521.82861328 | Val Loss=2.39065552 | Data=75.08312988 | Physics=13.13594640 | Val RMSE: 2.71182251 | ‚àö(Val Loss) = 1.54617453 | Current Learning Rate: 0.0002\nEpoch 620/1000 | Train Loss=7466.19213867 | Val Loss=2.37206006 | Data=74.52338028 | Physics=15.98198356 | Val RMSE: 2.71177077 | ‚àö(Val Loss) = 1.54014933 | Current Learning Rate: 0.0002\nEpoch 621/1000 | Train Loss=6864.62451172 | Val Loss=2.34946966 | Data=68.50580406 | Physics=16.67938440 | Val RMSE: 2.71168661 | ‚àö(Val Loss) = 1.53279793 | Current Learning Rate: 0.0002\n\n Epoch :  620 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 622/1000 | Train Loss=7454.03857422 | Val Loss=2.33861971 | Data=74.39652824 | Physics=18.76105275 | Val RMSE: 2.71164942 | ‚àö(Val Loss) = 1.52925467 | Current Learning Rate: 0.0002\nEpoch 623/1000 | Train Loss=6892.76953125 | Val Loss=2.44094372 | Data=68.79969597 | Physics=9.83764611 | Val RMSE: 2.71118021 | ‚àö(Val Loss) = 1.56235194 | Current Learning Rate: 0.0002\nEpoch 624/1000 | Train Loss=6660.58166504 | Val Loss=2.38521814 | Data=66.47340679 | Physics=13.19076780 | Val RMSE: 2.71127176 | ‚àö(Val Loss) = 1.54441512 | Current Learning Rate: 0.0002\nEpoch 625/1000 | Train Loss=6988.82202148 | Val Loss=2.38813639 | Data=69.75462723 | Physics=12.29782117 | Val RMSE: 2.71176839 | ‚àö(Val Loss) = 1.54535961 | Current Learning Rate: 0.0002\nEpoch 626/1000 | Train Loss=7699.54931641 | Val Loss=2.36757684 | Data=76.85862732 | Physics=14.77525661 | Val RMSE: 2.71172667 | ‚àö(Val Loss) = 1.53869319 | Current Learning Rate: 0.0002\nEpoch 627/1000 | Train Loss=7141.19580078 | Val Loss=2.39603758 | Data=71.28083420 | Physics=12.08380320 | Val RMSE: 2.71123457 | ‚àö(Val Loss) = 1.54791391 | Current Learning Rate: 0.0002\nEpoch 628/1000 | Train Loss=7386.25830078 | Val Loss=2.35929513 | Data=73.72698975 | Physics=14.88000455 | Val RMSE: 2.71150017 | ‚àö(Val Loss) = 1.53599977 | Current Learning Rate: 0.0002\nEpoch 629/1000 | Train Loss=7305.75732422 | Val Loss=2.36374998 | Data=72.92481995 | Physics=12.09348603 | Val RMSE: 2.71166635 | ‚àö(Val Loss) = 1.53744912 | Current Learning Rate: 0.0002\nEpoch 630/1000 | Train Loss=7764.51171875 | Val Loss=2.46808386 | Data=77.51173019 | Physics=12.18074078 | Val RMSE: 2.71199918 | ‚àö(Val Loss) = 1.57101369 | Current Learning Rate: 0.0002\nEpoch 631/1000 | Train Loss=7607.95874023 | Val Loss=2.43425488 | Data=75.94388962 | Physics=13.90514490 | Val RMSE: 2.71197534 | ‚àö(Val Loss) = 1.56020987 | Current Learning Rate: 0.0002\nEpoch 632/1000 | Train Loss=7144.70825195 | Val Loss=2.40954995 | Data=71.31159592 | Physics=13.49759894 | Val RMSE: 2.71190524 | ‚àö(Val Loss) = 1.55227256 | Current Learning Rate: 0.0002\nEpoch 633/1000 | Train Loss=7091.08032227 | Val Loss=2.39008880 | Data=70.77806282 | Physics=12.08164628 | Val RMSE: 2.71184826 | ‚àö(Val Loss) = 1.54599118 | Current Learning Rate: 0.0002\nEpoch 634/1000 | Train Loss=7160.73510742 | Val Loss=2.37333536 | Data=71.47574997 | Physics=10.80094437 | Val RMSE: 2.71179509 | ‚àö(Val Loss) = 1.54056334 | Current Learning Rate: 0.0002\nEpoch 635/1000 | Train Loss=7400.06323242 | Val Loss=2.35865474 | Data=73.86235809 | Physics=15.57675818 | Val RMSE: 2.71174383 | ‚àö(Val Loss) = 1.53579128 | Current Learning Rate: 0.0002\nEpoch 636/1000 | Train Loss=7626.51855469 | Val Loss=2.34704733 | Data=76.12075806 | Physics=19.16404372 | Val RMSE: 2.71170044 | ‚àö(Val Loss) = 1.53200758 | Current Learning Rate: 0.0002\nEpoch 637/1000 | Train Loss=6946.91162109 | Val Loss=2.41377521 | Data=69.34472275 | Physics=6.42703360 | Val RMSE: 2.71130824 | ‚àö(Val Loss) = 1.55363286 | Current Learning Rate: 0.0002\nEpoch 638/1000 | Train Loss=6955.27416992 | Val Loss=2.37992072 | Data=69.41959381 | Physics=13.84988931 | Val RMSE: 2.71135998 | ‚àö(Val Loss) = 1.54269922 | Current Learning Rate: 0.0002\nEpoch 639/1000 | Train Loss=7096.03515625 | Val Loss=2.43248129 | Data=70.81931496 | Physics=17.36629624 | Val RMSE: 2.71198630 | ‚àö(Val Loss) = 1.55964136 | Current Learning Rate: 0.0002\nEpoch 640/1000 | Train Loss=7565.51245117 | Val Loss=2.59333730 | Data=75.51696777 | Physics=14.75244552 | Val RMSE: 2.71220231 | ‚àö(Val Loss) = 1.61038423 | Current Learning Rate: 0.0002\nEpoch 641/1000 | Train Loss=7627.29077148 | Val Loss=2.47928238 | Data=76.13334656 | Physics=16.26015143 | Val RMSE: 2.71207547 | ‚àö(Val Loss) = 1.57457376 | Current Learning Rate: 0.0002\n\n Epoch :  640 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 642/1000 | Train Loss=7524.30175781 | Val Loss=2.65320325 | Data=75.10178757 | Physics=16.37798534 | Val RMSE: 2.71226883 | ‚àö(Val Loss) = 1.62886560 | Current Learning Rate: 0.0002\nEpoch 643/1000 | Train Loss=7111.97583008 | Val Loss=2.39963198 | Data=70.98327065 | Physics=14.77132052 | Val RMSE: 2.71187377 | ‚àö(Val Loss) = 1.54907453 | Current Learning Rate: 0.0002\nEpoch 644/1000 | Train Loss=7172.63378906 | Val Loss=2.41693997 | Data=71.59157562 | Physics=13.49193432 | Val RMSE: 2.71192765 | ‚àö(Val Loss) = 1.55465102 | Current Learning Rate: 0.0002\nEpoch 645/1000 | Train Loss=7167.76367188 | Val Loss=2.58164310 | Data=71.55043793 | Physics=7.87261371 | Val RMSE: 2.71216106 | ‚àö(Val Loss) = 1.60674918 | Current Learning Rate: 0.0002\nEpoch 646/1000 | Train Loss=7166.66625977 | Val Loss=2.51507902 | Data=71.52863312 | Physics=14.25413566 | Val RMSE: 2.71209717 | ‚àö(Val Loss) = 1.58590007 | Current Learning Rate: 0.0002\nEpoch 647/1000 | Train Loss=7734.08886719 | Val Loss=2.47349644 | Data=77.19893837 | Physics=16.73916602 | Val RMSE: 2.71201420 | ‚àö(Val Loss) = 1.57273531 | Current Learning Rate: 0.0002\nEpoch 648/1000 | Train Loss=7461.30126953 | Val Loss=2.35311508 | Data=74.47508621 | Physics=15.61391475 | Val RMSE: 2.71161199 | ‚àö(Val Loss) = 1.53398669 | Current Learning Rate: 0.0002\nEpoch 649/1000 | Train Loss=7233.30029297 | Val Loss=2.44644976 | Data=72.19983673 | Physics=11.81962288 | Val RMSE: 2.71193695 | ‚àö(Val Loss) = 1.56411314 | Current Learning Rate: 0.0002\nEpoch 650/1000 | Train Loss=6673.15148926 | Val Loss=2.34680104 | Data=66.60788155 | Physics=6.01241225 | Val RMSE: 2.71153307 | ‚àö(Val Loss) = 1.53192723 | Current Learning Rate: 0.0002\nEpoch 651/1000 | Train Loss=7632.23901367 | Val Loss=2.42886949 | Data=76.18304443 | Physics=15.57183601 | Val RMSE: 2.71188736 | ‚àö(Val Loss) = 1.55848312 | Current Learning Rate: 0.0002\nEpoch 652/1000 | Train Loss=7080.26245117 | Val Loss=2.39932203 | Data=70.66659164 | Physics=13.81184597 | Val RMSE: 2.71182418 | ‚àö(Val Loss) = 1.54897451 | Current Learning Rate: 0.0002\nEpoch 653/1000 | Train Loss=7092.48974609 | Val Loss=2.35744667 | Data=70.79227448 | Physics=12.09115083 | Val RMSE: 2.71158481 | ‚àö(Val Loss) = 1.53539789 | Current Learning Rate: 0.0002\nEpoch 654/1000 | Train Loss=7543.59301758 | Val Loss=2.42594695 | Data=75.29303360 | Physics=18.20076475 | Val RMSE: 2.71189928 | ‚àö(Val Loss) = 1.55754519 | Current Learning Rate: 0.0002\nEpoch 655/1000 | Train Loss=7621.92944336 | Val Loss=2.34966230 | Data=76.08408356 | Physics=13.98315067 | Val RMSE: 2.71150088 | ‚àö(Val Loss) = 1.53286088 | Current Learning Rate: 0.0002\nEpoch 656/1000 | Train Loss=7483.34350586 | Val Loss=2.34462619 | Data=74.70204544 | Physics=11.52086077 | Val RMSE: 2.71154881 | ‚àö(Val Loss) = 1.53121722 | Current Learning Rate: 0.0002\nEpoch 657/1000 | Train Loss=6797.98059082 | Val Loss=2.33587813 | Data=67.84411430 | Physics=15.21942860 | Val RMSE: 2.71153927 | ‚àö(Val Loss) = 1.52835798 | Current Learning Rate: 0.0002\nEpoch 658/1000 | Train Loss=7403.39111328 | Val Loss=2.44329715 | Data=73.90152359 | Physics=11.40286981 | Val RMSE: 2.71189809 | ‚àö(Val Loss) = 1.56310499 | Current Learning Rate: 0.0002\nEpoch 659/1000 | Train Loss=7003.53515625 | Val Loss=2.35443878 | Data=69.89709663 | Physics=16.54267504 | Val RMSE: 2.71166229 | ‚àö(Val Loss) = 1.53441811 | Current Learning Rate: 0.0002\nEpoch 660/1000 | Train Loss=6909.03076172 | Val Loss=2.34336519 | Data=68.95429039 | Physics=15.08219960 | Val RMSE: 2.71162105 | ‚àö(Val Loss) = 1.53080535 | Current Learning Rate: 0.0002\nEpoch 661/1000 | Train Loss=7644.23339844 | Val Loss=2.46208596 | Data=76.30699158 | Physics=13.07524940 | Val RMSE: 2.71194506 | ‚àö(Val Loss) = 1.56910360 | Current Learning Rate: 0.0002\n\n Epoch :  660 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 662/1000 | Train Loss=6827.04174805 | Val Loss=2.33461356 | Data=68.14100456 | Physics=10.86375229 | Val RMSE: 2.71151853 | ‚àö(Val Loss) = 1.52794421 | Current Learning Rate: 0.0002\nEpoch 663/1000 | Train Loss=7365.68994141 | Val Loss=2.33492589 | Data=73.52040100 | Physics=15.04666488 | Val RMSE: 2.71156049 | ‚àö(Val Loss) = 1.52804637 | Current Learning Rate: 0.0002\nEpoch 664/1000 | Train Loss=7155.84570312 | Val Loss=2.44946718 | Data=71.42882919 | Physics=9.65954180 | Val RMSE: 2.71190858 | ‚àö(Val Loss) = 1.56507742 | Current Learning Rate: 0.0002\nEpoch 665/1000 | Train Loss=7410.81616211 | Val Loss=2.41420412 | Data=73.97567368 | Physics=11.50517390 | Val RMSE: 2.71184802 | ‚àö(Val Loss) = 1.55377090 | Current Learning Rate: 0.0002\nEpoch 666/1000 | Train Loss=6815.32812500 | Val Loss=2.37173891 | Data=68.01792145 | Physics=14.42815723 | Val RMSE: 2.71178365 | ‚àö(Val Loss) = 1.54004514 | Current Learning Rate: 0.0002\nEpoch 667/1000 | Train Loss=7300.18432617 | Val Loss=2.49788356 | Data=72.86983109 | Physics=11.11864682 | Val RMSE: 2.71203995 | ‚àö(Val Loss) = 1.58046937 | Current Learning Rate: 0.0002\nEpoch 668/1000 | Train Loss=7011.71582031 | Val Loss=2.45662737 | Data=69.98941040 | Physics=9.42728539 | Val RMSE: 2.71197748 | ‚àö(Val Loss) = 1.56736314 | Current Learning Rate: 0.0002\nEpoch 669/1000 | Train Loss=6750.31750488 | Val Loss=2.34768248 | Data=67.37287903 | Physics=11.05387899 | Val RMSE: 2.71158957 | ‚àö(Val Loss) = 1.53221488 | Current Learning Rate: 0.0002\nEpoch 670/1000 | Train Loss=7486.91235352 | Val Loss=2.43292189 | Data=74.73897743 | Physics=10.14618341 | Val RMSE: 2.71190858 | ‚àö(Val Loss) = 1.55978262 | Current Learning Rate: 0.0002\nEpoch 671/1000 | Train Loss=7093.89770508 | Val Loss=2.40525556 | Data=70.80333519 | Physics=13.82099049 | Val RMSE: 2.71184754 | ‚àö(Val Loss) = 1.55088866 | Current Learning Rate: 0.0002\nEpoch 672/1000 | Train Loss=7476.82543945 | Val Loss=2.34803581 | Data=74.63618088 | Physics=11.53489015 | Val RMSE: 2.71145821 | ‚àö(Val Loss) = 1.53233016 | Current Learning Rate: 0.0002\nEpoch 673/1000 | Train Loss=7075.95825195 | Val Loss=2.39840841 | Data=70.61656952 | Physics=18.27511272 | Val RMSE: 2.71182680 | ‚àö(Val Loss) = 1.54867959 | Current Learning Rate: 0.0002\nEpoch 674/1000 | Train Loss=7200.25366211 | Val Loss=2.34814835 | Data=71.86796188 | Physics=13.87421974 | Val RMSE: 2.71143842 | ‚àö(Val Loss) = 1.53236687 | Current Learning Rate: 0.0002\nEpoch 675/1000 | Train Loss=7317.80004883 | Val Loss=2.33772016 | Data=73.04230118 | Physics=14.99149687 | Val RMSE: 2.71145463 | ‚àö(Val Loss) = 1.52896047 | Current Learning Rate: 0.0002\nEpoch 676/1000 | Train Loss=7292.56250000 | Val Loss=2.40746832 | Data=72.79179001 | Physics=12.75412967 | Val RMSE: 2.71182275 | ‚àö(Val Loss) = 1.55160189 | Current Learning Rate: 0.0002\nEpoch 677/1000 | Train Loss=6915.91381836 | Val Loss=2.38051033 | Data=69.02544785 | Physics=11.97787699 | Val RMSE: 2.71176124 | ‚àö(Val Loss) = 1.54289031 | Current Learning Rate: 0.0002\nEpoch 678/1000 | Train Loss=7261.43823242 | Val Loss=2.34768558 | Data=72.47890854 | Physics=14.60478107 | Val RMSE: 2.71135402 | ‚àö(Val Loss) = 1.53221595 | Current Learning Rate: 0.0002\nEpoch 679/1000 | Train Loss=6962.04760742 | Val Loss=2.33428574 | Data=69.48338699 | Physics=15.86705189 | Val RMSE: 2.71138215 | ‚àö(Val Loss) = 1.52783692 | Current Learning Rate: 0.0002\nEpoch 680/1000 | Train Loss=7112.65527344 | Val Loss=2.39543486 | Data=70.98877144 | Physics=15.23702909 | Val RMSE: 2.71177483 | ‚àö(Val Loss) = 1.54771924 | Current Learning Rate: 0.0002\nEpoch 681/1000 | Train Loss=7421.78295898 | Val Loss=2.37066698 | Data=74.08215714 | Physics=13.51906785 | Val RMSE: 2.71172404 | ‚àö(Val Loss) = 1.53969705 | Current Learning Rate: 0.0002\n\n Epoch :  680 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 682/1000 | Train Loss=7013.94677734 | Val Loss=2.34819198 | Data=70.00833702 | Physics=11.74397214 | Val RMSE: 2.71133113 | ‚àö(Val Loss) = 1.53238118 | Current Learning Rate: 0.0002\nEpoch 683/1000 | Train Loss=7272.82006836 | Val Loss=2.37596416 | Data=72.59820175 | Physics=10.08652868 | Val RMSE: 2.71174145 | ‚àö(Val Loss) = 1.54141629 | Current Learning Rate: 0.0002\nEpoch 684/1000 | Train Loss=7158.84521484 | Val Loss=2.36227274 | Data=71.45461655 | Physics=12.64401064 | Val RMSE: 2.71173286 | ‚àö(Val Loss) = 1.53696871 | Current Learning Rate: 0.0002\nEpoch 685/1000 | Train Loss=7491.68750000 | Val Loss=2.34917974 | Data=74.77365494 | Physics=18.59816383 | Val RMSE: 2.71168447 | ‚àö(Val Loss) = 1.53270340 | Current Learning Rate: 0.0002\nEpoch 686/1000 | Train Loss=6874.31201172 | Val Loss=2.38452673 | Data=68.61344910 | Physics=10.65600539 | Val RMSE: 2.71135402 | ‚àö(Val Loss) = 1.54419124 | Current Learning Rate: 0.0002\nEpoch 687/1000 | Train Loss=6768.56042480 | Val Loss=2.36247993 | Data=67.55678940 | Physics=9.91130982 | Val RMSE: 2.71139455 | ‚àö(Val Loss) = 1.53703606 | Current Learning Rate: 0.0002\nEpoch 688/1000 | Train Loss=7076.94384766 | Val Loss=2.35095716 | Data=70.63506317 | Physics=13.54438186 | Val RMSE: 2.71149302 | ‚àö(Val Loss) = 1.53328311 | Current Learning Rate: 0.0002\nEpoch 689/1000 | Train Loss=7497.00122070 | Val Loss=2.39576077 | Data=74.83557320 | Physics=13.08128826 | Val RMSE: 2.71181393 | ‚àö(Val Loss) = 1.54782450 | Current Learning Rate: 0.0002\nEpoch 690/1000 | Train Loss=7085.10986328 | Val Loss=2.37456751 | Data=70.72303009 | Physics=8.24179637 | Val RMSE: 2.71175981 | ‚àö(Val Loss) = 1.54096317 | Current Learning Rate: 0.0002\nEpoch 691/1000 | Train Loss=6961.83325195 | Val Loss=2.35113740 | Data=69.48689270 | Physics=11.74935094 | Val RMSE: 2.71134210 | ‚àö(Val Loss) = 1.53334188 | Current Learning Rate: 0.0002\nEpoch 692/1000 | Train Loss=7098.45312500 | Val Loss=2.37113500 | Data=70.85714531 | Physics=8.42006406 | Val RMSE: 2.71172976 | ‚àö(Val Loss) = 1.53984904 | Current Learning Rate: 0.0002\nEpoch 693/1000 | Train Loss=7135.57714844 | Val Loss=2.34644008 | Data=71.21648979 | Physics=17.96771615 | Val RMSE: 2.71135092 | ‚àö(Val Loss) = 1.53180945 | Current Learning Rate: 0.0002\nEpoch 694/1000 | Train Loss=7073.80322266 | Val Loss=2.33611345 | Data=70.60970116 | Physics=9.52303927 | Val RMSE: 2.71143842 | ‚àö(Val Loss) = 1.52843499 | Current Learning Rate: 0.0002\nEpoch 695/1000 | Train Loss=7410.55175781 | Val Loss=2.39866042 | Data=73.97618103 | Physics=9.92112319 | Val RMSE: 2.71179938 | ‚àö(Val Loss) = 1.54876089 | Current Learning Rate: 0.0002\nEpoch 696/1000 | Train Loss=7055.68823242 | Val Loss=2.33760071 | Data=70.42283630 | Physics=13.31059925 | Val RMSE: 2.71142840 | ‚àö(Val Loss) = 1.52892137 | Current Learning Rate: 0.0002\nEpoch 697/1000 | Train Loss=7331.63793945 | Val Loss=2.39209843 | Data=73.18278503 | Physics=12.38418502 | Val RMSE: 2.71178460 | ‚àö(Val Loss) = 1.54664099 | Current Learning Rate: 0.0002\nEpoch 698/1000 | Train Loss=7419.05932617 | Val Loss=2.36944151 | Data=74.04998779 | Physics=16.53274698 | Val RMSE: 2.71173286 | ‚àö(Val Loss) = 1.53929901 | Current Learning Rate: 0.0002\nEpoch 699/1000 | Train Loss=7453.63378906 | Val Loss=2.35286593 | Data=74.40063095 | Physics=13.48195565 | Val RMSE: 2.71168971 | ‚àö(Val Loss) = 1.53390551 | Current Learning Rate: 0.0002\nEpoch 700/1000 | Train Loss=7230.55102539 | Val Loss=2.35745430 | Data=72.17491913 | Physics=10.66782912 | Val RMSE: 2.71131015 | ‚àö(Val Loss) = 1.53540039 | Current Learning Rate: 0.0002\nEpoch 701/1000 | Train Loss=7330.41577148 | Val Loss=2.35964727 | Data=73.16524887 | Physics=15.60020623 | Val RMSE: 2.71170497 | ‚àö(Val Loss) = 1.53611434 | Current Learning Rate: 0.0002\n‚úÖ Learning Rate updated to 0.001\n\n Epoch :  700 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 702/1000 | Train Loss=7656.82714844 | Val Loss=2.63826299 | Data=76.43324661 | Physics=12.99396946 | Val RMSE: 2.71233940 | ‚àö(Val Loss) = 1.62427306 | Current Learning Rate: 0.001\nEpoch 703/1000 | Train Loss=7279.30688477 | Val Loss=2.59557056 | Data=72.65718460 | Physics=11.79133348 | Val RMSE: 2.71230841 | ‚àö(Val Loss) = 1.61107743 | Current Learning Rate: 0.001\nEpoch 704/1000 | Train Loss=7115.64501953 | Val Loss=2.56479073 | Data=71.01947021 | Physics=13.59715208 | Val RMSE: 2.71231270 | ‚àö(Val Loss) = 1.60149646 | Current Learning Rate: 0.001\nEpoch 705/1000 | Train Loss=7420.37402344 | Val Loss=2.66040015 | Data=74.07109451 | Physics=12.35851531 | Val RMSE: 2.71156335 | ‚àö(Val Loss) = 1.63107336 | Current Learning Rate: 0.001\nEpoch 706/1000 | Train Loss=7400.62036133 | Val Loss=2.59352183 | Data=73.86230469 | Physics=17.60858103 | Val RMSE: 2.71237993 | ‚àö(Val Loss) = 1.61044145 | Current Learning Rate: 0.001\nEpoch 707/1000 | Train Loss=7449.14599609 | Val Loss=2.56619406 | Data=74.35176086 | Physics=15.79011658 | Val RMSE: 2.71179652 | ‚àö(Val Loss) = 1.60193443 | Current Learning Rate: 0.001\nEpoch 708/1000 | Train Loss=7135.98535156 | Val Loss=2.54147434 | Data=71.22075844 | Physics=16.42426174 | Val RMSE: 2.71181703 | ‚àö(Val Loss) = 1.59420025 | Current Learning Rate: 0.001\nEpoch 709/1000 | Train Loss=7132.13696289 | Val Loss=2.52150512 | Data=71.19171715 | Physics=9.83416446 | Val RMSE: 2.71189976 | ‚àö(Val Loss) = 1.58792484 | Current Learning Rate: 0.001\nEpoch 710/1000 | Train Loss=6751.30969238 | Val Loss=2.51067781 | Data=67.38513374 | Physics=8.77808127 | Val RMSE: 2.71190357 | ‚àö(Val Loss) = 1.58451188 | Current Learning Rate: 0.001\nEpoch 711/1000 | Train Loss=7388.35595703 | Val Loss=2.53149819 | Data=73.74438095 | Physics=15.03365100 | Val RMSE: 2.71207714 | ‚àö(Val Loss) = 1.59106827 | Current Learning Rate: 0.001\nEpoch 712/1000 | Train Loss=7113.12158203 | Val Loss=2.65616870 | Data=70.99377823 | Physics=12.78568039 | Val RMSE: 2.71245503 | ‚àö(Val Loss) = 1.62977564 | Current Learning Rate: 0.001\nEpoch 713/1000 | Train Loss=7031.39331055 | Val Loss=2.65186048 | Data=70.18335342 | Physics=8.60763217 | Val RMSE: 2.71247411 | ‚àö(Val Loss) = 1.62845337 | Current Learning Rate: 0.001\nEpoch 714/1000 | Train Loss=7399.08642578 | Val Loss=2.57625198 | Data=73.85249329 | Physics=13.46555209 | Val RMSE: 2.71221972 | ‚àö(Val Loss) = 1.60507071 | Current Learning Rate: 0.001\nEpoch 715/1000 | Train Loss=7401.23388672 | Val Loss=2.57986569 | Data=73.87177658 | Physics=15.24705561 | Val RMSE: 2.71228528 | ‚àö(Val Loss) = 1.60619605 | Current Learning Rate: 0.001\nEpoch 716/1000 | Train Loss=7210.04101562 | Val Loss=2.59001422 | Data=71.96403885 | Physics=13.82677287 | Val RMSE: 2.71195984 | ‚àö(Val Loss) = 1.60935211 | Current Learning Rate: 0.001\nEpoch 717/1000 | Train Loss=7659.07031250 | Val Loss=2.57919073 | Data=76.45380974 | Physics=13.05687722 | Val RMSE: 2.71236205 | ‚àö(Val Loss) = 1.60598588 | Current Learning Rate: 0.001\nEpoch 718/1000 | Train Loss=7234.11279297 | Val Loss=2.57311559 | Data=72.19489670 | Physics=17.88114852 | Val RMSE: 2.71236610 | ‚àö(Val Loss) = 1.60409343 | Current Learning Rate: 0.001\nEpoch 719/1000 | Train Loss=7332.27856445 | Val Loss=2.56815696 | Data=73.18590927 | Physics=13.63682772 | Val RMSE: 2.71235871 | ‚àö(Val Loss) = 1.60254705 | Current Learning Rate: 0.001\nEpoch 720/1000 | Train Loss=6793.36523438 | Val Loss=2.56567860 | Data=67.79443359 | Physics=15.11786784 | Val RMSE: 2.71237350 | ‚àö(Val Loss) = 1.60177362 | Current Learning Rate: 0.001\nEpoch 721/1000 | Train Loss=7517.61743164 | Val Loss=2.56010866 | Data=75.03467560 | Physics=15.95740950 | Val RMSE: 2.71236444 | ‚àö(Val Loss) = 1.60003400 | Current Learning Rate: 0.001\n\n Epoch :  720 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 722/1000 | Train Loss=7167.75952148 | Val Loss=2.59067988 | Data=71.54556847 | Physics=10.51210851 | Val RMSE: 2.71180820 | ‚àö(Val Loss) = 1.60955894 | Current Learning Rate: 0.001\nEpoch 723/1000 | Train Loss=7640.46337891 | Val Loss=2.55701327 | Data=76.26397705 | Physics=16.08046558 | Val RMSE: 2.71186638 | ‚àö(Val Loss) = 1.59906638 | Current Learning Rate: 0.001\nEpoch 724/1000 | Train Loss=7577.28613281 | Val Loss=2.53603649 | Data=75.63738251 | Physics=12.50925463 | Val RMSE: 2.71192026 | ‚àö(Val Loss) = 1.59249377 | Current Learning Rate: 0.001\nEpoch 725/1000 | Train Loss=7301.28149414 | Val Loss=2.52140188 | Data=72.86720276 | Physics=20.79377734 | Val RMSE: 2.71190643 | ‚àö(Val Loss) = 1.58789229 | Current Learning Rate: 0.001\nEpoch 726/1000 | Train Loss=7736.75170898 | Val Loss=2.68097639 | Data=77.22561646 | Physics=15.54200872 | Val RMSE: 2.71248770 | ‚àö(Val Loss) = 1.63736880 | Current Learning Rate: 0.001\nEpoch 727/1000 | Train Loss=7524.65063477 | Val Loss=2.65238237 | Data=75.10352516 | Physics=17.48693064 | Val RMSE: 2.71244979 | ‚àö(Val Loss) = 1.62861359 | Current Learning Rate: 0.001\nEpoch 728/1000 | Train Loss=7380.37500000 | Val Loss=2.62745762 | Data=73.65800476 | Physics=17.70114718 | Val RMSE: 2.71242261 | ‚àö(Val Loss) = 1.62094343 | Current Learning Rate: 0.001\nEpoch 729/1000 | Train Loss=7280.99829102 | Val Loss=2.53402376 | Data=72.67778015 | Physics=10.58366078 | Val RMSE: 2.71206069 | ‚àö(Val Loss) = 1.59186172 | Current Learning Rate: 0.001\nEpoch 730/1000 | Train Loss=6941.04516602 | Val Loss=2.52198219 | Data=69.27791405 | Physics=11.98015080 | Val RMSE: 2.71202493 | ‚àö(Val Loss) = 1.58807504 | Current Learning Rate: 0.001\nEpoch 731/1000 | Train Loss=7289.81518555 | Val Loss=2.50925970 | Data=72.76371765 | Physics=11.52716126 | Val RMSE: 2.71198010 | ‚àö(Val Loss) = 1.58406425 | Current Learning Rate: 0.001\nEpoch 732/1000 | Train Loss=7388.88891602 | Val Loss=2.51084638 | Data=73.74974823 | Physics=15.60959806 | Val RMSE: 2.71200609 | ‚àö(Val Loss) = 1.58456504 | Current Learning Rate: 0.001\nEpoch 733/1000 | Train Loss=7223.01733398 | Val Loss=2.50078273 | Data=72.09279633 | Physics=14.49400450 | Val RMSE: 2.71195269 | ‚àö(Val Loss) = 1.58138633 | Current Learning Rate: 0.001\nEpoch 734/1000 | Train Loss=7092.67773438 | Val Loss=2.48802209 | Data=70.78715897 | Physics=16.31494761 | Val RMSE: 2.71191740 | ‚àö(Val Loss) = 1.57734656 | Current Learning Rate: 0.001\nEpoch 735/1000 | Train Loss=7926.33813477 | Val Loss=2.79992580 | Data=79.11742210 | Physics=18.18589840 | Val RMSE: 2.71255922 | ‚àö(Val Loss) = 1.67329788 | Current Learning Rate: 0.001\nEpoch 736/1000 | Train Loss=7308.42822266 | Val Loss=2.74016356 | Data=72.94910431 | Physics=11.70797571 | Val RMSE: 2.71250796 | ‚àö(Val Loss) = 1.65534389 | Current Learning Rate: 0.001\nEpoch 737/1000 | Train Loss=7153.56567383 | Val Loss=2.69600439 | Data=71.39669037 | Physics=14.31362069 | Val RMSE: 2.71246505 | ‚àö(Val Loss) = 1.64195144 | Current Learning Rate: 0.001\nEpoch 738/1000 | Train Loss=7626.84399414 | Val Loss=2.65611601 | Data=76.12287521 | Physics=16.53177988 | Val RMSE: 2.71242571 | ‚àö(Val Loss) = 1.62975955 | Current Learning Rate: 0.001\nEpoch 739/1000 | Train Loss=7416.14355469 | Val Loss=2.53647566 | Data=74.02690887 | Physics=13.28158996 | Val RMSE: 2.71175694 | ‚àö(Val Loss) = 1.59263170 | Current Learning Rate: 0.001\nEpoch 740/1000 | Train Loss=7507.49951172 | Val Loss=2.51065922 | Data=74.93833542 | Physics=13.77509400 | Val RMSE: 2.71180582 | ‚àö(Val Loss) = 1.58450603 | Current Learning Rate: 0.001\nEpoch 741/1000 | Train Loss=7702.30102539 | Val Loss=2.70692039 | Data=76.88819504 | Physics=11.95841083 | Val RMSE: 2.71247721 | ‚àö(Val Loss) = 1.64527214 | Current Learning Rate: 0.001\n\n Epoch :  740 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 742/1000 | Train Loss=6971.87744141 | Val Loss=2.51101899 | Data=69.59408951 | Physics=6.26249194 | Val RMSE: 2.71179152 | ‚àö(Val Loss) = 1.58461952 | Current Learning Rate: 0.001\nEpoch 743/1000 | Train Loss=7117.19848633 | Val Loss=2.49385309 | Data=71.03519821 | Physics=14.32102159 | Val RMSE: 2.71180201 | ‚àö(Val Loss) = 1.57919383 | Current Learning Rate: 0.001\nEpoch 744/1000 | Train Loss=6926.59252930 | Val Loss=2.48344111 | Data=69.13424873 | Physics=10.56113951 | Val RMSE: 2.71184349 | ‚àö(Val Loss) = 1.57589376 | Current Learning Rate: 0.001\nEpoch 745/1000 | Train Loss=7638.01977539 | Val Loss=2.77571154 | Data=76.23635483 | Physics=16.52469220 | Val RMSE: 2.71253014 | ‚àö(Val Loss) = 1.66604662 | Current Learning Rate: 0.001\nEpoch 746/1000 | Train Loss=7640.97045898 | Val Loss=2.49338055 | Data=76.27207565 | Physics=14.61415132 | Val RMSE: 2.71185303 | ‚àö(Val Loss) = 1.57904422 | Current Learning Rate: 0.001\nEpoch 747/1000 | Train Loss=7602.76367188 | Val Loss=2.73373175 | Data=75.88562012 | Physics=16.56673733 | Val RMSE: 2.71250057 | ‚àö(Val Loss) = 1.65340006 | Current Learning Rate: 0.001\nEpoch 748/1000 | Train Loss=7470.08471680 | Val Loss=2.66052866 | Data=74.56442261 | Physics=12.84986189 | Val RMSE: 2.71245408 | ‚àö(Val Loss) = 1.63111269 | Current Learning Rate: 0.001\nEpoch 749/1000 | Train Loss=6764.74975586 | Val Loss=2.51482844 | Data=67.51962852 | Physics=9.59595647 | Val RMSE: 2.71179652 | ‚àö(Val Loss) = 1.58582103 | Current Learning Rate: 0.001\nEpoch 750/1000 | Train Loss=7039.94873047 | Val Loss=2.68480587 | Data=70.26628494 | Physics=10.26970716 | Val RMSE: 2.71247673 | ‚àö(Val Loss) = 1.63853776 | Current Learning Rate: 0.001\nEpoch 751/1000 | Train Loss=7092.55834961 | Val Loss=2.64902711 | Data=70.78830528 | Physics=13.83845729 | Val RMSE: 2.71243596 | ‚àö(Val Loss) = 1.62758327 | Current Learning Rate: 0.001\nEpoch 752/1000 | Train Loss=7535.95532227 | Val Loss=2.55359674 | Data=75.21930885 | Physics=15.77053927 | Val RMSE: 2.71225667 | ‚àö(Val Loss) = 1.59799778 | Current Learning Rate: 0.001\nEpoch 753/1000 | Train Loss=7011.97924805 | Val Loss=2.72666955 | Data=69.98298264 | Physics=13.42059230 | Val RMSE: 2.71263051 | ‚àö(Val Loss) = 1.65126300 | Current Learning Rate: 0.001\nEpoch 754/1000 | Train Loss=7440.49023438 | Val Loss=2.52602005 | Data=74.26939011 | Physics=12.48227718 | Val RMSE: 2.71213603 | ‚àö(Val Loss) = 1.58934581 | Current Learning Rate: 0.001\nEpoch 755/1000 | Train Loss=7260.58764648 | Val Loss=2.51316381 | Data=72.46335983 | Physics=18.40488358 | Val RMSE: 2.71209097 | ‚àö(Val Loss) = 1.58529615 | Current Learning Rate: 0.001\nEpoch 756/1000 | Train Loss=7542.90551758 | Val Loss=2.50116873 | Data=75.29154968 | Physics=13.80778975 | Val RMSE: 2.71204257 | ‚àö(Val Loss) = 1.58150840 | Current Learning Rate: 0.001\nEpoch 757/1000 | Train Loss=7280.41625977 | Val Loss=2.49003077 | Data=72.66694260 | Physics=14.90732904 | Val RMSE: 2.71200418 | ‚àö(Val Loss) = 1.57798314 | Current Learning Rate: 0.001\nEpoch 758/1000 | Train Loss=7219.67041016 | Val Loss=2.79698563 | Data=72.06526184 | Physics=9.80166466 | Val RMSE: 2.71257854 | ‚àö(Val Loss) = 1.67241907 | Current Learning Rate: 0.001\nEpoch 759/1000 | Train Loss=7059.27856445 | Val Loss=2.52490187 | Data=70.45507622 | Physics=13.61851207 | Val RMSE: 2.71210051 | ‚àö(Val Loss) = 1.58899403 | Current Learning Rate: 0.001\nEpoch 760/1000 | Train Loss=7110.82373047 | Val Loss=2.70500779 | Data=70.97643089 | Physics=10.68573095 | Val RMSE: 2.71156597 | ‚àö(Val Loss) = 1.64469075 | Current Learning Rate: 0.001\nEpoch 761/1000 | Train Loss=7211.10693359 | Val Loss=2.51581168 | Data=71.96666718 | Physics=17.67695302 | Val RMSE: 2.71218419 | ‚àö(Val Loss) = 1.58613110 | Current Learning Rate: 0.001\n\n Epoch :  760 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 762/1000 | Train Loss=7666.69628906 | Val Loss=2.62748623 | Data=76.52865410 | Physics=13.86608634 | Val RMSE: 2.71166039 | ‚àö(Val Loss) = 1.62095225 | Current Learning Rate: 0.001\nEpoch 763/1000 | Train Loss=6805.24829102 | Val Loss=2.58057785 | Data=67.92315674 | Physics=10.33464659 | Val RMSE: 2.71172976 | ‚àö(Val Loss) = 1.60641766 | Current Learning Rate: 0.001\nEpoch 764/1000 | Train Loss=7577.44702148 | Val Loss=2.56717658 | Data=75.63479996 | Physics=15.01583597 | Val RMSE: 2.71231890 | ‚àö(Val Loss) = 1.60224116 | Current Learning Rate: 0.001\nEpoch 765/1000 | Train Loss=7307.23657227 | Val Loss=2.55302405 | Data=72.93223953 | Physics=14.68025175 | Val RMSE: 2.71229267 | ‚àö(Val Loss) = 1.59781849 | Current Learning Rate: 0.001\nEpoch 766/1000 | Train Loss=7292.58325195 | Val Loss=2.54121518 | Data=72.78678894 | Physics=14.65342857 | Val RMSE: 2.71227193 | ‚àö(Val Loss) = 1.59411895 | Current Learning Rate: 0.001\nEpoch 767/1000 | Train Loss=7170.94067383 | Val Loss=2.52864742 | Data=71.57317734 | Physics=12.35168181 | Val RMSE: 2.71224618 | ‚àö(Val Loss) = 1.59017217 | Current Learning Rate: 0.001\nEpoch 768/1000 | Train Loss=7594.56616211 | Val Loss=2.51933122 | Data=75.80782700 | Physics=13.96152309 | Val RMSE: 2.71222854 | ‚àö(Val Loss) = 1.58724010 | Current Learning Rate: 0.001\nEpoch 769/1000 | Train Loss=7476.23071289 | Val Loss=2.55383754 | Data=74.62824631 | Physics=13.24393994 | Val RMSE: 2.71185374 | ‚àö(Val Loss) = 1.59807312 | Current Learning Rate: 0.001\nEpoch 770/1000 | Train Loss=6800.98217773 | Val Loss=2.53329754 | Data=67.87854195 | Physics=12.43103926 | Val RMSE: 2.71183634 | ‚àö(Val Loss) = 1.59163356 | Current Learning Rate: 0.001\nEpoch 771/1000 | Train Loss=7200.22900391 | Val Loss=2.51432538 | Data=71.86827469 | Physics=13.24849834 | Val RMSE: 2.71184039 | ‚àö(Val Loss) = 1.58566248 | Current Learning Rate: 0.001\nEpoch 772/1000 | Train Loss=6995.00268555 | Val Loss=2.67017412 | Data=69.81596756 | Physics=10.81211183 | Val RMSE: 2.71245670 | ‚àö(Val Loss) = 1.63406670 | Current Learning Rate: 0.001\nEpoch 773/1000 | Train Loss=7266.57324219 | Val Loss=2.63880563 | Data=72.52377701 | Physics=15.99471771 | Val RMSE: 2.71241426 | ‚àö(Val Loss) = 1.62444007 | Current Learning Rate: 0.001\nEpoch 774/1000 | Train Loss=7120.96850586 | Val Loss=2.60978413 | Data=71.07143974 | Physics=13.41312140 | Val RMSE: 2.71237230 | ‚àö(Val Loss) = 1.61548269 | Current Learning Rate: 0.001\nEpoch 775/1000 | Train Loss=7142.44726562 | Val Loss=2.54486418 | Data=71.29450417 | Physics=7.76784472 | Val RMSE: 2.71168423 | ‚àö(Val Loss) = 1.59526300 | Current Learning Rate: 0.001\nEpoch 776/1000 | Train Loss=7351.87109375 | Val Loss=2.57437181 | Data=73.38362122 | Physics=13.13585382 | Val RMSE: 2.71226072 | ‚àö(Val Loss) = 1.60448492 | Current Learning Rate: 0.001\nEpoch 777/1000 | Train Loss=7751.15429688 | Val Loss=2.54513407 | Data=77.36991692 | Physics=16.58985736 | Val RMSE: 2.71179771 | ‚àö(Val Loss) = 1.59534764 | Current Learning Rate: 0.001\nEpoch 778/1000 | Train Loss=7222.81420898 | Val Loss=2.59680152 | Data=72.08714294 | Physics=14.50777585 | Val RMSE: 2.71232986 | ‚àö(Val Loss) = 1.61145949 | Current Learning Rate: 0.001\nEpoch 779/1000 | Train Loss=6975.34887695 | Val Loss=2.53497839 | Data=69.61773682 | Physics=14.74150896 | Val RMSE: 2.71178889 | ‚àö(Val Loss) = 1.59216154 | Current Learning Rate: 0.001\nEpoch 780/1000 | Train Loss=7710.69726562 | Val Loss=2.61892343 | Data=76.96437454 | Physics=15.57123607 | Val RMSE: 2.71237230 | ‚àö(Val Loss) = 1.61830878 | Current Learning Rate: 0.001\nEpoch 781/1000 | Train Loss=7259.31030273 | Val Loss=2.52846575 | Data=72.45684052 | Physics=14.14854633 | Val RMSE: 2.71176863 | ‚àö(Val Loss) = 1.59011495 | Current Learning Rate: 0.001\n\n Epoch :  780 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 782/1000 | Train Loss=7574.65600586 | Val Loss=2.50835609 | Data=75.60124207 | Physics=19.66239338 | Val RMSE: 2.71181607 | ‚àö(Val Loss) = 1.58377910 | Current Learning Rate: 0.001\nEpoch 783/1000 | Train Loss=7427.29394531 | Val Loss=2.67532277 | Data=74.12610626 | Physics=17.38997325 | Val RMSE: 2.71243358 | ‚àö(Val Loss) = 1.63564134 | Current Learning Rate: 0.001\nEpoch 784/1000 | Train Loss=6911.65625000 | Val Loss=2.50882077 | Data=68.98607635 | Physics=12.17528978 | Val RMSE: 2.71176410 | ‚àö(Val Loss) = 1.58392572 | Current Learning Rate: 0.001\nEpoch 785/1000 | Train Loss=7661.52294922 | Val Loss=2.68817878 | Data=76.47196388 | Physics=16.95266079 | Val RMSE: 2.71244597 | ‚àö(Val Loss) = 1.63956666 | Current Learning Rate: 0.001\nEpoch 786/1000 | Train Loss=7593.22094727 | Val Loss=2.64457870 | Data=75.79890060 | Physics=10.74460969 | Val RMSE: 2.71240044 | ‚àö(Val Loss) = 1.62621605 | Current Learning Rate: 0.001\nEpoch 787/1000 | Train Loss=7006.96582031 | Val Loss=2.53783154 | Data=69.93243790 | Physics=15.30726663 | Val RMSE: 2.71167159 | ‚àö(Val Loss) = 1.59305727 | Current Learning Rate: 0.001\nEpoch 788/1000 | Train Loss=7330.07177734 | Val Loss=2.50119805 | Data=73.16457367 | Physics=14.83315859 | Val RMSE: 2.71173692 | ‚àö(Val Loss) = 1.58151770 | Current Learning Rate: 0.001\nEpoch 789/1000 | Train Loss=7499.27197266 | Val Loss=2.71298432 | Data=74.84642410 | Physics=17.91104666 | Val RMSE: 2.71247721 | ‚àö(Val Loss) = 1.64711392 | Current Learning Rate: 0.001\nEpoch 790/1000 | Train Loss=7153.17553711 | Val Loss=2.50092936 | Data=71.39407349 | Physics=18.05119108 | Val RMSE: 2.71178317 | ‚àö(Val Loss) = 1.58143270 | Current Learning Rate: 0.001\nEpoch 791/1000 | Train Loss=6939.41845703 | Val Loss=2.48155046 | Data=69.26027107 | Physics=12.60993668 | Val RMSE: 2.71179676 | ‚àö(Val Loss) = 1.57529378 | Current Learning Rate: 0.001\nEpoch 792/1000 | Train Loss=6794.84130859 | Val Loss=2.47134924 | Data=67.81695747 | Physics=10.64616565 | Val RMSE: 2.71184611 | ‚àö(Val Loss) = 1.57205260 | Current Learning Rate: 0.001\nEpoch 793/1000 | Train Loss=7255.53295898 | Val Loss=2.46387744 | Data=72.41350174 | Physics=17.96426497 | Val RMSE: 2.71186447 | ‚àö(Val Loss) = 1.56967425 | Current Learning Rate: 0.001\nEpoch 794/1000 | Train Loss=7653.40844727 | Val Loss=2.81184816 | Data=76.39287949 | Physics=14.60677129 | Val RMSE: 2.71256757 | ‚àö(Val Loss) = 1.67685664 | Current Learning Rate: 0.001\nEpoch 795/1000 | Train Loss=8020.79577637 | Val Loss=2.71811318 | Data=80.06666756 | Physics=15.79275050 | Val RMSE: 2.71249223 | ‚àö(Val Loss) = 1.64867008 | Current Learning Rate: 0.001\nEpoch 796/1000 | Train Loss=7324.19970703 | Val Loss=2.49660826 | Data=73.10614014 | Physics=14.30021699 | Val RMSE: 2.71183491 | ‚àö(Val Loss) = 1.58006585 | Current Learning Rate: 0.001\nEpoch 797/1000 | Train Loss=7092.83520508 | Val Loss=2.48126316 | Data=70.79571724 | Physics=12.22844267 | Val RMSE: 2.71183562 | ‚àö(Val Loss) = 1.57520258 | Current Learning Rate: 0.001\nEpoch 798/1000 | Train Loss=7548.03295898 | Val Loss=2.74093437 | Data=75.33857536 | Physics=15.15510091 | Val RMSE: 2.71250820 | ‚àö(Val Loss) = 1.65557671 | Current Learning Rate: 0.001\nEpoch 799/1000 | Train Loss=7264.16088867 | Val Loss=2.52819943 | Data=72.50514984 | Physics=14.25023954 | Val RMSE: 2.71212506 | ‚àö(Val Loss) = 1.59003127 | Current Learning Rate: 0.001\nEpoch 800/1000 | Train Loss=6811.32763672 | Val Loss=2.51627231 | Data=67.98217773 | Physics=10.27003856 | Val RMSE: 2.71207356 | ‚àö(Val Loss) = 1.58627629 | Current Learning Rate: 0.0001\nEpoch 801/1000 | Train Loss=7441.25097656 | Val Loss=2.33975601 | Data=74.27795029 | Physics=13.56508932 | Val RMSE: 2.71164489 | ‚àö(Val Loss) = 1.52962613 | Current Learning Rate: 0.0001\n\n Epoch :  800 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 802/1000 | Train Loss=7764.68530273 | Val Loss=2.32913017 | Data=77.51404190 | Physics=12.34647780 | Val RMSE: 2.71160626 | ‚àö(Val Loss) = 1.52614880 | Current Learning Rate: 0.0001\nEpoch 803/1000 | Train Loss=7358.90576172 | Val Loss=2.32068205 | Data=73.45972824 | Physics=10.00222026 | Val RMSE: 2.71157098 | ‚àö(Val Loss) = 1.52337849 | Current Learning Rate: 0.0001\nEpoch 804/1000 | Train Loss=7507.76611328 | Val Loss=2.31479979 | Data=74.94563293 | Physics=12.16709386 | Val RMSE: 2.71154404 | ‚àö(Val Loss) = 1.52144659 | Current Learning Rate: 0.0001\nEpoch 805/1000 | Train Loss=6619.03381348 | Val Loss=2.31003952 | Data=66.06138039 | Physics=10.23145987 | Val RMSE: 2.71135712 | ‚àö(Val Loss) = 1.51988137 | Current Learning Rate: 0.0001\nEpoch 806/1000 | Train Loss=7183.96362305 | Val Loss=2.31376219 | Data=71.71008301 | Physics=10.04436586 | Val RMSE: 2.71153665 | ‚àö(Val Loss) = 1.52110553 | Current Learning Rate: 0.0001\nEpoch 807/1000 | Train Loss=6998.46801758 | Val Loss=2.30913186 | Data=69.84896469 | Physics=14.87746699 | Val RMSE: 2.71137404 | ‚àö(Val Loss) = 1.51958275 | Current Learning Rate: 0.0001\nEpoch 808/1000 | Train Loss=6780.92724609 | Val Loss=2.30516982 | Data=67.67390633 | Physics=14.51700531 | Val RMSE: 2.71137834 | ‚àö(Val Loss) = 1.51827860 | Current Learning Rate: 0.0001\nEpoch 809/1000 | Train Loss=6997.01513672 | Val Loss=2.32274032 | Data=69.84092331 | Physics=9.83655017 | Val RMSE: 2.71157694 | ‚àö(Val Loss) = 1.52405393 | Current Learning Rate: 0.0001\nEpoch 810/1000 | Train Loss=7183.54833984 | Val Loss=2.37585807 | Data=71.69900131 | Physics=14.92731421 | Val RMSE: 2.71172166 | ‚àö(Val Loss) = 1.54138184 | Current Learning Rate: 0.0001\nEpoch 811/1000 | Train Loss=6972.71435547 | Val Loss=2.36048651 | Data=69.59192657 | Physics=13.88866015 | Val RMSE: 2.71170807 | ‚àö(Val Loss) = 1.53638744 | Current Learning Rate: 0.0001\nEpoch 812/1000 | Train Loss=6724.13354492 | Val Loss=2.34806180 | Data=67.11307907 | Physics=9.31416617 | Val RMSE: 2.71166825 | ‚àö(Val Loss) = 1.53233862 | Current Learning Rate: 0.0001\nEpoch 813/1000 | Train Loss=6923.44360352 | Val Loss=2.33408856 | Data=69.10197639 | Physics=12.24160118 | Val RMSE: 2.71161628 | ‚àö(Val Loss) = 1.52777243 | Current Learning Rate: 0.0001\nEpoch 814/1000 | Train Loss=7177.17358398 | Val Loss=2.38787079 | Data=71.63705444 | Physics=12.97855455 | Val RMSE: 2.71173692 | ‚àö(Val Loss) = 1.54527366 | Current Learning Rate: 0.0001\nEpoch 815/1000 | Train Loss=6983.17382812 | Val Loss=2.36221027 | Data=69.70322418 | Physics=9.16849155 | Val RMSE: 2.71167994 | ‚àö(Val Loss) = 1.53694832 | Current Learning Rate: 0.0001\nEpoch 816/1000 | Train Loss=7060.06103516 | Val Loss=2.35019326 | Data=70.46412849 | Physics=14.92601160 | Val RMSE: 2.71167612 | ‚àö(Val Loss) = 1.53303397 | Current Learning Rate: 0.0001\nEpoch 817/1000 | Train Loss=7243.87280273 | Val Loss=2.33679056 | Data=72.30436707 | Physics=13.47922787 | Val RMSE: 2.71162772 | ‚àö(Val Loss) = 1.52865648 | Current Learning Rate: 0.0001\nEpoch 818/1000 | Train Loss=6874.19409180 | Val Loss=2.32676864 | Data=68.61023712 | Physics=11.81550516 | Val RMSE: 2.71158791 | ‚àö(Val Loss) = 1.52537489 | Current Learning Rate: 0.0001\nEpoch 819/1000 | Train Loss=7382.09765625 | Val Loss=2.32058907 | Data=73.68452072 | Physics=15.02443894 | Val RMSE: 2.71156216 | ‚àö(Val Loss) = 1.52334797 | Current Learning Rate: 0.0001\nEpoch 820/1000 | Train Loss=7231.80224609 | Val Loss=2.31763387 | Data=72.18270874 | Physics=14.39052792 | Val RMSE: 2.71154976 | ‚àö(Val Loss) = 1.52237773 | Current Learning Rate: 0.0001\nEpoch 821/1000 | Train Loss=7258.90991211 | Val Loss=2.36674714 | Data=72.45088196 | Physics=16.01302062 | Val RMSE: 2.71169162 | ‚àö(Val Loss) = 1.53842354 | Current Learning Rate: 0.0001\n\n Epoch :  820 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 822/1000 | Train Loss=7140.78125000 | Val Loss=2.30802202 | Data=71.27354050 | Physics=13.72166145 | Val RMSE: 2.71148968 | ‚àö(Val Loss) = 1.51921761 | Current Learning Rate: 0.0001\nEpoch 823/1000 | Train Loss=7504.46826172 | Val Loss=2.34583783 | Data=74.90624809 | Physics=16.12017022 | Val RMSE: 2.71163797 | ‚àö(Val Loss) = 1.53161287 | Current Learning Rate: 0.0001\nEpoch 824/1000 | Train Loss=7120.25366211 | Val Loss=2.30695391 | Data=71.06154251 | Physics=18.56010644 | Val RMSE: 2.71147227 | ‚àö(Val Loss) = 1.51886594 | Current Learning Rate: 0.0001\nEpoch 825/1000 | Train Loss=7802.78393555 | Val Loss=2.33800507 | Data=77.88745308 | Physics=17.41757570 | Val RMSE: 2.71162176 | ‚àö(Val Loss) = 1.52905369 | Current Learning Rate: 0.0001\nEpoch 826/1000 | Train Loss=7320.57788086 | Val Loss=2.32622123 | Data=73.07078552 | Physics=13.80575911 | Val RMSE: 2.71158290 | ‚àö(Val Loss) = 1.52519548 | Current Learning Rate: 0.0001\nEpoch 827/1000 | Train Loss=6791.98803711 | Val Loss=2.30569983 | Data=67.79261971 | Physics=8.77565110 | Val RMSE: 2.71142149 | ‚àö(Val Loss) = 1.51845312 | Current Learning Rate: 0.0001\nEpoch 828/1000 | Train Loss=6763.54638672 | Val Loss=2.29592896 | Data=67.50010872 | Physics=14.65353043 | Val RMSE: 2.71139860 | ‚àö(Val Loss) = 1.51523232 | Current Learning Rate: 0.0001\nEpoch 829/1000 | Train Loss=7130.69970703 | Val Loss=2.34233451 | Data=71.17076111 | Physics=14.67305052 | Val RMSE: 2.71159625 | ‚àö(Val Loss) = 1.53046870 | Current Learning Rate: 0.0001\nEpoch 830/1000 | Train Loss=7673.42114258 | Val Loss=2.32862401 | Data=76.59577751 | Physics=16.22940474 | Val RMSE: 2.71156359 | ‚àö(Val Loss) = 1.52598298 | Current Learning Rate: 0.0001\nEpoch 831/1000 | Train Loss=7553.02001953 | Val Loss=2.31881189 | Data=75.39806366 | Physics=12.04209059 | Val RMSE: 2.71153498 | ‚àö(Val Loss) = 1.52276456 | Current Learning Rate: 0.0001\nEpoch 832/1000 | Train Loss=6867.16015625 | Val Loss=2.31449914 | Data=68.53839874 | Physics=12.75164053 | Val RMSE: 2.71152377 | ‚àö(Val Loss) = 1.52134776 | Current Learning Rate: 0.0001\nEpoch 833/1000 | Train Loss=7206.71484375 | Val Loss=2.32960773 | Data=71.93539810 | Physics=11.76851174 | Val RMSE: 2.71159363 | ‚àö(Val Loss) = 1.52630520 | Current Learning Rate: 0.0001\nEpoch 834/1000 | Train Loss=7291.15380859 | Val Loss=2.37078857 | Data=72.78145218 | Physics=10.43467703 | Val RMSE: 2.71171117 | ‚àö(Val Loss) = 1.53973651 | Current Learning Rate: 0.0001\nEpoch 835/1000 | Train Loss=7422.84204102 | Val Loss=2.35049653 | Data=74.08665085 | Physics=18.01121245 | Val RMSE: 2.71165657 | ‚àö(Val Loss) = 1.53313291 | Current Learning Rate: 0.0001\nEpoch 836/1000 | Train Loss=7388.28491211 | Val Loss=2.33539701 | Data=73.74883270 | Physics=12.95066214 | Val RMSE: 2.71161008 | ‚àö(Val Loss) = 1.52820063 | Current Learning Rate: 0.0001\nEpoch 837/1000 | Train Loss=6961.55444336 | Val Loss=2.30753326 | Data=69.48315430 | Physics=12.40014275 | Val RMSE: 2.71143842 | ‚àö(Val Loss) = 1.51905668 | Current Learning Rate: 0.0001\nEpoch 838/1000 | Train Loss=7194.88232422 | Val Loss=2.30550814 | Data=71.81357193 | Physics=14.39618890 | Val RMSE: 2.71143627 | ‚àö(Val Loss) = 1.51838994 | Current Learning Rate: 0.0001\nEpoch 839/1000 | Train Loss=7544.74853516 | Val Loss=2.32997298 | Data=75.31228638 | Physics=13.75619630 | Val RMSE: 2.71159077 | ‚àö(Val Loss) = 1.52642488 | Current Learning Rate: 0.0001\nEpoch 840/1000 | Train Loss=7512.20532227 | Val Loss=2.31452465 | Data=74.98900414 | Physics=12.56039797 | Val RMSE: 2.71150827 | ‚àö(Val Loss) = 1.52135623 | Current Learning Rate: 0.0001\nEpoch 841/1000 | Train Loss=7170.16357422 | Val Loss=2.30987144 | Data=71.56564331 | Physics=14.68322716 | Val RMSE: 2.71148562 | ‚àö(Val Loss) = 1.51982617 | Current Learning Rate: 0.0001\n\n Epoch :  840 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 842/1000 | Train Loss=7090.84179688 | Val Loss=2.30632234 | Data=70.77108192 | Physics=15.87234828 | Val RMSE: 2.71146607 | ‚àö(Val Loss) = 1.51865804 | Current Learning Rate: 0.0001\nEpoch 843/1000 | Train Loss=7652.23217773 | Val Loss=2.34219027 | Data=76.38411522 | Physics=15.88388595 | Val RMSE: 2.71163535 | ‚àö(Val Loss) = 1.53042161 | Current Learning Rate: 0.0001\nEpoch 844/1000 | Train Loss=7381.64233398 | Val Loss=2.32898426 | Data=73.67740631 | Physics=16.55005127 | Val RMSE: 2.71159029 | ‚àö(Val Loss) = 1.52610099 | Current Learning Rate: 0.0001\nEpoch 845/1000 | Train Loss=7113.63378906 | Val Loss=2.31927085 | Data=71.00196457 | Physics=13.25712671 | Val RMSE: 2.71155334 | ‚àö(Val Loss) = 1.52291524 | Current Learning Rate: 0.0001\nEpoch 846/1000 | Train Loss=7138.20361328 | Val Loss=2.30393004 | Data=71.24865341 | Physics=13.21903814 | Val RMSE: 2.71136618 | ‚àö(Val Loss) = 1.51787019 | Current Learning Rate: 0.0001\nEpoch 847/1000 | Train Loss=7338.45263672 | Val Loss=2.30237460 | Data=73.24692535 | Physics=16.10956804 | Val RMSE: 2.71138334 | ‚àö(Val Loss) = 1.51735783 | Current Learning Rate: 0.0001\nEpoch 848/1000 | Train Loss=7057.76904297 | Val Loss=2.30148268 | Data=70.44248581 | Physics=14.43160768 | Val RMSE: 2.71139669 | ‚àö(Val Loss) = 1.51706386 | Current Learning Rate: 0.0001\nEpoch 849/1000 | Train Loss=7265.03710938 | Val Loss=2.32189560 | Data=72.51449966 | Physics=14.33156734 | Val RMSE: 2.71156049 | ‚àö(Val Loss) = 1.52377677 | Current Learning Rate: 0.0001\nEpoch 850/1000 | Train Loss=7390.90771484 | Val Loss=2.31426096 | Data=73.77644730 | Physics=12.61294503 | Val RMSE: 2.71153140 | ‚àö(Val Loss) = 1.52126956 | Current Learning Rate: 0.0001\nEpoch 851/1000 | Train Loss=7812.29907227 | Val Loss=2.31009388 | Data=77.99259186 | Physics=10.83346122 | Val RMSE: 2.71151543 | ‚àö(Val Loss) = 1.51989925 | Current Learning Rate: 0.0001\nEpoch 852/1000 | Train Loss=6966.11572266 | Val Loss=2.30643177 | Data=69.52533150 | Physics=14.44352357 | Val RMSE: 2.71149540 | ‚àö(Val Loss) = 1.51869404 | Current Learning Rate: 0.0001\nEpoch 853/1000 | Train Loss=7179.90649414 | Val Loss=2.30400896 | Data=71.66408157 | Physics=14.00544106 | Val RMSE: 2.71148157 | ‚àö(Val Loss) = 1.51789618 | Current Learning Rate: 0.0001\nEpoch 854/1000 | Train Loss=7086.42651367 | Val Loss=2.36307216 | Data=70.73139381 | Physics=12.50951303 | Val RMSE: 2.71167159 | ‚àö(Val Loss) = 1.53722870 | Current Learning Rate: 0.0001\nEpoch 855/1000 | Train Loss=7272.66845703 | Val Loss=2.37581348 | Data=72.58993912 | Physics=15.09179598 | Val RMSE: 2.71170545 | ‚àö(Val Loss) = 1.54136741 | Current Learning Rate: 0.0001\nEpoch 856/1000 | Train Loss=7355.54956055 | Val Loss=2.35500598 | Data=73.41611099 | Physics=16.92937484 | Val RMSE: 2.71165729 | ‚àö(Val Loss) = 1.53460288 | Current Learning Rate: 0.0001\nEpoch 857/1000 | Train Loss=6788.85668945 | Val Loss=2.34661269 | Data=67.76116753 | Physics=8.66102708 | Val RMSE: 2.71163845 | ‚àö(Val Loss) = 1.53186572 | Current Learning Rate: 0.0001\nEpoch 858/1000 | Train Loss=7128.10937500 | Val Loss=2.39670634 | Data=71.14606857 | Physics=13.97058265 | Val RMSE: 2.71176434 | ‚àö(Val Loss) = 1.54812992 | Current Learning Rate: 0.0001\nEpoch 859/1000 | Train Loss=7103.02294922 | Val Loss=2.32369518 | Data=70.89940643 | Physics=11.08346786 | Val RMSE: 2.71156144 | ‚àö(Val Loss) = 1.52436709 | Current Learning Rate: 0.0001\nEpoch 860/1000 | Train Loss=7212.81225586 | Val Loss=2.36347222 | Data=71.98802567 | Physics=16.92074738 | Val RMSE: 2.71168327 | ‚àö(Val Loss) = 1.53735888 | Current Learning Rate: 0.0001\nEpoch 861/1000 | Train Loss=7183.42871094 | Val Loss=2.34473610 | Data=71.69718552 | Physics=14.82165778 | Val RMSE: 2.71163177 | ‚àö(Val Loss) = 1.53125310 | Current Learning Rate: 0.0001\n\n Epoch :  860 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 862/1000 | Train Loss=7452.75000000 | Val Loss=2.33128834 | Data=74.38583183 | Physics=18.19431177 | Val RMSE: 2.71159077 | ‚àö(Val Loss) = 1.52685571 | Current Learning Rate: 0.0001\nEpoch 863/1000 | Train Loss=6931.97021484 | Val Loss=2.30349708 | Data=69.18314552 | Physics=15.58739263 | Val RMSE: 2.71140242 | ‚àö(Val Loss) = 1.51772761 | Current Learning Rate: 0.0001\nEpoch 864/1000 | Train Loss=7223.45922852 | Val Loss=2.32279873 | Data=72.09997177 | Physics=13.44650050 | Val RMSE: 2.71156073 | ‚àö(Val Loss) = 1.52407312 | Current Learning Rate: 0.0001\nEpoch 865/1000 | Train Loss=7331.84545898 | Val Loss=2.30460382 | Data=73.18470001 | Physics=13.35443091 | Val RMSE: 2.71140361 | ‚àö(Val Loss) = 1.51809216 | Current Learning Rate: 0.0001\nEpoch 866/1000 | Train Loss=6993.04467773 | Val Loss=2.30354452 | Data=69.79884720 | Physics=11.82362474 | Val RMSE: 2.71141362 | ‚àö(Val Loss) = 1.51774323 | Current Learning Rate: 0.0001\nEpoch 867/1000 | Train Loss=7040.70874023 | Val Loss=2.30347061 | Data=70.27368736 | Physics=13.24196003 | Val RMSE: 2.71142673 | ‚àö(Val Loss) = 1.51771891 | Current Learning Rate: 0.0001\nEpoch 868/1000 | Train Loss=7212.18017578 | Val Loss=2.30195737 | Data=71.98367310 | Physics=16.44489190 | Val RMSE: 2.71142435 | ‚àö(Val Loss) = 1.51722026 | Current Learning Rate: 0.0001\nEpoch 869/1000 | Train Loss=6743.07849121 | Val Loss=2.31221795 | Data=67.29528046 | Physics=14.58759467 | Val RMSE: 2.71151853 | ‚àö(Val Loss) = 1.52059793 | Current Learning Rate: 0.0001\nEpoch 870/1000 | Train Loss=7266.04492188 | Val Loss=2.30943346 | Data=72.53075409 | Physics=10.37968992 | Val RMSE: 2.71150112 | ‚àö(Val Loss) = 1.51968205 | Current Learning Rate: 0.0001\nEpoch 871/1000 | Train Loss=7505.79174805 | Val Loss=2.34262371 | Data=74.91663742 | Physics=18.00352240 | Val RMSE: 2.71163797 | ‚àö(Val Loss) = 1.53056324 | Current Learning Rate: 0.0001\nEpoch 872/1000 | Train Loss=7597.10888672 | Val Loss=2.32918239 | Data=75.83967972 | Physics=11.43631140 | Val RMSE: 2.71159434 | ‚àö(Val Loss) = 1.52616596 | Current Learning Rate: 0.0001\nEpoch 873/1000 | Train Loss=6802.46276855 | Val Loss=2.30159926 | Data=67.89206314 | Physics=12.62680009 | Val RMSE: 2.71139979 | ‚àö(Val Loss) = 1.51710224 | Current Learning Rate: 0.0001\nEpoch 874/1000 | Train Loss=7306.19750977 | Val Loss=2.32128930 | Data=72.92714691 | Physics=13.41983913 | Val RMSE: 2.71156192 | ‚àö(Val Loss) = 1.52357781 | Current Learning Rate: 0.0001\nEpoch 875/1000 | Train Loss=6997.54907227 | Val Loss=2.31351161 | Data=69.84198189 | Physics=12.93195750 | Val RMSE: 2.71152925 | ‚àö(Val Loss) = 1.52102315 | Current Learning Rate: 0.0001\nEpoch 876/1000 | Train Loss=7558.55151367 | Val Loss=2.30957055 | Data=75.45078278 | Physics=13.84373817 | Val RMSE: 2.71151280 | ‚àö(Val Loss) = 1.51972711 | Current Learning Rate: 0.0001\nEpoch 877/1000 | Train Loss=7353.33251953 | Val Loss=2.30608773 | Data=73.40036774 | Physics=12.60959678 | Val RMSE: 2.71149564 | ‚àö(Val Loss) = 1.51858079 | Current Learning Rate: 0.0001\nEpoch 878/1000 | Train Loss=7700.68823242 | Val Loss=2.30301499 | Data=76.86249352 | Physics=20.51161094 | Val RMSE: 2.71147776 | ‚àö(Val Loss) = 1.51756883 | Current Learning Rate: 0.0001\nEpoch 879/1000 | Train Loss=7062.07519531 | Val Loss=2.30920410 | Data=70.48630714 | Physics=13.88266338 | Val RMSE: 2.71135163 | ‚àö(Val Loss) = 1.51960659 | Current Learning Rate: 0.0001\nEpoch 880/1000 | Train Loss=7052.25341797 | Val Loss=2.30499864 | Data=70.39006615 | Physics=12.60092997 | Val RMSE: 2.71136165 | ‚àö(Val Loss) = 1.51822221 | Current Learning Rate: 0.0001\nEpoch 881/1000 | Train Loss=7457.88623047 | Val Loss=2.30250239 | Data=74.43954849 | Physics=17.56108444 | Val RMSE: 2.71137476 | ‚àö(Val Loss) = 1.51739991 | Current Learning Rate: 0.0001\n\n Epoch :  880 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 882/1000 | Train Loss=6981.32592773 | Val Loss=2.31723022 | Data=69.67857933 | Physics=13.62754737 | Val RMSE: 2.71154928 | ‚àö(Val Loss) = 1.52224517 | Current Learning Rate: 0.0001\nEpoch 883/1000 | Train Loss=7186.44287109 | Val Loss=2.31160522 | Data=71.73087311 | Physics=12.93464212 | Val RMSE: 2.71152377 | ‚àö(Val Loss) = 1.52039635 | Current Learning Rate: 0.0001\nEpoch 884/1000 | Train Loss=7512.14208984 | Val Loss=2.30722713 | Data=74.99019814 | Physics=11.49161101 | Val RMSE: 2.71150208 | ‚àö(Val Loss) = 1.51895595 | Current Learning Rate: 0.0001\nEpoch 885/1000 | Train Loss=7421.90966797 | Val Loss=2.30592203 | Data=74.08117485 | Physics=16.41828829 | Val RMSE: 2.71132731 | ‚àö(Val Loss) = 1.51852632 | Current Learning Rate: 0.0001\nEpoch 886/1000 | Train Loss=7164.72045898 | Val Loss=2.30729675 | Data=71.51673126 | Physics=10.73283606 | Val RMSE: 2.71150184 | ‚àö(Val Loss) = 1.51897883 | Current Learning Rate: 0.0001\nEpoch 887/1000 | Train Loss=7781.76733398 | Val Loss=2.30388641 | Data=77.68276787 | Physics=13.98719589 | Val RMSE: 2.71148348 | ‚àö(Val Loss) = 1.51785588 | Current Learning Rate: 0.0001\nEpoch 888/1000 | Train Loss=7270.30346680 | Val Loss=2.30146885 | Data=72.56369400 | Physics=16.87043416 | Val RMSE: 2.71146846 | ‚àö(Val Loss) = 1.51705933 | Current Learning Rate: 0.0001\nEpoch 889/1000 | Train Loss=6785.07824707 | Val Loss=2.30761671 | Data=67.71353149 | Physics=16.35992239 | Val RMSE: 2.71129870 | ‚àö(Val Loss) = 1.51908422 | Current Learning Rate: 0.0001\nEpoch 890/1000 | Train Loss=7415.55932617 | Val Loss=2.30364704 | Data=74.01592445 | Physics=17.40288659 | Val RMSE: 2.71148133 | ‚àö(Val Loss) = 1.51777697 | Current Learning Rate: 0.0001\nEpoch 891/1000 | Train Loss=7081.84082031 | Val Loss=2.30522299 | Data=70.68180466 | Physics=15.69537515 | Val RMSE: 2.71130919 | ‚àö(Val Loss) = 1.51829612 | Current Learning Rate: 0.0001\nEpoch 892/1000 | Train Loss=6928.56835938 | Val Loss=2.30179000 | Data=69.14564514 | Physics=18.28028320 | Val RMSE: 2.71134138 | ‚àö(Val Loss) = 1.51716506 | Current Learning Rate: 0.0001\nEpoch 893/1000 | Train Loss=7197.25097656 | Val Loss=2.29980302 | Data=71.83531952 | Physics=16.12580295 | Val RMSE: 2.71135783 | ‚àö(Val Loss) = 1.51651013 | Current Learning Rate: 0.0001\nEpoch 894/1000 | Train Loss=6911.95483398 | Val Loss=2.29950428 | Data=68.98780441 | Physics=12.17323346 | Val RMSE: 2.71138382 | ‚àö(Val Loss) = 1.51641166 | Current Learning Rate: 0.0001\nEpoch 895/1000 | Train Loss=7197.59765625 | Val Loss=2.29914689 | Data=71.83901596 | Physics=15.66620988 | Val RMSE: 2.71139479 | ‚àö(Val Loss) = 1.51629376 | Current Learning Rate: 0.0001\nEpoch 896/1000 | Train Loss=7812.54614258 | Val Loss=2.32097816 | Data=77.99095917 | Physics=13.68374514 | Val RMSE: 2.71155548 | ‚àö(Val Loss) = 1.52347565 | Current Learning Rate: 0.0001\nEpoch 897/1000 | Train Loss=7109.33374023 | Val Loss=2.37583613 | Data=70.96053505 | Physics=12.28621872 | Val RMSE: 2.71171188 | ‚àö(Val Loss) = 1.54137480 | Current Learning Rate: 0.0001\nEpoch 898/1000 | Train Loss=7331.54492188 | Val Loss=2.43682218 | Data=73.17395020 | Physics=17.49217545 | Val RMSE: 2.71178770 | ‚àö(Val Loss) = 1.56103241 | Current Learning Rate: 0.0001\nEpoch 899/1000 | Train Loss=7623.43383789 | Val Loss=2.39339161 | Data=76.09471130 | Physics=16.83694639 | Val RMSE: 2.71174192 | ‚àö(Val Loss) = 1.54705906 | Current Learning Rate: 0.0001\nEpoch 900/1000 | Train Loss=7009.65698242 | Val Loss=2.32271433 | Data=69.95833969 | Physics=16.23258064 | Val RMSE: 2.71157050 | ‚àö(Val Loss) = 1.52404535 | Current Learning Rate: 0.0001\nEpoch 901/1000 | Train Loss=7265.42187500 | Val Loss=2.36485147 | Data=72.51661682 | Physics=15.33219278 | Val RMSE: 2.71168709 | ‚àö(Val Loss) = 1.53780735 | Current Learning Rate: 0.0001\n\n Epoch :  900 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 902/1000 | Train Loss=7499.95922852 | Val Loss=2.34587479 | Data=74.86478424 | Physics=13.46590079 | Val RMSE: 2.71163964 | ‚àö(Val Loss) = 1.53162491 | Current Learning Rate: 0.0001\nEpoch 903/1000 | Train Loss=6810.36401367 | Val Loss=2.33557391 | Data=67.97518539 | Physics=9.52926667 | Val RMSE: 2.71161628 | ‚àö(Val Loss) = 1.52825844 | Current Learning Rate: 0.0001\nEpoch 904/1000 | Train Loss=7327.87036133 | Val Loss=2.32770634 | Data=73.13845444 | Physics=17.30588224 | Val RMSE: 2.71158981 | ‚àö(Val Loss) = 1.52568221 | Current Learning Rate: 0.0001\nEpoch 905/1000 | Train Loss=7696.83325195 | Val Loss=2.31813598 | Data=76.82602119 | Physics=19.15404643 | Val RMSE: 2.71151280 | ‚àö(Val Loss) = 1.52254260 | Current Learning Rate: 0.0001\nEpoch 906/1000 | Train Loss=7074.18212891 | Val Loss=2.31417346 | Data=70.60944939 | Physics=12.25905833 | Val RMSE: 2.71149731 | ‚àö(Val Loss) = 1.52124071 | Current Learning Rate: 0.0001\nEpoch 907/1000 | Train Loss=7380.95581055 | Val Loss=2.31302595 | Data=73.67444992 | Physics=14.23934275 | Val RMSE: 2.71149731 | ‚àö(Val Loss) = 1.52086353 | Current Learning Rate: 0.0001\nEpoch 908/1000 | Train Loss=7643.28198242 | Val Loss=2.33657527 | Data=76.29392624 | Physics=16.79520528 | Val RMSE: 2.71162558 | ‚àö(Val Loss) = 1.52858603 | Current Learning Rate: 0.0001\nEpoch 909/1000 | Train Loss=7069.66015625 | Val Loss=2.31225109 | Data=70.56093025 | Physics=14.50893218 | Val RMSE: 2.71148705 | ‚àö(Val Loss) = 1.52060878 | Current Learning Rate: 0.0001\nEpoch 910/1000 | Train Loss=6732.57348633 | Val Loss=2.36454701 | Data=67.18903160 | Physics=14.85135713 | Val RMSE: 2.71172714 | ‚àö(Val Loss) = 1.53770840 | Current Learning Rate: 0.0001\nEpoch 911/1000 | Train Loss=7197.44482422 | Val Loss=2.41056347 | Data=71.83397293 | Physics=16.94799796 | Val RMSE: 2.71179271 | ‚àö(Val Loss) = 1.55259895 | Current Learning Rate: 0.0001\nEpoch 912/1000 | Train Loss=7232.46386719 | Val Loss=2.37893224 | Data=72.19078827 | Physics=12.40356549 | Val RMSE: 2.71172643 | ‚àö(Val Loss) = 1.54237878 | Current Learning Rate: 0.0001\nEpoch 913/1000 | Train Loss=7179.70629883 | Val Loss=2.35554051 | Data=71.66745758 | Physics=10.10596112 | Val RMSE: 2.71166897 | ‚àö(Val Loss) = 1.53477705 | Current Learning Rate: 0.0001\nEpoch 914/1000 | Train Loss=7368.91040039 | Val Loss=2.30511093 | Data=73.55353928 | Physics=14.39559482 | Val RMSE: 2.71146989 | ‚àö(Val Loss) = 1.51825917 | Current Learning Rate: 0.0001\nEpoch 915/1000 | Train Loss=7355.22070312 | Val Loss=2.33661079 | Data=73.41969681 | Physics=12.31535925 | Val RMSE: 2.71161127 | ‚àö(Val Loss) = 1.52859771 | Current Learning Rate: 0.0001\nEpoch 916/1000 | Train Loss=7221.80493164 | Val Loss=2.32476640 | Data=72.09083557 | Physics=8.70562686 | Val RMSE: 2.71157312 | ‚àö(Val Loss) = 1.52471852 | Current Learning Rate: 0.0001\nEpoch 917/1000 | Train Loss=7742.55468750 | Val Loss=2.31622458 | Data=77.28584862 | Physics=17.24386617 | Val RMSE: 2.71154094 | ‚àö(Val Loss) = 1.52191472 | Current Learning Rate: 0.0001\nEpoch 918/1000 | Train Loss=7294.96264648 | Val Loss=2.31043386 | Data=72.81462860 | Physics=13.75353545 | Val RMSE: 2.71151638 | ‚àö(Val Loss) = 1.52001119 | Current Learning Rate: 0.0001\nEpoch 919/1000 | Train Loss=7076.50268555 | Val Loss=2.30266261 | Data=70.62763405 | Physics=16.12266907 | Val RMSE: 2.71134043 | ‚àö(Val Loss) = 1.51745272 | Current Learning Rate: 0.0001\nEpoch 920/1000 | Train Loss=7125.34082031 | Val Loss=2.31157255 | Data=71.11486244 | Physics=16.29647273 | Val RMSE: 2.71152186 | ‚àö(Val Loss) = 1.52038562 | Current Learning Rate: 0.0001\nEpoch 921/1000 | Train Loss=7611.53393555 | Val Loss=2.30721641 | Data=75.98139572 | Physics=13.21971966 | Val RMSE: 2.71149874 | ‚àö(Val Loss) = 1.51895237 | Current Learning Rate: 0.0001\n\n Epoch :  920 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 922/1000 | Train Loss=6969.59521484 | Val Loss=2.30500650 | Data=69.56814957 | Physics=9.08550935 | Val RMSE: 2.71133399 | ‚àö(Val Loss) = 1.51822484 | Current Learning Rate: 0.0001\nEpoch 923/1000 | Train Loss=7893.86804199 | Val Loss=2.30802846 | Data=78.79818535 | Physics=17.89942513 | Val RMSE: 2.71150327 | ‚àö(Val Loss) = 1.51921964 | Current Learning Rate: 0.0001\nEpoch 924/1000 | Train Loss=7473.50878906 | Val Loss=2.31955266 | Data=74.59645653 | Physics=16.52900393 | Val RMSE: 2.71154881 | ‚àö(Val Loss) = 1.52300775 | Current Learning Rate: 0.0001\nEpoch 925/1000 | Train Loss=7241.57910156 | Val Loss=2.35214615 | Data=72.28162003 | Physics=13.29369486 | Val RMSE: 2.71167326 | ‚àö(Val Loss) = 1.53367078 | Current Learning Rate: 0.0001\nEpoch 926/1000 | Train Loss=6977.12988281 | Val Loss=2.31153989 | Data=69.63813782 | Physics=12.78805340 | Val RMSE: 2.71148562 | ‚àö(Val Loss) = 1.52037489 | Current Learning Rate: 0.0001\nEpoch 927/1000 | Train Loss=7399.03271484 | Val Loss=2.33553767 | Data=73.85480118 | Physics=14.07155229 | Val RMSE: 2.71161556 | ‚àö(Val Loss) = 1.52824664 | Current Learning Rate: 0.0001\nEpoch 928/1000 | Train Loss=6838.40332031 | Val Loss=2.30572104 | Data=68.25168228 | Physics=12.41452507 | Val RMSE: 2.71142769 | ‚àö(Val Loss) = 1.51846004 | Current Learning Rate: 0.0001\nEpoch 929/1000 | Train Loss=7062.53491211 | Val Loss=2.30658984 | Data=70.48567390 | Physics=17.50119927 | Val RMSE: 2.71144629 | ‚àö(Val Loss) = 1.51874614 | Current Learning Rate: 0.0001\nEpoch 930/1000 | Train Loss=7557.77954102 | Val Loss=2.32975984 | Data=75.44156075 | Physics=14.92095335 | Val RMSE: 2.71158695 | ‚àö(Val Loss) = 1.52635503 | Current Learning Rate: 0.0001\nEpoch 931/1000 | Train Loss=7182.65063477 | Val Loss=2.31992793 | Data=71.68533325 | Physics=18.12129137 | Val RMSE: 2.71155119 | ‚àö(Val Loss) = 1.52313101 | Current Learning Rate: 0.0001\nEpoch 932/1000 | Train Loss=6937.76904297 | Val Loss=2.30441380 | Data=69.24626923 | Physics=11.77915370 | Val RMSE: 2.71139526 | ‚àö(Val Loss) = 1.51802957 | Current Learning Rate: 0.0001\nEpoch 933/1000 | Train Loss=7623.60571289 | Val Loss=2.31768727 | Data=76.09866714 | Physics=15.44437589 | Val RMSE: 2.71154547 | ‚àö(Val Loss) = 1.52239525 | Current Learning Rate: 0.0001\nEpoch 934/1000 | Train Loss=7339.24511719 | Val Loss=2.31164575 | Data=73.25960541 | Physics=12.46403179 | Val RMSE: 2.71151996 | ‚àö(Val Loss) = 1.52040970 | Current Learning Rate: 0.0001\nEpoch 935/1000 | Train Loss=7114.17480469 | Val Loss=2.31849694 | Data=71.00715065 | Physics=13.57701317 | Val RMSE: 2.71154022 | ‚àö(Val Loss) = 1.52266109 | Current Learning Rate: 0.0001\nEpoch 936/1000 | Train Loss=7350.52978516 | Val Loss=2.31243372 | Data=73.37279892 | Physics=12.13930684 | Val RMSE: 2.71143079 | ‚àö(Val Loss) = 1.52066886 | Current Learning Rate: 0.0001\nEpoch 937/1000 | Train Loss=7426.38793945 | Val Loss=2.32655644 | Data=74.13587761 | Physics=9.18460745 | Val RMSE: 2.71156836 | ‚àö(Val Loss) = 1.52530539 | Current Learning Rate: 0.0001\nEpoch 938/1000 | Train Loss=7445.55639648 | Val Loss=2.31856155 | Data=74.31588173 | Physics=17.21386647 | Val RMSE: 2.71153831 | ‚àö(Val Loss) = 1.52268231 | Current Learning Rate: 0.0001\nEpoch 939/1000 | Train Loss=7449.82763672 | Val Loss=2.31237626 | Data=74.36274147 | Physics=14.36444892 | Val RMSE: 2.71151209 | ‚àö(Val Loss) = 1.52064991 | Current Learning Rate: 0.0001\nEpoch 940/1000 | Train Loss=7543.79882812 | Val Loss=2.30802083 | Data=75.29789734 | Physics=17.35845471 | Val RMSE: 2.71149206 | ‚àö(Val Loss) = 1.51921713 | Current Learning Rate: 0.0001\nEpoch 941/1000 | Train Loss=6792.81555176 | Val Loss=2.30711412 | Data=67.79316711 | Physics=14.41179585 | Val RMSE: 2.71132302 | ‚àö(Val Loss) = 1.51891875 | Current Learning Rate: 0.0001\n\n Epoch :  940 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 942/1000 | Train Loss=7574.07543945 | Val Loss=2.34072208 | Data=75.60592651 | Physics=13.81611099 | Val RMSE: 2.71165085 | ‚àö(Val Loss) = 1.52994180 | Current Learning Rate: 0.0001\nEpoch 943/1000 | Train Loss=7542.78784180 | Val Loss=2.38226056 | Data=75.29525375 | Physics=11.94187349 | Val RMSE: 2.71174407 | ‚àö(Val Loss) = 1.54345739 | Current Learning Rate: 0.0001\nEpoch 944/1000 | Train Loss=7417.98144531 | Val Loss=2.35932302 | Data=74.04972649 | Physics=10.36201933 | Val RMSE: 2.71168661 | ‚àö(Val Loss) = 1.53600883 | Current Learning Rate: 0.0001\nEpoch 945/1000 | Train Loss=7535.23461914 | Val Loss=2.34205604 | Data=75.21262741 | Physics=17.10674481 | Val RMSE: 2.71163702 | ‚àö(Val Loss) = 1.53037775 | Current Learning Rate: 0.0001\nEpoch 946/1000 | Train Loss=7193.04052734 | Val Loss=2.32865191 | Data=71.79420090 | Physics=14.50728081 | Val RMSE: 2.71159077 | ‚àö(Val Loss) = 1.52599216 | Current Learning Rate: 0.0001\nEpoch 947/1000 | Train Loss=7108.76855469 | Val Loss=2.30328894 | Data=70.95312691 | Physics=14.07766116 | Val RMSE: 2.71141481 | ‚àö(Val Loss) = 1.51765907 | Current Learning Rate: 0.0001\nEpoch 948/1000 | Train Loss=7513.91967773 | Val Loss=2.32097316 | Data=75.00756073 | Physics=11.53388755 | Val RMSE: 2.71156096 | ‚àö(Val Loss) = 1.52347410 | Current Learning Rate: 0.0001\nEpoch 949/1000 | Train Loss=7312.00366211 | Val Loss=2.30184746 | Data=72.98731613 | Physics=12.65162381 | Val RMSE: 2.71138692 | ‚àö(Val Loss) = 1.51718402 | Current Learning Rate: 0.0001\nEpoch 950/1000 | Train Loss=7019.89184570 | Val Loss=2.30004549 | Data=70.06247711 | Physics=15.52771089 | Val RMSE: 2.71138978 | ‚àö(Val Loss) = 1.51659012 | Current Learning Rate: 0.0001\nEpoch 951/1000 | Train Loss=7433.46704102 | Val Loss=2.31809974 | Data=74.19850349 | Physics=14.64327744 | Val RMSE: 2.71154547 | ‚àö(Val Loss) = 1.52253067 | Current Learning Rate: 0.0001\nEpoch 952/1000 | Train Loss=7372.36083984 | Val Loss=2.31159067 | Data=73.59031677 | Physics=12.52650233 | Val RMSE: 2.71151972 | ‚àö(Val Loss) = 1.52039158 | Current Learning Rate: 0.0001\nEpoch 953/1000 | Train Loss=6797.94433594 | Val Loss=2.30214810 | Data=67.84502602 | Physics=13.91411877 | Val RMSE: 2.71136236 | ‚àö(Val Loss) = 1.51728308 | Current Learning Rate: 0.0001\nEpoch 954/1000 | Train Loss=7366.77612305 | Val Loss=2.31160665 | Data=73.53446579 | Physics=12.74145323 | Val RMSE: 2.71152020 | ‚àö(Val Loss) = 1.52039683 | Current Learning Rate: 0.0001\nEpoch 955/1000 | Train Loss=7046.33105469 | Val Loss=2.30658388 | Data=70.33416176 | Physics=9.96349198 | Val RMSE: 2.71149588 | ‚àö(Val Loss) = 1.51874423 | Current Learning Rate: 0.0001\nEpoch 956/1000 | Train Loss=6924.30688477 | Val Loss=2.30319071 | Data=69.10953140 | Physics=13.40729936 | Val RMSE: 2.71133590 | ‚àö(Val Loss) = 1.51762664 | Current Learning Rate: 0.0001\nEpoch 957/1000 | Train Loss=7439.55639648 | Val Loss=2.30237818 | Data=74.25597000 | Physics=17.59773437 | Val RMSE: 2.71137619 | ‚àö(Val Loss) = 1.51735890 | Current Learning Rate: 0.0001\nEpoch 958/1000 | Train Loss=7543.33105469 | Val Loss=2.31355214 | Data=75.29971504 | Physics=12.99744601 | Val RMSE: 2.71153188 | ‚àö(Val Loss) = 1.52103651 | Current Learning Rate: 0.0001\nEpoch 959/1000 | Train Loss=7466.67504883 | Val Loss=2.30132818 | Data=74.53182220 | Physics=14.25249578 | Val RMSE: 2.71135712 | ‚àö(Val Loss) = 1.51701295 | Current Learning Rate: 0.0001\nEpoch 960/1000 | Train Loss=7426.64331055 | Val Loss=2.31068897 | Data=74.12550163 | Physics=18.03286780 | Val RMSE: 2.71151662 | ‚àö(Val Loss) = 1.52009511 | Current Learning Rate: 0.0001\nEpoch 961/1000 | Train Loss=7497.54980469 | Val Loss=2.30630946 | Data=74.84580994 | Physics=10.20956955 | Val RMSE: 2.71149611 | ‚àö(Val Loss) = 1.51865387 | Current Learning Rate: 0.0001\n\n Epoch :  960 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 962/1000 | Train Loss=7521.31738281 | Val Loss=2.30269289 | Data=75.07755661 | Physics=14.39401299 | Val RMSE: 2.71147585 | ‚àö(Val Loss) = 1.51746261 | Current Learning Rate: 0.0001\nEpoch 963/1000 | Train Loss=7340.52636719 | Val Loss=2.30004287 | Data=73.26589203 | Physics=16.92983310 | Val RMSE: 2.71145892 | ‚àö(Val Loss) = 1.51658916 | Current Learning Rate: 0.0001\nEpoch 964/1000 | Train Loss=7143.62573242 | Val Loss=2.29833722 | Data=71.30081177 | Physics=14.29650831 | Val RMSE: 2.71144700 | ‚àö(Val Loss) = 1.51602674 | Current Learning Rate: 0.0001\nEpoch 965/1000 | Train Loss=7467.06201172 | Val Loss=2.32746434 | Data=74.53106117 | Physics=17.35303138 | Val RMSE: 2.71158290 | ‚àö(Val Loss) = 1.52560294 | Current Learning Rate: 0.0001\nEpoch 966/1000 | Train Loss=6973.54956055 | Val Loss=2.31429625 | Data=69.60056114 | Physics=14.05039980 | Val RMSE: 2.71152043 | ‚àö(Val Loss) = 1.52128112 | Current Learning Rate: 0.0001\nEpoch 967/1000 | Train Loss=7188.77856445 | Val Loss=2.34841418 | Data=71.75776291 | Physics=10.29080146 | Val RMSE: 2.71164441 | ‚àö(Val Loss) = 1.53245366 | Current Learning Rate: 0.0001\nEpoch 968/1000 | Train Loss=7564.43041992 | Val Loss=2.33372307 | Data=75.49870300 | Physics=20.72012208 | Val RMSE: 2.71160054 | ‚àö(Val Loss) = 1.52765274 | Current Learning Rate: 0.0001\nEpoch 969/1000 | Train Loss=7077.09155273 | Val Loss=2.32311177 | Data=70.64021301 | Physics=10.65254076 | Val RMSE: 2.71156454 | ‚àö(Val Loss) = 1.52417576 | Current Learning Rate: 0.0001\nEpoch 970/1000 | Train Loss=7901.44384766 | Val Loss=2.31522489 | Data=78.87840652 | Physics=14.96959567 | Val RMSE: 2.71153331 | ‚àö(Val Loss) = 1.52158630 | Current Learning Rate: 0.0001\nEpoch 971/1000 | Train Loss=7901.99890137 | Val Loss=2.31001449 | Data=78.88083839 | Physics=17.01432241 | Val RMSE: 2.71151090 | ‚àö(Val Loss) = 1.51987314 | Current Learning Rate: 0.0001\nEpoch 972/1000 | Train Loss=7334.35205078 | Val Loss=2.30537438 | Data=73.21090698 | Physics=12.30240771 | Val RMSE: 2.71137691 | ‚àö(Val Loss) = 1.51834595 | Current Learning Rate: 0.0001\nEpoch 973/1000 | Train Loss=6682.46166992 | Val Loss=2.37056041 | Data=66.69796562 | Physics=8.15971701 | Val RMSE: 2.71170878 | ‚àö(Val Loss) = 1.53966248 | Current Learning Rate: 0.0001\nEpoch 974/1000 | Train Loss=6864.20043945 | Val Loss=2.39987898 | Data=68.51180840 | Physics=10.27278715 | Val RMSE: 2.71180820 | ‚àö(Val Loss) = 1.54915428 | Current Learning Rate: 0.0001\nEpoch 975/1000 | Train Loss=7075.17211914 | Val Loss=2.37474728 | Data=70.61551094 | Physics=14.35497497 | Val RMSE: 2.71174383 | ‚àö(Val Loss) = 1.54102147 | Current Learning Rate: 0.0001\nEpoch 976/1000 | Train Loss=6639.26672363 | Val Loss=2.31980968 | Data=66.26583099 | Physics=8.30597397 | Val RMSE: 2.71154952 | ‚àö(Val Loss) = 1.52309215 | Current Learning Rate: 0.0001\nEpoch 977/1000 | Train Loss=7408.94531250 | Val Loss=2.37036562 | Data=73.95429993 | Physics=13.86021713 | Val RMSE: 2.71174884 | ‚àö(Val Loss) = 1.53959918 | Current Learning Rate: 0.0001\nEpoch 978/1000 | Train Loss=7445.62524414 | Val Loss=2.40976977 | Data=74.32277298 | Physics=12.18054756 | Val RMSE: 2.71181297 | ‚àö(Val Loss) = 1.55234337 | Current Learning Rate: 0.0001\nEpoch 979/1000 | Train Loss=7058.92333984 | Val Loss=2.33329630 | Data=70.45684052 | Physics=12.22164323 | Val RMSE: 2.71162105 | ‚àö(Val Loss) = 1.52751315 | Current Learning Rate: 0.0001\nEpoch 980/1000 | Train Loss=7057.74584961 | Val Loss=2.37856627 | Data=70.44282341 | Physics=13.78241893 | Val RMSE: 2.71168971 | ‚àö(Val Loss) = 1.54226017 | Current Learning Rate: 0.0001\nEpoch 981/1000 | Train Loss=7268.43798828 | Val Loss=2.42903233 | Data=72.55401230 | Physics=10.57122779 | Val RMSE: 2.71179581 | ‚àö(Val Loss) = 1.55853534 | Current Learning Rate: 0.0001\n\n Epoch :  980 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  1.0019,  -3.5995, -11.3073],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563],\n        [  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9908,  -3.6061, -10.8017],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9997545   -3.6670835  -14.181376  ]\n [  1.017321    -3.6561015  -14.1789875 ]\n [  0.98335093  -3.677263   -14.183639  ]\n [  0.9918498   -3.6719964  -14.182465  ]\n [  0.99420863  -3.6705313  -14.182139  ]\n [  0.97456807  -3.6826885  -14.184859  ]\n [  0.95600367  -3.6940708  -14.187476  ]\n [  1.0027694   -3.6652079  -14.180963  ]\n [  0.9554059   -3.6944363  -14.187561  ]\n [  0.9701287   -3.685412   -14.185486  ]\n [  0.9533667   -3.6956801  -14.187851  ]] \n\nFinal Test RMSE:  1.6645255088806152\nEpoch 982/1000 | Train Loss=6777.15270996 | Val Loss=2.33624744 | Data=67.64274597 | Physics=9.73153351 | Val RMSE: 2.71159911 | ‚àö(Val Loss) = 1.52847874 | Current Learning Rate: 0.0001\nEpoch 983/1000 | Train Loss=7139.37866211 | Val Loss=2.38053584 | Data=71.26390839 | Physics=10.18089676 | Val RMSE: 2.71170378 | ‚àö(Val Loss) = 1.54289854 | Current Learning Rate: 0.0001\nEpoch 984/1000 | Train Loss=7595.69897461 | Val Loss=2.35664797 | Data=75.82524872 | Physics=11.45053460 | Val RMSE: 2.71165013 | ‚àö(Val Loss) = 1.53513777 | Current Learning Rate: 0.0001\nEpoch 985/1000 | Train Loss=7385.06469727 | Val Loss=2.33906102 | Data=73.71256638 | Physics=15.74936261 | Val RMSE: 2.71160412 | ‚àö(Val Loss) = 1.52939892 | Current Learning Rate: 0.0001\nEpoch 986/1000 | Train Loss=7356.20678711 | Val Loss=2.30294585 | Data=73.42628860 | Physics=14.78204669 | Val RMSE: 2.71142983 | ‚àö(Val Loss) = 1.51754594 | Current Learning Rate: 0.0001\nEpoch 987/1000 | Train Loss=7321.49731445 | Val Loss=2.30295372 | Data=73.08001709 | Physics=14.22296555 | Val RMSE: 2.71143651 | ‚àö(Val Loss) = 1.51754856 | Current Learning Rate: 0.0001\nEpoch 988/1000 | Train Loss=7603.50854492 | Val Loss=2.30249763 | Data=75.89494705 | Physics=17.83026395 | Val RMSE: 2.71143889 | ‚àö(Val Loss) = 1.51739836 | Current Learning Rate: 0.0001\nEpoch 989/1000 | Train Loss=7257.23095703 | Val Loss=2.35803032 | Data=72.43558884 | Physics=15.02119384 | Val RMSE: 2.71168995 | ‚àö(Val Loss) = 1.53558791 | Current Learning Rate: 0.0001\nEpoch 990/1000 | Train Loss=7246.05541992 | Val Loss=2.36897683 | Data=72.32342148 | Physics=14.84515716 | Val RMSE: 2.71172047 | ‚àö(Val Loss) = 1.53914809 | Current Learning Rate: 0.0001\nEpoch 991/1000 | Train Loss=7564.15380859 | Val Loss=2.35045648 | Data=75.49515343 | Physics=21.40398001 | Val RMSE: 2.71166539 | ‚àö(Val Loss) = 1.53311980 | Current Learning Rate: 0.0001\nEpoch 992/1000 | Train Loss=7316.32934570 | Val Loss=2.31528449 | Data=73.02869415 | Physics=13.63066655 | Val RMSE: 2.71149731 | ‚àö(Val Loss) = 1.52160585 | Current Learning Rate: 0.0001\nEpoch 993/1000 | Train Loss=7463.03784180 | Val Loss=2.33613086 | Data=74.49591827 | Physics=13.29174293 | Val RMSE: 2.71161532 | ‚àö(Val Loss) = 1.52844071 | Current Learning Rate: 0.0001\nEpoch 994/1000 | Train Loss=7510.44628906 | Val Loss=2.32543302 | Data=74.96254921 | Physics=18.57548347 | Val RMSE: 2.71157718 | ‚àö(Val Loss) = 1.52493703 | Current Learning Rate: 0.0001\nEpoch 995/1000 | Train Loss=7197.52661133 | Val Loss=2.31748128 | Data=71.84106064 | Physics=13.28595120 | Val RMSE: 2.71154428 | ‚àö(Val Loss) = 1.52232754 | Current Learning Rate: 0.0001\nEpoch 996/1000 | Train Loss=7334.44653320 | Val Loss=2.30566144 | Data=73.20881271 | Physics=14.81397337 | Val RMSE: 2.71138453 | ‚àö(Val Loss) = 1.51844049 | Current Learning Rate: 0.0001\nEpoch 997/1000 | Train Loss=7088.32250977 | Val Loss=2.31383324 | Data=70.75138664 | Physics=11.67793656 | Val RMSE: 2.71152878 | ‚àö(Val Loss) = 1.52112889 | Current Learning Rate: 0.0001\nEpoch 998/1000 | Train Loss=7433.17602539 | Val Loss=2.30911446 | Data=74.19751358 | Physics=13.54837313 | Val RMSE: 2.71150684 | ‚àö(Val Loss) = 1.51957703 | Current Learning Rate: 0.0001\nEpoch 999/1000 | Train Loss=6688.87402344 | Val Loss=2.30390739 | Data=66.76121521 | Physics=9.16699363 | Val RMSE: 2.71133995 | ‚àö(Val Loss) = 1.51786280 | Current Learning Rate: 0.0001\n‚úÖ Saved last model at epoch 1000 \nEpoch 1000/1000 | Train Loss=6975.96752930 | Val Loss=2.30627584 | Data=69.62788010 | Physics=12.03068164 | Val RMSE: 2.71144533 | ‚àö(Val Loss) = 1.51864278 | Current Learning Rate: 0.0001\n‚úÖ Metrics saved successfully!\nPlot losses after training 3:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA/8AAAIjCAYAAABViau2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xd4FNXXB/Dv1mRTNgXSCCFA6CH03ktIqNKki3QBQQTEgvJDioqidBFeUAmioICgSO899CJdek1CS0jPtnn/2MxkZnZ2sxsSNiHn8zw8kNnZmbuzs2HPveeeK2MYhgEhhBBCCCGEEEJeW3JnN4AQQgghhBBCCCEFi4J/QgghhBBCCCHkNUfBPyGEEEIIIYQQ8pqj4J8QQgghhBBCCHnNUfBPCCGEEEIIIYS85ij4J4QQQgghhBBCXnMU/BNCCCGEEEIIIa85Cv4JIYQQQgghhJDXHAX/hBBCCCGEEELIa46Cf0IIeY0NHjwYZcuWzdNzp02bBplMlr8NKuL2798PmUyG/fv3c9vsvcZ37tyBTCZDTExMvrapbNmyGDx4cL4esyiLiYmBTCbDnTt3nN0UuxTE56yofXaL2ntGCCFFFQX/hBDiBDKZzK4//CCzuDGZTPjuu+9QsWJFaDQahIWFYfTo0UhNTbXr+TVq1ECZMmXAMIzVfZo2bYqAgAAYDIb8anaBOHr0KKZNm4akpCRnN4XDBmwymQyHDx+2eJxhGISEhEAmk6Fz5855OscPP/yQ750l+endd9+FXC7H8+fPBdufP38OuVwOFxcXZGZmCh67desWZDIZPv3001fZVKfQ6XRYsGABateuDa1WC29vb4SHh+Odd97B1atXnd08u8XFxeGTTz5B69at4enpmevv5qNHj6JZs2Zwc3NDYGAgxo0bJ/l7KysrCx9//DFKlSoFjUaDhg0bYteuXQX4SgghxR0F/4QQ4gSrVq0S/GnXrp3k9qpVq77UeZYvX45r167l6blTpkxBRkbGS53/ZSxYsAAffvghqlevjgULFqBv377YsWMHnj59atfzBwwYgPv37+PQoUOSj9+5cwexsbHo06cPlEplntv5MtfYXkePHsX06dMlg/9r165h+fLlBXp+W1xdXbF69WqL7QcOHMCDBw/g4uKS52PnJfgfOHAgMjIyEBoamufz2qtZs2ZgGAZHjhwRbD969Cjkcjn0ej1OnToleIzdt1mzZgCc/zkrSD179sQHH3yA6tWr4+uvv8b06dPRokULbNu2DceOHeP2e5XvWV5cu3YN33zzDR4+fIiIiAib+547dw5t27ZFeno65s6di+HDh2PZsmXo1auXxb6DBw/G3LlzMWDAACxYsAAKhQIdO3aU7EwjhJD8kPdvO4QQQvLsrbfeEvx87Ngx7Nq1y2K7WHp6Otzc3Ow+j0qlylP7AECpVL5UUPyyfv/9d4SHh2PDhg1cCvPMmTNhMpnsen7//v0xefJkrF69Gi1atLB4fM2aNWAYBgMGDHipdr7MNc4PLxNc54eOHTti3bp1WLhwoeB+Wb16NerWrWt3Z83LSktLg7u7OxQKBRQKxSs5JxvAHz58GF26dOG2HzlyBDVq1EBGRgYOHz7M7cfuK5fL0aRJEwDO/5wVlJMnT2Lz5s348ssvLbIcvv/+e0FH1qt8z/Kibt26ePbsGXx9fbF+/XrJQJ716aefwsfHB/v374dWqwVgnpozYsQI7Ny5E1FRUQCAEydO4Pfff8e3336LSZMmAQDefvttVK9eHR999BGOHj1a8C+MEFLs0Mg/IYQUUq1atUL16tVx+vRptGjRAm5ubtyX6L///hudOnVCqVKl4OLigrCwMMycORNGo1FwDPF8dHbe+XfffYdly5YhLCwMLi4uqF+/Pk6ePCl4rtS8YZlMhrFjx+Kvv/5C9erV4eLigvDwcGzfvt2i/fv370e9evXg6uqKsLAw/N///Z9Dc5HlcjlMJpNgf7lcbnegFBISghYtWmD9+vXQ6/UWj69evRphYWFo2LAh7t69i3fffReVK1eGRqNBiRIl0KtXL7vmIEvN+U9KSsLgwYPh5eUFb29vDBo0SHLU/t9//8XgwYNRvnx5uLq6IjAwEEOHDsWzZ8+4faZNm4YPP/wQAFCuXDku1Z5tm9Sc/1u3bqFXr17w9fWFm5sbGjVqhC1btgj2YesXrF27Fl9++SVKly4NV1dXtG3bFjdu3Mj1dbP69euHZ8+eCdKVdTod1q9fj/79+0s+x2QyYf78+QgPD4erqysCAgIwcuRIJCYmcvuULVsWly5dwoEDB7jX3KpVKwA5Uw4OHDiAd999F/7+/ihdurTgMfF7t23bNrRs2RKenp7QarWoX7++IGPh+vXr6NmzJwIDA+Hq6orSpUujb9++ePHihdXXXqZMGYSEhFiM/B85cgRNmzZFkyZNJB8LDw+Ht7c3gJf/nB0+fBj169cXfM6kGAwGzJw5k/vMly1bFp9++imysrK4fSZOnIgSJUoIpsq89957kMlkWLhwIbctISEBMpkMS5YssXptbt68CcA8tUZMoVCgRIkS3M/i94y9JlJ/+Pe6PfeRNXq9HlevXkVcXFyu+3p6esLX1zfX/ZKTk7lOXDbwB8xBvYeHB9auXcttW79+PRQKBd555x1um6urK4YNG4bY2Fjcv38/1/MRQoijXr+uZkIIeY08e/YMHTp0QN++ffHWW28hICAAgPnLsoeHByZOnAgPDw/s3bsXU6dORXJyMr799ttcj7t69WqkpKRg5MiRkMlkmD17Nnr06IFbt27lOpJ9+PBhbNiwAe+++y48PT2xcOFC9OzZE/fu3eO+0J89exbt27dHUFAQpk+fDqPRiBkzZsDPz8/u1z5kyBCMHDkS//d//4eRI0fa/Ty+AQMG4J133sGOHTsE884vXLiAixcvYurUqQDMo5RHjx5F3759Ubp0ady5cwdLlixBq1atcPnyZYeyLRiGQdeuXXH48GGMGjUKVatWxcaNGzFo0CCLfXft2oVbt25hyJAhCAwMxKVLl7Bs2TJcunQJx44dg0wmQ48ePfDff/9hzZo1mDdvHkqWLAkAVq9lQkICmjRpgvT0dIwbNw4lSpTAypUr8cYbb2D9+vXo3r27YP+vv/4acrkckyZNwosXLzB79mwMGDAAx48ft+v1li1bFo0bN8aaNWvQoUMHAOZA+8WLF+jbt68gaGSNHDkSMTExGDJkCMaNG4fbt2/j+++/x9mzZ3HkyBGoVCrMnz8f7733Hjw8PPDZZ58BAHf/s9599134+flh6tSpSEtLs9rGmJgYDB06FOHh4Zg8eTK8vb1x9uxZbN++Hf3794dOp0N0dDSysrLw3nvvITAwEA8fPsTmzZuRlJQELy8vq8du1qwZNmzYgKysLLi4uECn0+HkyZMYPXo00tPT8dFHH4FhGMhkMiQmJuLy5csYNWpUrtfVns/ZhQsXEBUVBT8/P0ybNg0GgwGff/65xXUCgOHDh2PlypV488038cEHH+D48eOYNWsWrly5go0bNwIAmjdvjnnz5uHSpUuoXr06AODQoUOQy+U4dOgQxo0bx20DIJlRw2JT+H/77Tc0bdrUoeyGHj16oEKFCoJtp0+fxvz58+Hv789ts+c+subhw4eoWrUqBg0alG91JS5cuACDwYB69eoJtqvVatSqVQtnz57ltp09exaVKlUSdBIAQIMGDQCYpw+EhITkS7sIIYTDEEIIcboxY8Yw4l/JLVu2ZAAwS5cutdg/PT3dYtvIkSMZNzc3JjMzk9s2aNAgJjQ0lPv59u3bDACmRIkSzPPnz7ntf//9NwOA+eeff7htn3/+uUWbADBqtZq5ceMGt+38+fMMAGbRokXcti5dujBubm7Mw4cPuW3Xr19nlEqlxTGt+eSTTxi1Ws0oFApmw4YNdj1H7Pnz54yLiwvTr18/i2MDYK5du8YwjPT1jI2NZQAwv/zyC7dt3759DABm37593DbxNf7rr78YAMzs2bO5bQaDgWnevDkDgFmxYgW3Xeq8a9asYQAwBw8e5LZ9++23DADm9u3bFvuHhoYygwYN4n4eP348A4A5dOgQty0lJYUpV64cU7ZsWcZoNApeS9WqVZmsrCxu3wULFjAAmAsXLlici2/FihUMAObkyZPM999/z3h6enKvp1evXkzr1q259nXq1Il73qFDhxgAzG+//SY43vbt2y22h4eHMy1btrR67mbNmjEGg0HyMfZaJSUlMZ6enkzDhg2ZjIwMwb4mk4lhGIY5e/YsA4BZt26dzdcsZfHixYLrzd43d+/eZS5fvswAYC5dusQwDMNs3rzZ4jW+zOesW7dujKurK3P37l1u2+XLlxmFQiE45rlz5xgAzPDhwwXnmTRpEgOA2bt3L8MwDPP48WMGAPPDDz8wDGO+dnK5nOnVqxcTEBDAPW/cuHGMr68vd/2kmEwm7ndYQEAA069fP2bx4sWCtrLE75nYkydPmDJlyjARERFMamoqwzCO3UdS2N+F/M+OPdatW2fxO0D8GP+zy+rVqxcTGBjI/RweHs60adPGYr9Lly5Z/b1PCCEvi9L+CSGkEHNxccGQIUMstms0Gu7fKSkpePr0KZo3b4709HS7qmj36dMHPj4+3M/NmzcHYE4Xz01kZCTCwsK4n2vUqAGtVss912g0Yvfu3ejWrRtKlSrF7VehQgVuZDg3CxcuxNy5c3HkyBH069cPffv2xc6dOwX7uLi44H//+5/N4/j4+KBjx47YtGkTNzLMMAx+//131KtXD5UqVQIgvJ56vR7Pnj1DhQoV4O3tjTNnztjVZtbWrVuhVCoxevRobptCocB7771nsS//vJmZmXj69CkaNWoEAA6fl3/+Bg0aCOaZe3h44J133sGdO3dw+fJlwf5DhgyBWq3mfnbkXmD17t0bGRkZ2Lx5M1JSUrB582arKf/r1q2Dl5cX2rVrh6dPn3J/6tatCw8PD+zbt8/u844YMSLXueK7du1CSkoKPvnkE7i6ugoeY9Pt2ZH9HTt2ID093e7zA8J5/4A5rT84OBhlypRBlSpV4Ovry6X+i4v92WLP52zHjh3o1q0bypQpw+1XtWpVREdHC461detWAOa0fr4PPvgAALgpIX5+fqhSpQoOHjzItVehUODDDz9EQkICrl+/DsA88t+sWTObU3hkMhl27NiBL774Aj4+PlizZg3GjBmD0NBQ9OnTx+6VK4xGI/r164eUlBRs3LgR7u7uAF7+PipbtiwYhsnX1STYwo1SdThcXV0FhR0zMjKs7sc/FiGE5CcK/gkhpBALDg4WBGasS5cuoXv37vDy8oJWq4Wfnx9XLNDWHGUWP1gAwHUE2DNXVvxc9vnscx8/foyMjAyLtF0AktvEMjIy8Pnnn2P48OGoV68eVqxYgTZt2qB79+5cgHX9+nXodDo0bNgw1+MNGDAAaWlp+PvvvwGYK7HfuXNHUOgvIyMDU6dORUhICFxcXFCyZEn4+fkhKSnJruvJd/fuXQQFBcHDw0OwvXLlyhb7Pn/+HO+//z4CAgKg0Wjg5+eHcuXKAbDvfbR2fqlzsStH3L17V7D9Ze4Flp+fHyIjI7F69Wps2LABRqMRb775puS+169fx4sXL+Dv7w8/Pz/Bn9TUVDx+/Nju87LXyhZ27jmbxm7tOBMnTsSPP/6IkiVLIjo6GosXL7brPahevTq8vb0FAT47z10mk6Fx48aCx0JCQiQ/Q2K5fc6ePHmCjIwMVKxY0WI/8ft/9+5dyOVyi89fYGAgvL29BfdE8+bNubT+Q4cOoV69eqhXrx58fX1x6NAhJCcn4/z581wnkS0uLi747LPPcOXKFTx69Ahr1qxBo0aNsHbtWowdOzbX5wPm1RD27t3L1ehg5ed9lF/Yzjx+HQVWZmamoLNPo9FY3Y9/LEIIyU80558QQgoxqS+ASUlJaNmyJbRaLWbMmIGwsDC4urrizJkz+Pjjj+2qhm9ttJThFfoqiOfa48qVK0hKSuJGwJVKJdavX482bdqgU6dO2LdvH9asWQN/f39uiURbOnfuDC8vL6xevRr9+/fH6tWroVAo0LdvX26f9957DytWrMD48ePRuHFjeHl5QSaToW/fvnavLpAXvXv3xtGjR/Hhhx+iVq1a8PDwgMlkQvv27Qv0vHz59X72798fI0aMQHx8PDp06MAVtBMzmUzw9/fHb7/9Jvm4I3Uh8jNAmjNnDgYPHoy///4bO3fuxLhx4zBr1iwcO3aMKyYoRS6Xo3Hjxjh69Ci37B+/un2TJk3w888/c7UAunXrZld7CuJzZk+xzWbNmmH58uW4desWDh06hObNm0Mmk6FZs2Y4dOgQSpUqBZPJZFfwzxcUFIS+ffuiZ8+eCA8Px9q1axETE2OzFsBff/2Fb775BjNnzkT79u0Fj+XnfZRfgoKCAECyiGBcXJwgEyooKAgPHz6U3A+AYF9CCMkvFPwTQkgRs3//fjx79gwbNmwQFNy6ffu2E1uVw9/fH66urpIV4+2pIs8GKPxq1+7u7ti6dSuaNWuG6OhoZGZm4osvvrBrmTsXFxe8+eab+OWXX5CQkIB169ahTZs2CAwM5PZZv349Bg0ahDlz5nDbMjMz7U5N5gsNDcWePXuQmpoqGP2/du2aYL/ExETs2bMH06dP5woPAuBSq/nsXSGBPb/4XAC46SAFtZZ69+7dMXLkSBw7dgx//PGH1f3CwsKwe/duNG3aNNfg3ZHXbet8AHDx4sVcM08iIiIQERGBKVOm4OjRo2jatCmWLl2KL774wubzmjVrhm3btmHTpk14/PixoMJ9kyZN8Nlnn2Hr1q3IyMiwK+XfHn5+ftBoNJL3i/j9Dw0NhclkwvXr17kMEMBcHDIpKUlwT7BB/a5du3Dy5El88sknAMzF/ZYsWYJSpUrB3d0ddevWzVO7VSoVatSogevXr+Pp06eCzyHff//9h0GDBqFbt24WSwUCjt1Hr0r16tWhVCpx6tQp9O7dm9uu0+lw7tw5wbZatWph3759SE5OFhT9Ywtt1qpV65W1mxBSfFDaPyGEFDHsiCB/BFCn0+GHH35wVpMEFAoFIiMj8ddff+HRo0fc9hs3bmDbtm25Pj8iIgIBAQH4/vvvBam7JUqUwIoVK/D06VNkZGQI1lXPzYABA6DX6zFy5Eg8efJEkPLPtlk8orpo0SKLpRPt0bFjRxgMBsEyaEajEYsWLbI4J2A5kjt//nyLY7LznO3pjOjYsSNOnDiB2NhYbltaWhqWLVuGsmXLolq1ava+FId4eHhgyZIlmDZtms33pnfv3jAajZg5c6bFYwaDQfAa3d3d89QBwxcVFQVPT0/MmjWLS6lmsdc+OTkZBoNB8FhERATkcrlkarYYG9B/8803cHNzEwRuDRo0gFKpxOzZswX7viyFQoHo6Gj89ddfuHfvHrf9ypUr2LFjh2Dfjh07ArC8t+bOnQsA6NSpE7etXLlyCA4Oxrx586DX67mOjObNm+PmzZtYv349GjVqlGv1/uvXrwvaxUpKSkJsbCx8fHysjs6npqaie/fuCA4OxsqVKyU7gRy5j6Q4stSfvby8vBAZGYlff/0VKSkp3PZVq1YhNTUVvXr14ra9+eabMBqNWLZsGbctKysLK1asQMOGDanSPyGkQNDIPyGEFDFNmjSBj48PBg0ahHHjxkEmk2HVqlX5lnafH6ZNm4adO3eiadOmGD16NIxGI77//ntUr14d586ds/lcpVKJ77//Hn369EFERARGjhyJ0NBQXLlyBT///DMiIiLw4MEDdO3aFUeOHLFYKktKy5YtUbp0afz999/QaDTo0aOH4PHOnTtj1apV8PLyQrVq1RAbG4vdu3cL1iK3V5cuXdC0aVN88sknuHPnDqpVq4YNGzZYzB/XarVo0aIFZs+eDb1ej+DgYOzcuVMyg4MdZf3ss8/Qt29fqFQqdOnShesU4Pvkk0+4ZffGjRsHX19frFy5Erdv38aff/4Jubzg+v2lljMUa9myJUaOHIlZs2bh3LlziIqKgkqlwvXr17Fu3TosWLCAqxdQt25dLFmyBF988QUqVKgAf39/tGnTxqE2abVazJs3D8OHD0f9+vXRv39/+Pj44Pz580hPT8fKlSuxd+9ejB07Fr169UKlSpVgMBiwatUqKBQK9OzZM9dzNGjQAGq1GrGxsWjVqpUgMHZzc0PNmjURGxsLb29vm7UHHDV9+nRs374dzZs3x7vvvguDwYBFixYhPDwc//77L7dfzZo1MWjQICxbtoybNnTixAmsXLkS3bp1Q+vWrQXHbd68OX7//XdERERwNSDq1KkDd3d3/Pfff1aLOfKdP38e/fv3R4cOHdC8eXP4+vri4cOHWLlyJR49eoT58+dbndowffp0XL58GVOmTOFqdbDCwsLQuHFjh+4jKY4u9cdmf1y6dAmAOaBna5BMmTKF2+/LL79EkyZN0LJlS7zzzjt48OAB5syZg6ioKMHUhYYNG6JXr16YPHkyHj9+jAoVKmDlypW4c+cOfvrpp1zbQwgheeKcRQYIIYTwWVvqLzw8XHL/I0eOMI0aNWI0Gg1TqlQp5qOPPmJ27NiR6zJ07PJW3377rcUxATCff/4597O1JcjGjBlj8VzxcnMMwzB79uxhateuzajVaiYsLIz58ccfmQ8++IBxdXW1chWEDh48yERHRzNarZZxcXFhqlevzsyaNYtJT09ntm3bxsjlciYqKorR6/V2He/DDz9kADC9e/e2eCwxMZEZMmQIU7JkScbDw4OJjo5mrl69avG67Fnqj2EY5tmzZ8zAgQMZrVbLeHl5MQMHDuSWk+Mv9ffgwQOme/fujLe3N+Pl5cX06tWLefTokcV7wTAMM3PmTCY4OJiRy+WCZdGkrv3NmzeZN998k/H29mZcXV2ZBg0aMJs3bxbsw74W8fJ27D3Cb6cU/lJ/toiX+mMtW7aMqVu3LqPRaBhPT08mIiKC+eijj5hHjx5x+8THxzOdOnViPD09GQDcsn+2zm1t2bhNmzYxTZo0YTQaDaPVapkGDRowa9asYRiGYW7dusUMHTqUCQsLY1xdXRlfX1+mdevWzO7du22+Nr7GjRszAJhPP/3U4rFx48YxAJgOHTpYPPayn7MDBw4wdevWZdRqNVO+fHlm6dKlksfU6/XM9OnTmXLlyjEqlYoJCQlhJk+eLFgalMUuXzh69GjB9sjISAYAs2fPHqvXgZWQkMB8/fXXTMuWLZmgoCBGqVQyPj4+TJs2bZj169cL9hW/Z4MGDWIASP4Rv3577iMpji71Z609Ul+lDx06xDRp0oRxdXVl/Pz8mDFjxjDJyckW+2VkZDCTJk1iAgMDGRcXF6Z+/frM9u3b7WoPIYTkhYxhCtFQESGEkNdat27dcOnSJcl5yoQQQgghpODQnH9CCCEFQrxO9fXr17F161a0atXKOQ0ihBBCCCnGaOSfEEJIgQgKCsLgwYNRvnx53L17F0uWLEFWVhbOnj0ruTY5IYQQQggpOFTwjxBCSIFo37491qxZg/j4eLi4uKBx48b46quvKPAnhBBCCHECGvknhBBCCCGEEEJeczTnnxBCCCGEEEIIec1R8E8IIYQQQgghhLzmaM5/PjGZTHj06BE8PT0hk8mc3RxCCCGEEEIIIa85hmGQkpKCUqVKQS63PbZPwX8+efToEUJCQpzdDEIIIYQQQgghxcz9+/dRunRpm/tQ8J9PPD09AZgvulardXJrrNPr9di5cyeioqKgUqmc3RxCJNF9SooCuk8JqlQB4uKAoCDg6lVnt0YS3aekKKD7lBR2hfkeTU5ORkhICBeP2kLBfz5hU/21Wm2hD/7d3Nyg1WoL3Y1LCIvuU1IU0H1KwKZXyuVAIf2/n+5TUhTQfUoKu6Jwj9oz9ZwK/hFCCCGEEEIIIa85Cv4JIYQQQgghhJDXHAX/hBBCCCGEEELIa47m/BNCCCGE5MXJk4DRCCgUzm4JIeQ1wTAMDAYDjEajs5tCePR6PZRKJTIzM1/5e6NQKKBUKvNlOXkK/gkhhBBC8iIoyNktIIS8RnQ6HeLi4pCenu7sphARhmEQGBiI+/fv50sQ7ig3NzcEBQVBrVa/1HEo+CeEEEIIIYQQJzKZTLh9+zYUCgVKlSoFtVrtlCCTSDOZTEhNTYWHhwfk8lc3c55hGOh0Ojx58gS3b99GxYoVX+r8FPwTQgghhBBCiBPpdDqYTCaEhITAzc3N2c0hIiaTCTqdDq6urq80+AcAjUYDlUqFu3fvcm3IKwr+CSGEEELyYtkyIDUV8PAA3nnH2a0hhLwGXnVgSYqG/LovKPgnhBBCCMmLGTOAhw+B4GAK/gkhhBR61LVECCGEEEIIIYS85ij4J4QQQgghhBBSaJQtWxbz5893djNeOxT8E0IIIYQQQghxmEwms/ln2rRpeTruyZMn8c5LTqdq1aoVxo8f/1LHeN3QnH9CCCGEEEIIIQ6Li4vj/v3HH39g6tSpuHbtGrfNw8OD+zfDMDAajVAqcw9B/fz88rehBICTR/7Lli0r2UM0ZswYAEBmZibGjBmDEiVKwMPDAz179kRCQoLgGPfu3UOnTp3g5uYGf39/fPjhhzAYDIJ99u/fjzp16sDFxQUVKlRATEyMRVsWL16MsmXLwtXVFQ0bNsSJEycK7HUTQgghhBBCiC0MwyBdZ3DKH4Zh7GpjYGAg98fLywsymYz7+erVq/D09MS2bdtQt25duLi44PDhw7h58ya6du2KgIAAeHh4oH79+ti9e7fguOK0f5lMhh9//BHdu3eHm5sbKlasiE2bNr3U9f3zzz8RHh4OFxcXlC1bFnPmzBE8/sMPP6BixYpwdXVFUFAQBg0axD22fv16REREQKPRoESJEoiMjERaWtpLtedVcOrI/8mTJ2E0GrmfL168iHbt2qFXr14AgAkTJmDLli1Yt24dvLy8MHbsWPTo0QNHjhwBABiNRnTq1AmBgYE4evQo4uLi8Pbbb0OlUuGrr74CANy+fRudOnXCqFGj8Ntvv2HPnj0YPnw4goKCEB0dDcDcSzVx4kQsXboUDRs2xPz58xEdHY1r167B39//FV8VQgghhBBCSHGXoTei2tQdTjn35RnRcFPnT6j4ySef4LvvvkP58uXh4+OD+/fvo2PHjvjyyy/h4uKCX375BV26dMG1a9dQpkwZq8eZPn06Zs+ejW+//RaLFi3CgAEDcPfuXfj6+jrcptOnT6N3796YNm0a+vTpg6NHj+Ldd99FiRIlMHjwYJw6dQrjxo3DqlWr0KRJEzx9+pTroIiLi0O/fv0we/ZsdO/eHSkpKTh06JDdHSbO5NTgX5zO8fXXXyMsLAwtW7bEixcv8NNPP2H16tVo06YNAGDFihWoWrUqjh07hkaNGmHnzp24fPkydu/ejYCAANSqVQszZ87Exx9/jGnTpkGtVmPp0qUoV64c15NTtWpVHD58GPPmzeOC/7lz52LEiBEYMmQIAGDp0qXYsmULfv75Z3zyySev8IoQQgghhBBCyOtjxowZaNeuHfezr68vatasyf08c+ZMbNy4EZs2bcLYsWOtHmfw4MHo168fAOCrr77CwoULceLECbRv397hNs2dOxdt27bF//73PwBApUqVcPnyZXz77bcYPHgw7t27B3d3d3Tu3Bmenp4ICQlBWFgYAHPwbzAY0KNHD4SGhgIAIiIiHG6DMxSaOf86nQ6//vorJk6cCJlMhtOnT0Ov1yMyMpLbp0qVKihTpgxiY2PRqFEjxMbGIiIiAgEBAdw+0dHRGD16NC5duoTatWsjNjZWcAx2H7b4g06nw+nTpzF58mTucblcjsjISMTGxlptb1ZWFrKysrifk5OTAQB6vR56vf6lrkVBYttWmNtYWDxNzUJiuh4V/T1y35nkK7pPSVFA9ylRApABYAAYCul9QPcpKQroPjW/doZhYDKZYDKZAAAuChkuTmuXyzMLhotCxrXDXuz+4r/r1KkjOFZqaiqmT5+OrVu3coF0RkYG7t69K9iPvR6s6tWrcz9rNBpotVrEx8fbbKf4GKwrV67gjTfeEDzWuHFjzJ8/H3q9Hm3btkVoaCjKly+P6OhoREdHo23btvD09ERERATatm2LiIgIREVFoV27dnjzzTfh4+Pj0PVyhMlkAsMw0Ov1UCgUgscc+dwUmuD/r7/+QlJSEgYPHgwAiI+Ph1qthre3t2C/gIAAxMfHc/vwA3/2cfYxW/skJycjIyMDiYmJMBqNkvtcvXrVantnzZqF6dOnW2zfuXMn3Nzccn/BTrZr1y5nN6HQez/W/PGYWtuAEq5ObkwxRfcpKQroPi2+mvj6wkUuR5a3N45u3ers5thE9ykpCorzfapUKhEYGIjU1FTodDpnNwcpmY4/JzMzEwzDcIOi6enpAMyBK7sNME/t3r9/P2bOnIly5cpBo9Fg0KBBSE1N5fYzmUzIzMwUPM9gMAh+Zs8h3sbfX6fTST5uNBqRlZUleCwjIwOAeVBXoVBg7969OHz4MPbu3YupU6di2rRp2Lt3L7y8vLBu3TocP34c+/btw8KFCzFlyhTs3r2bywTIbzqdDhkZGTh48KBFfTv2Otuj0AT/P/30Ezp06IBSpUo5uyl2mTx5MiZOnMj9nJycjJCQEERFRUGr1TqxZbbp9Xrs2rUL7dq1g0qlcnZzCrX3Y3cCAPwr10V0eEAue5P8RPcpKQroPiXo2BEAoAHQ0bktsYruU1IU0H1qDpzv378PDw8PuLoWzVEnV1dXyGQyLhZiB0Q9PT0F8dGpU6cwZMgQ9O/fH4A5E+D+/ftQq9XcfnK5HK6uroLnsaP9LJlMZrEPn1KpFByTLzw8HKdOnRI8dvbsWVSqVEkwgv/GG2/gjTfewBdffIESJUrg5MmT6NGjBwAgKioKUVFR+OKLL1CuXDns3r0bEyZMcOyi2SkzMxMajQYtWrSwuD+sdX5IKRTB/927d7F7925s2LCB2xYYGAidToekpCTB6H9CQgICAwO5fcRV+dnVAPj7iFcISEhIgFarhUajgUKhgEKhkNyHPYYUFxcXuLi4WGxXqVRF4pdWUWlnYaBSKelaOQndp6+HF+l6bL8Uh/bVg+Clef3eT7pPSVFA9ykpCorzfWo0GiGTySCXyyGXO3VBtjxj2y31N/81VaxYERs3bsQbb7wBmUyG//3vfzCZTNzrZ4l/lro2uV2vp0+f4t9//xVsCwoKwqRJk1C/fn18+eWX6NOnD2JjY7F48WL88MMPkMvl2Lx5M27duoUWLVrAx8cHmzdvhslkQuXKlXHy5Ens2bMHUVFR8Pf3x/Hjx/HkyRNUq1atwN47uVwOmUwm+Rlx5DNTKO6sFStWwN/fH506deK21a1bFyqVCnv27OG2Xbt2Dffu3UPjxo0BmOdlXLhwAY8fP+b22bVrF7RaLapVq8btwz8Guw97DLVajbp16wr2MZlM2LNnD7cPKX741ToVMpkTW0JI0Td2zRl8/OcFTFp33tlNIYQQQoiTzZ07Fz4+PmjSpAm6dOmC6Oho1KlTp0DOtXr1atSuXVvwZ/ny5ahTpw7Wrl2L33//HdWrV8fUqVMxY8YMbgq6t7c3NmzYgDZt2qBq1apYtmwZfvzxR4SHh0Or1eLgwYPo2LEjKlWqhClTpmDOnDno0KFDgbyG/OT0kX+TyYQVK1Zg0KBBUCpzmuPl5YVhw4Zh4sSJ8PX1hVarxXvvvYfGjRujUaNGAMypFtWqVcPAgQMxe/ZsxMfHY8qUKRgzZgw3Kj9q1Ch8//33+OijjzB06FDs3bsXa9euxZYtW7hzTZw4EYMGDUK9evXQoEEDzJ8/H2lpaVz1f1L86Iw5xT+KaOerpP8SUjBuzVlMaFcJ0eHWM1uI/YwmBtfiU1Al0BNyOXUUSTl0/SkAYNflhFz2JIQQQkhRNXjwYC54BoBWrVpJLn9XtmxZ7N27V7BtzJgxgp/v3Lkj+FnqOElJSTbbs3//fpuP9+zZEz179pR8rFmzZoLn8+sWVK1aFdu3b7d57MLK6WHN7t27ce/ePQwdOtTisXnz5qFz587o2bMnWrRogcDAQMHUAIVCgc2bN0OhUKBx48Z466238Pbbb2PGjBncPuXKlcOWLVuwa9cu1KxZE3PmzMGPP/7ILfMHAH369MF3332HqVOnolatWjh37hy2b99uUQSQFD0ZOiO+3nYV5+4nOfQ8nYEX/L9GI//j1pzF1fgUjFx12tlNeW18ve0KOi48hG93XnN2Uwghr9qAAUB0tPlvQgghpJBz+sh/VFSUZE8OYC4asXjxYixevNjq80NDQ7E1lwq7rVq1wtmzZ23uM3bsWJvrSpKiacn+G1h64CaWHriJO193yv0J2fjBv+w1Cv4T051fPfZ1s/zQbQDAkv038XH7Kk5uDSHklTpwAHj4EAgOdnZLCCGEkFw5feSfkIJ040lqnp7HT/s3OrjGaWFmpZ+NkNfai3Q9+i07hj9O3nN2UwghhBBCnIaCf/JaUyvydotn6XMCfr3x9YmYTa/PSyGQnv9GLP1w4AZibz3Dx39ecHZTCCGEEEKchoJ/8lpzUSry9Dz+yL/hNQr+AcvXci0+Bc9Ss5zQFsekZhmc3YRCZcpfF9Dwqz14kuL8985gNGHcmrOIOXLb2U2RlJyhd3YTCCGEEEKcjoJ/8tLuPktDxwWHsOn8I2c3xYJambdbnD/n3/Aap/3feJyK6PkHUfeL3c5pkJ2WHriJ6p/vQI8fjmDEL6dgML4+70le/XrsHh6nZGHBnv+c3RRsvRiPTecfYdo/l53dFEIIIYQQYgUF/+SlfbrxAi7HJWPcGttFFZ3BhRf8O5IinWV4PUf+TaJrcOL2cye1xDFfb7sKADhzLwm7LicUqiXj8jq1JL9cePDCqecHgKeFIPsgL249SUUSFcEkhBBCSDFBwT95ackZhTcd20WVc4tn6I029z1+6xluZRcIfG1H/i1+LpodGymFaApAXrNL8sulR8lOPT8AZBpsf7YKo/vP09FmzgE0/2afs5tCCCGEEPJKUPBPXlphDiCV8pxb3Nac8VtPUtFn2TG0mXMAeqNJMOf/dSr4J05+KAoFABPTLEdmC1M2hkrh3KUgDSbG6fUQMvVFr4Ps3P0kAOaOJD1NIyGEEEJIMUDBP8GNxynIzGVU3JbCXHDcyItu07Ksv8Zr8Sncv6v8b7ugcNnrNL9cnPZfFFxLSLHYVpiWX3TGyL/4nnR26npWERz593ZTcf9+kJhh9/Ne5nclIYQQQqS1atUK48ePd3YzXnsU/Bdze68mIHLuQQxbedLZTSkQel6QmGbn6KjRxGDftSfczwY7h8cNRhNGrTqN/ztw07FGFgCD0STdaWGR9/9ynQFGE4O3fjyOEb+ceqnj2HJdIvi39z15FVSKvNWVyKvtF+NRaco2wbYMXcEHpCYTY/UzlFUER/75bb79NNWu5+y8FI8q/9uOFYV0VQPiBCNGABMmmP8mhJBiqEuXLmjfvr3kY4cOHYJMJsO///770ueJiYmBt7f3Sx+nuKPgv5hbe/IBAODIjWdObknB4KeH2xv8i9mb9r/7SgK2X4rHrOzidC/r3wdJmLzhAp5LpL3b8jQ1C7Vn7ML7f5yzeMxyzv/LufwoGYdvPMWuywmCOgn56ZnE6zcWouCfP/KfZeMa5FebR/162mK6RrqdwX+Gzoh3fzuNP08/cPi8I389jfDPd+D+83SLx/ij4a+iAyQ/8N+r208tX5OU97KLmk6nVQ0I6/PPgblzzX8TQkgxNGzYMOzatQsPHlh+t1ixYgXq1auHGjVqOKFlRAoF/8VcoJcr9++8fmkvzN/1+XN503R5C/7tTTG3NwCz17CVp7DmxD2HV1E4fP0pUrIM2PJvnMV8eXHav4kXRebl/b+fmBM0FVRhxJRMy/eNHfm/+PAFPv/7osMdJHmRkJyJGf9cxu2naYLtKl5dCWv3wIt0PRrP2oOP1p8vkLal64x4kJie63v40+Fb2HohHh+sc7wd7AoLa07cs3hMsDpGIeqYYUldFn6Hhb0j/1QbgBBCyCvFMIAuzTl/7Pxe2LlzZ/j5+SEmJkawPTU1FevWrcOwYcPw7Nkz9OvXD8HBwXBzc0NERATWrFmTr5fq3r176Nq1Kzw8PKDVatG7d28kJOSsDnX+/Hm0bt0anp6e0Gq1qFu3Lk6dMmeu3r17F126dIGPjw/c3d0RHh6OrVu35mv7CgulsxtAnMtf68L9+1maDiU9XGzsLa2wfNW/8TgFE/44j/fbVkRktQAAwlH7VBtz/m2xd+RfLssp/NZzyVFMiqqMxmElrO5/6dELeLupEeytkXz8SfbyaYdvPHWgtcLg69CNp3ijZinuZ/7vcXGgaDQxUDpYvO4hb650QRVGTJUI/tlR9M6LDgMAnqbpsLh/nQI5P2vMb2dw6m4iNp1/hFNTInPawgizS3zd1RbPXXf6Ph6nZGHtqQeY/WbNfG/bL7F3sO1iPAY2CsXMbtWt7nfjcU6QazQxUMgdL1YotWoGP5A2mhioFNafzzAMZDLnFUk0mRjI5TLBCgX3nts3578Q9msQQgh5nenTga9K5b5fQfj0EaB2z3U3pVKJt99+GzExMfjss8+4/+PXrVsHo9GIfv36ITU1FXXr1sXHH38MrVaLLVu2YODAgQgLC0ODBg1euqkmk4kL/A8cOACDwYAxY8agT58+2L9/PwBgwIABqF27NpYsWQKFQoFz585BpTLX/xkzZgx0Oh0OHjwId3d3XL58GR4eHi/drsKIRv6LOX7AejXOcm51UfL+7+dw4eELDOfNP+fPe89r2r+9I9r8eOb03UT0W37M6r73n6ej08LDaPr1Xm7bi3Q9Tt99jvTsDIUAXseMI0XGnqflrLn+19mHgiCfvzKD0SRcpyEvI7aCkf8CGhVNydJbbBOPwF56WPBr3Z+6mwjAPK2Cj/+6rY38F3ShxW0X4wEAq47dtblfYnrOtQz/fDs3mu8Iqcr+/PtT/N68SBe+f+x9ZnJSJM3WAeHP+c98BTUTCCGEkNfV0KFDcfPmTRw4cIDbtmLFCvTs2RNeXl4IDg7GpEmTUKtWLZQvXx7vvfce2rdvj7Vr1+bL+ffs2YMLFy5g9erVqFu3Lho2bIhffvkFBw4cwMmT5rpm9+7dQ2RkJKpUqYKKFSuiV69eqFmzJvdY06ZNERERgfLly6Nz585o0aJFvrStsKGR/2JOz0vXfeun41g/qjHqlfV16BgvO8f33P0keLgoUcH/5XrYktItg0SDSTgqqzeaoDOY4O4ivPV1NgJXe5eVkzswmskuM8Y3aMUJnLufhGBvDQ591BpBXhokJJsDzYsPX9j9vvDnyO+9+hib/41Dl+zRf368pTOaBJkAeZmTfvdZTvCfH3PaY28+w5S/LmB6l6rcNsm0f9F7YnTi3BN+xoO1qSWvsnmXHr3Aw8QMRIUHWjyWyFsVIFNvwohfTuHO150cOr5URxS/Q4B/HzxITEezb/YJ9s0ymJCQnIk3vj+C/g3KYFJ0ZbvP7WjWQIbOCI1amIZgMDJwUUIw8p9pY7WCU3ee40p8Ct5qWMbu85JipHRp4OFDIDgYkJjvSgghL0XlZh6Bd9a57VSlShU0adIEP//8M1q1aoUbN27g0KFDmDFjBgDAaDTiq6++wtq1a/Hw4UPodDpkZWXBzc3+c9hy5coVhISEICQkhNtWrVo1eHt748qVK6hfvz4mTpyI4cOHY9WqVYiMjESvXr0QFhYGABg3bhxGjx6NnTt3IjIyEj179nxt6xTQyH8xJx6lO377eYGe73FypiB4eJ6mQ7fFRxA594BDweO9Z+mY8Mc5wfxrqZhAMOc/y4i2cw4g/PMdFuui2ypWl5e0/9xkSBRIuxKXDAB4mJSBDL1RcJ3YToDcZBmMFgXZYm/xijnyg3+DSTAibU8nR0JyJtrM2Y8fD90CANx9lnP99fkQ/Pf/8RhuPknDWz/nZG9IBf/iANSZK//x77F0K1NLXmXXRKeFh/HOqtO4KJENkR+1EaRWFsgQjPznvNq/zj602DdLb8TifTfwPE2H7/fdsPu8Wy/EodaMXTjw35PcdwYwd9d/qDF9B87eSxRsZ98vfoeFrdUK3lwai//9dRF7rz62u62EEEJIvpDJzKn3zvjj4BS9YcOG4c8//0RKSgpWrFiBsLAwtGzZEgDw7bffYsGCBfj444+xb98+nDt3DtHR0dDpXt1SxdOmTcOlS5fQqVMn7N27F9WqVcPGjRsBAMOHD8etW7cwcOBAXLhwAfXq1cOiRYteWdteJQr+izmdKOBT5mEOsL1uPE5Bg6/2YGhMzrKC/JHIW0/sK7oFAJ0XHcLGsw8xiVe4TGr+srjg373swPhf0ci7zZF/OyNLxoEQjx+8sufmtz/LYBIUUUu0cx33t348jq0XzCng1YO1AHJqBwDC1yJ+zfa8zpmbL+PWkzR8seUKAGEwaTCakJIpzL64/TSNm8ZgD6kRcnFHDWA5UitOq2cYxiLdvKBYKyp55l4iRvxyCnefpRV42r+U/ySWSBQXgMyL7ZfisWS/cDlL/pQafieeOMMGYIsDOv575t3fzuBFhh6Dfj5hdR+jicHs7Vex79pjLNxzHXojg9nbrwmuP9s5kWXnyD9LKluHEEIIIWa9e/eGXC7H6tWr8csvv2Do0KFctt6RI0fQtWtXvPXWW6hZsybKly+P//77L9/OXbVqVdy/fx/379/ntl2+fBlJSUmoVq0at61SpUqYMGECdu7ciR49emDFihXcYyEhIRg1ahQ2bNiADz74AMuXL8+39hUmFPwXc+KR/4Jarg0Avt9rHuU7ejNnJJofKJx/YN+8bZOJQXL2aPAd3si/QqKH0tpSf+JQLD9G/u2dHgAIR0/ZEUj+tcgyCEf+7Q3aTt7JGeWsHGAO/tk56gajSZj2bzAJpkXYk3lxPUHYQcPvQNhw5iEipu3kqsGfv5+E1t/tR7fFR7h9dl9OwM5L8Xa9Fpa4QwGwnHcubvunGy+g5oydOHmnYDNZAOv3WI8fjmLX5QS8t+aszUKLBUV8msuPkpGWx7nt4jZ/s124nCX/dfN/p1gL/lUOFpYEALUi9/+u1py4hx/238SQFTkdjG5qheA9YtuXZefIP8veDjhCCCGkOPLw8ECfPn0wefJkxMXFYfDgwdxjFStWxK5du3D06FFcuXIFI0eOFFTit5fRaMS5c+cEf65cuYLIyEhERERgwIABOHPmDE6cOIG3334bLVu2RL169ZCRkYGxY8di//79uHv3Lo4cOYKTJ0+ialXzNNPx48djx44duH37Ns6cOYN9+/Zxj71uKPgv5iyC/zwUbbMnljGZGGH6OXs+XtB93s6RtUuPkrl/lymRM1dIMu3fZCX4F7XZVvCfkJwpGCW0xpFrx28Le2x+IJ6lNwlSqRPzMIpdOdBcQ4EN/sVr0OuNjKBYnT1p+w8ShVMK+NdtwZ7rAIDJGy4AADb/a56j9l92h0Gm3ojhv5zCO6tO40WG9OuReg+lqv1bpP2L3tA1J8w9vwt2X7f6WvIL/32Xmp5xR7Q0YH7URrCH+CzDV56U3s+OD7BUBxj/eamikX+2c8tdLRX8G6GUO/5fT4hvzqoYJhMj2W6p3zHuLkrBe2TI48h/Xj6DhBBCSHEybNgwJCYmIjo6GqVK5axSMGXKFNSpUwfR0dFo1aoVAgMD0a1bN4ePn5qaitq1awv+dOnSBTKZDH///Td8fHzQokULREZGonz58vjjjz8AAAqFAs+ePcPbb7+NSpUqoXfv3ujQoQOmT58OwNypMGbMGFStWhXt27dHpUqV8MMPP+TLNSlsqOBfMZcfI/+5pbvP3fUflh28KVklnH/+i4/sG/m/xNuPn9otNeeeH9zy547z2/zz4duYtU04ksm39+pjdFl0GDsntLTZLkfWAOcHEll6E0wmRjTybxIEuEl2jDqKq+1XDsxJ+2cYxiL41xlMgmkfRlGAl5Kph7taCTlvOgJ/5FicSSCmEAV4/PM/T9PBS6OyeI6LUs7dJ/vjZLi2+4bkaHVuI/8sR6Zi5BW/0+a+qHOEa4co7VxpYym8G49TYGKASgGeL9UufoeIycTg0YtMyf2SMw2S7wWf1L2dZTDBVaUAwzCC4H/R3hv488wDrB7eUPL6Z+mFI/96owkqO0b1A7SuuPnE3JHSfPY+BHtr8Ps7jQT3p7ijBQDUSrkge0Tn4Jx/Vn5MmSCEEEJeZ40bN5bsnPf19cVff/1l87nsknzWDB48WJBNIFamTBn8/fffko+p1WqsWbPG6nNf1/n9Umjkv5jTGcwfUHauvzhAdJTUB37hnutWgzX+iOKzVNtfrhnGHCDzl1Pjz2eXDv5zji9VOA4AZmy+bPO8QM7otS16B66dsOq6kVt+jL+Nf83sSTkWZx5Uzg4eM/UmpOmMFtkLOoNJ0GHAn/N//3k66n2xG+/9fhYmE4MXGXrBvnJZ7pkO/ADPZGIEHUvWll104UXFG+8o8MOBW5L7iUf+X9Voupi400ZcbBEAZDKZINPE1nVLTNMhcu5BRM076NDyjpJ45+Rnkfh5ugh2e5Ii3SnAJ9UpyLYvyyDsBPrzjLni+eSNFySnwmQZTIL6FvYuwcnvgHiYlIETd57jiWjZRangPyVTL2gHe5/zr2+WwZhrBgS/voVUfRFCCCGEkMKOgv9ijv1Czc7NzUvaP5+9a8WzX7z5QYXU3G6+0b+eQavv9gm+hKdkGbhjySW+kPNfTzLv+AURK9pbGwCAIO09y2CyCJKSRdfCnpRj/rUM8nJFgNYFbtnLnD1NybIY3dQZrc/5X3HkDrIMJmz5Nw4zNl9Gzek7ubXkAaCkh4vNLJGkdJ0gQErJNAiCN2tp/y5K+34lWab9S+9X0NPrxZ02DxIzLPaRyYRLEeqNJtx4nIK/zz20CDi38+ohJKXrkZimwz/nH3EdN45kl/BH3dlChDIZ4OumFuw3bdNlzN1lu+iO1HnZDgVro+YqhVzy90GWwSi41/hZAzqDCT8euoWbEsU/0yUyQPjnNpkYySyR5AyD4PeA3sCm/fOey+Q+tYHfAZeXmgWEEEIIIc5GwX8xx36p92CD/1wCummbLuG4aF4tP36xt+gdGzjoRWn5tkbftl+Kx/3nGdh6MU6wnR39lxqM449m80f+86Ow4f3n6Xj75xM4fP2p+ZgOpf0LR/7F100cHNuT9s8PZnZMaAGZTMaN8j5JzZJM++dff72VwnUxR+8I/gbMHQW2ruHdZ+mCx5MydMKfrXRmqO0N/nOp9v+qiAPGh4kZMIkCXhkgus4mRM49iPd/P4edl4XFbv45n7OWb3KmHv2WH8N7a85icXaxzAwHsgH4zWDn4LupFHAVrXt/+MZTLNxzXZBFIyZ1b7PHtHbfqxRyi6kogDlg59+L/OB//u7/8MWWK+i1NNbyfBKvnf/cZ1bS8pNFI/96iZF/AJJ1PfidFPwOOHumKRBCCCGEFDb0DaaYyxn5NwcE1gI6hmHwxvdHEHP0Dqb/Yz1NXjwSag038i9IO2ck6wKw52eJpwewBe1yS/vnj6YnpunsKuJny6R153Hwvyd466fjAByc85+W05ZMvckigErOEKZCOzLyr1EpoHU1z+Eu6WEO/nstjbXoUNAZhcE/P9BJkEgF5z9fZzTZnCJy93m6IDBLStcLXuPLj/zbOee/oEf+eddAlj0VQura8T9X7MgzAJy9lyTY78bjnBHv5Aw9rsabl+vbccncSZDhQLV+4TSL7OBfovo+S3wNp/9zCXN3XrM4FitD4jPMp1LIrIz8C+tZ8As6/nX2IQBhij13PonXzr/HrE2TSM7UC+5zg5HBlL8uCFYdMT/f8nXw28+/BhT8E0IIIaQoom8wxRxb8M09l5H/ozef4V72fObLccmCx/hf79/68TgSknOfQyyV9g9YT/23FTh+uvEiZm6+LBj5/3TjBSSl6ywyC1gf/fkvhq885dCya+IR3YdJwhRve4L/03cT8enGC4LnZhmMggwFwPI1iufcS2E7M/ij5xX9Pbh/HxNlbOhF0w34bRAv6QcIA1OD0bKAIN/z1Cxh8J+ht8gEkKK2VQmPJ1MvTB3nj/y/zPz/dJ3BrvfRZGIwLOYkPvrzXwDmwD/U17zyBP86mR+TCV47/14WZyzwR7f5nVU+7ubOHEeCf37nVnp22r+7WmH1nue//w8S07HiyB0s3HsDeqNJMiX+WnwKjCbGaq0LayP/j1MyBcUH+feJtaKEgLWRf/70GSvBf4bBIvPi12P3LPaTer61e4HS/gnn11+B7dvNfxNCCCGFHAX/xRz7xd3Dxpz/TL0R3+64xv0c7K0RPM4Ptv598ALT/7mU63nZUTbxl+tkK0X5MnXWA7Irccn46fBtPOVlBKw+fg/9lh+3WYPg0PWnuPNMujq7FHGwK46h7JnzP+jnE1h9XBh4ZOot5/yzwbG3W04VdrZDYOXRO2j69V7cFhU3Y68pf/T8kw5VuH8nS47884N/879TswwWHRtieqPJZtq/zmgSTB1IStcJrt8LK5kM1gJTN1GqeqZeeH7+PcgPEu2p9r/3agIG/nQc1+JT0PDLPei55Giuz/nvcQr2XH2MXdlp+yqFHDVDvAGYO3jE+K/dWrYFIBy95md/+GTP0xfPe1fbGIHmz4dn58Jr1Eqr2RD868nvi8rQGyWD4Ilrz+OzjResBshKufTI//R/LuPgf0+4n1PtLPgnNec/NYu3XJ+VrKGUTL3V688nvRqJczJKSBHSqhUQHW3+mxBCCCnkKPgv5ri0f7X0yP+NxymImncQ5+4ncdssgkjRc6TWOheTmvMP5Iz8v0jXC1J/pdbhbl6xpOBn8cjdlbhk3M0luF936n6ubWVVnbod2y7EISE5E3efpVkEqvbUEZAKdLIMlsHV/2VXufdwUXIdM2zHyOebLuFhUga+2HwZT1Ky0G3xEfxx8h7XceOiyvlYe7upUaeMNwAgQTSnW280CUZ72Q6IuFwCf8DcUWBr2kSW3sSlmgPme4b/Gq3N+beWQu4jKlKXpTcKg1UGmLn5MhiG4Ua5AWEQa83Y1Wdx6PpTRM8/iJQsA/598MIiy0NMIZpiopLLUL+sLwDg1B1h8C+DKO3fysi/QdQZ85RXyd7H3fz6xaPf/PdabPmhWzh+6xnuPE3DnOz0fXe1wmp9BP6157+89CwjFzyX8XVD7ez7CQB+P3nfagaIuKCkNWzaPz/rR+sqnJ5grcYE+9z91x7j4+wsDDETI8yksRbQZ0p0cljLtrG3sCkhhBBCSGFifQIoKRYsqv2LvmDP3HwF956nw9ddjaFNy+K7nf8hVWeAycRw1fUdmevOsp72b4DRxKDTokNIyTTg4Iet4eWmkkx37lKjFADzCD4gPTKYm1MSo7S2jP7tDPdv8Wi01HVgGAYyme3rlKm3HiS5qhTwcDGvoy5eEi1dZ8RXW6/g3P0knLufhDUjGgGwHA1mpwFcEU3XEK8ywI5Ci5dPs8bW9c4ymJAinvNvR9q/tWvk7aYSZCNk6I3IMgrP/9Ph2+hbP0Qw7cGeAnlSQWVKpgFevKwLMfHbpVLKueD/zL1Ei6Axi/ezcIQ950CZonbwK967Z99r4s+Bq0phdQnL5EwD+iw7hhLuaq4YnpuLUrIivrhd/IwE/lQIlUIGjSr3+x4wB+a5TVUBcjrEbj7JyWThT/9IzTJYnVrApv0PXnHS5jn4dUL4nUN8H//5L+49S8e28c1R2sc8hUNv5XNpz+sihBBCCClsaOS/mGPn/HtkF/wTj+ZefPgCAPDDgDoY3rw8AHPKKz+wy8so2MCfTuDozafc+VkpmQY8ScnCg8QMvMjQY9N5cwEwqSBOo1bgp0H1uVHCvAT/1gIne4jPJxUE8UdF45Jy5jN3qVkKkVX9AZg7QqxlDbiq5FwxRnFbTQyDa9kF4YCckVsX0bx5NpASz0XXGUyigovmf7NV3wO1rggvpcWCvrUk08ttpWtnGYzCtH/xnH9rI/9WroN45N9gYgTXk2VkGMH7kmYl0OOrGqS12JaYy+oK4naqFHJU9PeAWiFHus6IeF7dC5lMVHyP1z6pivws/vvFjlaLPwcBWheb7QSEVfBtzfnXS9wLgPk+zwn+5RbBv7X3LC3LIPjd0D48UHI/9j66+ywn+M/Kfp0pmXpU/3wHGn61x8pzjRbLYkrh3+fiehbs/P1Lj5KRkmXA3J05yx7SyD/J1f79wI4d5r8JIYSQQo6C/2JOPPLPD1afpmbhWZoOMhlQs7Q3XFUKblSVn6IrHpWztxRW/+XHJdP+417kjPBuyK7+LVXJW5PdHnY5u7zgFwx7WVLpxPyg4+5zc3BTwd8Di/rVhnd2QJtlsDHyr1TAI7ty/5W4ZPT+v5wl0BhAEGSyAZM4FVwcuLOjyHqjSRDcsEEzWzuhfjlfbBnXHF1rBSPQy9Wibak2Ok50BuGc/8R0nV3V/q0G/+5qi20zNluuOqEzmATBvz0F8qRGgnMN/kVZByq5DHK5DCU9zO3sv/w471FhwT/+Z4efgi++x4XBv8mirW5qBb7uUcNmO8U0NtL+hfPic/b55/wjxGcX4nNRyi2WCrR2rVKzDFxmyeAmZVEpwEN6v0w2+M+ZopOhN2Lq3xcRMW0nAOvTQVIzDbj44IXkY9Z8v++G4GcvjfDeYgubAtazGuxd0pQUA2+9BbRvb/6bEEJInrVq1Qrjx493djNeexT8F3MWaf+8L7vsqHKorxs02V/42SXk+MXIbK1vn1s1fam0f/5qAWwbrI38A+bU57yyFcA6Suo68F8fG1SwVeHZwnyZeqPVEcZ0nZHLypix+TJO3H7OPcYwjKAuAhu8iYN9cWcAO9KtE3U6vPvbGaw5cY8b+WcDWQAo7SMs8gjYHlXPMpgE1/ZafIogkJJayg2wPh/bh5eC75md6SFVWC9TbxIE/OKpElKkOgisZSbkHFcU/Ge/l2xHFD+AlMmEGTX8DA7+/SG+x/nLO7LBJltvo20Vf/z7eRSqB3sJrk1u3NVKVPT3lHzMWtr//x28hQ/Xm+fTqxRyuIoyS6zV1UjNMnBLfyrkMmjU0rPMUrMMiDlyG3N38UbcTQx+ib2b6+tJzdJjZ3bRxbzy0gjbdT+RH/znvjICIYQQUpx16dIF7du3l3zs0KFDkMlk+Pdf6bo8joiJiYFMJoNMJoNcLkdQUBD69OmDe/eEhbRbtWoFmUyGr7/+2uIYnTp1gkwmw7Rp07htt2/fRv/+/VGqVCm4urqidOnS6Nq1K65evcrtI5PJoFAo4OPjA4VCwbXj999/f+nX9apR8F/MWVT7N1gG/5UCcoIFrYYtPscLTESj1vxiYbmlx0qP/OcE/+k6IzJ0RkHlchYb9L9U8G9npXF7SM1L5l/PO9nV+UOyg3+23VkG6aXUAOBZWhZXjFHspKiwHHsuF9H1cOF1BqgVcpT3c+f2F1//yRsucME/P6MiJHsONJ84AObL1BsFnQNX4pIFAf+zNB2MJgZPU7PAMAzGrTmLaZsu2ZX2X7+sL2RW0ksy9EbB6Lg9c/6l5sDnNvIv7lRQZte/KOlhmYUiLlbHH/lP19uXpcAG0bHZyzVWK6WFMvt9lTqnNW4uCkzvGo5edUtjZIvygsesFSXkUynk0KiF/23wOzr4TEzOdVIqZBY1MljJmXpM+8cyi8MeG848RMzRO3l6LkurEXae8AuWWhvhNzGWS38SQgghxdGwYcOwa9cuPHjwwOKxFStWoF69eqhRw7FMRWu0Wi3i4uLw8OFD/Pnnn7h27Rp69eplsV9ISAhiYmIE2x4+fIg9e/YgKCiI26bX69GuXTu8ePECGzZswLVr1/DHH38gIiICSUlJguf/9NNPuHr1Kh4+fIi4uDjExcWhW7du+fK6XiUq+FfM5cz5twz+bz01px1X4K0V75k98s+OXhpNjMVyZWywo1bKLYIIhVwm2F88Wp6caRAURwPMAbDkyD8X/Oe9D8ue5fnsP5bt4P9Cdv2EaqXMI++CkX8rI4nP03TwcM39Y+qqknMj//yl/gAICuBpNUouy8O8hJvl62erzPvxgsoQX4mRf4mOE5nMXBMiKUPPzWcv6aHG01QdjmUHroD5Hpm26RJWHbuLzzpWxabzj2y+Pv7otkatQLC3Bg8SLVclyNQbBenzemPOvWiNdNq/cORfnMEi7jRSKYQj/3xZeqPgPudnRPADflsdFezrOPifubhl26oB3GN+ni64LqrnYI27WomSHi74tldNHPjvCf7v4C3uMWH9B+nPhVppOeffWvAP5GRQKOXWg39rWSBS2Hspt3ay1Aq5zcykqGoBFr+/APN95KpS2Hyu3mSi3nNCCCEFimEYZBhyX4WpIGiUGq5otS2dO3eGn58fYmJiMGXKFG57amoq1q1bh2+//RbPnj3D2LFjcfDgQSQmJiIsLAyffvop+vXr51CbZDIZAgPNNYSCgoIwbNgwjBs3DsnJydBqc2o4de7cGWvXrsWRI0fQtGlTAMDKlSsRFRUlyBS4dOkSbt68iT179iA0NBQAEBoayj2Hz9vbGwEBAdBqtZDLi+43AAr+izlbaf9PU8xfsvnzvdniemz6sVTAe+ZeElp/tx/7JrWC3iD8Yu2qlHMjrWqlnAuO2aAxJdOAxHRR8J+qk57znx1MiIMRKezxCwrDMJKBNBuQG00MLmTPTa5Z2hsAf+Rfeh11wBz0sR0ztpg7XHKuKx//ZxelgsskSMuSrsbOpf3zR/59LUf+pbImtK4qvMjIWaZRJgOaVSiJv849sshUWHXMnNb95dYrub4+/px/GYDyfh5Wg3/xCHqGzohr8SmYtO48Pu5QGW2qBCBTb4RKIYfRJP2+vRCN/IsDRHHHh83g32ASdAIl84J/fseD1D3OMhhN+PdBElKzDCjhrkaNYC/usbIl3XH05jOrz+XjB+DiYJxt45l7ifhuxzXJ50sV/Hso8T6w2NoOSrkcblYyWO4/t/9LTUkPF0HwnxtvNxUep0ivXlGupDuWvV0PY3greLDO3EtEk7CSNqv6G00M5PYWOCGEEELyIMOQgYarGzrl3Mf7H4ebyvL7n5hSqcTbb7+NmJgYfPbZZ1yHwbp162A0GtGvXz+kpqaibt26+Pjjj6HVarFlyxYMHDgQYWFhaNCgQZ7a9/jxY2zcuBEKhQIKhajYtVqNAQMGYMWKFVwgHxMTg9mzZwtS/v38/CCXy7F+/XqMHz/e4jivo6LbbUHyJEtvRFJWzpdyNujkj/xfjU/Gg8R0PEszf2ku4Z4T0LApsimZejAMg4/WS8/heZiUgTvP0ixGzhqHleD+rVEpuPP7Zqd1p2UZuOJirOdpOpsj/+I0dyn2BNAvw2BibM75v/kkFWk6I9zUCi6Tgs1YyNJbT/sP9tbYbDvbIas3MlwKucXIPy/tX6NWcB09qVn2j/yXlkz7lwj+s6eFsGnzbioFgrzNWQOOjPCKiav9ly/pLrlfpt5osQpDms6A3v8Xi2sJKRi56jTSsgyImncQXRYdtrrsm3jkX3ydxFMF2IrxUin4BpNwBQJ+pwl/Oxv8l5Iorqg3MtzofvVgL26ZTQD4oF0lVA+2XLEgN9aWquzxw1Ec59WW4FMrZRafN1tTJAQj/y7Sn1P+Eo7j2lRACYnijiwvjXR9gzISnVOA5X3Dx35OxDUxAGD/tScAbGcW5GfWECGEEFKUDR06FDdv3sSBAwe4bStWrEDPnj3h5eWF4OBgTJo0CbVq1UL58uXx3nvvoX379li7dq1D53nx4gU8PDzg7u6OgIAA7Nu3D2PGjIG7u+X3wqFDh2Lt2rVIS0vDwYMH8eLFC3Tu3FmwT3BwMBYuXIipU6fCx8cHbdq0wcyZM3Hr1i2L4w0YMAClS5eGVquFh4cHPDw8LOoNFAU08l/MfLzhErZcVEIX+AgjWlTg5rSyy8ml64xoP/8QAHBzw315X8a5kf9MA24+SbOZrm0wMoIR7a3jmmPDmZz5QG5qBZcZ4OmqxLM0c0V4tuCfq0qOTL0JT1OzJOdDc2n/ytyDf08X5Ust65ebLINJcpSQrQrPLplYPdgLiuzAjV2SL9NglJxbHKB1wc+D62P3FesFzSr6e+C/7KXL2NcnXuqPH9y4quRcAUHzUmyWbWaXhhPM+ZdK+5cInD1dVAAyuNR2tVLOFeh7GeIgLswv55d8vVAf+GtdsPVCvLngn6ijKF1n4LbpjQyi5x/ksgb4gSefOKAV1yIQZz3kNv8+mbe6gWDOv0Taf5i/B95qHIrZ23NG33dfSeDugzA/YdX8Eh4u2Pxec1T93/ZcaxzwswvEI/HW6i3wqbOzJfisrdwAAEkZ5uuoVMjhlksnXRlfN0yMqow/zzy0uo/GytSBMr5uktMPvG0UQ2Qzb8SfFwDYd/UxPu1Y1WpGDpC9DKCN6SSEEELIy9IoNTje/3juOxbQue1VpUoVNGnSBD///DNatWqFGzdu4NChQ5gxYwYAwGg04quvvsLatWvx8OFD6HQ6ZGVlwc0t98wCPk9PT5w5cwZ6vR7btm3Db7/9hi+//FJy35o1a6JixYpYv3499u3bh4EDB0KptPxOOmbMGLz99tvYv38/jh07hnXr1uGrr77Cpk2b0K5dO26/OXPmoFGjRvDw8ODS/kuVKuVQ+wsDCv6LGR9385fhxDTzyL1ONPLPdzu7QB2/6rvWNWfkPyWX9bVTswxcp4K7WoFqpbT47XhO8KFUyLjzs7UEMvVGLogtX9IDl7MLxUmNwLlmFx6zZ86/ewGP/E/ZeAFn7iVZbGfT/tnUY37VfP7Iv1QQvvzteqgc6CmYKy9W2seNC/7ZIMxy5D8nuHFV5oz8p+kMkqOXRhMDmUzY6ePn4YIapb2QlmVAlsGEB4kZSJUo+MeO/LPvoUoh597blyEO4kJL5AT/aqWcC+QyRQX/gJxRXBZ/usCtJ+Z73E2tEATi4mr/4qwO8SoRbHaFwkoeeAqvsyDF2px/nfkcrioF3m1VAdHhgTh15zk+/vOC4Fhh/tJZDyqFDDbicPM5eMG/uyiQzjKYcl2dQ6WQW3QS2Jp2z15HlUJmNe2fxdZ1sPV51qgU6NcgBOtPPxDcu1LTUszHtD7yz55H/HkBgOuPU3H/ebrN0f3c6g0QQgghL0smk9mVel8YDBs2DO+99x4WL16MFStWICwsDC1btgQAfPvtt1iwYAHmz5+PiIgIuLu7Y/z48dDpHMsKlcvlqFChAgCgatWquHnzJkaPHo1Vq1ZJ7j906FAsXrwYly9fxokTJ6we19PTE126dEGXLl3wxRdfIDo6Gl988YUg+A8MDET58uWL/Jz/ottykifsF+zEdGFALRUcs3FACd5oJjsavPzQbXT/4ajNc6Vk6rmRM3YptAxBmrPJovMhy2DiRlXZVN5naTrJkX824Mqt2r95mbGCncPz1znpDAg2UErMHk335QUj/JF/qSCDDZptpf0HaF25lPPVx+9lH9f6nH9h2r/B6simr5uam8cOmP/z+evdptgxvgWXcSGZ9p/dZvbeUivlXLaIvaTiZ62oA4Gf5q1S5AT/GXqjxSoEX2yxXlOADf69NCp80qEKl1GQmK6DwWjCg+xl38TXyaLaf/Z7UKeMt+R5+DE1P2sgTWJlAva1hPl5SHaciEf+WSpF7r/OO0Xk9FCLPxM6o0mwhKfkOSSKeNrCdnQobKT9s7yzPxu2PqvuLkrM6lED12Z2ENQekMpMAXI6O6VIrRaikMvQoKwvAGD/tceS2Tzsyg4U/BNCCCE5evfuDblcjtWrV+OXX37B0KFDufn/R44cQdeuXfHWW2+hZs2aKF++PP77779cjpi7Tz75BH/88QfOnLGs3wMA/fv3x4ULF1C9enVUq1bNrmPKZDJUqVIFaWlpL92+woiC/2KGHQlLTNcLvsRbGxmXywBv3jzb1lX87T5XSqYBuuy0fjYwEaQ564zc8nhsajg7sgwAoSWyg38rBf/YXyi5FfxTKWR2TQ2wRxs7Xj9/dD9Tb8LoX09zVdX5hevYkcdMveWSe0DONbGVteDhorB4bfYW/EvPkp5uAEinr8vlMigVcu69lJ7zLwy2pNL+g71tp5EpJYJY/ooHMpkMpXjHSMnUc/dApt6ER9mp/NPfCM91ygG7ooWbWoFRLcPwba+aAMwj1p9uvIBm3+zD0RtPLZektFLwz1/rikMftbZ5zpRM23P+NbyRb6VET0heg//DH7fmVpoALNP+7z1Lx8S152weI7fq+TltEbZbqZBLVvvn35tsdoetzzPbESaXywSZKdbuKW9bI/9Ky4KhLko5WlXxAwDsu/YE+uwAn60xoVEpuM41W8UACSGEkOLGw8MDffr0weTJkxEXF4fBgwdzj1WsWBG7du3C0aNHceXKFYwcORIJCdantdorJCQE3bt3x9SpUyUf9/HxQVxcHPbs2SP5+Llz59C1a1esX78ely9fxo0bN/DTTz/h559/RteuXQX7JiUlISEhAfHx8dyfothBQMF/McOO/J+9l4QjN3LSya0tw+XrrhYUF7MWeEhJydRz6ezsKD1/5YAM3hJobHDHLwpXmhv5l17qj5Vb2r9KLpcs6iXlu+zgz5r5fWth/ajGNvdZ1K82GpYzjx5ei0/Btovx3GPC+gnm9+JFhl4ykGADV1sj/xqVwqIAW+4j/+b9U63M+Qekq9az2MBOqtq/uCCbWiG3GLW3dWxAeu65OJ2e/5oSkrN4HSlGLq2/jK+b1aCwVog3AHMhRiAnEGY7x5LSdVh7ylyf4ostVyzaxO/48HZToXONnDVjraWgs/jTZXQGE7cKRE7wn/N+igP6iGAvwTQcPqXCeul5d7XComij+Jr+fvI+9lx9bLPtaqUc/RqUsbkPYNl5ZF7qz/I+5t8v7LW3lcnjzsse4D/XWiePj805/3KLY6qVctQLNX92bz1J5T6XwT4aHPiwFY5+0oa7blTwjwAAHjwwp/ZIrG9NCCHFzbBhw5CYmIjo6GjBfPgpU6agTp06iI6ORqtWrRAYGIhu3brlyzknTJiALVu2WE3r9/b2liwICAClS5dG2bJlMX36dDRs2BB16tTBggULMH36dHz22WcWr61KlSoIDg5GUFAQgoKCsGjRonx5Da8SzfkvZtiR54SULIz45RQAc8V4pVwmWHqPxa/0z1oyoA5GSyyPJfa/vy9hfGRFADmByfjIirj/PB17rj6G0cQgPTtF29NFGPyrFXL4ZweJiel6LmjWuioFS6UBlsFC4/IlEOTtig3ZhcOUClmuUwOCvTX4qkcEWlbyw6R157ntM7qGY+rfl7if3VQKqxXHWSqFnAvIxcXQ+MEIO53iWWqWZCDBTgvwsDF6rVErLYJ9i4J/vMddlXKuMyFNZ4DJSuqytQATsMziUClkXPvFgb555F+4zVY1d9a295rgjx2HkKQpjcpBXjb3fZySyY3iZuqNuJ+dql/aR4NS3hpcjU+xeE75ku44dz9JMOcfyHl/+NX8L8cl4/t9NwTPZ4P/oU3LYWoXyzQyW0tLiu/fvsticWlGe25qi6vaevD/95imVtfcVdsY+c+vaS8qhQyVAjxx9n/tcPLOc7yz6rTkfn6eLojjrdphDv5z2uCuVuDrnjWwYM91bmlJb27Ov62R/5x7iV8HwtpzvDW25vybn8PvlHBRyuHL1kXhZUepFXKuzgT7nogLHxJCCCHFXePGjSXrB/n6+uKvv/6y+dz9+/fbfHzw4MGCbAJWo0aNBOfM7Tjnzp3j/l2yZEksWLDA5v6AeUlvk8mE5ORkmvNPihapkTCVQg6ZTAYXieChhEQQ2CEiCLN71rDrfPN3X+fOAZjTcJe8VZd7PDl7FNRTNFfczUUhnAqgN38JH9G8PMr7uWNki/LcMVxEc3bXvNMIEbx10K2lHPO1qOSHlpX8LLa/3bgsosMDBMcKkFiKjU+tlHOB2OMU4bKF/AJk7LVNTNdjxubLVo/nYWOudPVgrUXmgzjt38XKnH9ztX/pAMb2yL/5eOzIP3+kWjwCq1ZYpv1L3VNiFfw9ULsEg2/fjMDoVmGCx8T/qeiNDBfcPk7J4tLqg300CLLyXpXNTuNmOzDY+0PrqoJUbP23qKbD+ezRevG1ZtlKXbfIItAZYTIxOXP+eZ03/NF8lUImyMIRs5X2n1vnl73Y4pE+7mqbGSl+4pF/hUzQvvfaVkSXmqUE9SDY6UW20/5zHnuvjbljsVONIKvFBG1NmWFH8Pkj/y5KBTdVIDlTz01B4r8P7L8dqX1ACCGEEFIYUPBfzPhKjLqygapUIGMtbZo/r90e/C/+KoWM++LNjoyLA0R3tTJ72ThzZXU2MArQumLvB60wuWNVbl9XpfDYgHD021Ult1n127y/9Y+C+Du+eHRbTKWQc8djq/yz+Nffx01tEWhGVQuAp6sS7aoF8J4jHYjP7lkDzSv6WQR24iCQPyLsqsoJ/vVG4frzfDaD/+zXxgax/ADLnjn/1l6Po8a1NQd/o1uFcR1ANx6b0/hLeqjhplYKagPwlSspTP9y480lzy2zg89a8L+wb227jwGYp8Cw97hGMPKfc4NILUnHZyvt31pAvW5UY6vLE0pRKXPOYSubwDLtX3id2KPw7x02K8lm8M+7lxqHlcCBD1thXu9aVp/D7/RzUcpRwT9n2pIpuxNJPPLPdkIwjLneCCCsQ8G+Fir4RwghhJCihoL/YsZHIrBhAwx+INO3fggmRVXC+HaVJI8T7GDwr+YFJuZlS4Rp8eLUcA8XJfdFPzXLwM2Hlpq77yoxR5ofzPu4qSU7PfhsBf8mifQl22nxMq6d8S9EI/+8dijkMkH1fwAo6emCU1MisWxgTnaEZIeNUo7e9UMAWI7qpmbpLfZluaoUFku8SbEVEKpEo8/8AEtc2V+tlMNdrRR0cvjaqMDuiPfbVsTWcc0xKaoy1wHErvUenD2/PUArPfIv7rziX5PcOor4rN03kdUCcHlGNPdz+ZLueD+7s4I1onk57rqk6QySc/75QbOtexSw7PThz+m3lhVQv6wvhjcvZ/O4fPyOJPFoO784objzSKpwIQDIeTdGvewq+zar/YvOGVrCHWql9cwe/rFaVPLD7oktuZ/ZtH3+e69WyqHkZass2JOduSS3HPk3WqmXQYqZ6dOBiRPNfxNCCCGFHAX/xYy4OByQExjwg4UhTcthbJuKVkf+g7xytv/4dr1czysOPth5zezIs3heu7uLgksrTs0ycNMDpNJ7+cESG5zwOwm83dSCoFuKtRFcQHpur7h4muBYvCJ37Pxzri2izhdxkK2Sy+CiVFjM6y7vlzNSHaB1Qczg+tzPCtG+T1OFa6YKg39zcJNbIGlP2j/L1si/SiGHXC7jajoAjgXXtijkMlQrpZVcyrFUdrq/1OuUySw7Bfj3FX8uuUIu42pPSLF1Hd3USvSqWxoA8M2bNSw6aTQqBdcJlp5lRGb21BZrBf9yD/5z7oPJHarg0vSczgf+koJi/IC+fXigxeNNK5Tg/p3Fm7IgDrjltoL/7HN0qB4IV5Uc3esEAwAGNQlFlUBPrBzagPtdYyv4t1b/wtq0Bn4bA7TCNrEZPfz7l73G4ntUJRj5p4J/hGf5cmDePPPfhBBCSCFHwT/hvtjyv/DmNrKvVsrRPjwQYX7uaFaxpN3nYInTdD1dxMG/UpAufvFhMgDLdG3A2sg/fyRXlWuROUdH/m1VdFcpcta2Fz9VvIydONCxNkJbgbfKwvFPI9GkQs41Fy+9VjfUR/AzP7hjr7vUa+KzOfIvulb8OexSBf8AYWZHfhWf4xMvd8gG862r+CPYW4PG5XMCWJXCchqIVNV5wJzJYGtuu61OIwCY1SMCxz9ti/plfS2Wv3NRKbjpBmk6A1cfgt+BIkj7z3VJy5y2eLgqBZ8LceFJa6+hXlkfjG1dgfu5V93S+G14IzTP/oyzqyQAlgE3/9VZpP1nv44fBtTBualR8Pc0d760qRKA7eNbCOptdKlRCs0rlkTXWqUgZu29sHZP8YP/QFGHD1s7QjznH7CsjaIUTFtil/qj4J8QQgghRQtV+yfcl/+pXarh2M1n6FyzlM2Ah7V0YF0wDGO1+jifeD6yRfAvGtHzcDFXsVfKZdzcWleV3Erwz/tirmTn/AvT/qVGm/mrG9gK4qQC5fGRFbH1QhyXFeCmVuRUv1fKLUbArckQzbmXWuMeMGdi7LycgOrBWovH+IXHlr9dD5FV/QWP84NGNmDLbdTSnqX+WEFerlxFfQ8XJRRyGXdd2CKS/PfXVlX6vLIYVVfLufYc+qg1MvRGhH++AwAgl5nvGf77X5L3evnZGVqNymbRuNxei1Ih57IMpAoxuqsVeAJzcP5fvLleQbUgreD5/P1t4Qf/4nYl2wr+eft6uiqRlpVzT7LnjxnSAA8S01GG1+llq4imViM9JUAmy33ljWqltFg1rCF+PXbXotCi1eDfyjE1vIwOcbaHUWrOvyqnMCkf/55nM6SsLZNJCCGEEFJY0ch/MTSkkjDgZIOK+mV98V7bipIBtjVs4J9b9X9xMOIqChzEAZabWgmZTCZI860c4GmxNjmQ9zn/noJ0X+sBiUZlGXCE+Xng/OdReKOmeXSyWpAWs3vWwKweEfBwUUoWBfy+v2URuHS9MB1bHFizGoeVwKaxTfHrsIYWj/FH3ttVC7DojJEa+Wd5uigxuElZwTaFXGYzNZ9/vEGNQ9Ghes4a9y4queBx9v3gXw9x5oCjpDqb2GCf+5n3OuVyYcApgwwymUwwuluSd394C0b+VTaD3NxG/vnEWR1uaiUXeJ6//wI6owmerkqE+Gp4z+EX/LM/7V/cLlu16fj7al1VgqJ+7DEVchlCS7gLrr3tqvzi4N/x91yqY8Va8C/1ewEAN60CEHbwAPw5/znHZGsQuItW2OC3X0kj/4QQQggpoij4L4ZqlWDQkpeqn1s6sT161w/B5RnRmBRViZtvzWcR+IjOqVbKBUEIu6QX/8t+1SDLUW9AOPLPLvPGf00+7iqL4L+8n7sgmLIVxE3tXA0V/D3wTc8IwXYPFyUXAJUp4Ybe9UPQr0EZAJYjn4v710HnGpZpzBX9PQU/2wqSapT2thiRBCzT/sXEc/75mlUsabF0nq+72mowBQinMkxoV0kQLLoo5YJ6C+y5KwXmTFuQWlISAD5qXxlNK5Swq4aEmLjzRjy6zH897D/5a8DzA0N+p4CXaORfXAPDkeBfHMyW8FBzQebJO88BAOGltIIAWzjn3/60f0fqKgiCf41K0E5b94G5UyVnX36fjEXwb2MlAmv49xXLVhZGaAlzVgK/or9GUIzS/L5WCTR/5rrVMtcdcOMF+myWUWqWsINU0CFCI/+EEEIIKaIo+C+mBAHhS47EstzUSoxtUxFHJ7fF5veaCTIIxKO94jm6aoVc0A72Sz4/iKgUIAyUWWV8c86jlhj593ZTC4q4fdaxKnaObyFok63R3TIl3LB7Ykv0qV/G4rHA7I6O6qW8BNvFI//ebtLTAGZ2q45KATnBilTAkxvxuvFiLoLg3/w65/WpiQblfDGja3WLJcvEa7SLpfBWE9C6qgRLIbooFZKdKu3Dc7IDrAWU5Uu647fhjRDJW+aQjw3q2GwLPnGwbyu1nA2u+e8JvyZEmRI5qe1ajVIQcLarFoC5vWtyPzsyhaGEaIUIdjlCANh79TEAy/uIXyU/t3vjCW9ZydplvAEASwbUgVIuw3e9alp5lmXav0oic8MaqQKcgGWQbq3avy1qheV7aOtzumtCS1yYFiX4neGilHP3W+XsoH/96CbY/F4ztM2eHsN/jYbsm1k8TUIlGPlng38a+SeEEEJI0eL04P/hw4d46623UKJECWg0GkRERODUqVPc4wzDYOrUqQgKCoJGo0FkZCSuX78uOMbz588xYMAAaLVaeHt7Y9iwYUhNTRXs8++//6J58+ZwdXVFSEgIZs+ebdGWdevWoUqVKnB1dUVERAS2bt1aMC+6EOB/4c+PkX+x6sFeqFYqZ6RenM7On1dd0d8Dwd4aQTvY4IE/Vzy0hHSRPbVSjr0ftETTCiUwrq25WBk/+PNxUwm+4OuMJigVwvR0R9Z25xvVMgwxQ+pjQCNhx4B4zr+14wd7a7BiSAPuZ1Ue0qP1Do38m69L99qlsXZkY/h5ulikL4vTo8XY7ArAPPrLX/JMrZQLRqjZczcs78ttszZ6m9tc8L/HNMXm95pZ1DQALDuTbKWks3eiq5WU8MZhOcUBjSZGsBSct5tK0LHjyMi/uI5CCXcXQXq5SiHjlm/ktvGOL8+ltsbpe4ncv9kCix0ignBxejTezF51QIpF2r9EZXtrrF1ncVX+vKT9S02BsVVfRK2Uw9NVJSjGKZPJcHpKJE581pbrFPBwUaJ6sJfksdiAvhGvQCQgzFxgXwul/RNCCCGkqHFq8J+YmIimTZtCpVJh27ZtuHz5MubMmQMfn5xq5bNnz8bChQuxdOlSHD9+HO7u7oiOjkZmZs766QMGDMClS5ewa9cubN68GQcPHsQ777zDPZ6cnIyoqCiEhobi9OnT+PbbbzFt2jQsW7aM2+fo0aPo168fhg0bhrNnz6Jbt27o1q0bLl68+GouxiumFqVqFwR+gC8eIX0/siI6RQShflkf/DSoPuRymaAd7Bd1fqBYxkaF/fJ+HvhteCNudF4855+PDV5V+RD8a9QKtKrsb5GSLS5gaKt6Pn9fYy5V+KVkOTDyLzWS6+jIv3hUlN/3oBC9j+z5VAo5/ninEWZ0DUd4KenpG7mNMrvbCNo8RCPQNlcUyH46v5Ajv/4DW4keAC49ShaMbvu4qeHllrfgn39cAPDljfwDwBs1gy2yW/idQbZS8IGcSvw1SwuzB3LrVOEfV6tRCYLuXIN/3nWW8er9u6tfPu3fkWvLN7VzNXSqEYTVw831Mbzd1BbX3hp25P+9NhUwqmUYt53f8cK+ltw63QghhJDiYvDgwZDJzDWVVCoVAgIC0K5dO/z8888wOThNLiYmBt7e3vnSrlatWmH8+PH5cqzXhVOD/2+++QYhISFYsWIFGjRogHLlyiEqKgphYeYvXQzDYP78+ZgyZQq6du2KGjVq4JdffsGjR4/w119/AQCuXLmC7du348cff0TDhg3RrFkzLFq0CL///jsePTJXiv7tt9+g0+nw888/Izw8HH379sW4ceMwd+5cri0LFixA+/bt8eGHH6Jq1aqYOXMm6tSpg++///6VX5dXgR9o5RYc5BU/6BYHdqEl3LF4QB2sG9WES7N2kUjDN/IC09I+1oN/MX7gwAb2PeoEQ62U463sUXqVaGpAfhIUuFPILNYY5+MHruLq//Zgiy1+GF1Z8nH+tZAKIMVzl21V+geA5ExhkUJxh4WbKO2a1bB8CbzduKzV0dvclh+0RVygTVzbQAp/5Fbcpl7ZI+UjW4Zx9ScAy5F/RzrOxMtNerooBVkFUp1b/KA5t5H/ub1r4Z0W5fHT4Pp2twkQThvxdFUK7hdrq0+w+Gn4AxqaP1e1QryhkMsEj+Ut7T9v/z35ebpgcf86guUw7cUVAXRRYlJUJW57Ou9zyY38U9o/IYQQwmnfvj3i4uJw584dbNu2Da1bt8b777+Pzp07w2Aw5H4A8ko4dam/TZs2ITo6Gr169cKBAwcQHByMd999FyNGjAAA3L59G/Hx8YiMjOSe4+XlhYYNGyI2NhZ9+/ZFbGwsvL29Ua9eTpGwyMhIyOVyHD9+HN27d0dsbCxatGgBtTrny3d0dDS++eYbJCYmwsfHB7GxsZg4caKgfdHR0Vwng1hWVhaysnLm2CYnJwMA9Ho99Hrry2o5G9s2fsyikqNA2uzjxq+izeR6Dv6XfR+NAnq9Hs/Tcq6xUmaCXm9f76ECOfu5q2TQ6/X4uls1TO9cBa4q87FlYHj7SF+DvF4XV0XOsX3d1Xb/0kvLcvz+iaxSEmc+aw1PV5Xkc2W84N5kNFjso9MLOxwCPaWPw+LXZjDf7wbBzx68oE9h5X2vFuSJy3EpUClk3LKDOn1O28R/28NVJUdm9v2hkll/roxttzHndYv3/bxzFfSuWwo1gr3w45E73HYPtRz8Wo5yJvf72hqDwSC4ln4eSotjCYox5nKu0l5qfNjOPO3FkTalZupyfjAZIWNy7hc5bJ/TjdfJElmlJHrUDkIZHw30ej183FQ5QTNjdPg6ySH8rI9uUa7Af7fqDCbJc6Rm6rjtcpn5PcmSuF9J8aNo3hx49gwoUQLGQnof0H1KigK6T82vnWEYmEwmbrScYRgwGRlOaY9Mo7FrOW/A3E61Wg1/f/PUzKCgINSqVQsNGjTgMgCGDx8OAJg3bx5iYmJw69Yt+Pr6onPnzvjmm2/g4eGB/fv3Y8iQIebzZ5976tSp+Pzzz7Fq1SosWrQI165dg7u7O1q3bo158+Zx57TVNmvZB3/++SemTZuGGzduICgoCGPHjhXEg0uWLMH8+fNx//59eHl5oVGjRtiwYQNMJhPWr1+PmTNn4saNG3Bzc0Pt2rWxceNGuLvbv2qaI0wmE5js74IKUV0kRz43Tg3+b926hSVLlmDixIn49NNPcfLkSYwbNw5qtRqDBg1CfHw8ACAgQFgALCAggHssPj7e4k1XKpXw9fUV7FOuXDmLY7CP+fj4ID4+3uZ5xGbNmoXp06dbbN+5cyfc3OwfoXaWRw/ug038SHj0AFu33sv3c6SmAOwtdv/OHWzdesvm/hmpCrA52f+dP4m0G0Dcs5xtjtZgGFRRBpkMOLR3p+Tjj3nHPrJvN3IGJ81tloF5yboP5uMoDZl2HMe877Xrt7DVeOMlzmn7+BdPHsH988JHHsbJwd4LnUKM0CRcxNat1qe7dPEHUlIU6BhiwtatW3EhTgbA/Eto69atSEvMOd71q1ew9cVli2P0DARcdXK0DTZhw20F4jOAZ1dPYKuwnAd27drlwCvMeT8vnjsD413xyKz5Ghj0emzduhUljOZ2KmTW3+dHF4Db8Tmv79KZE3jmlnOsg4cP4550HUqrrWRt3boVDx7kHPve1QvYmvCv1ec8eZxQIHVIkrLM51BmX4cLz3PadP2/q9iaesXqc4OZnH1PHI9FqAfAvoUKfc77cejAAVy2L/OeczfV3C4AGFzRiCr669gqvkHyjfk8SS+SRdc4+3N56y62br0NAHiSYL5v/r1wET5PLwBw7D4lr5m+fXP+XcjrBNF9SoqC4nyfKpVKBAYGIjU1FTqduWPelJGBhNZtnNKegH17Iddoct8R5uDTYDBwg6GsevXqoXr16li3bh169+4NANDpdPjqq68QGhqKO3fuYNKkSZgwYQLmzJmD6tWrY9asWfjqq69w8uRJAIC7uzuSk5ORkpKCjz/+GBUrVsSTJ0/w2WefYeDAgVi3bp3VdhkMBuh0Oot2AcC5c+fQt29ffPLJJ+jevTtOnDiBSZMmwc3NDf3798fZs2fx/vvvY+nSpWjQoAGSkpIQGxuLlJQUxMfHY8CAAZg+fTo6d+6MlJQUxMbG4sWLFzAaHc/itYdOp0NGRgYOHjxoMaiYnp5u93GcGvybTCbUq1cPX331FQCgdu3auHjxIpYuXYpBgwY5s2m5mjx5sqBnKDk5GSEhIYiKioJWKz2nuTDQ6/XYtWsXKpYvi/1x5oC/Ulg5dOwgnTL+MrIMJsy7uBsA4OIbhI4drVccB4CVD0/gTmoSAODNTpHwcVNj/ZPTOHTjGQCgY8eODp0/t72/u3oISDf3pnbulLP3+7HmzgK5XI6OHaMdOicfe5xypUqiY8e6du3rH1waHTtWz/M5rfGrlojULANaV/azeOyU6Qr+PX4fADD/nQ52HW8E79+N03XYN/8ImlUogY4da+BQ1iWce/4QAFCrRgQ61pcuNvd29t/DTQz0JkaQQs/ep+3atYNKZV89hjnXDiH1ufn9bNakIRqW8xU8zl5jtVqNjh1bo43eiNon7qN1JT+U97PeS5t19hHW3zZ3hnSMbIUyvhp8dML85aRho8aoG+pj9blibBsA8/38OPYutty/BgDoEtlMckUL9jmlggLRsWMtu8/liJatM6B1VcLTVQWP60/x47UzAIDq4dXQsXGo1edFGU249dNJ3HyShv5dWgtqZ2x8dgb3/nsKAIhs2wZBEkuA2nI1PgVzL8QCABrWrytZ6DG/sNdYq/VEx45NLLb7+OVc+73pF3D2WRwqVamKdg2CHb5PCXnV8vL7lJBXje5TIDMzE/fv34eHhwdcXc3/Z5qUSiQ4qT1aT0/I7RzQVKlUUCqVkjFQtWrVcOHCBe6xjz/+mHusevXqyMzMxLvvvovly5cDAPz9/SGXy1GxYkXBcd59913Bz2w2uFwuh4eHB6QolUqo1WrJdi1btgxt2rTBzJkzAQB16tTB7du3sXjxYowaNQrPnj2Du7s7evXqBU9PTzAMgxo1asDT0xM3btyAwWBAv379EBpq/p7UuHFju65VXmVmZkKj0aBFixbc/cGS6tywxqnBf1BQEKpVqybYVrVqVfz5558AgMDAQABAQkICgoJylgpLSEhArVq1uH0eP34sOIbBYMDz58+55wcGBiIhQfjRYX/ObR/2cTEXFxe4uFjOjVapVEXil5ZGreL9W1kgbeYf8n5SRq7n4M8l99O6QSaTYVbPGpi78z8MbVYu39uYwZtCIHVsuUx6u6OCvDR2H6duaIkCeS+aVLQeOH0QXQWJGQb0rFM6T+f291Lh5JRIKOXmQi9evPoJGhf7Pg/Wqgw48nkyV7g3B/+eGherz5PJco47qlVFyX34NC45xymp1UCtVmNwk7K4+SQV9cv75VqIzxqVSgW1MudXcGlfT5uvValQFNjvlrJ+/N8HOf92Vdn+3aBSAWtHNYHRxFjUDinJK7KncVE73HaNS8595GrnfZRXX3avjm93XMPsN2tKnidAm/MZVrKpdjI5t62o/N4nxRvdp6QoKM73qdFohEwmg1wuhzy7vozM3R2Vz5x2SnscSftni/3Jrazuw39s9+7dmDVrFq5evYrk5GQYDAZkZmYiMzMTbm5u3H7iY50+fRrTpk3D+fPnkZiYyKXyP3jwwCKetHZuvqtXr6Jr166Cx5o1a4YFCxaAYRhER0cjNDQUFSpUQPv27REVFYW2bdtCq9Widu3aaNu2LWrWrIno6GhERUXhzTffFBStz29yuZwrqCj+jDjymXFqwb+mTZvi2rVrgm3//fcf14NSrlw5BAYGYs+ePdzjycnJOH78ONe70rhxYyQlJeH06ZwPxt69e2EymdCwYUNun4MHDwrmQ+zatQuVK1fm3qTGjRsLzsPuU9C9OM7CL+olrlSfn6a/EQ6lXIYPonLPLEhKz5l7zP6yKe3jhrl9aqF6sJe1p+VZhs72PHx7f+FZ826rMJT0UGNCu0q57rt/UivMfrMGetezviRbQfF2U+P7/nXQukreR1ZVCjl3vfirF+S1Ynte8FeGsFXt39H3lV8QkS32N+2NcKwa1tDhwJ+tID8oezQ9JTPnd5JWY7svVp7HTgZHqRwo+AeY33upoqH81T5etuCfSx6L/9lrQMNQnP1fO9TMXjGB9cvQBuhSs5TgM8zePi9Rn5IQQgixi0wmg9zNzSl/XvZ7MOvKlSvc9Os7d+6gc+fOqFGjBv7880+cPn0aixcvBgBuqoOUtLQ0REdHQ6vV4rfffsPJkyexcePGXJ/3Mjw9PXHmzBmsWbMGQUFBmDZtGpo3b46kpCQoFArs2rUL27ZtQ7Vq1bBo0SJUrlwZt2/fLpC25CenBv8TJkzAsWPH8NVXX+HGjRtYvXo1li1bhjFjxgAw3/Djx4/HF198gU2bNuHChQt4++23UapUKXTr1g2AOVOgffv2GDFiBE6cOIEjR45g7Nix6Nu3L0qVKgUA6N+/P9RqNYYNG4ZLly7hjz/+wIIFCwRp+++//z62b9+OOXPm4OrVq5g2bRpOnTqFsWPHvvLr8ioI134vuNtgUJOyuDQjGq0r5x5YJqa/2iIv6Xrbc3Je9lfeR+2r4ORnkSjlnft8qbIl3dG7XohdwVZh58mrhq/OwxJveT4vL/h3lejQCstO7Y8Ol87msYa/otvLBuAfRFXCulGN8WmnqgCEHRG5/Sf7qi4lf2WOvGY1AOaVEVgvu9Sf6hV0Ikld/xaV/LCoX2348lZqeHV3NCkS2rQBwsPNfxNCCBHYu3cvLly4gJ49ewIwj96bTCbMmTMHjRo1QqVKlbjV2Vhqtdpi3vzVq1fx7NkzfP3112jevDmqVKlikfntqKpVq+LIkSOCbUeOHEGlSpW4gnpKpRKRkZGYPXs2zp07h3v37mHv3r0AzN8bmjZtiunTp+Ps2bNQq9Vch0Rh5tS0//r162Pjxo2YPHkyZsyYgXLlymH+/PkYMGAAt89HH32EtLQ0vPPOO0hKSkKzZs2wfft2wVyH3377DWPHjkXbtm0hl8vRs2dPLFy4kHvcy8sLO3fuxJgxY1C3bl2ULFkSU6dOxTvvvMPt06RJE6xevRpTpkzBp59+iooVK+Kvv/5C9er5P/+6MOAHZY4sV5YX9mYWGF/x0lm5jdzltrSaPfKr17QoKQwj/65qy/OuGdEIOy8noHvtYIeOGx0egO92uKJRed/cd86FSiFH/bI5xxnQsAy2XohDl5qlcn1uftyP9lDxfjeoXqLHwUsw8u/4fcA/98t0QhQUBjT0TwD89x/w8CHw4oWzW0IIIU6VlZWF+Ph4GI1GJCQkYPv27Zg1axY6d+6Mt982V3qqUKEC9Ho9Fi1ahC5duuDIkSNYunSp4Dhly5ZFamoq9uzZg5o1a8LNzQ1lypSBWq3GokWLMGrUKFy8eJGbq5+bJ0+e4Ny5c4JtQUFB+OCDD1C/fn3MnDkTffr0QWxsLL7//nv88MMPAIDNmzfj1q1baNGiBXx8fLB582aYTCZUrlwZx48fx549exAVFQV/f38cP34cT548QdWqVV/+QhYwpwb/ANC5c2d07tzZ6uMymQwzZszAjBkzrO7j6+uL1atX2zxPjRo1cOjQIZv79OrVC7169bLd4NcEf3TPRSJl1xnUSjl0BpNgfXBnKoTxRpHAH4FXK17de8kfXdZI3NP+Wle81ch68TprPF1VOPJJmwIJQL3d1Ngyrrld+76qtH9+un1egnaWVpN/I/+F6aNYDPvzCCGEkFxt374dQUFBUCqV8PHxQc2aNbFw4UIMGjSIm1dfs2ZNzJ07F9988w0mT56MFi1aYNasWVznAGAekB01ahT69OmDZ8+e4fPPP8e0adMQExODTz/9FAsXLkSdOnXw3Xff4Y033si1XatXr7aIE2fOnIkpU6Zg7dq1mDp1KmbOnImgoCDMmDEDgwcPBgB4e3tjw4YNmDZtGjIzM1GxYkX8+OOPCA8Px7Vr13Dw4EHMnz8fycnJCA0NxZw5c9Chg32Fs53J6cE/cY5XlfbviF+HNcSsbVcw/Y3wV3K+5hVL4tD1p2hjZa77qxppfd0I0v5f4cg/f1651Bz0l1EYRp4Vr2zknx/859fIv+PHURXyKTA0558QQggxi4mJQUxMjF37TpgwARMmTBBsGzhwoODnJUuWYMmSJYJt/fr1Q79+/QTbmFz+M96/f7/Nx3v27MlNSRBr1qyZ4Pkmk4mrql+1alVs377d5rELKwr+iylBMa0CLPjniAblfLHx3aav7HwL+9bGFlsp186P94okftr/y6SNO0rBG6Uu7IFjXrzEILxDHC34Z025kjnLJ+Zl+gv/d1RhirNl9IuBEEIIIUUUBf/FVGEc+X/VfNzVkmng375ZA9M2XcKygfWc0Kqiz1lz/l/DeF+gdkjBLR/Dx++wyUu6Piu0hDsW9K3FrZDgKP40h7weoyBQQhAhhBBCiioK/oupV7XUX1HUq14IetQpXShSvYsiD37w/woj8peZn16Y7ZzQAqfuJOLNuq9mGUj+e/ayU1+61nKsuKLY7Ddr4ElKFir4e7zUcQpCbqmGhBBCCCGFDQX/xZQw7f/1DJpeBgX+eccfpdXx18krYK/re1YpwBOVAjxf2fn4UyZMTg5we9cLcer5pbD9IRT7E0IIIaSooaivmBKm/dPIP8k//M6kEu4ur+y8/LXYSd4J6iVQgCvh9exkIoQQQsjrj0b+iyka+ScFRSaT4c/RTZCcoUegl+srO++QpmVx4vZzRIcHvrJzvo74c/5pLXvr6MoQQgghpKih4L+Y4n/Bpzn/JL/VDX01xen43NRKrBza4JWf93XDr8xvenWzNooMKvhHBKZOBVJTAY/CV5eCEEIIEaPgv5iiav+EkNzQ6LZ1NOefAADeecfZLSCEEELsRlFfMcVfRotG/gkhUgK1r27aRlHB/uakKRGEEEIIKWoo+C+m+MtUudDIPyGE55ehDfBl9+qIKO3l7KYUOpT2TwghhDgmJiYG3t7eBXb8/fv3QyaTISkpqcDO8bqgqK+Y4o/ovcq12AkhhV+LSn4Y0DDU2c0o1CjtnwAA4uKABw/MfxNCSDE1ePBgyGQyyGQyqNVqVKhQATNmzIDBYHgl52/SpAni4uLg5ZX/gxZ37tyBTCbDuXPn8v3YzkBz/ospV5UCZ/7XDgq5TDAFgBBCiHWy7MR/iv0JAKB+feDhQyA42NwJQAghxVT79u2xYsUKZGVlYevWrRgzZgxUKhUmT55c4OdWq9UIDKTVnuxBQ77FmK+7Gl4albObQQghRQal/RNCCHlVGIaBPsvolD+MgyluLi4uCAwMRGhoKEaPHo3IyEhs2rRJsM+OHTtQtWpVeHh4oH379ojLzpo6ePAgVCoV4uPjBfuPHz8ezZs3BwDcvXsXXbp0gY+PD9zd3REeHo6tW7cCkE77P3LkCFq1agU3Nzf4+PggOjoaiYmJAID169cjIiICGo0GJUqUQGRkJNLS0hx6vaysrCyMGzcO/v7+cHV1RbNmzXDy5Enu8cTERAwYMAB+fn7QaDSoWLEiVqxYAQDQ6XQYO3YsgoKC4OrqitDQUMyaNStP7bAXjfwTQgghjqK8f0IIIQXMoDNh2fsHnHLudxa0hMol70XBNRoNnj17xv2cnp6O7777DqtWrYJcLsdbb72FSZMm4bfffkOLFi1Qvnx5rFq1Ch9++CEAQK/X47fffsPs2bMBAGPGjIFOp8PBgwfh7u6Oy5cvw8PKMqvnzp1D27ZtMXToUCxYsABKpRL79u2D0WhEXFwc+vXrh9mzZ6N79+5ISUnBoUOHHO7sYH300Uf4888/sXLlSoSGhmL27NmIjo7GjRs34Ovri//973+4fPkytm3bhpIlS+LGjRvIyMgAACxcuBCbNm3C2rVrUaZMGdy/fx/379/PUzvsRcE/IYQQYica+CeEEEKsYxgGe/bswY4dO/Dee+9x2/V6PZYuXYqwsDAAwNixYzFjxgzu8WHDhmHFihVc8P/PP/8gMzMTvXv3BgDcu3cPPXv2REREBACgfPnyVtswe/Zs1KtXDz/88AO3LTw8HABw5swZGAwG9OjRA6Gh5vpG7DEdlZaWhiVLliAmJgYdOnQAACxfvhy7du3CTz/9hA8//BD37t1D7dq1Ua9ePQBA2bJlueffu3cPFStWRLNmzSCTybj2FCQK/gkhhBAH0bg/IYSQgqZUy/HOgpZOO7cjNm/eDA8PD+j1ephMJvTv3x/Tpk3jHndzc+MCfwAICgrC48ePuZ8HDx6MKVOm4NixY2jUqBFiYmLQu3dvuLu7AwDGjRuH0aNHY+fOnYiMjETPnj1Ro0YNybacO3cOvXr1knysZs2aaNu2LSIiIhAdHY2oqCi8+eab8PHxcej1AsDNmzeh1+vRtGlTbptKpUKDBg1w5coVAMDo0aPRs2dPnDlzBlFRUejWrRuaNGnCveZ27dqhcuXKaN++PTp37oyoqCiH2+EImvNPCCGE2EmWPemfsv4JIYQUNJlMBpWLwil/ZA4WuWndujXOnTuH69evIyMjAytXruQCd8AcFItfGz/V3t/fH126dMGKFSuQkJCAbdu2YejQodzjw4cPx61btzBw4EBcuHAB9erVw6JFiyTbotForLZToVBg165d2LZtG6pVq4ZFixahcuXKuH37tkOv114dOnTA3bt3MWHCBDx69Aht27bFpEmTAAB16tTB7du3MXPmTGRkZKB379548803C6QdLAr+CSGEEEIIIYTkmbu7OypUqIAyZcpAqcxbcvnw4cPxxx9/YNmyZQgLCxOMqANASEgIRo0ahQ0bNuCDDz7A8uXLJY9To0YN7Nmzx+p5ZDIZmjZtiunTp+Ps2bNQq9XYuHGjw+0NCwuDWq3GkSNHuG16vR4nT55EtWrVuG1+fn4YNGgQfv31V8yfPx/Lli3jHtNqtejTpw+WL1+OP/74A3/++SeeP3/ucFvsRWn/hBBCiIMYSvwnhBBC8lV0dDS0Wi2++OILQT0AwFz5v0OHDqhUqRISExOxb98+VK1aVfI4kydPRkREBN59912MGjUKarUa+/btQ69evXDz5k3s2bMHUVFR8Pf3x/Hjx/HkyROrx2Jdu3YNaWlpcHd3h1xuHj8PDw/H6NGj8eGHH8LX1xdlypTB7NmzkZ6ejmHDhgEApk6dirp16yI8PBxZWVnYvHkzd665c+ciKCgItWvXhlwux7p16xAYGAhvb++XvJLWUfBPCCGE2InNgqS0f0IIISR/yeVyDB48GF999RXefvttwWNGoxFjxozBgwcPoNVq0b59e8ybN0/yOJUqVcLOnTvx6aefokGDBtBoNGjYsCH69esHrVaLgwcPYv78+UhOTkZoaCjmzJnDFeyzpn///hbb7t+/j6+//homkwkDBw5ESkoK6tWrhx07dnA1BNRqNSZPnow7d+5Ao9GgefPm+P333wEAnp6emD17Nq5fvw6FQoH69etj69atXOdCQaDgnxBCCLGTjOr9E0IIIQIxMTE2Hx88eDAGDx4s2NatWzfJ5fUePnyIjh07IigoSLDd2vx+AGjVqpXFsVq2bClIx2d5e3tj+/btNtvLV7ZsWTAMA5PJhOTkZGi1WovgfOHChVi4cKHk86dMmYIpU6ZIPjZixAiMGDHC7rbkBwr+CSGEEAfRwD8BAOzZAxgMQB7ntxJCCDF78eIFLly4gNWrV2PTpk3Obs5ri/63IoQQQuzkYPFj8rqrXNnZLSCEkNdC165dceLECYwaNQrt2rVzdnNeWxT8E0IIIQ6iOf+EEEJI/tm/f7+zm1As0FJ/hBBCiJ3YgX+q9k8IIYSQooZG/gkhhBA7Udo/EVi9GkhPB9zcAIlK0IQQ4iipIniE5Nd9QcE/IYQQ4ij6bkYA4KOPgIcPgeBgCv4JIS9FpVIBANLT06HRaJzcGlLYpKenA8i5T/KKgn9CCCHETrLsoX+K/QkhhOQnhUIBb29vPH78GADg5ubG/Z9DnM9kMkGn0yEzM9Niqb+CxDAM0tPT8fjxY3h7e0OhULzU8Sj4J4QQQuxEX8MIIYQUlMDAQADgOgBI4cEwDDIyMqDRaJzSKePt7c3dHy+Dgn9CCCHEQTQnkxBCSH6TyWQICgqCv78/9Hq9s5tDePR6PQ4ePIgWLVq8dOq9o1Qq1UuP+LMo+CeEEELsRUP/hBBCCphCoci3YI/kD4VCAYPBAFdX11ce/OcnWuqPEEIIcRAN/BNCCCGkqKHgnxBCCLGTDFTwjxBCCCFFEwX/hBBCiJ2o8DIhhBBCiioK/gkhhBAHUdo/IYQQQooaKvhHCCGE2Ikd+Gco8Z8AALvsUj4sv0QIIYQUNAr+CSGEEDtR2j8ROHXK2S0ghBBC7EZp/4QQQoiDKO2fEEIIIUUNBf+EEEKInWSgoX9CCCGEFE0U/BNCCCGEEEIIIa85mvNPCCGE2Imd889Q3j8BgJEjgefPAV9f4P/+z9mtIYQQQmyi4J8QQgixEyX9E4EtW4CHD4HgYGe3hBBCCMkVpf0TQgghDqJxf0IIIYQUNRT8E0IIIfbKzvunrH9CCCGEFDUU/BNCCCGEEEIIIa85Cv4JIYQQO7Fz/hlK/CeEEEJIEUPBPyGEEGInGVX8I4QQQkgRRcE/IYQQ4iCa808IIYSQosapwf+0adMgk8kEf6pUqcI9npmZiTFjxqBEiRLw8PBAz549kZCQIDjGvXv30KlTJ7i5ucHf3x8ffvghDAaDYJ/9+/ejTp06cHFxQYUKFRATE2PRlsWLF6Ns2bJwdXVFw4YNceLEiQJ5zYQQQoouWXbiP8X+hBBCCClqnD7yHx4ejri4OO7P4cOHuccmTJiAf/75B+vWrcOBAwfw6NEj9OjRg3vcaDSiU6dO0Ol0OHr0KFauXImYmBhMnTqV2+f27dvo1KkTWrdujXPnzmH8+PEYPnw4duzYwe3zxx9/YOLEifj8889x5swZ1KxZE9HR0Xj8+PGruQiEEEKKBEr7J4QQQkhRpXR6A5RKBAYGWmx/8eIFfvrpJ6xevRpt2rQBAKxYsQJVq1bFsWPH0KhRI+zcuROXL1/G7t27ERAQgFq1amHmzJn4+OOPMW3aNKjVaixduhTlypXDnDlzAABVq1bF4cOHMW/ePERHRwMA5s6dixEjRmDIkCEAgKVLl2LLli34+eef8cknn7yiK0EIIaSooLR/AgDo1w9ITAR8fJzdEkIIISRXTg/+r1+/jlKlSsHV1RWNGzfGrFmzUKZMGZw+fRp6vR6RkZHcvlWqVEGZMmUQGxuLRo0aITY2FhEREQgICOD2iY6OxujRo3Hp0iXUrl0bsbGxgmOw+4wfPx4AoNPpcPr0aUyePJl7XC6XIzIyErGxsVbbnZWVhaysLO7n5ORkAIBer4der3+pa1KQ2LYV5jYSQvcpKaxMJhP3N92nBF99lfPvQnof0H1KigK6T0lhV5jvUUfa5NTgv2HDhoiJiUHlypURFxeH6dOno3nz5rh48SLi4+OhVqvh7e0teE5AQADi4+MBAPHx8YLAn32cfczWPsnJycjIyEBiYiKMRqPkPlevXrXa9lmzZmH69OkW23fu3Ak3Nzf7LoAT7dq1y9lNICRXdJ+Swua/BzIACty7dw+7dt0BQPcpKRroPiVFAd2npLArjPdoenq63fs6Nfjv0KED9+8aNWqgYcOGCA0Nxdq1a6HRaJzYstxNnjwZEydO5H5OTk5GSEgIoqKioNVqndgy2/R6PXbt2oV27dpBpVI5uzmESKL7lBRWd/bfwtb7NxASEoJ27SrRfUoKPfp9SooCuk9JYVeY71E2A90eTk/75/P29kalSpVw48YNtGvXDjqdDklJSYLR/4SEBK5GQGBgoEVVfnY1AP4+4hUCEhISoNVqodFooFAooFAoJPeRqkXAcnFxgYuLi8V2lUpV6G4IKUWlnaR4o/uUFDZKpQKAeXoYe2/SfUqKArpPSVFA9ykp7ArjPepIe5xe7Z8vNTUVN2/eRFBQEOrWrQuVSoU9e/Zwj1+7dg337t1D48aNAQCNGzfGhQsXBFX5d+3aBa1Wi2rVqnH78I/B7sMeQ61Wo27duoJ9TCYT9uzZw+1DCCGE8FHBPwIAqFIF0GrNfxNCCCGFnFOD/0mTJuHAgQO4c+cOjh49iu7du0OhUKBfv37w8vLCsGHDMHHiROzbtw+nT5/GkCFD0LhxYzRq1AgAEBUVhWrVqmHgwIE4f/48duzYgSlTpmDMmDHcqPyoUaNw69YtfPTRR7h69Sp++OEHrF27FhMmTODaMXHiRCxfvhwrV67ElStXMHr0aKSlpXHV/wkhhBA+BhT9EwCpqUBKivlvQgghpJBzatr/gwcP0K9fPzx79gx+fn5o1qwZjh07Bj8/PwDAvHnzIJfL0bNnT2RlZSE6Oho//PAD93yFQoHNmzdj9OjRaNy4Mdzd3TFo0CDMmDGD26dcuXLYsmULJkyYgAULFqB06dL48ccfuWX+AKBPnz548uQJpk6divj4eNSqVQvbt2+3KAJICCGkeJPJnN0CQgghhJC8cWrw//vvv9t83NXVFYsXL8bixYut7hMaGoqtW7faPE6rVq1w9uxZm/uMHTsWY8eOtbkPIYQQAlDaPyGEEEKKnkI1558QQggpzGSgoX9CCCGEFE0U/BNCCCEOooF/QgghhBQ1FPwTQgghdmLn/FPaPyGEEEKKGgr+CSGEEDtR0j8hhBBCiioK/gkhhBAH0VJ/hBBCCClqKPgnhBBC7MQt9UexPyGEEEKKGKcu9UcIIYQUJVTtnwgsXQpkZAAajbNbQgghhOSKgn9CCCHEQTTwTwAAnTs7uwWEEEKI3SjtnxBCCLGTjAb+CSGEEFJEUfBPCCGEOIihtf4IIYQQUsRQ2j8hhBDiIAr9CQDg9GlApwPUaqBuXWe3hhBCCLGJgn9CCCHETjLK+yd8XbsCDx8CwcHAgwfObg0hhBBiE6X9E0IIIQ6irH9CCCGEFDUU/BNCCCF2Ysf9KfYnhBBCSFFDwT8hhBBiJ8r6J4QQQkhRRcE/IYQQ4iCq9k8IIYSQooaCf0IIIcRONPBPCCGEkKKKgn9CCCHEQTTuTwghhJCihoJ/QgghxE7cUn8U/RNCCCGkiKHgnxBCCLETFfwjhBBCSFFFwT8hhBDiIIaG/gkhhBBSxCid3QBCCCGkqGAH/qnYPwEAXLlivhkoJYQQQkgRQME/IYQQYi8K8gifp6ezW0AIIYTYjdL+CSGEEAfRyD8hhBBCihoK/gkhhBA70bg/IYQQQooqSvsnhBBCHEQF/wgAYO5cIDkZ0GqBiROd3RpCCCHEJgr+CSGEEDuxU/4p7Z8AMAf/Dx8CwcEU/BNCCCn0KO2fEEIIsZOMEv8JIYQQUkRR8E8IIYQ4iAb+CSGEEFLUUPBPCCGE2InS/gkhhBBSVFHwTwghhNiJkv4JIYQQUlRR8E8IIYQ4jIb+CSGEEFK0UPBPCCGE2ElGQ/+EEEIIKaIo+CeEEEIcRHP+CSGEEFLUUPBPCCGE2Ild6o9if0IIIYQUNUpnN4AQQggpMijtn/DVqQOEhAB+fs5uCSGEEJIrCv4JIYQQBzGU908AYNMmZ7eAEEIIsRul/RNCCCF2Ygf+KfQnhBBCSFFDwT8hhBBCCCGEEPKao+CfEEIIsZMse60/yvonhBBCSFFDc/4JIYQQO1G9PyLwxhvAkyfmgn80/58QQkghR8E/IYQQ4iAa+CcAgDNngIcPgeBgZ7eEEEIIyRWl/RNCCCF2ys76p2r/hBBCCClyKPgnhBBC7CSjvH9CCCGEFFEU/BNCCCGEEEIIIa85Cv4JIYQQO8mo5B8hhBBCiigK/gkhhBAH0ZR/QgghhBQ1FPwTQgghduIK/lG9f0IIIYQUMRT8E0IIIYQQQgghr7lCE/x//fXXkMlkGD9+PLctMzMTY8aMQYkSJeDh4YGePXsiISFB8Lx79+6hU6dOcHNzg7+/Pz788EMYDAbBPvv370edOnXg4uKCChUqICYmxuL8ixcvRtmyZeHq6oqGDRvixIkTBfEyCSGEvAYo7Z8QQgghRU2hCP5PnjyJ//u//0ONGjUE2ydMmIB//vkH69atw4EDB/Do0SP06NGDe9xoNKJTp07Q6XQ4evQoVq5ciZiYGEydOpXb5/bt2+jUqRNat26Nc+fOYfz48Rg+fDh27NjB7fPHH39g4sSJ+Pzzz3HmzBnUrFkT0dHRePz4ccG/eEIIIUWGLDvvn4J/AgCYOBH4/HPz34QQQkgh5/TgPzU1FQMGDMDy5cvh4+PDbX/x4gV++uknzJ07F23atEHdunWxYsUKHD16FMeOHQMA7Ny5E5cvX8avv/6KWrVqoUOHDpg5cyYWL14MnU4HAFi6dCnKlSuHOXPmoGrVqhg7dizefPNNzJs3jzvX3LlzMWLECAwZMgTVqlXD0qVL4ebmhp9//vnVXgxCCCGFGtX6JwITJwLTplHwTwghpEhQOrsBY8aMQadOnRAZGYkvvviC23769Gno9XpERkZy26pUqYIyZcogNjYWjRo1Qmzs/7d33+FRlWkfx7/TMumdJJQAkd6RaqyIQETs2JB1sbIquCJWXMW2Lq6+dlB0Leiqa9u1IQIRFEVCL9JB6QkJoaQnkynn/WPIkJgASURmBn6f64Ik5zwzc8/MnXKf5z7PyaJbt24kJyf7xmRkZHDbbbexdu1aTj31VLKysmrcR9WYqtMLKisrWbZsGRMmTPDtN5vNDBo0iKysrMPG7XA4cDgcvq+LiooAcDqdOJ3Oxr0Yx0FVbIEco4jyVAKV2+0GwGN4lKcSFJSnEgyUpxLoAjlHGxKTX4v/Dz/8kOXLl7NkyZJa+3JzcwkJCSE2NrbG9uTkZHJzc31jqhf+Vfur9h1pTFFREeXl5Rw4cAC3213nmA0bNhw29kmTJvHYY4/V2j579mzCw8MPe7tAkZmZ6e8QRI5KeSqBZuVeE2Bh3759vvxUnkowUJ5KMFCeSqALxBwtKyur91i/Ff87d+7kzjvvJDMzk9DQUH+F0WgTJkxgfLU2v6KiIlJTUxkyZAjR0dF+jOzInE4nmZmZDB48GJvN5u9wROqkPJWAtTqXaZt/Jj4+gcGDeypPT3bFxd4FIEwmiIrydzR10s9TCQbKUwl0gZyjVR3o9eG34n/ZsmXs2bOHXr16+ba53W5++OEHJk+ezKxZs6isrKSgoKDG7H9eXh4pKSkApKSk1FqVv+pqANXH/PYKAXl5eURHRxMWFobFYsFisdQ5puo+6mK327Hb7bW222y2gEuIugRLnHJyU55KoLFaD/7aNJl8uak8PYl17w7Z2dC8Oeza5e9ojkh5KsFAeSqBLhBztCHx+G3Bv/POO4/Vq1ezcuVK378+ffowcuRI3+c2m405c+b4brNx40Z27NhBeno6AOnp6axevbrGqvyZmZlER0fTuXNn35jq91E1puo+QkJC6N27d40xHo+HOXPm+MaIiIiAd4JXREREJBj5beY/KiqKrl271tgWERFBQkKCb/tNN93E+PHjiY+PJzo6mjvuuIP09HROO+00AIYMGULnzp257rrrePrpp8nNzeWhhx5izJgxvln5W2+9lcmTJ3Pfffdx4403MnfuXD7++GO+/vpr3+OOHz+eUaNG0adPH/r168cLL7xAaWkpN9xww3F6NUREJKjoUn8iIiISZPy+2v+RPP/885jNZoYPH47D4SAjI4NXXnnFt99isTB9+nRuu+020tPTiYiIYNSoUTz++OO+MWlpaXz99dfcddddvPjii7Ro0YI33niDjIwM35irr76a/Px8Jk6cSG5uLj179mTmzJm1FgEUEZGTW9XEv6HqX0RERIJMQBX/33//fY2vQ0NDmTJlClOmTDnsbVq1asWMGTOOeL8DBgxgxYoVRxwzduxYxo4dW+9YRUTk5KO2fxEREQlWfjvnX0REJFgZmvgXERGRIKPiX0REpN409S8iIiLBScW/iIhIA2niX0RERIKNin8REZF6qjrn31Dfv4iIiAQZFf8iIiL1pKZ/ERERCVYBtdq/iIhIMNC8vwDwxRdQWQkhIf6ORERE5KhU/IuIiNST6WDfv7r+BYDevf0dgYiISL2p7V9ERKSe1PYvIiIiwUrFv4iISANp4l9ERESCjdr+RURE6smkqX+pbvp0KC+HsDC48EJ/RyMiInJEKv5FREQaSif9C8Ctt0J2NjRvDrt2+TsaERGRI1Lbv4iISD1Vzfyr9BcREZFg06jif+fOneyqdoR78eLFjBs3jtdff/2YBSYiIhJoTFryT0RERIJUo4r/a6+9lu+++w6A3NxcBg8ezOLFi/nb3/7G448/fkwDFBERCTTq+hcREZFg06jif82aNfTr1w+Ajz/+mK5du7JgwQLef/99pk2bdizjExERCRy+tn9V/yIiIhJcGlX8O51O7HY7AN9++y0XX3wxAB07dmT37t3HLjoREZEAoqZ/ERERCVaNKv67dOnC1KlT+fHHH8nMzOT8888HICcnh4SEhGMaoIiISKBR27+IiIgEm0YV///85z957bXXGDBgACNGjKBHjx4AfPnll77TAURERE40JpPm/kVERCQ4WRtzowEDBrB3716KioqIi4vzbR89ejTh4eHHLDgREZFApJl/ERERCTaNmvkvLy/H4XD4Cv/t27fzwgsvsHHjRpKSko5pgCIiIoGiat5ftb8AEBkJUVHejyIiIgGuUcX/JZdcwrvvvgtAQUEB/fv359lnn+XSSy/l1VdfPaYBioiIBAp1/UsNGzZAUZH3o4iISIBrVPG/fPlyzjrrLAA+/fRTkpOT2b59O++++y4vvfTSMQ1QREQk0Bjq+xcREZEg06jiv6ysjKioKABmz57N5Zdfjtls5rTTTmP79u3HNEAREZFAYdLF/kRERCRINar4b9u2LZ9//jk7d+5k1qxZDBkyBIA9e/YQHR19TAMUEREJFGr7FxERkWDVqOJ/4sSJ3HPPPbRu3Zp+/fqRnp4OeLsATj311GMaoIiISKBR178AcO+9cPPN3o8iIiIBrlGX+rviiis488wz2b17Nz169PBtP++887jsssuOWXAiIiKBRBP/UsN//gPZ2dC8OTzzjL+jEREROaJGFf8AKSkppKSksGvXLgBatGhBv379jllgIiIigcrQxf5EREQkyDSq7d/j8fD4448TExNDq1ataNWqFbGxsTzxxBN4PJ5jHaOIiEhgODj1r7Z/ERERCTaNmvn/29/+xptvvslTTz3FGWecAcD8+fN59NFHqaio4MknnzymQYqIiAQCrfYvIiIiwapRxf8777zDG2+8wcUXX+zb1r17d5o3b87tt9+u4l9ERE5omvgXERGRYNOotv/9+/fTsWPHWts7duzI/v37f3dQIiIigcjka/tX+S8iIiLBpVHFf48ePZg8eXKt7ZMnT6Z79+6/OygREREREREROXYa1fb/9NNPM2zYML799lvS09MByMrKYufOncyYMeOYBigiIhIoqs7417y/iIiIBJtGzfyfc845bNq0icsuu4yCggIKCgq4/PLLWbt2Lf/+97+PdYwiIiIBwWTSgn8iIiISnBo18w/QrFmzWgv7rVq1ijfffJPXX3/9dwcmIiISsDT1LwDDhsH+/RAf7+9IREREjqrRxb+IiMjJxrfgn3/DkEDx2mv+jkBERKTeGtX2LyIicjJS07+IiIgEKxX/IiIiDaRL/YmIiEiwaVDb/+WXX37E/QUFBb8nFhERkYCm9f5EREQkWDWo+I+JiTnq/j//+c+/KyAREZFAp3l/AaBPH8jNhZQUWLrU39GIiIgcUYOK/7fffvuPikNERCQIeKf+1fUvgLfwz872dxQiIiL1onP+RURE6klt/yIiIhKsVPyLiIg0kKHGfxEREQkyKv5FRETqqWriX23/IiIiEmxU/IuIiNSTSX3/IiIiEqRU/IuIiDSQZv5FREQk2Kj4FxERqSfN+4uIiEiwUvEvIiIiIiIicoJT8S8iIlJPVaf8G+r7FxERkSDj1+L/1VdfpXv37kRHRxMdHU16ejrffPONb39FRQVjxowhISGByMhIhg8fTl5eXo372LFjB8OGDSM8PJykpCTuvfdeXC5XjTHff/89vXr1wm6307ZtW6ZNm1YrlilTptC6dWtCQ0Pp378/ixcv/kOes4iIBC+TGv+luqefhn/9y/tRREQkwPm1+G/RogVPPfUUy5YtY+nSpQwcOJBLLrmEtWvXAnDXXXfx1Vdf8cknnzBv3jxycnK4/PLLfbd3u90MGzaMyspKFixYwDvvvMO0adOYOHGib8zWrVsZNmwY5557LitXrmTcuHHcfPPNzJo1yzfmo48+Yvz48TzyyCMsX76cHj16kJGRwZ49e47fiyEiIkFD8/4CwLXXws03ez+KiIgEOL8W/xdddBEXXHAB7dq1o3379jz55JNERkaycOFCCgsLefPNN3nuuecYOHAgvXv35u2332bBggUsXLgQgNmzZ7Nu3Tree+89evbsydChQ3niiSeYMmUKlZWVAEydOpW0tDSeffZZOnXqxNixY7niiit4/vnnfXE899xz3HLLLdxwww107tyZqVOnEh4ezltvveWX10VERALTobZ//8YhIiIi0lBWfwdQxe1288knn1BaWkp6ejrLli3D6XQyaNAg35iOHTvSsmVLsrKyOO2008jKyqJbt24kJyf7xmRkZHDbbbexdu1aTj31VLKysmrcR9WYcePGAVBZWcmyZcuYMGGCb7/ZbGbQoEFkZWUdNl6Hw4HD4fB9XVRUBIDT6cTpdP6u1+KPVBVbIMcoojyVQFV1WpmBoTyVoKA8lWCgPJVAF8g52pCY/F78r169mvT0dCoqKoiMjOSzzz6jc+fOrFy5kpCQEGJjY2uMT05OJjc3F4Dc3NwahX/V/qp9RxpTVFREeXk5Bw4cwO121zlmw4YNh4170qRJPPbYY7W2z549m/Dw8Po9eT/KzMz0dwgiR6U8lUCzqxTASkV5hS8/lacnr8jsbExuN4bFQknz5v4O54iUpxIMlKcS6AIxR8vKyuo91u/Ff4cOHVi5ciWFhYV8+umnjBo1innz5vk7rKOaMGEC48eP931dVFREamoqQ4YMITo62o+RHZnT6SQzM5PBgwdjs9n8HY5InZSnEqjW7S7imZ8XEhoayuDBpytPT3LWtDRM2dkYzZvj2rrV3+HUST9PJRgoTyXQBXKOVnWg14ffi/+QkBDatm0LQO/evVmyZAkvvvgiV199NZWVlRQUFNSY/c/LyyMlJQWAlJSUWqvyV10NoPqY314hIC8vj+joaMLCwrBYLFgsljrHVN1HXex2O3a7vdZ2m80WcAlRl2CJU05uylMJNFar99emAb7cVJ6KCQI+B5SnEgyUpxLoAjFHGxKPXxf8q4vH48HhcNC7d29sNhtz5szx7du4cSM7duwgPT0dgPT0dFavXl1jVf7MzEyio6Pp3Lmzb0z1+6gaU3UfISEh9O7du8YYj8fDnDlzfGNERETg0KX+tN6fiIiIBBu/zvxPmDCBoUOH0rJlS4qLi/nggw/4/vvvmTVrFjExMdx0002MHz+e+Ph4oqOjueOOO0hPT+e0004DYMiQIXTu3JnrrruOp59+mtzcXB566CHGjBnjm5W/9dZbmTx5Mvfddx833ngjc+fO5eOPP+brr7/2xTF+/HhGjRpFnz596NevHy+88AKlpaXccMMNfnldREQkMFWt9i8iIiISbPxa/O/Zs4c///nP7N69m5iYGLp3786sWbMYPHgwAM8//zxms5nhw4fjcDjIyMjglVde8d3eYrEwffp0brvtNtLT04mIiGDUqFE8/vjjvjFpaWl8/fXX3HXXXbz44ou0aNGCN954g4yMDN+Yq6++mvz8fCZOnEhubi49e/Zk5syZtRYBFBERAV3qT0RERIKPX4v/N99884j7Q0NDmTJlClOmTDnsmFatWjFjxowj3s+AAQNYsWLFEceMHTuWsWPHHnGMiIic3A7N/Kv6FxERkeAScOf8i4iIBKqqc/5FREREgo2KfxERkQZS27+IiIgEGxX/IiIi9aQF/0RERCRYqfgXERFpIE38i4iISLDx64J/IiIiwaRq4t9Q378ALFkCbjdYLP6ORERE5KhU/IuIiNST2v6lhqZN/R2BiIhIvantX0REpIE07y8iIiLBRsW/iIhIvXmn/tX1LyIiIsFGbf8iIiL1pLZ/qeH116GkBCIjYfRof0cjIiJyRCr+RUREGkgL/gkAjz8O2dnQvLmKfxERCXhq+xcREaknTfyLiIhIsFLxLyIi0kCa9xcREZFgo+JfRESknkxVJ/2r+hcREZEgo+JfRESkntT2LyIiIsFKxb+IiEgDaeJfREREgo2KfxERkXrydf1rtX8REREJMir+RURE6smkxn8REREJUir+RUREGkjz/iIiIhJsrP4OQEREJFiYNPEv1bVvDzExkJzs70hERESOSsW/iIhIA+mUfwFg7lx/RyAiIlJvavsXERFpIEON/yIiIhJkVPyLiIjUk9r+RUREJFip+BcREWkgtf2LiIhIsNE5/yIiIvVk0tS/VDdyJOzdC4mJ8P77/o5GRETkiFT8i4iINJAm/gWAefMgOxuaN/d3JCIiIkeltn8REZF68s37q/oXERGRIKPiX0REpJ7U9S8iIiLBSsW/iIhIA+lSfyIiIhJsVPyLiIjUk+lg479W+xcREZFgo+JfRESkntT2LyIiIsFKxb+IiEgDaeJfREREgo2KfxERkXrSxL+IiIgEKxX/IiIiDWTopH8REREJMlZ/ByAiIhI0Dk79q/QXAG65BQoLISbG35GIiIgclYp/ERGRejKp8V+qe+QRf0cgIiJSb2r7FxERaSB1/YuIiEiwUfEvIiJST7rUn4iIiAQrFf8iIiL1pNpfREREgpWKfxERkUbQiv9CixbedpAWLfwdiYiIyFGp+BcREaknk/r+RUREJEip+BcREWkETfyLiIhIMFHxLyIiUk/V5/1V+4uIiEgwUfEvIiJST+r6FxERkWCl4l9ERKQRtOCfiIiIBBMV/yIiIvVkqtb4r9JfREREgomKfxERkfpS27+IiIgEKRX/IiIijaCufxEREQkmKv5FRETqSQv+iYiISLCy+jsAERGRYKSJf+G998DhALvd35GIiIgclYp/ERGReqox8a++fxkwwN8RiIiI1Jtf2/4nTZpE3759iYqKIikpiUsvvZSNGzfWGFNRUcGYMWNISEggMjKS4cOHk5eXV2PMjh07GDZsGOHh4SQlJXHvvfficrlqjPn+++/p1asXdrudtm3bMm3atFrxTJkyhdatWxMaGkr//v1ZvHjxMX/OIiISvEzq+xcREZEg5dfif968eYwZM4aFCxeSmZmJ0+lkyJAhlJaW+sbcddddfPXVV3zyySfMmzePnJwcLr/8ct9+t9vNsGHDqKysZMGCBbzzzjtMmzaNiRMn+sZs3bqVYcOGce6557Jy5UrGjRvHzTffzKxZs3xjPvroI8aPH88jjzzC8uXL6dGjBxkZGezZs+f4vBgiIhJUNO8vIiIiwcSvbf8zZ86s8fW0adNISkpi2bJlnH322RQWFvLmm2/ywQcfMHDgQADefvttOnXqxMKFCznttNOYPXs269at49tvvyU5OZmePXvyxBNPcP/99/Poo48SEhLC1KlTSUtL49lnnwWgU6dOzJ8/n+eff56MjAwAnnvuOW655RZuuOEGAKZOncrXX3/NW2+9xQMPPFArdofDgcPh8H1dVFQEgNPpxOl0HvsX6xipii2QYxRRnkqgcjkPdZVVKk9PeqZ583zn/BvnnOPvcOqkn6cSDJSnEugCOUcbElNAnfNfWFgIQHx8PADLli3D6XQyaNAg35iOHTvSsmVLsrKyOO2008jKyqJbt24kJyf7xmRkZHDbbbexdu1aTj31VLKysmrcR9WYcePGAVBZWcmyZcuYMGGCb7/ZbGbQoEFkZWXVGeukSZN47LHHam2fPXs24eHhjXsBjqPMzEx/hyByVMpTCTQON1T96pw7Zy4hFuXpyWzITTcRtm8f5QkJzH7zTX+Hc0TKUwkGylMJdIGYo2VlZfUeGzDFv8fjYdy4cZxxxhl07doVgNzcXEJCQoiNja0xNjk5mdzcXN+Y6oV/1f6qfUcaU1RURHl5OQcOHMDtdtc5ZsOGDXXGO2HCBMaPH+/7uqioiNTUVIYMGUJ0dHQDn/3x43Q6yczMZPDgwdhsNn+HI1In5akEqrJKF/ctngvAuQMH8tO8ucrTk5g1NBSA0NBQLrjgAj9HUzf9PJVgoDyVQBfIOVrVgV4fAVP8jxkzhjVr1jB//nx/h1Ivdrsdex2X9rHZbAGXEHUJljjl5KY8lUATYhxaKsdmsx78qDw92Zkg4HNAeSrBQHkqgS4Qc7Qh8fh1wb8qY8eOZfr06Xz33Xe0aNHCtz0lJYXKykoKCgpqjM/LyyMlJcU35rer/1d9fbQx0dHRhIWFkZiYiMViqXNM1X2IiIhUpyv9iYiISDDxa/FvGAZjx47ls88+Y+7cuaSlpdXY37t3b2w2G3PmzPFt27hxIzt27CA9PR2A9PR0Vq9eXWNV/szMTKKjo+ncubNvTPX7qBpTdR8hISH07t27xhiPx8OcOXN8Y0RERKpf6U+1v4iIiAQTv7b9jxkzhg8++IAvvviCqKgo3zn6MTExhIWFERMTw0033cT48eOJj48nOjqaO+64g/T0dE477TQAhgwZQufOnbnuuut4+umnyc3N5aGHHmLMmDG+tvxbb72VyZMnc99993HjjTcyd+5cPv74Y77++mtfLOPHj2fUqFH06dOHfv368cILL1BaWupb/V9EREREREQkWPm1+H/11VcBGDBgQI3tb7/9Ntdffz0Azz//PGazmeHDh+NwOMjIyOCVV17xjbVYLEyfPp3bbruN9PR0IiIiGDVqFI8//rhvTFpaGl9//TV33XUXL774Ii1atOCNN97wXeYP4OqrryY/P5+JEyeSm5tLz549mTlzZq1FAEVEREBt/yIiIhJc/Fr8G/X4yyk0NJQpU6YwZcqUw45p1aoVM2bMOOL9DBgwgBUrVhxxzNixYxk7duxRYxIRkZNT9bZ/Nf6LiIhIMAmIBf9ERESCgQnT0QeJiIiIBCAV/yIiIo2gtn8REREJJn5t+xcREQkmJk38S3W7dvk7AhERkXrTzL+IiEgjaOJfREREgomKfxERkXqqPvGvtn8REREJJir+RURE6smkvn8REREJUjrnX0REpBEMNf7LY49BYSHExMAjj/g7GhERkSNS8S8iIlJPavuXGv71L8jOhubNVfyLiEjAU9u/iIhIPanrX0RERIKVin8REZFG0MS/iIiIBBMV/yIiIvWkBf9EREQkWKn4FxERaQyd9C8iIiJBRMW/iIhII6j0FxERkWCi4l9ERKQB1PkvIiIiwUjFv4iISCOo619ERESCiYp/ERGRBtDEv4iIiAQjq78DEBERCUaa+BfOOQf27oXERH9HIiIiclQq/kVERBrAZDKBYWCo71/ef9/fEYiIiNSb2v5FREQaQG3/IiIiEoxU/IuIiDSC5v1FREQkmKj4FxERaYCqS/2p619ERESCiYp/ERGRBjCp8V+qDBwIXbp4P4qIiAQ4LfgnIiIi0hibNkF2NhQW+jsSERGRo9LMv4iISENo4l9ERESCkIp/ERGRRtCl/kRERCSYqPgXERFpgKqJf5X+IiIiEkxU/IuIiDSASW3/IiIiEoRU/IuIiDSCuv5FREQkmKj4FxERaYCqS/0ZavwXERGRIKLiX0REpAHU9i8iIiLBSMW/iIhII6jtX0RERIKJ1d8BiIiIBBNN/IvPxIlQUgKRkf6ORERE5KhU/IuIiDSCJv6F0aP9HYGIiEi9qe1fRESkAUxVJ/2r+hcREZEgouJfRESkAdT2LyIiIsFIbf8iIiKNoEv9Cbt3g9sNFgs0bervaERERI5IM/8iIiINUdX1r9pf+vaF1FTvRxERkQCn4l9ERKQB1PYvIiIiwUjFv4iISCNo5l9ERESCiYp/ERGRBvCt9i8iIiISRFT8i4iINIIm/kVERCSYqPgXERFpAJNvwT+V/yIiIhI8VPyLiIg0gJr+RUREJBip+BcREWkEzfuLiIhIMFHxLyIi0gC+Bf9U/cvJ6uOPYf9+f0chIiINpOJfRESkAdT2Lye1tWvh0Udh5EgoKPB3NCIi0gAq/kVERBrB0NS/zJkDa9Z4P54sOnaEBx+E0lL405/gwAF/RyQiIvVk9XcAIiIiwcSkqX+p0qGDvyM4vgwDLBYYMQLMZpgyBa67Dv79b4iL83d0IiJyFJr5FxERaQRd6U9OOiYTeDzeAwBXXw233eY99/+669QBICISBPxa/P/www9cdNFFNGvWDJPJxOeff15jv2EYTJw4kaZNmxIWFsagQYPYvHlzjTH79+9n5MiRREdHExsby0033URJSUmNMT///DNnnXUWoaGhpKam8vTTT9eK5ZNPPqFjx46EhobSrVs3ZsyYccyfr4iInAi8U/+q/eWkUnW0y2SCigrvAYBrr4Vx42DvXh0AEBEJAn4t/ktLS+nRowdTpkypc//TTz/NSy+9xNSpU1m0aBERERFkZGRQUVHhGzNy5EjWrl1LZmYm06dP54cffmD06NG+/UVFRQwZMoRWrVqxbNkynnnmGR599FFef/1135gFCxYwYsQIbrrpJlasWMGll17KpZdeypo1a/64Jy8iIkFJbf/i88EH8MYb3o8nMsPwJv6sWXDDDTBgADz0EKxYAVddBXfd5S38r7tOiwCKiAQwvxb/Q4cO5e9//zuXXXZZrX2GYfDCCy/w0EMPcckll9C9e3feffddcnJyfB0C69evZ+bMmbzxxhv079+fM888k5dffpkPP/yQnJwcAN5//30qKyt566236NKlC9dccw1//etfee6553yP9eKLL3L++edz77330qlTJ5544gl69erF5MmTj8vrICIiwUdt/8J998Ett3g/nshMJvjiCxg+HBIS4Oab4aOP4PbbYcsWuOIKGDMGiorg4ouhsNDfEYuISB0CdsG/rVu3kpuby6BBg3zbYmJi6N+/P1lZWVxzzTVkZWURGxtLnz59fGMGDRqE2Wxm0aJFXHbZZWRlZXH22WcTEhLiG5ORkcE///lPDhw4QFxcHFlZWYwfP77G42dkZNQ6DaE6h8OBw+HwfV1UVASA0+nE6XT+3qf/h6mKLZBjFFGeSiCrmvh3upSnJzsr3nwwAFeA5kGjfp56PN4F/aqOcOXnY/nHPzCeeALP2LHgdmOdMAHPhRfiadHCO374cEzl5Zg/+gj3vn0QHv4HPBs5Uen3vgS6QM7RhsQUsMV/bm4uAMnJyTW2Jycn+/bl5uaSlJRUY7/VaiU+Pr7GmLS0tFr3UbUvLi6O3NzcIz5OXSZNmsRjjz1Wa/vs2bMJD4JfeJmZmf4OQeSolKcSiBwVFsDEwoULaRGhPD2ZDamoIAyoqKhgdoCvFVTfPG357bd4bDayTz8dw2YDwFpSwun797MwPh7rW29x5oMPkt27N6sGDIBvviFx9WoOtGuHOz4e6w034Fq9Glav/gOfjZyo9PNUAl0g5mhZWVm9xwZs8R/oJkyYUKNboKioiNTUVIYMGUJ0dLQfIzsyp9NJZmYmgwcPxnbwl7pIoFGeSiD7x5p5FDod9O9/GtlrFipPT2LW0FAAQkNDueCCC/wcTd0a9PPU48Hy1FOYSkro0b8/xtChEBICu3ZhdToZ7HZj+b//w7jsMppPnkxziwV++QXLtGl4+vbFGDjw+DwpOeHo974EukDO0aoO9PoI2OI/JSUFgLy8PJo2berbnpeXR8+ePX1j9uzZU+N2LpeL/fv3+26fkpJCXl5ejTFVXx9tTNX+utjtdux2e63tNpst4BKiLsESp5zclKcSiEwHV/yzWr2/QpWnYoKAz4Gj5mnVon7ffw9XXIH1mWe8X198MaSlwbXXYh09GjIyML3xxqFFo957D7Ztw9ylCwT4ayCBTz9PJdAFYo42JB6/Lvh3JGlpaaSkpDBnzhzftqKiIhYtWkR6ejoA6enpFBQUsGzZMt+YuXPn4vF46N+/v2/MDz/8UONciMzMTDp06EBcXJxvTPXHqRpT9TgiIiIiJzSTCSorwW6Ht9/2fpw8GaZPB7fbu8jflVfC8uXeKxy88QbccQe8/DJMmwYtWvj7GYiIyFH4dea/pKSEX375xff11q1bWblyJfHx8bRs2ZJx48bx97//nXbt2pGWlsbDDz9Ms2bNuPTSSwHo1KkT559/PrfccgtTp07F6XQyduxYrrnmGpo1awbAtddey2OPPcZNN93E/fffz5o1a3jxxRd5/vnnfY975513cs455/Dss88ybNgwPvzwQ5YuXVrjcoAiIiJw6FJ/Wu1fTiiG4W3x//BD78r+FgssWQL33gtWK1x6KTzyCKSmwsSJ0KyZt+D/6Sfo1s3f0YuISD34tfhfunQp5557ru/rqnPoR40axbRp07jvvvsoLS1l9OjRFBQUcOaZZzJz5kxCD55jB95L+Y0dO5bzzjsPs9nM8OHDeemll3z7Y2JimD17NmPGjKF3794kJiYyceJERo8e7Rtz+umn88EHH/DQQw/x4IMP0q5dOz7//HO6du16HF4FEREJJqajDxEJPiYTLFwIN90EU6ZA//7eFftHjIAHHvDuv/BCePpp7wGBhARwOCAszN+Ri4hIPfm1+B8wYADGEaZOTCYTjz/+OI8//vhhx8THx/PBBx8c8XG6d+/Ojz/+eMQxV155JVdeeeWRAxYRETnIQFP/coJZt857fv/w4RAV5d02bx6cdRaMGwcuF1xwATRp4t1XbTJGREQCX8Ce8y8iIhKIqhb8U9u/kJICzZt7PwazqmSurISKikNFfVmZdxG/t96CvDx49FGYNevQ7UzqgxERCSYq/kVEREQaY+lS2LXL+zGYVRXxw4Z5i/wHHvB+HR7u/VhWBmefDa1awcErLomISPAJ2Ev9iYiIBDJN/EvQqrqs39q1sHkzxMR4F/Dr0MG7ev/tt4PH413Yz+2GL7/0dje8+qrO8RcRCWIq/kVERBpAnc4S9Ewm+O9/vUV+fDyUloLZDC++CNdf713p/69/hf/9z3sFgP37ITNThb802mcrdpEaF06f1vH+DkXkpKa2fxERkUY40oK1IgFtxQrvqv6PPw4LFsBnn8Fll8Hll3tn+a+7Dtavh3/+EyZN8p7W0KtXjbtQ/v+xnG4PI99YyJ0frvB3KL/b3A153PXRKq6YmvWH5o3D5Wbn/rJjfr+VLg9rsgv9kvNuj4HbU/fjGobB2pxCKpzuY/64v+aXcMu7S1m/u+iY3/exMH/zXoa++CMjXl/IgdJKf4cTVFT8i4iINEDVzL9KH+Evf4Err/R+DERuNy63h+V7TRSUOb2r9QNs3AidOnln+ePioHdvb4v/2LFw332wdau3zf+aa+CKKyAtjR37ynhu9kZKHC7yix2c+c/vGPP+cpxuj+/hCsudNQqRUoeLvSWOGiF9vGQno99d6vuDPa+oAtfB+yivdFPqcNUYv7+0kl0Hjn1B1xjrdxexfV9pvccbhsFPv+ylsNxZY/uOfWUUljn53/JdXPfmojqf3+rsQn76ZR9frMxhd2E5e4oqAPhoyQ7+s3gHAC63h6Xb9rNs+wEqXZ5a93Eki7fuZ9ve+j+XtTmFDHl+HrPW5jJj9W5enrMZz8GitKjCyXcb99R676r8d1m27/Od+8t9n+cWVnDu/33PQ5+vblDsh/Pk1+s56+nv+OmXvQBUON18vHSn77U7EsMwWLRlX52F9MOfr+HCl+fz9erdvzvG3x5AWJtTyLOzN+Jw1X7cCqebQc/N49IpP/le6+r+tzybYS/N55lZ3tv/9vVfseMAr837FY/HYEt+CUUVzlr3AfDj5nwue+UnNuUV+2Kc8L/VZK7LY9hLh79aWoXTTVll3e/5H23agq2s311E1pZ9TD8G78vJRG3/IiIiDWBCff9y0NdfQ3a2d8X/AFNYWslN7y6laOkKztmynL/ZmvL6n/sCcKCsktjVqzHl5kKrVizfvp/CChetB5xP1Nvv8eo7PzD0+hi6t4gFIMRq5vq3F7Nlbynb9pXRMzWW7IJysgvKSY0P54GhHdm2t5RLpvxE64Rwvhh7Jl//vJv7Pl1FpdvDxIu6cN1prTAMg/v++zMAdttaRvRN5do3FvGXc06ha7MYHvzfamxWM+/e2I+uzWMAuGTKfHbuL2fRg+eRHF370oKrdxXSNDaU3MIKujSL9l2N43Ccbg8Lft1H+ikJhFjNLN9xgLjwENISI+oc73J7KK5wcaCskosnzycmzMb8+wcSarPgdHvwGAZ2q6XO236+Mpu7PlrFkM7JvP7nPmT9uo+sLfuYPHczNosZx8GCfdRbi5l91zlYzIdi35J/qDC/8z8rWbxtPx2So9h4sEA7t0MS7y3czuTvfgEgKcrOW9f3pWvzGDblFfPZimzuGNgWgOdmb+KMtomc2zEJgOU7DnDVa1kAdG8Rw91DOnDaKfFc9dpCbGYTL197Kk1jap7icet7y9i5v5y//HuZb9uzmZu4+cw0ShwuPlyyk8RIO9/ceRZrsgtpmxRJanw4JQ4X367P893mP0t2sK/Ewd1DOjD5u81s3VvK1r2lpETZSWxkHen2GHy9ejfvZm0H4KHP1/DdPQN4+6dt/HPmBqJCrcz461mkxofXuN3stbnMXpfHuR2ScLjcjP94FRd0S+GVkb19YypdHj5auhOAV777lQu7NztsHGWVLkKtFszm2jm4u7CcB/67mk15xXw+5gzAe1Bh9jrvaxMdamNE/5bYLCbKK93MXJNLdJiNrQcP0KzJKfR9P1Z56PM1ALw5fysrdxawKa+YOXefw+pdhXRuFs1lrywAYNu+Mj5asoPT2yTy3s39a8V23ZuLAbjjgxW8e1M/hr30I3tLvAfnPIb3oF5MmA3wHqx4ec4v3JPRnpvfWYrTbTBz3FlEhdrweAxyiyp4f9F2DAMWbd1PXLiNTk2jMQF3nNeO/y3fRe9WcbRNiqoVh2EYfLpsFyt3FjCiX0u27i1l854SLu3ZjOToUMoq3VjNJib8bzXfrt/ju93CLfu47rRWvq/dHgPDMLBaDs1xT/nuFz5YtIPXrvO+tyPfWMR1p7WiZXw4czfsYdzgdqzYUUCfVnG0S64d24lExb+IiEhjaOpfAthTszayaeNOFr17N3ZXJVMcpfDnD1m8dT9PLCjiufgWtHn7bZy3jGbUW6spdrjoZirhBVs46zbm8OZUb3F4SmIE34w7iy0Hi5AvV+Xw5aoc3+P8d/ku7svowCNfrqWw3MmqXYXsKa7gHzPWU1rpnc2c+MUaBnVKqjEzOf3nHBZt2QfAa/O2EGW3UuxwgQMumjyf09IS+NNprXwzxTe8vYRBnZKYtymfezI60CE5it2FFVwy5SfffT56UWcGdU5m5/5yTm0ZS6jNgmEY3PPJz2QXlPH29f14csY63lu4g3szOjCkczKXv7KAmDAbSx8ahM1ixu0xuOeTVVjNJlrGh/P6D1soqXQRE2bD6TbYW1LJPZ+sYum2AxSWOwmxmpl+x5mYzSYmzVjPX85uw7fr85i5JtdXqM9el8e/s7bx8BdrfbE6qs3U/5pfyt0fr2TS5d2ZtTaXORv24Kg2A714234A3/0BrMku5MeDM9wAe4od3DhtCQseGMilU36irNLNq9//6tv/xvytLH94MPERIXyx4tBM/M+7Chn11mImXtiZVTsLABj34Uo++ks64C3ISivdNWbsq3tj/lbf53tLHFw8eT67CytolxTJ7LvOZs76vBrPtSqmNdlFNToe/i9zM0Oamxle56PUlrkuj+837uHejA58vHQn/5ixoUYchmHw3UZvgVhc4eKDxTu4uEczxn6wHKvZzJ2D2nH/f3+muMLFp8t20aaJ9+DPjNW5rNpZQOXBbpQ9RYc6VyLs3oM8eUUVTP95NxldkmkR5z2gsHJnASNeX8jFPZpx1+D2bMkv4fS2iQB4PAa3v7+cFTu8r+//lmezJb/EV/gDzFizmynf/0L75Cii7FbmbDhU3AI8O3sTeUUVPHJRF9LbJADU6BZYtv0AAKPfXcbKnQXYLIcOQFR1isz/ZS/FFU6iQm24PQZTvvuFHdVOk9iYV8zbP23zFf5VPl22i1/zS2geG8ZHS3ayY38ZM9fm+va/PPcX8osdTP85B6e79i/GqkJ9TU4RczfsIcxm4co+LfhqVQ7X9m9JXHgIHVKiWJtTxFPfeN/H9xft8N3+f8t3kRARwsa8Ynq3iuOnX/bVuP9FW/ZjGAYmk4kvVmb7DorM+OtZtIgLw+0xeGbWRgAunfIT8REhFJY7fQfOAN/ziQ618tTw7ny3YQ/Lth/gf7efTmx4SK3nFMxMhk7aOiaKioqIiYmhsLCQ6Ohof4dzWE6nkxkzZnDBBRdgs9n8HY5InZSnEsjOeeY7tu8r4+Nb+rF7zQLl6cmsRYtDM/+7dvk7Gh+Px6DjwzMxVZTzfzNeIKV4H132/Ir1+lGce8rVZBeUc9+8aYzMX03ZBRfyp/K27A2P5dZF/+Wi9T9w+5jJ/Ow5NEv62nW9a8z4/taIfi19BQbAvRkdeGbWRixmE22aRLApr4Q2TSLo1jyGz1fmHPZ+GiLEYqZP6zgW/FqzELCaTbgOtkg3ibJzfpcU/r1we533MebcNkz5zluMvn1DX05NjeXTZbv4+9frGxTL6W0SiA611SiIGsJiNh32vO7DGX32KUxbsI1Kl4evxp7JiH8tpMTh4v2b+zPyjUV13qZlfDgvjTiVMe8vJ7ug7mK+yvWnt2ZTXjG/5peQFBXK6uzCBsUH8MEt/Xl3wXZmrs2lSZSd/GLHEceHWgweubgrV/VtRYXTzYdLdjK0awrNYsN8rfxmk4l/zFh/1Hi+HHsGV7ya5SviAcJDLJRV1u/8eLPJO+v9W2PPbesrGs9u34RHL+rMN2tyfcUlgN3q7ep45orubN5Two59ZY3OjbpMubYXv+wp4flvNzX4tvcMac/q7EJmrc07+uDjrHqO2CwmIuxW7+lKR1D1Ws+5+xzaNInkilcXsPTggZBBnZL5eVcBTWNCWbWr4fkL8PzVPbjs1BZAYP9t2pA6VMX/MaLiX+TYUZ5KIKsq/j+6pR+5Kv5PbgFa/Fc43XR8eCYAf/3pP1ywYT6vpF/JpLmv83m70/lbxlgAHl38HzJ2ryFx0xo2NmlNUsl+brjyUf5026VM+N+h87D7pcWzeOv+Go8x9ty2bN1bWuM86DCbhfJqM9aDOiVxdvsmTKw24w1w2anN+aza7HOVEf1asnz7gRoz3H8Uu9VMSkwo2/cdeT2B2we0Ye6GPWzILWZgxyTmVpuR/evAtrw095cj3PrwXr+uN6//sIW/X9aVvCIH17+9mCP9Rd61eTTv3dSf/y7P5onp63zbI0IsrH40g2v+tbDWe1RdqM1MhbNh6wL8VnK0nbxqM+Et48NrzBz/Vs/UWNbvLsLh8vDeTf3505u1D0oM7JhEmM1SI4/+cs4p7DpQztc/76ZjShRPX9Hde957AyqWs9ol8uPmvbW2pyVG+Frp6xpns5hqzV73ax3v6774PRIj7bXWwBjWrWmdawm0iAtj14EjH6A5Fkwmjph3bZpE8Gt+/daGeOv6PpzVrgm3vLuU7zfmA/D29X1JjQ9nyPPz6vX+WcwmVkwcTKjVgtVsYk1OIcNfXVBnRwFAn1ZxLN1+gFvPacPSbft9hX9dzmibwDntm/DzrkL2ljhYuGW/7z66t4glPMRSoxsAvAckvhhzBs1iwwL6b9OG1KFa8E9ERKQBqpopdexcAor7UNFdfRG+A+Pvo8QeTrOivfzt3Fu4YvUcvt38IU1jQnm03wiuPnsst1z+EFm33M3td07lrKuGcObBduUq1YvKjC7JLH1oEPdkdOCuwe1oleDtELi4RzPGD25f43bDe7Xg4h7N6NQ0mq7No7m4RzNG9GvJwxd2psNvzqu1mk1c0bsFbZLqPve+vsae25bTD7ZF16XfwUvNOVyeoxb+rRPCGTeoPf+97XS+HX8Ob13fl3dv7Efz2DCevKwrdw1uz/BeLeq87UsjTuWDm/vz433n8vQV3X3brWYTU67txZAuKXx62+l0TInmnPZNasT8zZ1n0TYpkpToUL658ywu7dmMh4d1JjY8hK7Nav5hHxsegtlsotvBNRJ+a9Ll3djyjwv48b6Bvtc8KcpOqO1QCbDp70N5+oruTBjasc7XrnlsGOMHt+ez28/wbXv4ws7MvfucOh/zH5d1I9RmZuXOAhwuDy3jwzmjbQIvXtMTs4kasfZPi+fRi7vUuP1r87bw9c/egnhDbjEXT6678LdZTEy+9lTO7dCEjC7JnNIkggu6pQD4Cvpr+qb6xofZLMwZfw5dmx96DV8Z2YvbB7TBajYxol9L/jqwHTFhNlIOri+REh3Kv/7cp8bjtk6ouXYAwJW9686D6sae26bWtsnXnkr3FrXfuzdH9eWFq3tyb0YHYsMPX2ie1S6RxMgQnrq8W52XoW3TJIKYMBsThnassf3601vz9BXd2fz3oXw0+jSWPzzYd+pDdQ9d2LnWtqjQQ2eN/3VgW1rEhTFuUDsGdkzGZjHTpkmkb3+nptG0TYpk6p96k5YYwcU9mtHlYA5XPzWhSpdm0USH2gixmjGbTXRvEctzV/WssSZHj9RYAP52QSd6Hvx86rxfaxT+VWsUVHfzmacw+uw2TL62Fx+OTmfx387jx/vO5dPbTmfiRZ3563ntmHR5NwYeXBsDIL/YwYD/+75ei0YGC53zLyIi0gBHW1BM5LgzDLBYYN06+Phj3FddS3hlOWUhYfxtcBsyu/ekee4epp52Bae1jOHqN/7Ov00wqO017Ihryo64pjw49mxuOVgcHu7A1ovX9OSSnocWN2ybFMV3dw/A6fFgt1rILaxg2oJtlFW6GDuwHUO7NQW8xexvpSVG+Gb4v7tnAHHhNmLDQ5i55tBCc6nxYTXONV/8t/Po9+ScWve16MHziAsPYW+Jg2axYSzbfoDFr2URarPQMj6chy/sTG5ROREhVoZ0SeG295bxzRpvG/b1p7fmjLaJvDhnE9f2a8Ww7k2pdHnYW+IgMdJOiNVMiNVM2yRvQXN2+yb89MBA32M/e1UP1u0u8l0SbfodZ7JjfxlDu6b4flYkR4dy36fehQ4fuagzw7o3rfUcJl7YhT+96V2ErFPTaGaNOxu3xyDEauaFa071jevVKo5h3ZuS9es+CsoqueWsNAA6Nz1U0D57ZQ/SmkSwamcBV/dJxWw20STKzn9vP511OUX0TI3lHzPWM23BNsC7oONVfbxF8q/5Jb77uaZvKvef35G4iEPnPL94TU++WpXDFb1bYLWYee6qHoz/eBV/v7QriZF29pU6GNEvFafbwyNfejs+ql6LS3o25/yuKZQ63PR6IhOAbi1iSIys+5zq3846p5+SQI/UWBZu2cfKnQVMvLAzF3ZvVmMRvl/2FDNj9aEW+2v7t2RDbjErdxbw4AUdMZtNPHZxFx76fC0PDO1IVKiN+87vyD1DOgBgNpu447x2AGzMLSYuwkZMuI1/XNaNGat38+RlXUmODmXQc/N8M/NbJ12AyWTisl7NGfXWYiYM7UR0mI1l2/fzn8U7fbFc3rsFj37l7dro1jyGezI6YDKZmHZDPxZv3UeTKDtvzd/G0G4pdEiJokOK9/tx/ua9ZB1cHyP9lATf5+2SInnr+r7YDi5sFxZiwWMYvLNgOyt3FhBiNfPF2DOpdHkID7Ew6ZtD6yJUP+DS/xTvAZ8vxp5JqcOF2WTink9W8afTWnFuhyTuGNiW9xftYP/BK3TMvutsHv9qHWtyCrnpzFMYf/C1q5Icba/1+ZAuKQzp4j0ws6e4gs+WZ9OndTzDX/UuSnhuhyZ8vynfl4fVXdSjGRd2b8o/ZqynxOHm75d2ZV+pg8QI+8Hv4601xlvMJr7+65msyS7ikS/X+LpVzm7fpMa4pKiaC4iGWM2M6NeSa/qm8uPmvfz5rcW+2JKiQ3E6j3wKQrBQ8S8iItIImveXgGEywYEDMGAA7N2Lfc06nv95J6+lX4nFPoSQSwYw4u676Tv2z7S//iFM/VvRZuxYpmzNZ8zgO7iwe1PaV5uJN5lMzL7rbHYdKOO/y7P5+ufdWM0mhnatXbSazSbsZu9CaCkxoTUK4yM5q32i7zzo6rN6tw9oy7rdRVzSszmX9GzG/tJKZqzOJdRmJikqlA9Hn8Z7C7dz6zltGPXWYlrEhfmuAtAs1nvgoHerOJY+NIhIu7XGit9VJl7U+eAlBMu55exTaB4bxuDOyTXGNImy17rd4Tx1eTeufC2Lq/q0oGvzGN+VCqqEWM08MLSj95Jqh+kU6JASxZK/DfJ9bTGbaqz+X8VmMTPl2l6Ad8G3qisNnNOhCYmRIfRtHc/lvZpjMpno1TKuxm0j7Vb6pXk7H+4a3J5dB8oZ3qvmlSruGtye7fvKuOGMNM7vmlLr8b3vy6HbXN6rBWe2S6RJpL3GgdFRp7emrNLN9J9z+FO1ldjtVgt2q4WHhnVi275S+qcl1LjdKYnhhNqsNIsN49U/9WLE6wtZuv0Aj1/ShT+ntwa8lxZc8MtehnSuHV+bJpEM7ZrCzLW5nN4mgW7NY3h5xKmszSkk42Dx2btVfK0DUnWt0F9VfIP3IMK1/Vv6vn7mih489Plqnri0qy/+09sksvGJob77uqJ3Cy7t2ZwR/1rItf1bEh1q468D27K7sIJJl3fz5WZ8RAjnH/ze6t0qvlYc/7i8G7e8u5Rbzkrjyt6pOD0eyivd2CxmX+Ff9d4A9EyN4/Gv1jJ+cAci7VY4mMrjBrXjhW83M7GO2Xzw5kek3VsavnNjP9/2u4d04M7z2vHS3F847ZR4msaE8crIXoc9EH5t/1bM25TP2e2a1DkmKSqUv5zTBo/H4Jq+qVgtJp64pCtllW4i7HWXpiaTib8N61zjPgB6tYqtderCKYkRtIgLp0VcOCkxodz8zlLuy+hQ5/fT4R6rT+tD3ztjz21Xr9sFC53zf4zonH+RY0d5KoFs4P99z5a9pXxwU1/y12UpT09mgXTOf2EhvPACTJpE6SWXMzXHzHUrvyHxogyWN2vGqXY7loICePllqKyEf/8b10MPM+W5T7j+0v7ERNQ9+1pY7uSZWRsY3DmFc34zc/Z7eDwG//pxC71axdG3de2Cpz5KHC5CLN6Z+caoWiH8WCiucBIeYq13gfFHMAwDw6i7kA10Hy/eznPfrObffzmTdimxvu2F5U5+zS+pdSDjaBwuNyEWc0B0au0pqiA2PKTReXqseDwGG3KL6ZgSFZQ5cjhfrMwmxGKmVUIET85Yx4ShnWodgGuMHzfn43B6GHTwwGAg/23akDpUM/8iIiINceL8zSQnkpgYGDcODIPwJ55g2ZWP82P3s/lv60I6P/MM5gMHICEBHn/c+/G667BedRV3xhz5j+SYMBt/v7TbMQ/XbDbxl3NqnwPdEJGHmSWsr2NZGEaF+r8YMJlMdZ73HQwuO7UZ9t0raZ1Q87zzmDBbgwt/wNcREQiSokOPPug4MJtNdG4WuBOUjVW9E+X9m087Zvd7Vrtjd7AzkKj4FxERaQRDjf8yYoS33T6u4cXJHyImBu6+m/37Cvn3lIlMuPpveF54mO+bNSMjPx9LWpq38AcIDfX+ExGRk4aKfxERkQYI0ok1+SM884x/Htft9i7w5/GA+TetxFFR5I69h4+W7mLSR09iDG6NMyEBz/DhWFTsi4ic1HSpPxERkUbQijniF++8A2PGgMPhLfw9ta/dXhEaxuT0q/nPgGuw3HILzX/4wXuwQERETmoq/kVERBogEBaQkpOUywWrV8PSpTBx4mEPAFS6DMpCwvhw8J/w3H8/vZ9/HtN//+unoEVEJFCo+BcREWkAlf7iN1YrPPYYXHQRZGXBhAl1HgBwur2fu8Mj8dxzDxuuuQajc92X9xIRkZOHin8REZFGUNu/0LEjREd7Px4PLhdERMDVV0O3bjB9OjzxhPfSfdUOAFQV/yEWE0RFsemqq6BTp+MTo4iIBCwt+CciItIAVV3/Wu1fKCmB4mLvx+PBaoWPPoLJk70HHUpK4LXXvMX/E0+A3Q4ej6/4t1kOzvHoVBUREUEz/yIiIg1iUuO/+Mvq1XDrrTBqFPz737Bli7cL4LvvvGsAHOwAcFS6AAix6s88ERE5RL8VREREGkFt/3Lc7dgBkZEwdCjEx0NoKDz5JPTuDW+84f3c4cBpeA9Q+Wb+RUREUPEvIiLSIOqgluOu6khTbCyEhHgPAgC43RATA5MmeVv+33gDHn+8dtu/iIgIKv5FREQaRRP/8oeq3lpSdcSpc2ewWOCZZ+DAAe/n4D33/9RTvacD3HrroQX/rDpSJSIih2jBPxERkUZQ27/8YQzDW/B//z3MmeM9t/+CC2DkSPjiC0hPh5tugttvh9at4a23oKIC7r4bEhKo3LYFgBDN/IuISDUq/kVERBrApL5/+aOZTPC//3kL/KFDISXFO6ufmQmvvw4//ggjRsDo0eB0ei/x9+WXkJAAQKXa/kVEpA4q/kVERBpBl/qTP8zWrTBhAvzzn94CH7yX9Gva1Hu5v27dYMEC2LYNCgqgbVto1sx3c6fLm5s2rfYvIiLVqPgXERFpAN+8v2p/+aNUVkJcnLfw37wZzj3X2/I/aZJ3/6pV0KMHdO9e58195/xr5l9ERKpR8S8iItIA6voXn6lTobwcwsJ+3/1UnePvcnln9vftg+xs+Oknb7v/BRfAq696xy5eDE895f3Xvn2dd3dowT8V/yIicoiKfxERkUbQxL9w4YXH5n5MJli4EG67DbKy4PTTvYv6nXMODB/uPc+/yuefQ16e9xJ/h+FwVZ3zryNVIiJyiIp/ERGRBtDMv/whqmb+MzPhoovgmmsgJwfy872z/cXF8M038K9/eRf8S04+7F05teCfiIjUQcW/iIhIIxi61p8cS127egv6d97xFv+XX+69fN+HH8KZZ0KHDt7Z/h9+OOy5/lVU/IuISF1U/IuIiDSA6eCSfyr9hWXLvIvzhYRA7971v13VOf5uN1gs3m0REfDMMzBwIHz8MVx1FVx7rfffunXeAwMWC8TGHvXunW5vdmrBPxERqU6/FURERBpAbf/ic8kl3vPzL7mkYbczmWD2bG9h/9FHh7Z36ABDh3pn910u8Hhn8OncGRIS6lX4A1RqwT8REamDfiuIiIg0grr+5XeJjfWe0//MM9C3L8ya5e0EuPFG73n9GzaA2dyoRKt0qe1fRERq028FERGRBqia+FftL79Lv37w9dfwxhvQujXccw8MGeI9zz89Hf7xD+9lBBvRanLonH+1qYiIyCE6519ERKQh1PcvDVV1jv+yZbBihffz00+HTp2gZ0/45BOYO9c7+3/ttVBSAj16eFv/G8Gptn8REamDin8REZFG0Gr/J6+fdxXw3YZ8/sqhTpDDqir8//c/uOMOaNrUu7jfAw/AF194DwKAd6G/gQPhT3/ybr/ySoiKalR8TpcW/BMRkdpU/IuIiDRAIM/7ezwGRRVOYsND/B0KHo+B2Vzz1TIMA9MJ0Dlx8eSfALjJ4SLyaINNJvjxR/jLX7yt/LfcAkuXetv+Bw3yHhQ4//xDi/t16wZdunjP92+kSl3qT0RE6qDiX0REpBEcTg9f7TAz4z8r+fPpaUz/OYd5G/NJb5PI3hIHZhPccvYp9E9L4N5PV4EBT1/Rnfm/7OW9hds5vU0iM9fkEhZi4S/nnEJRuYvT2yYQHWpjybb9lFS4mLcpn8tObU5RhZPt+8qIDrPRPjmSjinRvjgKyir52+drOKddE+ZtzmfmmlzeuaEfZ7ZL9I1xuT3c+eFKdheW8+AFnQi1Wfhhcz7hNgtX921JblEFf/n3UuxWC2YTlFW6+ecV3Xl93hZyCsu5+axTuLhHsyO+HuWVbv6zeAfpbRJ4f9F2vliZw9vX96VP63gcLjd/n76emWtz6ZgSRWG5E4vZRP+0BJrGhDK0awoeA8LtFl7I3Ez75Egu69WcKXN/Ia/IwaMXd2F9bhHPzd7EXYPb06tlLF+uysFmMbM6u5C0xAiWbttPXpGD3YXltIwPp1lsGJf0bMaHi3eyv7SSp4Z3p0mUHQC3x+C/y3bhMQzaJkVy339/5uYzT6F7ixi6No9h+75S7vv0Z/qnxbNjfxmhNgvXpbfi02W76NUy7lAOuDy+4v/N+Vt5KXMDb9zQn74tYymqdPPSt5vZtG0PkzZ/Q/Pbb8dxw4189eUiLrn9SmzXX8/+onJiLrmEvE++oOy0M3hx9kZuP689nZpG13ht3/hxCx8v3cllp7YgPsJGTJgNl8cg0m7loyU7sVrM/OXsU+jaPIYvV+WwcmcBADa1/YuISDUmQ32Lx0RRURExMTEUFhYSHR199Bv4idPpZMaMGVxwwQXYbDZ/hyNSJ+WpBLLhry5g2fYDjbptx5QoNuQWH3Z/UpSdtkmRLPh13xHvZ0CHJrRtEsnyHQdYvqOgzjEXdEsht7CClvHh5BRWsHjr/jrHtU4Ip6DcSUGZ87CPZzGbGHdeO6LDbOQXO9hb4qDc6cZuNZNX5CDSbmXznmI25ZXUuJ3daubiHs2Y/8tedhdWHPE5HUmPFjFs31/mi9FuNeM4uKJ9fYXZLKTGhwFwoMxJfrGjznFDu6Ywc21uvRbZz5oyiqYl+yhKSKbHTf/CMJlpUZjH1QfWs6lZO74KaQ5A731bGdE9iRkkcMekMaxPSuP/ho8nbeNKPnn/fgCuu/oJfmrdE4CLezSjb1o8y7cfwO0x+HJVzlFjsVlMXNi9GZ+tyPZt++Dm/vRtFaOfpxLw9HtfAl0g52hD6lDN/IuIiDTANX1TWb2r0NdaHWm3UuKouTDb+MHtWbhlX60i/reFf4/UWDAMVu0qBGBPsYM9hylKq/t+Yz7fb8w/4pgZq3MBDntwID4ihKJyJ9v2ldWIe/u+Mv67fFeNsW6PwbOZm44a1285XB4+WbbL93j7SyuxW83cPaQ9JRUuXpr7S523S4gIYV9ppe/rqten+v3+1tntmzCwQxNe+2FLnQcayp3uWgcn6vLNmtxa23qkxrLq4Gx6XUornBgmMx3yt/HK55PYnNiSZe4ITG2bE2W3siwhjWXZ0CNnFWbDw5t9L2F/aSUJ9ghmdDyTXdFNyI1M8N3fl6ty6iz4OyRH0STKTk5hOVvySwHomRpLmM1C1pZ9NQp/gIRI+1Gfr4iInDxU/IuIiDTAlX1S6ZcWT+ba3eRvWce4awYxe8NetuSX0qtVHB7D4NwOSdw2oA3zN+9lTXYhLo9BcnQoO/aXcUpiBJhg4a/7mHhRZ0yYeDdrG2e1b8Ka7ELmbthD05hQLu7RjHbJUazOLiTMZiHUZqagzMmXq3IIsZqpdHno3DSavSUODGDZtgMM696Uzs2iWbRlHzaLGZMJsg+UEx9hZ0iXZN6av5WwEAtX902lTZNIsgvKyVyXR4jFzJAuybSICwfg7iHteePHrSREhjD67FOY9tM25m3KJyrUSpMoO4mRdsJsFoornDSPC2NvSSUWs4nzOiYxc00uu4squH1AGzbnlfDTL3tplRjBlb1bEGLxztiHhVgAuHNQe7J+3cestbkkRtpZsm0/6W0SuPGMNNbmFNIiLpwKp5vvN+7hl/wSzuuUTEmFizCbheZxYSzdtp9ereIwm0x0SI7CbDZxTb+WvLNgG2e3b4LFbKJVQjgut8FnK7IJs1loGhNKiNVM52bRrMku4u5PVjKkcwqdm0bz3cY9bM4rIcRqpkVcGIXlTu7J6MCpqbGszSliXU4RD3+xhlCbhfRTEnyr6YeGWLk1qZK/TnmQzDMvYf+Nf6FjZAK3tEukX1o872ZtZ8Eve+nj2kz3vF944sKOLItsTrfXvqRvizBuuXAMuMxc1SqOUocbm8XED5v3UljupENyFD1bxnJfRocaaznkFlbw0y97uaBbU8xmeGfBNjbkFrNzfxlX9G5BqM1Ch5QonM7Dd3SIiMjJRW3/x4ja/kWOHeWpBAPl6cmpxkKGxcXe1fwdDhgzBpKSYPLkQ4OdTsjLg9JS6NDBu+3CC2HGDOjbF9atg/nzvZf1+42qP89+7wKJylMJBspTCXSBnKNq+xcRERH5A9S4gkHVpfhcLsjNhbPPPrRv1iyYORPeegsSEuCUU+Dbb+GTT+Df/4ayMhg2DNq1q/NxToSrIoiISGBR8X+Sce3di6W4GMPlwjCbMSoqKF20CMPhwBwdTXifPpisVvB4MNxu3AcO4C4qwnC5vJch8niwJiVhjorCHBKCp7IST2kZFevWEpKaChYL1iZJAJjtIWC14iktpXLrNjCbcB8owNY0BZPdji0lBSwWHJs24Skqwtq0KbbmzTGZTHgqKnDt3YfJasESG4s5NBRnXh6uvXuxJibizM7BU1qCOTwcc1gYlvh4LHFxlK9ahTU+HqOyEkt8PObISErmzMEcGUlo167gcuEuLfXG7qjEfkoa7sJCnLl5uAsKwDAwWczYO3SgbOkyPOVl2NPSsCYn4ykrwxRix2Q2YbjdWOPjcebkYGvRAgDD7caxcSOG2w0uF/ZOnbDGeVeF9lRW4ikqwl1UjKesDAwDT3ERloQE7O3agWHgysujYsMGrEnJWCIj8DgcVO7YgSU6BlvzZthSUnDt2YMpJAR3UTHWhHg85eV4SkuxNW+O4XLj3rcXZ3Y2nrIyDI8Hc1gY5tBQrElJWGJjqdyxk4oN6wnt1JnQzp3A46Hkhx+xNW+GNT4eS0IChtOJKz8f9969GG43JqsVk92OOSwMw+3GtScfe/t2WGJjcWZnU7F+PSGtWmEOC8OZsxtLXCz2du0wysup2LgJkz0EW7Nm3vG7duE+cABbaiqWmBhc+flYIiMxDAPD4cBkt1OxZg1GdDQheXtwbNqE22KhcssWDJcLW7NmuAsLsbdpgzkqCqOiAsfWbdiapmCOjMSWnOyNMTcXZ94ebM2be6/LdvA629bERNyFhd7nZLNhstvB7ca1bx9Gebn3MmRmM9bkZEx2O57SUt9Yo6KC8rVrCe3QAXNkJO79+/E4KjEqHZQtXYqtaVOsCQlYYmMxPB5MZjPOvDzs7dpjMpu8722TJoSkpuI6cAD3vn3YmjbFFBaGyWzGU1mJe+9eXPv2YQoJwRIZ6b1/lxN7WhquAwfA5fJ+P23fDhYLrj35WJOaENKyJa49ewjt3Bl3cQnuAwewxMZgCrFjiY3BmZODUenElbsbT3mFN3cqKrClpHhfh7AwLHFxGGVlGE4nptBQcLu9uWoy+d4rx9atmMPCvfdtNmOOisKanIwzO4fyVSuxt22LJTYW1+7d3vc4Pp6KtetwH9hP5fYdhHZoj61ZMzyOSlx787FERRHSqhWG2+3N5ZISbz63aOGN4+Brj8mEp6QEx6+/Yg4NxRwZhSUqEnNkJEZlJZ6SErBawTBwZmd7X4/8fDxlZdjbtcMcFkbJggXYkpKwJiZiCg3FlZeHMycHj8NBaKfO2Jqm4Ny9G3NEBOaISMwR4ZhMJpx5e6jcuhVb82YYLhfOnTsJ69EDS0yMf36IS2AqK4P8fPj5Z9i40Xv5vnfega5d4YknIDISnnwS7r0XnnkGRo/2d8QiInISUvF/kjnwrzdo88EH/Pr3JzGFhIBhYPyB5wOabDZvMeypvTiTyWYDsxnDcWhxK3NMjK/AqcFq9c6sHInZXOfjHDG+0FCMysoG365ezGYssbF4SktrPMfaQZio17LSx1pdr6nN5m1T/Z3MMTG+QvKwj2exgNt92PtoDex87rkGPa4lIQFPUdHhc/o3OWIKD8eoqDhsfh72fn7He2aJi/MdaAIwh4eDxYKn+PArwAcyc2Skt/iuS32+bwOV2Yw5PLzO52aJiaHV++9hbtXKD4FJQIqOhilTICMDZs+G/fu9Rf5550Hbtt6fqx995D1AICIi4icq/n9jypQpPPPMM+Tm5tKjRw9efvll+vXr5++wjhlPWRmGyYTJMLxFL2BLTcWWnEzl9u24fvuHidWKJSbG2w1gMYMBrj17jli0VVdVPJlCQ70z0E0ScWzb7u0sqNpns2FNSsK1Zw+ewkMrOptCQjA8Hm/x4HL5imn3gQNYEhOwJiR6n095Oa59+7wFnM0GbjeW+Hjc+/d7OxWaNsUSGYljyxZf3KaQEExWq3dmEzDZ7d6DEeD7Y98cE+PrQjAqKjCFhXnj9ngwWSzeovG3bDbvTKrNhqew0BtDNeaoKG+xZzZjDgnBmZdX435C2rTBme1drdlkNmNr1QqjvBxndrb39TpYdJpCQrzvn8WCyW7HOPg8sFi8s/AREd4YKsoxysqpzM4GpxNLQgLWxEQqt2+vO/6q9yQkxDtDerAA9jgr8ZSUYrJYMIWF4s7fe/AJmbG3aYPj118xWSzYWrTAlZfnex+tSUkYhsc7vuo9jIvDvW+f972oo4i2JCRgVFbiKi/HcnDm196uHa49e3BmZ2NN9L7vVQeILE0SMUrL8FRUeO/34OtgiY31fm02e//VcRCq+utmstu9TQIeD0ZFxZEPilXFbLGAx4O9fXvweHAVHMBdUIjJZMKorMQUFoZRXn5o7MFumqpcwen05WDVNmtCAkZ5OR6Hw/selpX5XquqLgxrSjJGhYOQU9JwZufgyvWuTu47aGaxHDrQ4XZ7D/RZrYS0boU5xI4zJwdzZCSu3Fw85eVHPpBR9R6ZTIS0bOlNk9xcTCEhvtl6TCZsqak4d+wAsxlrYqL3Z4nL5e24SEnB1rQplVu34i4s9L4/0dHeTpyq73mz2Zu3djvuvXtr54bFQkjr1t7bFBfjLi725as5IgJPZSUYBrbkZJzZ2Vji4jBHR+HcsfPQ94zL5b1Pw8ASE4MlIQFLTAwVa9d636/QUO/7fjBXqn4WWJok4iks8h6srKjAXVjIgf98SMID9x/+dZOTw3PPQVGRt/gfPx62bIE9e6BVK0hMPDTOYoGYGEhNPZTXausXEZHjTMV/NR999BHjx49n6tSp9O/fnxdeeIGMjAw2btxIUlKSv8M7JpKfeJxlffsw5IwzMHJ2Y7JZCe3a1VuseDzeGUmTCZPFAmaLr/W1OuNg4W44HAcPCli8f1g7HJgsFm8xYTJ5Ty0oL8dT4SAkrbXvfgyPx9ueu3u394/wuHhvm3tFBY7NmzFHRmJNTMQcGQl4CxpPSQmW6GjM4eHeP+AtlhpxuUtK8BQWYk1JAbMZk8mEu7AQT0UF1qQkbxFfXo6nogJzRATg7Qav2LgJa3IStmrvr6eyEsfGTYR27OA7IOAuKcUc7m3PrlqEyVPiPe3AtW8f5nDvfZrtIb7buPbuxbV3r7dFOTrKW9hYLDVfS5cLV9UBArcbW9Om3k4JkwmT2XxonNOJu6QES1QUhsvlLfidzkOPlZeHOTIKk8WMOSys1vvuLinBqKzEGh9/6HHz872nacTF4S4u9p5OsH+/9zSLqKjDnm9qGAau3bu9B1BCw7BERuAuKsJksfiKMMf69ZhCw7C3b+d97Ssrce/bhzk8HEtMDJ7yclz5+d42/uJicLm8BwWKi72nBzidfDNjBkMPLqpSlZ+e0lIsB8+xNTweDJcLc0iIL08q1q/HkpBASGqqNzer8VRU4Ny509tmDhgOh/c0mMhILPHxvvfGMAw8JSW4CwuxJiT4DlQZbrc3xsJC78Egmw1LfDx4PLUeq+o9w2rFU1TkK2zdBQW49uzxnYZhlJfjzMnxvg8JCZijo2u97u6iIpy7c7G3bVMrf2q9Lzk53rb16GhfkesuKMCSkFDn+2kYBng83gL8YEGMxULltm0YlZWEpKV5v6fLyrxt/ge/dwzD8B0Yc+7ciTkmBltSEu6CAt+BPk95OZVbt2I92Gpf9Z7hcnkPkplM3te6tNSbSwe3Vb2XptBQXPn5GJWV3tNXIiK8B86qxW5UVnpvd/C0CdxuzGFhuIuKfN9vntJSPA4Hlrg478EQw8BTXu7LIwCPw4Frz55Dp/BUVHhzoLgEk9VCSMuWvu/74lmzyB53F2VLl3Lowmxy0nruOcjOhubNvcV/aqr3X3WVld7W/59+8rb+q+gXERE/0Wr/1fTv35++ffsy+eBKvR6Ph9TUVO644w4eeOCBI942GFb7NwyDX3/4gTWLfqZlairmasWlSCDxeDzs2LlTeXqCCubfOp5yBwfenQmYCD+9KwcKC4iLizvC4mz1KfT+wBfkaF0d8rv0emEi9uJCHFExLB/3eK39iT8vIXL3DhLWrmD9iFspa9ri+ARmGLj3FuIuLMXAwOFwYD/Y3VSbqXaaGgf/M5m8XUQmDqXpwQOL3n+H7uLo6ngc3y7l4gmnKocMaueK6eAnJsBk9uVFhaOC0NBQ777qP7uq31etn5em6oPq2HcwV4+Uf/ViqvlwgMmoeuS6HruObQY1n0P1ISbAbDr4u+RIgf72dakjhsM9jgnv99of+e3WmF9npsMscFrnS3j4Bzj6a9c4lhAzp7/5JC6XS6v9n0gqKytZtmwZEyZM8G0zm80MGjSIrKysWuMdDgeOaudxFxUVAd7LQATqNXXLXeW8+O1PdMw/jewt/o5G5Gi6KE8lAEVCh5HeTw+eZbJNp3GftLp5bNgBp8fG8j1da+yLLdhJ6pLVlIVE8d2wFzlgaQV7jnOAgTkXIVK3CH8HIFKb1VlCt4pirIa3bA7EOq8hMan4P2jv3r243W6Sk5NrbE9OTmbDhg21xk+aNInHHnus1vbZs2cTXq01NZBUGpXsjchmq2u1Xx5fx/XrJ4gnRUVOCtFlBk0KD836NFb1ydRaO46kIY9bzwmkw8YiR2Y4fR8t5T/X2FVsh2/PGI7HYqUypLDW/oao8/2pc7b+EJcVisNqbjbqmLX87d3VNa/qm+E01Rxj/CYGk1F7228f77fPRX8bnLgM6pcP1b/+7e1rDPrt9mq7jzDv74vhWPzMPtxXDbrrul4T4+Dmwz2ReqjX7xMj8H7e+553A38YHPXnyDF6km6bg1mzHISYvKeZZmZmHps7PobKqq8fdRQq/htpwoQJjB8/3vd1UVERqampDBkyJKDb/gcOHMjcuXMZOHBgwLWsiFRxOp3KUwl4ylMJ//YZKCskPDaCa6fe6O9w6qQ8lWCgPJVAFmoJxeVykZmZyeDBgwMuR6s60OtDxf9BiYmJWCwW8vLyamzPy8sjJSWl1ni73Y7dbq+13WazBVxCVBdtiibEFEJ0WHRAxyknN6fVqTyVgKc8lapz1c0mE9FhgXngX3kqwUB5KoGual2CQKz1GhKPVtI6KCQkhN69ezNnzhzfNo/Hw5w5c0hPT/djZCIiIiIiIiK/j2b+qxk/fjyjRo2iT58+9OvXjxdeeIHS0lJuuOEGf4cmIiIiIiIi0mgq/qu5+uqryc/PZ+LEieTm5tKzZ09mzpxZaxFAERERERERkWCi4v83xo4dy9ixY/0dhoiIiAS6Xr0gNRWaNPF3JCIiIkel4l9ERESkMb780t8RiIiI1JsW/BMRERERERE5wan4FxERERERETnBqfgXEREREREROcHpnH8RERGRxrj4YsjP9y74p/P/RUQkwKn4FxEREWmM5cshOxuaN/d3JCIiIkeltn8RERERERGRE5yKfxEREREREZETnIp/ERERERERkROcin8RERERERGRE5yKfxEREREREZETnIp/ERERERERkROcin8RERERERGRE5zV3wGcKAzDAKCoqMjPkRyZ0+mkrKyMoqIibDabv8MRqZPyVIKB8lTweA59DNDf/8pTCQbKUwl0gZyjVfVnVT16JCr+j5Hi4mIAUlNT/RyJiIiIHFe7d0NMjL+jEBGRk1hxcTExR/ldZDLqc4hAjsrj8ZCTk0NUVBQmk8nf4RxWUVERqamp7Ny5k+joaH+HI1In5akEA+WpBAPlqQQD5akEukDOUcMwKC4uplmzZpjNRz6rXzP/x4jZbKZFixb+DqPeoqOjAy5xRX5LeSrBQHkqwUB5KsFAeSqBLlBz9Ggz/lW04J+IiIiIiIjICU7Fv4iIiIiIiMgJTsX/ScZut/PII49gt9v9HYrIYSlPJRgoTyUYKE8lGChPJdCdKDmqBf9ERERERERETnCa+RcRERERERE5wan4FxERERERETnBqfgXEREREREROcGp+BcRERERERE5wan4P8lMmTKF1q1bExoaSv/+/Vm8eLG/Q5KTxKRJk+jbty9RUVEkJSVx6aWXsnHjxhpjKioqGDNmDAkJCURGRjJ8+HDy8vJqjNmxYwfDhg0jPDycpKQk7r33Xlwu1/F8KnISeeqppzCZTIwbN863TXkqgSA7O5s//elPJCQkEBYWRrdu3Vi6dKlvv2EYTJw4kaZNmxIWFsagQYPYvHlzjfvYv38/I0eOJDo6mtjYWG666SZKSkqO91ORE5Db7ebhhx8mLS2NsLAw2rRpwxNPPEH1dcaVo3K8/fDDD1x00UU0a9YMk8nE559/XmP/scrJn3/+mbPOOovQ0FBSU1N5+umn/+inVm8q/k8iH330EePHj+eRRx5h+fLl9OjRg4yMDPbs2ePv0OQkMG/ePMaMGcPChQvJzMzE6XQyZMgQSktLfWPuuusuvvrqKz755BPmzZtHTk4Ol19+uW+/2+1m2LBhVFZWsmDBAt555x2mTZvGxIkT/fGU5AS3ZMkSXnvtNbp3715ju/JU/O3AgQOcccYZ2Gw2vvnmG9atW8ezzz5LXFycb8zTTz/NSy+9xNSpU1m0aBERERFkZGRQUVHhGzNy5EjWrl1LZmYm06dP54cffmD06NH+eEpygvnnP//Jq6++yuTJk1m/fj3//Oc/efrpp3n55Zd9Y5SjcryVlpbSo0cPpkyZUuf+Y5GTRUVFDBkyhFatWrFs2TKeeeYZHn30UV5//fU//PnViyEnjX79+hljxozxfe12u41mzZoZkyZN8mNUcrLas2ePARjz5s0zDMMwCgoKDJvNZnzyySe+MevXrzcAIysryzAMw5gxY4ZhNpuN3Nxc35hXX33ViI6ONhwOx/F9AnJCKy4uNtq1a2dkZmYa55xzjnHnnXcahqE8lcBw//33G2eeeeZh93s8HiMlJcV45plnfNsKCgoMu91u/Oc//zEMwzDWrVtnAMaSJUt8Y7755hvDZDIZ2dnZf1zwclIYNmyYceONN9bYdvnllxsjR440DEM5Kv4HGJ999pnv62OVk6+88ooRFxdX4/f9/fffb3To0OEPfkb1o5n/k0RlZSXLli1j0KBBvm1ms5lBgwaRlZXlx8jkZFVYWAhAfHw8AMuWLcPpdNbI0Y4dO9KyZUtfjmZlZdGtWzeSk5N9YzIyMigqKmLt2rXHMXo50Y0ZM4Zhw4bVyEdQnkpg+PLLL+nTpw9XXnklSUlJnHrqqfzrX//y7d+6dSu5ubk18jQmJob+/fvXyNPY2Fj69OnjGzNo0CDMZjOLFi06fk9GTkinn346c+bMYdOmTQCsWrWK+fPnM3ToUEA5KoHnWOVkVlYWZ599NiEhIb4xGRkZbNy4kQMHDhynZ3N4Vn8HIMfH3r17cbvdNf4YBUhOTmbDhg1+ikpOVh6Ph3HjxnHGGWfQtWtXAHJzcwkJCSE2NrbG2OTkZHJzc31j6srhqn0ix8KHH37I8uXLWbJkSa19ylMJBFu2bOHVV19l/PjxPPjggyxZsoS//vWvhISEMGrUKF+e1ZWH1fM0KSmpxn6r1Up8fLzyVH63Bx54gKKiIjp27IjFYsHtdvPkk08ycuRIAOWoBJxjlZO5ubmkpaXVuo+qfdVPz/IHFf8ictyNGTOGNWvWMH/+fH+HIlLDzp07ufPOO8nMzCQ0NNTf4YjUyePx0KdPH/7xj38AcOqpp7JmzRqmTp3KqFGj/BydCHz88ce8//77fPDBB3Tp0oWVK1cybtw4mjVrphwV8SO1/Z8kEhMTsVgstVakzsvLIyUlxU9Ryclo7NixTJ8+ne+++44WLVr4tqekpFBZWUlBQUGN8dVzNCUlpc4crton8nstW7aMPXv20KtXL6xWK1arlXnz5vHSSy9htVpJTk5WnorfNW3alM6dO9fY1qlTJ3bs2AEcyrMj/c5PSUmpteCvy+Vi//79ylP53e69914eeOABrrnmGrp168Z1113HXXfdxaRJkwDlqASeY5WTgf43gIr/k0RISAi9e/dmzpw5vm0ej4c5c+aQnp7ux8jkZGEYBmPHjuWzzz5j7ty5tVqievfujc1mq5GjGzduZMeOHb4cTU9PZ/Xq1TV+8GZmZhIdHV3rD2GRxjjvvPNYvXo1K1eu9P3r06cPI0eO9H2uPBV/O+OMM2pdKnXTpk20atUKgLS0NFJSUmrkaVFREYsWLaqRpwUFBSxbtsw3Zu7cuXg8Hvr3738cnoWcyMrKyjCba5YZFosFj8cDKEcl8ByrnExPT+eHH37A6XT6xmRmZtKhQwe/t/wDWu3/ZPLhhx8adrvdmDZtmrFu3Tpj9OjRRmxsbI0VqUX+KLfddpsRExNjfP/998bu3bt9/8rKynxjbr31VqNly5bG3LlzjaVLlxrp6elGenq6b7/L5TK6du1qDBkyxFi5cqUxc+ZMo0mTJsaECRP88ZTkJFF9tX/DUJ6K/y1evNiwWq3Gk08+aWzevNl4//33jfDwcOO9997zjXnqqaeM2NhY44svvjB+/vln45JLLjHS0tKM8vJy35jzzz/fOPXUU41FixYZ8+fPN9q1a2eMGDHCH09JTjCjRo0ymjdvbkyfPt3YunWr8b///c9ITEw07rvvPt8Y5agcb8XFxcaKFSuMFStWGIDx3HPPGStWrDC2b99uGMaxycmCggIjOTnZuO6664w1a9YYH374oREeHm689tprx/351kXF/0nm5ZdfNlq2bGmEhIQY/fr1MxYuXOjvkOQkAdT57+233/aNKS8vN26//XYjLi7OCA8PNy677DJj9+7dNe5n27ZtxtChQ42wsDAjMTHRuPvuuw2n03mcn42cTH5b/CtPJRB89dVXRteuXQ273W507NjReP3112vs93g8xsMPP2wkJycbdrvdOO+884yNGzfWGLNv3z5jxIgRRmRkpBEdHW3ccMMNRnFx8fF8GnKCKioqMu68806jZcuWRmhoqHHKKacYf/vb32pc/kw5Ksfbd999V+ffoqNGjTIM49jl5KpVq4wzzzzTsNvtRvPmzY2nnnrqeD3FozIZhmH4p+dARERERERERI4HnfMvIiIiIiIicoJT8S8iIiIiIiJyglPxLyIiIiIiInKCU/EvIiIiIiIicoJT8S8iIiIiIiJyglPxLyIiIiIiInKCU/EvIiIiIiIicoJT8S8iIiIiIiJyglPxLyIiIkHJZDLx+eef+zsMERGRoKDiX0RERBrs+uuvx2Qy1fp3/vnn+zs0ERERqYPV3wGIiIhIcDr//PN5++23a2yz2+1+ikZERESORDP/IiIi0ih2u52UlJQa/+Li4gBvS/6rr77K0KFDCQsL45RTTuHTTz+tcfvVq1czcOBAwsLCSEhIYPTo0ZSUlNQY89Zbb9GlSxfsdjtNmzZl7NixNfbv3buXyy67jPDwcNq1a8eXX375xz5pERGRIKXiX0RERP4QDz/8MMOHD2fVqlWMHDmSa665hvXr1wNQWlpKRkYGcXFxLFmyhE8++YRvv/22RnH/6quvMmbMGEaPHs3q1av58ssvadu2bY3HeOyxx7jqqqv4+eefueCCCxg5ciT79+8/rs9TREQkGJgMwzD8HYSIiIgEl+uvv5733nuP0NDQGtsffPBBHnzwQUwmE7feeiuvvvqqb99pp51Gr169eOWVV/jXv/7F/fffz86dO4mIiABgxowZXHTRReTk5JCcnEzz5s254YYb+Pvf/15nDCaTiYceeognnngC8B5QiIyM5JtvvtHaAyIiIr+hc/5FRESkUc4999waxT1AfHy87/P09PQa+9LT01m5ciUA69evp0ePHr7CH+CMM87A4/GwceNGTCYTOTk5nHfeeUeMoXv37r7PIyIiiI6OZs+ePY19SiIiIicsFf8iIiLSKBEREbXa8I+VsLCweo2z2Ww1vjaZTHg8nj8iJBERkaCmc/5FRETkD7Fw4cJaX3fq1AmATp06sWrVKkpLS337f/rpJ8xmMx06dCAqKorWrVszZ86c4xqziIjIiUoz/yIiItIoDoeD3NzcGtusViuJiYkAfPLJJ/Tp04czzzyT999/n8WLF/Pmm28CMHLkSB555BFGjRrFo48+Sn5+PnfccQfXXXcdycnJAFNv1JMAAAEoSURBVDz66KPceuutJCUlMXToUIqLi/npp5+44447ju8TFREROQGo+BcREZFGmTlzJk2bNq2xrUOHDmzYsAHwrsT/4Ycfcvvtt9O0aVP+85//0LlzZwDCw8OZNWsWd955J3379iU8PJzhw4fz3HPP+e5r1KhRVFRU8Pzzz3PPPfeQmJjIFVdccfyeoIiIyAlEq/2LiIjIMWcymfjss8+49NJL/R2KiIiIoHP+RURERERERE54Kv5FRERERERETnA6519ERESOOZ1VKCIiElg08y8iIiIiIiJyglPxLyIiIiIiInKCU/EvIiIiIiIicoJT8S8iIiIiIiJyglPxLyIiIiIiInKCU/EvIiIiIiIicoJT8S8iIiIiIiJyglPxLyIiIiIiInKC+39W7fxKmNf6LwAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA/8AAAIjCAYAAABViau2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xd4FNX6B/DvbE3dNNKAECD0EHrvJSTUS5MugoBKEwFFRfkhRUHxihQpFwtBL6CAWC7SewtFmvTeIQFCets2vz82M5mZnd3shoRN4P08Dw9kdnbm7Oxs2Pec97yHYVmWBSGEEEIIIYQQQl5YClc3gBBCCCGEEEIIIcWLgn9CCCGEEEIIIeQFR8E/IYQQQgghhBDygqPgnxBCCCGEEEIIecFR8E8IIYQQQgghhLzgKPgnhBBCCCGEEEJecBT8E0IIIYQQQgghLzgK/gkhhBBCCCGEkBccBf+EEEIIIYQQQsgLjoJ/Qgh5yQwfPhwVK1Ys1HNnzJgBhmGKtkGl3N69e8EwDPbu3ctvc/Qa37p1CwzDIC4urkjbVLFiRQwfPrxIj1maxcXFgWEY3Lp1y9VNcUhxfM5K22e3tL1nhBBSGlDwTwghJQTDMA79EQaZLxuz2Yx///vfqFq1Ktzd3REREYExY8YgIyPDoefXqVMHFSpUAMuyNvdp2bIlgoODYTQai6rZxeLw4cOYMWMGUlJSXN0UHhewMQyDgwcPWj3OsizCwsLAMAy6d+9eqHMsXbq0yDtLitLYsWOhUCjw9OlT0fanT59CoVBAq9UiJydH9NiNGzfAMAw++uij59lUl9Dr9Vi4cCHq168PnU4HX19fREZG4s0338SlS5dc2raHDx/iww8/RPv27eHt7V3g79vDhw+jVatW8PDwQEhICCZMmCD7uyg3NxcffPABypYtC3d3dzRt2hQ7duwoxldCCCHyKPgnhJAS4qeffhL96dSpk+z2mjVrPtN5vv32W1y+fLlQz502bRqys7Of6fzPYuHChZgyZQpq166NhQsXYuDAgdi2bRuePHni0POHDBmCu3fv4sCBA7KP37p1C/Hx8RgwYABUKlWh2/ks19hRhw8fxsyZM2WD/8uXL+Pbb78t1vPb4+bmhjVr1lht37dvH+7duwetVlvoYxcm+B86dCiys7MRHh5e6PM6qlWrVmBZFocOHRJtP3z4MBQKBQwGA/7++2/RY9y+rVq1AuD6z1lx6tu3L959913Url0bn3/+OWbOnIk2bdpgy5YtOHLkCL/f83zPOJcvX8YXX3yB+/fvIyoqyu6+p0+fRseOHZGVlYX58+dj1KhRWLFiBfr162e17/DhwzF//nwMGTIECxcuhFKpRNeuXWU7yAghpDgV/psNIYSQIvXqq6+Kfj5y5Ah27NhhtV0qKysLHh4eDp9HrVYXqn0AoFKpnikoflY///wzIiMjsXHjRj6Fefbs2TCbzQ49f/DgwZg6dSrWrFmDNm3aWD2+du1asCyLIUOGPFM7n+UaF4VnCa6LQteuXbF+/XosWrRIdL+sWbMGDRs2dLiz5lllZmbC09MTSqUSSqXyuZyTC+APHjyIHj168NsPHTqEOnXqIDs7GwcPHuT34/ZVKBRo0aIFANd/zorL8ePHsWnTJnz22WdWWQ7ffPONqCPreb5nnIYNGyIpKQn+/v7YsGGDbCDP+eijj+Dn54e9e/dCp9MBsEy3eeONN7B9+3bExMQAAI4dO4aff/4ZX375Jd577z0AwGuvvYbatWvj/fffx+HDh4v/hRFCSB4a+SeEkFKkXbt2qF27Nk6cOIE2bdrAw8OD/xL9xx9/oFu3bihbtiy0Wi0iIiIwe/ZsmEwm0TGk89G5eef//ve/sWLFCkRERECr1aJx48Y4fvy46Lly84YZhsH48ePx+++/o3bt2tBqtYiMjMTWrVut2r937140atQIbm5uiIiIwH/+8x+n5iIrFAqYzWbR/gqFwuFAKSwsDG3atMGGDRtgMBisHl+zZg0iIiLQtGlT3L59G2PHjkX16tXh7u6OgIAA9OvXz6E5yHJz/lNSUjB8+HD4+PjA19cXw4YNkx21/+effzB8+HBUrlwZbm5uCAkJwYgRI5CUlMTvM2PGDEyZMgUAUKlSJT7Vnmub3Jz/GzduoF+/fvD394eHhweaNWuGv/76S7QPV79g3bp1+Oyzz1C+fHm4ubmhY8eOuHbtWoGvmzNo0CAkJSWJUpv1ej02bNiAwYMHyz7HbDZjwYIFiIyMhJubG4KDg/HWW28hOTmZ36dixYo4f/489u3bx7/mdu3aAcifcrBv3z6MHTsWQUFBKF++vOgx6Xu3ZcsWtG3bFt7e3tDpdGjcuLEoY+Hq1avo27cvQkJC4ObmhvLly2PgwIFITU21+dorVKiAsLAwq5H/Q4cOoWXLlmjRooXsY5GRkfD19QXw7J+zgwcPonHjxqLPmRyj0YjZs2fzn/mKFSvio48+Qm5uLr/P5MmTERAQIJoq8/bbb4NhGCxatIjflpiYCIZhsGzZMpvX5vr16wAsU2uklEolAgIC+J+l7xl3TeT+CO91R+4jW7y9veHv71/gfmlpaXzHLBf4A5ag3svLC+vWreO3bdiwAUqlEm+++Sa/zc3NDSNHjkR8fDzu3r1b4PkIIaSovHjdyoQQ8oJLSkpCly5dMHDgQLz66qsIDg4GYPmy7OXlhcmTJ8PLywu7d+/G9OnTkZaWhi+//LLA465Zswbp6el46623wDAM5s2bhz59+uDGjRsFjmQfPHgQGzduxNixY+Ht7Y1Fixahb9++uHPnDv+F/tSpU+jcuTNCQ0Mxc+ZMmEwmzJo1C4GBgQ6/9tdffx1vvfUW/vOf/+Ctt95y+HlCQ4YMwZtvvolt27aJ5p2fPXsW586dw/Tp0wFYRikPHz6MgQMHonz58rh16xaWLVuGdu3a4cKFC05lW7Asi549e+LgwYMYPXo0atasid9++w3Dhg2z2nfHjh24ceMGXn/9dYSEhOD8+fNYsWIFzp8/jyNHjoBhGPTp0wdXrlzB2rVr8fXXX6NMmTIAYPNaJiYmokWLFsjKysKECRMQEBCAVatW4V//+hc2bNiA3r17i/b//PPPoVAo8N577yE1NRXz5s3DkCFDcPToUYdeb8WKFdG8eXOsXbsWXbp0AWAJtFNTUzFw4EBR0Mh56623EBcXh9dffx0TJkzAzZs38c033+DUqVM4dOgQ1Go1FixYgLfffhteXl74+OOPAYC//zljx45FYGAgpk+fjszMTJttjIuLw4gRIxAZGYmpU6fC19cXp06dwtatWzF48GDo9XrExsYiNzcXb7/9NkJCQnD//n1s2rQJKSkp8PHxsXnsVq1aYePGjcjNzYVWq4Ver8fx48cxZswYZGVl4f333wfLsmAYBsnJybhw4QJGjx5d4HV15HN29uxZxMTEIDAwEDNmzIDRaMQnn3xidZ0AYNSoUVi1ahVeeeUVvPvuuzh69Cjmzp2Lixcv4rfffgMAtG7dGl9//TXOnz+P2rVrAwAOHDgAhUKBAwcOYMKECfw2ALIZNRwuhX/16tVo2bKlU9kNffr0QZUqVUTbTpw4gQULFiAoKIjf5sh99KzOnj0Lo9GIRo0aibZrNBrUq1cPp06d4redOnUK1apVE3USAECTJk0AWKYPhIWFPXObCCHEISwhhJASady4caz013Tbtm1ZAOzy5cut9s/KyrLa9tZbb7EeHh5sTk4Ov23YsGFseHg4//PNmzdZAGxAQAD79OlTfvsff/zBAmD/97//8ds++eQTqzYBYDUaDXvt2jV+25kzZ1gA7OLFi/ltPXr0YD08PNj79+/z265evcqqVCqrY9ry4YcfshqNhlUqlezGjRsdeo7U06dPWa1Wyw4aNMjq2ADYy5cvsywrfz3j4+NZAOyPP/7Ib9uzZw8LgN2zZw+/TXqNf//9dxYAO2/ePH6b0WhkW7duzQJgV65cyW+XO+/atWtZAOz+/fv5bV9++SULgL1586bV/uHh4eywYcP4nydOnMgCYA8cOMBvS09PZytVqsRWrFiRNZlMotdSs2ZNNjc3l9934cKFLAD27NmzVucSWrlyJQuAPX78OPvNN9+w3t7e/Ovp168f2759e7593bp145934MABFgC7evVq0fG2bt1qtT0yMpJt27atzXO3atWKNRqNso9x1yolJYX19vZmmzZtymZnZ4v2NZvNLMuy7KlTp1gA7Pr16+2+ZjlLliwRXW/uvrl9+zZ74cIFFgB7/vx5lmVZdtOmTVav8Vk+Z7169WLd3NzY27dv89suXLjAKpVK0TFPnz7NAmBHjRolOs97773HAmB3797NsizLPnr0iAXALl26lGVZy7VTKBRsv3792ODgYP55EyZMYP39/fnrJ8dsNvO/w4KDg9lBgwaxS5YsEbWVI33PpB4/fsxWqFCBjYqKYjMyMliWde4+Ksj69eutPtfSx4SfR06/fv3YkJAQ/ufIyEi2Q4cOVvudP3/e5u9yQggpLpT2TwghpYxWq8Xrr79utd3d3Z3/d3p6Op48eYLWrVsjKyvLoSraAwYMgJ+fH/9z69atAVjSxQsSHR2NiIgI/uc6depAp9PxzzWZTNi5cyd69eqFsmXL8vtVqVKFHxkuyKJFizB//nwcOnQIgwYNwsCBA7F9+3bRPlqtFv/3f/9n9zh+fn7o2rUr/vzzT35kmGVZ/Pzzz2jUqBGqVasGQHw9DQYDkpKSUKVKFfj6+uLkyZMOtZmzefNmqFQqjBkzht+mVCrx9ttvW+0rPG9OTg6ePHmCZs2aAYDT5xWev0mTJqJ55l5eXnjzzTdx69YtXLhwQbT/66+/Do1Gw//szL3A6d+/P7Kzs7Fp0yakp6dj06ZNNlP+169fDx8fH3Tq1AlPnjzh/zRs2BBeXl7Ys2ePw+d94403CpwrvmPHDqSnp+PDDz+Em5ub6DEu3Z4b2d+2bRuysrIcPj8gnvcPWNL6y5UrhwoVKqBGjRrw9/fnU/+lxf7sceRztm3bNvTq1QsVKlTg96tZsyZiY2NFx9q8eTMAS1q/0LvvvgsA/JSQwMBA1KhRA/v37+fbq1QqMWXKFCQmJuLq1asALCP/rVq1sjuFh2EYbNu2DZ9++in8/Pywdu1ajBs3DuHh4RgwYIDDK1eYTCYMGjQI6enp+O233+Dp6QmgaO8je7hijHK1Ndzc3ETFGrOzs23uJzwWIYQ8DxT8E0JIKVOuXDlRYMY5f/48evfuDR8fH+h0OgQGBvLFAu3NUeYIgwUAfEeAI3Nlpc/lns8999GjR8jOzrZK2wUgu00qOzsbn3zyCUaNGoVGjRph5cqV6NChA3r37s0HWFevXoVer0fTpk0LPN6QIUOQmZmJP/74A4ClEvutW7dEhf6ys7Mxffp0hIWFQavVokyZMggMDERKSopD11Po9u3bCA0NhZeXl2h79erVrfZ9+vQp3nnnHQQHB8Pd3R2BgYGoVKkSAMfeR1vnlzsXt3LE7du3Rduf5V7gBAYGIjo6GmvWrMHGjRthMpnwyiuvyO579epVpKamIigoCIGBgaI/GRkZePTokcPn5a6VPdzccy6N3dZxJk+ejO+++w5lypRBbGwslixZ4tB7ULt2bfj6+ooCfG6eO8MwaN68ueixsLAw2c+QVEGfs8ePHyM7OxtVq1a12k/6/t++fRsKhcLq8xcSEgJfX1/RPdG6dWs+rf/AgQNo1KgRGjVqBH9/fxw4cABpaWk4c+YM30lkj1arxccff4yLFy/iwYMHWLt2LZo1a4Z169Zh/PjxBT4fsKyGsHv3br5GB6co7yN7uA46YW0ETk5OjqgDz93d3eZ+wmMRQsjzQHP+CSGklJH7spiSkoK2bdtCp9Nh1qxZiIiIgJubG06ePIkPPvjAoWr4tkZLWUGhr+J4riMuXryIlJQUfgRcpVJhw4YN6NChA7p164Y9e/Zg7dq1CAoK4pdItKd79+7w8fHBmjVrMHjwYKxZswZKpRIDBw7k93n77bexcuVKTJw4Ec2bN4ePjw8YhsHAgQMdXl2gMPr374/Dhw9jypQpqFevHry8vGA2m9G5c+diPa9QUb2fgwcPxhtvvIGEhAR06dKFL2gnZTabERQUhNWrV8s+7kxdiKIMpr766isMHz4cf/zxB7Zv344JEyZg7ty5OHLkCF9MUI5CoUDz5s1x+PBhftk/YXX7Fi1a4IcffuBrAfTq1cuh9hTH58yRYputWrXCt99+ixs3buDAgQNo3bo1GIZBq1atcODAAZQtWxZms9mh4F8oNDQUAwcORN++fREZGYl169YhLi7Obi2A33//HV988QVmz56Nzp07ix4ryvuooHYDwMOHD60ee/jwoSi7KTQ0FPfv35fdD4BoX0IIKW4U/BNCyAtg7969SEpKwsaNG0UFt27evOnCVuULCgqCm5ubbMV4R6rIcwGKsDK2p6cnNm/ejFatWiE2NhY5OTn49NNPHVrmTqvV4pVXXsGPP/6IxMRErF+/Hh06dEBISAi/z4YNGzBs2DB89dVX/LacnByHU5OFwsPDsWvXLmRkZIhG/y9fvizaLzk5Gbt27cLMmTP5woMA+NRqIUdXSODOLz0XAH46SHGtpd67d2+89dZbOHLkCH755Reb+0VERGDnzp1o2bJlgcG7M6/b3vkA4Ny5cwVmnkRFRSEqKgrTpk3D4cOH0bJlSyxfvhyffvqp3ee1atUKW7ZswZ9//olHjx6JKty3aNECH3/8MTZv3ozs7GyHUv4dERgYCHd3d9n7Rfr+h4eHw2w24+rVq3wGCGApDpmSkiK6J7igfseOHTh+/Dg+/PBDAJbifsuWLUPZsmXh6emJhg0bFqrdarUaderUwdWrV/HkyRPR51DoypUrGDZsGHr16mW1VCDg3H30LGrXrg2VSoW///4b/fv357fr9XqcPn1atK1evXrYs2cP0tLSREX/uOKZ9erVK7Z2EkKIFKX9E0LIC4AbERSOAOr1eixdutRVTRJRKpWIjo7G77//jgcPHvDbr127hi1bthT4/KioKAQHB+Obb74Rpe4GBARg5cqVePLkCbKzs0XrqhdkyJAhMBgMeOutt/D48WNRyj/XZumI6uLFi62WTnRE165dYTQaRcugmUwmLF682OqcgPVI7oIFC6yOyc1zdqQzomvXrjh27Bji4+P5bZmZmVixYgUqVqyIWrVqOfpSnOLl5YVly5ZhxowZdt+b/v37w2QyYfbs2VaPGY1G0Wv09PQsVAeMUExMDLy9vTF37lw+/ZrDXfu0tDQYjUbRY1FRUVAoFLJp3FJcQP/FF1/Aw8NDFOQ1adIEKpUK8+bNE+37rJRKJWJjY/H777/jzp07/PaLFy9i27Zton27du0KwPremj9/PgCgW7du/LZKlSqhXLly+Prrr2EwGPiOjNatW+P69evYsGEDmjVrVmD1/qtXr4raxUlJSUF8fDz8/Pxsjs5nZGSgd+/eKFeuHFatWiXbCeTMffQsfHx8EB0djf/+979IT0/nt//000/IyMhAv379+G2vvPIKTCYTVqxYwW/Lzc3FypUr0bRpU6r0Twh5rmjknxBCXgAtWrSAn58fhg0bhgkTJoBhGPz0009FlnZfFGbMmIHt27ejZcuWGDNmDEwmE7755hvUrl0bp0+ftvtclUqFb775BgMGDEBUVBTeeusthIeH4+LFi/jhhx8QFRWFe/fuoWfPnjh06JDVslpy2rZti/Lly+OPP/6Au7s7+vTpI3q8e/fu+Omnn+Dj44NatWohPj4eO3fuFK1F7qgePXqgZcuW+PDDD3Hr1i3UqlULGzdutJo/rtPp0KZNG8ybNw8GgwHlypXD9u3bZTM4uFHWjz/+GAMHDoRarUaPHj34TgGhDz/8kF92b8KECfD398eqVatw8+ZN/Prrr1Aoim8sQG45Q6m2bdvirbfewty5c3H69GnExMRArVbj6tWrWL9+PRYuXMjXC2jYsCGWLVuGTz/9FFWqVEFQUBA6dOjgVJt0Oh2+/vprjBo1Co0bN8bgwYPh5+eHM2fOICsrC6tWrcLu3bsxfvx49OvXD9WqVYPRaMRPP/0EpVKJvn37FniOJk2aQKPRID4+Hu3atRMFxh4eHqhbty7i4+Ph6+trt/aAs2bOnImtW7eidevWGDt2LIxGIxYvXozIyEj8888//H5169bFsGHDsGLFCn7a0LFjx7Bq1Sr06tUL7du3Fx23devW+PnnnxEVFcXXgGjQoAE8PT1x5coVm8Uchc6cOYPBgwejS5cuaN26Nfz9/XH//n2sWrUKDx48wIIFC2xObZg5cyYuXLiAadOm8bU6OBEREWjevLlT95EtXEbH+fPnAVgCeq6uyLRp0/j9PvvsM7Ro0QJt27bFm2++iXv37uGrr75CTEyMaDpC06ZN0a9fP0ydOhWPHj1ClSpVsGrVKty6dQvff/99gdeMEEKKlItWGSCEEFIAW0v9RUZGyu5/6NAhtlmzZqy7uztbtmxZ9v3332e3bdtW4DJ03FJ/X375pdUxAbCffPIJ/7OtJcjGjRtn9VzpcnMsy7K7du1i69evz2o0GjYiIoL97rvv2HfffZd1c3OzcRXE9u/fz8bGxrI6nY7VarVs7dq12blz57JZWVnsli1bWIVCwcbExLAGg8Gh402ZMoUFwPbv39/qseTkZPb1119ny5Qpw3p5ebGxsbHspUuXrF6XI0v9sSzLJiUlsUOHDmV1Oh3r4+PDDh06lF9OTrjU371799jevXuzvr6+rI+PD9uvXz/2wYMHVu8Fy7Ls7Nmz2XLlyrEKhUK0LJrctb9+/Tr7yiuvsL6+vqybmxvbpEkTdtOmTaJ9uNciXd6Ou0eE7ZQjXOrPHulSf5wVK1awDRs2ZN3d3Vlvb282KiqKff/999kHDx7w+yQkJLDdunVjvb29WQD8sn/2zm1r2bg///yTbdGiBevu7s7qdDq2SZMm7Nq1a1mWZdkbN26wI0aMYCMiIlg3NzfW39+fbd++Pbtz5067r02oefPmLAD2o48+snpswoQJLAC2S5cuVo896+ds3759bMOGDVmNRsNWrlyZXb58uewxDQYDO3PmTLZSpUqsWq1mw8LC2KlTp4qWBuVwyxeOGTNGtD06OpoFwO7atcvmdeAkJiayn3/+Odu2bVs2NDSUValUrJ+fH9uhQwd2w4YNon2l79mwYcNYALJ/pK/fkfvIFlvnkPvKfODAAbZFixasm5sbGxgYyI4bN45NS0uz2i87O5t977332JCQEFar1bKNGzdmt27dWmBbCCGkqDEsW4KGhQghhLx0evXqhfPnz8vOUyaEEEIIIUWD5vwTQgh5bqRrWl+9ehWbN29Gu3btXNMgQgghhJCXBI38E0IIeW5CQ0MxfPhwVK5cGbdv38ayZcuQm5uLU6dOya5NTgghhBBCigYV/COEEPLcdO7cGWvXrkVCQgK0Wi2aN2+OOXPmUOBPCCGEEFLMaOSfEEIIIYQQQgh5wdGcf0IIIYQQQggh5AVHwT8hhBBCCCGEEPKCozn/RcRsNuPBgwfw9vYGwzCubg4hhBBCCCGEkBccy7JIT09H2bJloVDYH9un4L+IPHjwAGFhYa5uBiGEEEIIIYSQl8zdu3dRvnx5u/tQ8F9EvL29AVguuk6nc3FrbDMYDNi+fTtiYmKgVqtd3RxCZNF9Sko6ukcJatQAHj4EQkOBS5dc3RordI+Sko7uUVIalIb7NC0tDWFhYXw8ag8F/0WES/XX6XQlPvj38PCATqcrsTcwIXSfkpKO7lECLrVSoQBK4P/7dI+Sko7uUVIalKb71JGp51TwjxBCCCGEEEIIecFR8E8IIYQQQgghhLzgKPgnhBBCCCGEEEJecDTnnxBCCCHEWcePAyYToFS6uiWEkBcIy7IwGo0wmUyubgqBZc6/SqVCTk6Oy94TpVIJlUpVJMvJU/BPCCGEEOKs0FBXt4AQ8oLR6/V4+PAhsrKyXN0UkodlWYSEhODu3btFEnwXloeHB0JDQ6HRaJ7pOBT8E0IIIYQQQogLmc1m3Lx5E0qlEmXLloVGo3FpsEkszGYzMjIy4OXlBYXi+c+YZ1kWer0ejx8/xs2bN1G1atVnagcF/4QQQgghhBDiQnq9HmazGWFhYfDw8HB1c0ges9kMvV4PNzc3lwT/AODu7g61Wo3bt2/zbSksCv4JIYQQQpy1YgWQkQF4eQFvvunq1hBCXhCuCjBJyVZU9wUF/4QQQgghzpo1C7h/HyhXjoJ/QgghpQJ1LRFCCCGEEEIIIS84Cv4JIYQQQgghhJQYFStWxIIFC1zdjBcOBf+EEEIIIYQQQpzGMIzdPzNmzCjUcY8fP443n3FKVbt27TBx4sRnOsaLhub8E0IIIYQQQghx2sOHD/l///LLL5g+fTouX77Mb/Py8uL/zbIsTCYTVKqCQ9DAwMCibSgB4OKR/4oVK8r2EI0bNw4AkJOTg3HjxiEgIABeXl7o27cvEhMTRce4c+cOunXrBg8PDwQFBWHKlCkwGo2iffbu3YsGDRpAq9WiSpUqiIuLs2rLkiVLULFiRbi5uaFp06Y4duxYsb1uQgghhBBCCLGHZVlk6Y0u+cOyrENtDAkJ4f/4+PiAYRj+50uXLsHb2xtbtmxBw4YNodVqcfDgQVy/fh09e/ZEcHAwvLy80LhxY+zcuVN0XGnaP8Mw+O6779C7d294eHigatWq+PPPP5/p+v7666+IjIyEVqtFxYoV8dVXX4keX7p0KapXr46QkBCEhobilVde4R/bsGEDoqKi4O7ujoCAAERHRyMzM/OZ2vM8uHTk//jx4zCZTPzP586dQ6dOndCvXz8AwKRJk/DXX39h/fr18PHxwfjx49GnTx8cOnQIAGAymdCtWzeEhITg8OHDePjwIV577TWo1WrMmTMHAHDz5k1069YNo0ePxurVq7Fr1y6MGjUKoaGhiI2NBWDppZo8eTKWL1+Opk2bYsGCBYiNjcXly5cRFBT0nK8KIYQQQggh5GWXbTCh1vRtLjn3hVmx8NAUTaj44Ycf4t///jcqV64MPz8/3L17F127dsVnn30GrVaLH3/8ET169MDly5dRoUIFm8eZOXMm5s2bhy+//BKLFy/GkCFDcPv2bfj7+zvdphMnTqB///6YMWMGBgwYgMOHD2Ps2LEICAjA8OHD8ffff2PChAlYtWoVoqKiYDAY+Bj04cOHGDRoEObNm4fevXsjPT0dBw4ccLjDxJVcGvxL0zk+//xzREREoG3btkhNTcX333+PNWvWoEOHDgCAlStXombNmjhy5AiaNWuG7du348KFC9i5cyeCg4NRr149zJ49Gx988AFmzJgBjUaD5cuXo1KlSnxPTs2aNXHw4EF8/fXXfPA/f/58vPHGG3j99dcBAMuXL8dff/2FH374AR9++OFzvCKEEEIIIYQQ8uKYNWsWOnXqxP/s7++PunXr8j/Pnj0bv/32G/7880+MHz/e5nGGDx+OQYMGAQDmzJmDRYsW4dixY+jcubPTbZo/fz46duyI//u//wMAVKtWDRcuXMCXX36J4cOH486dO/D09ET37t3Bsix0Oh0aNmwIwBL8G41G9OnTB+Hh4QCAqKgop9vgCiVmzr9er8d///tfTJ48GQzD4MSJEzAYDIiOjub3qVGjBipUqID4+Hg0a9YM8fHxiIqKQnBwML9PbGwsxowZg/Pnz6N+/fqIj48XHYPbhyv+oNfrceLECUydOpV/XKFQIDo6GvHx8Tbbm5ubi9zcXP7ntLQ0AIDBYIDBYHima1GcuLaV5DaWNLlGMy48TEOdcj5QKhhXN+elQPcpKenoHiUqAAwAFoCxBN4HdI+Sko7uUTGDwQCWZWE2m2E2mwEAWiWDczM6FfDM4qFVMnw7HMXtL/27QYMGomNlZGRg5syZ2Lx5Mx9IZ2dn4/bt26L9uOvBqV27Nv+zu7s7dDodEhIS7LZTegzOxYsX8a9//Uv0WPPmzbFgwQIYDAZ07NgR4eHhqFKlCjp06IDu3bvzUw6ioqLQsWNHREVFISYmBp06dcIrr7wCPz8/p66XM8xmM1iWhcFggFKpFD3mzGeoxAT/v//+O1JSUjB8+HAAQEJCAjQaDXx9fUX7BQcHIyEhgd9HGPhzj3OP2dsnLS0N2dnZSE5Ohslkkt3n0qVLNts7d+5czJw502r79u3b4eHhUfALdrEdO3a4ugmlRtwVBU4lKdC5vAldwkp+Os+LhO5TUtLRPfryauHvD61CgVxfXxzevNnVzbGJ7lFS0tE9aqFSqRASEoKMjAzo9XpXNwfpOc4/JycnByzL8oOiWVlZACyBK7cNsEzt3rt3L2bPno1KlSrB3d0dw4YNQ0ZGBr+f2WxGTk6O6HlGo1H0M3cO6Tbh/nq9XvZxk8mE3Nxc0WPZ2dkALIO6SqUSu3fvxsGDB7F7925Mnz4dM2bMwO7du+Hj44P169fj6NGj2LNnDxYtWoRp06Zh586dfCZAUdPr9cjOzsb+/fut6ttx19kRJSb4//7779GlSxeULVvW1U1xyNSpUzF58mT+57S0NISFhSEmJgY6nc6FLbPPYDBgx44d6NSpE9RqtaubUyq8E78dALD/kQaL3+ro4ta8HOg+JSUd3aMEXbsCANwBdHVtS2TRPUpKOrpHxXJycnD37l14eXnBzc3N1c0pFDc3NzAMw8dC3ICot7e3KD76+++/8frrr2Pw4MEALJkAd+/ehUaj4fdTKBRwc3MTPY8b7ecwDGO1j5BKpRIdUygyMhJ///236LFTp06hWrVqohH8Hj16oF27dvj0008REBCA48ePo0+fPgCAmJgYxMTE4NNPP0WlSpWwc+dOTJo0ybmL5qCcnBy4u7ujTZs2VveHrc4POSUi+L99+zZ27tyJjRs38ttCQkKg1+uRkpIiGv1PTExESEgIv4+0Kj+3GoBwH+kKAYmJidDpdHB3d4dSqYRSqZTdhzuGHK1WC61Wa7VdrVaXil9gpaWdJYmCYeiaPWd0n75YLj5Mw43HmehWJ9TVTSkydI+Sko7uUVLS0T1qYTKZwDAMFAoFFAqXLshWaFy75f4WvqaqVavit99+w7/+9S8wDIP/+7//g9ls5l8/R/qz3LUp6Ho9efIE//zzj2hbaGgo3nvvPTRu3BifffYZBgwYgPj4eCxZsgRLly6FQqHApk2bcOPGDbRq1QoqlQoHDhyA2WxGzZo1cfz4cezatQsxMTEICgrC0aNH8fjxY9SqVavY3juFQgEmLxaRfl6c+fyUiDtr5cqVCAoKQrdu3fhtDRs2hFqtxq5du/htly9fxp07d9C8eXMAlnkZZ8+exaNHj/h9duzYAZ1Oh1q1avH7CI/B7cMdQ6PRoGHDhqJ9zGYzdu3axe9DCADQdH9Cnk2XhQcwbs1JHLv51NVNIYQQQoiLzJ8/H35+fmjRogV69OiB2NhYNGjQoFjOtWbNGtSvX1/059tvv0WDBg2wbt06/Pzzz6hduzamT5+OWbNm8VPQfX19sXHjRkRHR6NZs2ZYsWIF1q5di8jISOh0Ouzfvx9du3ZFtWrVMG3aNHz11Vfo0qVLsbyGouTykX+z2YyVK1di2LBhUKnym+Pj44ORI0di8uTJ8Pf3h06nw9tvv43mzZujWbNmACypFrVq1cLQoUMxb948JCQkYNq0aRg3bhw/Kj969Gh88803eP/99zFixAjs3r0b69atw19//cWfa/LkyRg2bBgaNWqEJk2aYMGCBcjMzOSr/xMC4IUq9peSpcdrPxxDr3rlMKJVJVc354WgN5px40kGqgd7g2FenHulOFxKSEOTSs4vy0MIIYSQkmv48OF88AwA7dq1k13+rmLFiti9e7do27hx40Q/37p1S/Sz3HFSUlLstmfv3r12H+/bty/69u0r+1irVq2wd+9evl6BTqfjR/Vr1qyJrVu32j12SeXy4H/nzp24c+cORowYYfXY119/DYVCgb59+yI3NxexsbFYunQp/7hSqcSmTZswZswYNG/eHJ6enhg2bBhmzZrF71OpUiX89ddfmDRpEhYuXIjy5cvju+++45f5A4ABAwbg8ePHmD59OhISElCvXj1s3brVqgggKd1+O3UPadlGDGtRsVDPV7xAAd3yfTfwz71U/HMvlYL/IvLOz6ew5VwCPu1VG682K55iLy+KUrAMLiEFGzIEePIEKFMGWL3a1a0hhBBCCuTy4D8mJka2JwewFI1YsmQJlixZYvP54eHh2FxAld127drh1KlTdvcZP3683XUlSenGsiwm/XIGANC+ehAqBDi/IsOLNJqbkUvL6hS1LecsK4ws33edgv8C2PqdT0ipsm8fcP8+UK6cq1tCCCGEOKREzPknpLiZzPnBxpPM3EId4wXK+qeR12JkMDm3Ju7LoiQE/Jv+eYD+/4lHQmoh1i8ihBBCCCnlKPgnLwWjIPjPNRQuOHuR0v7Nro/DXlh64/MN/lmWLRGBdUGEn0FXtXb8mlM4dvMpPv3rgotaQAghhBDiOhT8k5eCcDRWX8iR2Rdp5F8u/LqTlIV7yVkuaItzMnKNrm6CXQbT8wttWZZF32WH0WvpYVF2i6tce5SOYT8cw4nbyVaPCTtFXN1XkZJF014IIYQQ8vKh4J8UqbmbL2Lwt0dKXOqz0SQc+TcV6hiKFyj6lwZfOQYT2ny5B62+2ANjCXvvhL47cAO1P9mG3ksPYdSqv0vcfQY835H/9FwjTt5JwZm7KfjnXspzO68tr8cdx74rj9F32WGrx0TB//NsFCGEEEIIAUDBPyli/9l/A4evJ2Hf5ceuboqIMEjMLmTw/yIt9WeWRP9JmXr+3znPOW3dGZ/+dREAcOpOCnZeTOSL7JUkhc0sKQzhFJZz91Of23ltufs02+ZjwutiMpece8xgMuPc/VSYS0DmBCGEEEJIcaLgnxSLkjYiaxB8sc/MtR/830vOwoGr1p0XL9Kcf+nIv3DOeGmYP85Jz3m507dzBB1ZJ++kuK4hDhB2VDzvugj2TP/jHLovPoi4w7dc3RRCCCGEkGJFwT8pFiUtThamsmcWMGe81Rd7MPT7Yzhw9bEoEC5pr+lZCAc5jSazqDPA+BznrDsjNds60C+pbX1ecgVB9Om7Ka5riAP0pvyOitwSFPyvPXYXAKgIICGEEEJeeBT8E15mrhE3n2QW+vniEeOSFSkLi7A5WjBu6PfHMGzlcf7nF2nkX8hgYkXTAIwlNP35amK61baS2tbnJVufH1A/FUzdKImEAX9JCv45ztxKZjNbIl8DIYQQUlq1a9cOEydOdHUzXngU/BNen6WH0f7fe3H+QeHmDpfkQMxodnzkX2j/lfz0f2em/C/bex1j/nvC5RXYzWZWNsWaFZRc0xvNos6Rwrb57bWn8Mqy4qs6fyUxw2pbSZo7/rwt3nUVPb45yP8s7AgoTtl6U6Gm9Qjvw8IW3SwOqkLU8uiz7DCaf7EXBcwgIi+6N94AJk2y/E0IIS+pHj16oHPnzrKPHThwAAzD4J9//nnm88TFxcHX1/eZj/Oyo+Cf8C7njaz+euJ+oZ5fklOwhW3L1BduqThnRv6/2HoJW84lYNfFxEKdS+hReg4++u2s0wXdjCYzOn29D//65qBVsCYM0PUms6hzxFiIgDo9x4D/nXmAv28n41ZS4bNH7EnOsh7ZLskdTrYUVefIVzuuiH7Wm8wOr9Rw7VEGRsQdd3qqQI7BhMaf7UT0/H1299Morf9rKUkj/8KPcqivG/9vR2sRnL6bgvQcI26mv5jZQMRBn3wCzJ9v+ZsQQl5SI0eOxI4dO3Dv3j2rx1auXIlGjRqhTp06LmgZkUPBP7GSkGa7Yrc9wmreJS1DXi+a81/Ipf4K8aIK29EgtHjXNaw5egfdFx8seGeBR+m5uP44E5cS0nFSsu66MMjRm8wwGAVp/4XoxLmdlMX/uzAjqY5IkynuZ8pr67n7qfjkj3PPJfX91pNMzN50AQmpOU4/99qjDNSbtR2Ldl0thpYBmXoT7iVnFbjf8JXHsPvSI7z63VGnjn81MQMZuUbcTsqy29GgVVv/16IvQcG/kM5Nzf/7rgPXTth5o2JKX+cTIYSQUoRlAX2ma/44WAC6e/fuCAwMRFxcnGh7RkYG1q9fj5EjRyIpKQmDBg1CuXLl4OHhgaioKKxdu7ZIL9WdO3fQs2dPeHl5QafToX///khMzB+EO3PmDNq3bw9vb2/odDo0bNgQf//9NwDg9u3b6NGjB/z8/ODp6YnIyEhs3ry5SNtXUqhc3QBS8jxIcT6oAUpOhX+WZTHh59Pw1CjxeV9LT6No5N+JtH8h6fJ4jpj0yxk8Ts/Fm20ibO7zMDUbSRl61C7nI/v44/Rc/t9GkxkqmVFVOcJAeP/Vx2haOYD/WRh86Y1mGEQj/86/TuFov6GYMkAycqzfN66tXMfIk0w9lgxuUCzn5wxYEY/EtFycupOMjWNbyu6jN5qhUVm/T3M2X0R6jhHzd1zBhI5Vi7xt7/x8CnsvP8aCAfXQq345m/vdS7Z08GXkGmEysw4vY+kmCOrTc4zw89TI7qdVKa22iYP/kpMvL2zXnadZiAj0sru/cKlQBz+KhBBCSOEYsoA5ZV1z7o8eABrPAndTqVR47bXXEBcXh48//hhM3mDZ+vXrYTKZMGjQIGRkZKBhw4b44IMPoNPp8Ndff2Ho0KGIiIhAkyZNnrmpZrOZD/z37dsHo9GIcePGYcCAAdi7dy8AYMiQIahfvz6WLVsGpVKJ06dPQ622DACMGzcOer0e+/fvh6enJy5cuAAvL/vfB0or+upCrNx4nFGoNa+Fwb8rpwDcT8nG/848wM/H7/JLoQlHKe0V/LO3zF1hU8znbL5k9/Hmc3ej++KDuJ0XQOcaTTh5JxlJGZagv2KZ/F+85x+kOXzeJEHwv/lsgijgEv7bYDKL3q/CpKULR/6La86/3PsmnaJw/jmsdZ+YZnlfpEvrCVPdbc2/L0wHkjP2XrbUqFiw80oBe+ZrMHsHHqQ4lu0jfGvlMjE4bjIj/08y8juxhMv+FeZ3TVHKEXwWchyom5AlyOah/0AJIYQQYMSIEbh+/Tr27cufFrhy5Ur07dsXPj4+KFeuHN577z3Uq1cPlStXxttvv43OnTtj3bp1RXL+Xbt24ezZs1izZg0aNmyIpk2b4scff8S+fftw/LilePedO3cQHR2NGjVqoGrVqujXrx/q1q3LP9ayZUtERUWhcuXK6N69O9q0aVMkbStpaOSfWEnLMWLoD0exelQzp54nDCALM288NcuAE3eeol21ICieIXVcGF/pTWa4qZUwCAIMLhU/NcsAHw+16Ln2AnxH51M7E/wKOxsuPkxDeIAnvtx6Gd8dvAmGAQ683140Mnn81lPUDfN16NhPM/ODrZtPMrFk9zVMjqkOwHrNdeFrK8x7d+uJcOT/2TNATt5JwZzTSnhXe4IONUMBWEaapaTvl6mYg2t7hEUUM/VGq3sLcDiD7pmplArsvfwIPu5q1K/gZ3ff1GwDlu69hk97RRV4XOG9mJZttPmYVpL1sOrwLXzy53n+Zy7z5Odjd/DZ5ouIe70xGob7F3h+IP8zwzg4DSfXaIKSYWxmzOQIPgs5NjIS9EYzfoy/hVZVy8BDnf/fZsnIdSIuU748cP8+UK4cIDPXlRBCnpnawzIC76pzO6hGjRpo0aIFfvjhB7Rr1w7Xrl3DgQMHMGvWLACAyWTCnDlzsG7dOty/fx96vR65ubnw8HD8HPZcvHgRYWFhCAsL47fVqlULvr6+uHjxIho3bozJkydj1KhR+OmnnxAdHY1+/fohIsKSmTthwgSMGTMG27dvR3R0NPr27fvC1imggQsi69C1JKefI5xXX1Dqd67RhIep4tHG8WtPYkTc3/jP/hsOn9NsZjFv6yWsO35X/jx5X+yFwW1WrgmLd11F3Vnb8cdpcXFDewW/HB35dyb4FaYQa9WWVGkuhZ5lgRuPM0X7JKY5NiXDbGZx47G48F78jfz3VFp8Te9k1kZGrhHdFx/A3M0XRW0GiqYI35v/PYnEbAYjVp3MP6dM8C99v1xV/J9lWdE9n2Wj1sPz6pp4kpGL4SuPo/fSw1aPyaXcy11bOcL7RDryL5xOI037Fwb+wjZ8uPEs0nOMmLD2tEPnB4Bxa04idsF+PqvHnhyDCe2/3Iu+y+NtdsoJVx4QdooJrT12B5/+dRGdFxwQfR5LcI1TQgghLwKGsaTeu+KPk7WuRo4ciV9//RXp6elYuXIlIiIi0LZtWwDAl19+iYULF+KDDz7Anj17cPr0acTGxkKvf37LFM+YMQPnz59Ht27dsHv3btSqVQu//fYbAGDUqFG4ceMGhg4dirNnz6JRo0ZYvHjxc2vb80TBPwFQNKm3BlHwbz8Ke+PHE2g+d7eogv2Bq08AAF87kbK8/UIilu69jvd//YcfERSemwsyDJK0f65S+tSNZ0XHsxv8O/hN35ngPzkrP4BS5v2SFQaRuUazKDgR7m/PJ3+ex+Ld1wAAkWV1AMS1A+yl/TsSvK8+chvn7qfxHTXC+gJGkxnpksDwTlKW1TZ7MmSKMsqlmedIgjVpWj3Lskh18Jo9C+k1ExaV3HUxEaN/OoHkTL3daSXOKOg4KYLXLN33oUxNj0wHlwkU3ttDvjuKg3mfWcD+dBop6efMkUCes/lsAq4kZmD7BdsraVxKSMPM/53H7kuP8CA1B2fupoiCdtG5HShEeCVvJRRA3LFjYktYZVNCCCHERfr37w+FQoE1a9bgxx9/xIgRI/gsvUOHDqFnz5549dVXUbduXVSuXBlXrjj+fb8gNWvWxN27d3H3bv5g4IULF5CSkoJatWrx26pVq4ZJkyZh+/bt6NOnD1auXMk/FhYWhtGjR2Pjxo1499138e233xZZ+0oSSvsnACAq+MYxm1mn0u9FAaSdADhLb8T+K5a5yXsuPbIqdOfoclsAcEQwmp2pN8FLqxIFYtyXefGobH4QII2h7AXujqbDO1PwLlkQNHNtFZ4n12gSpSInO1jN/qcjt/l/Vw/xxvkHaXiSYX0uIC/tX3BOR6YtSLMKhCPCey8/Rv//xOPdmOoY174Kbidlou2Xe1HGS4O/p3UCABy4+hgpWQb0qCtfxEarUojeJ0A+uJSuFy9t+5QN/2DDiXv4bWyLAtPfn4X0vhGOgo9cZakk6++lKbK0f2mnhz25RsvUF86WcwlW+9jKVJCSfjZf/f4obn3eDYB4ZYuCPivSINvRbBHh+3v7SabN/TovOAAA+N+Z/FTJpxnWnx2WZUWvyVYnhL+gsKGwE6EUrjRJCCGEFAsvLy8MGDAAU6dORVpaGoYPH84/VrVqVWzYsAGHDx+Gn58f5s+fj8TERFFg7giTyYTTp0+Ltmm1WkRHRyMqKgpDhgzBggULYDQaMXbsWLRt2xaNGjVCdnY2pkyZgldeeQWVKlXCvXv3cPz4cfTt2xcAMHHiRHTp0gXVqlVDcnIy9uzZg5o1az7rJSmRaOSfAJAPWPVOzt12JO3fbGZxWDClIFiXv8Z2WZ/8fzs6Enjs5lP+39zItqiquME6oBYem5UkYttbgiwj18gX4bNHrgPBVmeIcIQ2P0tB0HlhMIuKx8mtdV+Q6sHeACzt544lTG82mMyiczrSySFdEk14zb/Zcw1mFvhy22UA+Rkdws6Hod8fw9trT/FFDqWkc8YB+Tn/0jna0mBswwnLPNwle67beilFQnq/J6Zbj64npuZY3W+FZWsUmyMsPii8p++nZGPeNusClI4uf2mvc0w4dcBoZmEys/xnTVoA0Cr4d/B3jfB1336aVWAGhPCee5Jp/dmVtsPW59/XIz/4Fx6T0v4JIYSQfCNHjkRycjJiY2NRtmz+AM+0adPQoEEDxMbGol27dggJCUGvXr2cPn5GRgbq168v+tOjRw8wDIM//vgDfn5+aNOmDaKjo1G5cmX88ssvAAClUomkpCS89tprqFatGvr3748uXbpg5syZACydCuPGjUPNmjXRuXNnVKtWDUuXLi2Sa1LS0Mg/AQAYZL70csXyCnMMuQDyXnIWui8+KAp4hcFboLcWD/LWTr+ckF5gYTuzmcXFhPzq908yclGpjKcoQMmRCahtjTKevZeKEauO2zxfjsGMhp/uxLGPOyLI283mfnKZC7lG+SX6UrLzAwluNFc8bcEsGuVNKUQKe3iAB7QqBXKNZjzJyEWYv4co7V9vNNtdqSFLb4RaqYBa0H5uqTjhMWxRK/OzR6TTS+4+zUZ4gPUyMsJl8n6Mv4XEtBzZkf+C0v7zFW+UJg2K7z6Vr57vzMj/mbspCPDSoLyfdTGcgoJ/pYIB8naxvNeW4oMJqdmybXA0Zd9u8C84htHE4pXlh3H3aRYOvN8BXloVcgyCzBNJ+w0ODqELO8I2n32IvZcfYVKnahjSNJzfbqvjUDjyz2UQSPe19VyN4B4WZhyUkNVNCSGEkBKhefPmsh3z/v7++P333+0+l1uSz5bhw4eLsgmkKlSogD/++EP2MY1Gg7Vr19p87os6v18OjfwTAPlf6oW1PZxJv7ccg5X9N2f5vutWwavwy7zwOU8LSG83mVnkGE2iQIYb+ZeOnAO25+sLn//q90dF8+JtEWYbyJELkGxdS+Ec/vxlCYXTFiRp/w6O/PsJKs0rFQqU8dICAB7nZS6I0v7tLPWXnmNA6y/2oN/yeACWFRJYlsU9OyP/QrlGE5QK8drwwgwRW+nmwpHr6X+cx5I912WnI0iX1CuuZQYLIr2/7j7NstqHYRwP/o/dfIqeSw7h1e+Oyj6eXUCavrATRJjlwY3wl/HSiPZ/4kBGC2A/M0Y4TUNvNOPUnRQ8ydDj9N0UeGjE/cyFHvkXnCNLb8KTDD0+/u2caB/plBROkmDkn3u/HB35F/5OuS14byntnxBCCCGlCQX/BEB+yr5GqeADL6eDf7Pt0WPLsa2zCIQjt/YqiQvtuJCIqBnb8OtJcaV+LoApqOCfkLCVqdmOjaqrCqiD4MwUihSZOf/SkX9hwJOabXCoOKOw2nqDCr4o420J/p+kywT/RrP4vRMc/9C1JCRlWgK4hTstKyT875+HoqCHZVmbr+9ecrYoyyAlWy96fbZGsOXS/uVYpf3buDbFvcSe1ch/snXwDzBWaf8n7yRj9yXrwnUr9lumKdxKshzn7tMsbDufwPemZ+vtfzaly11yuM4WrjOIk5JlwDs/n8Lmsw/tHtdePQvh+yzsvFMrGXhq7Qf/0rft+uMMfH/wptVIfEEZDwBw9VG67PYkQZu4a+LoyL/w/b2TlP/eUto/IYQQQkoTCv4JgPwv9Rqlgk+5thf8bz33EHM2XxSNtBaU9i+d9wuIv8wLv2DLze/mvPHj38jSm/B/v4tH/Pg5/5LgWXpskUJ8eVdIlj4xm1lMXncaS/Zcs3kup0b+zeJRW2GgZGbtd4zwz8sLxNaMaooALy0CBSP/RpNZ9L5Zj/znn0+4tCC3CsO/8+byC59vKyi8k5QlKn6XkmUQXQtbc821MveKHMfT/ouXI2n/DGMd5PZZehgj4v4WZQqwLIv9V/Kr6BtNZrSetwdv/XQCey9bCmVKMyakHVK2Rv650fkAycg/APxx+gHGrj5ptV1I+jp1bvlBvcEovqc4OQYzPDXijj+55QaFOn61D7M3XcDqo3dE2x0pTCgMzoWSBGn/3O8nR0f+hZ/J20/zMwto5J8QQgghpQkF/wRA/pd6tUoQ/NsImE/dScbo/57Eiv03cPJOsuAY8l/+OXL1A4QjbcLOA3vBvy38yL/M0l225vkbzGaHgmkhpSTQOnbrKTaevM8XuJN77baCnZQs63nQRknmgnQ00pHl/rgAu6yvOwAg0NsS7H382zlcl1bql8z5F76PwiXOONLXn2WnWNztpEzRsn0p2QaHMjw0MvUR5FhV+xcEvcI5Z8Udo0k7Px6mZst3OAl2E2YpCGsoZOSKp0YI59KfzVsaUzoC7ucpDuaF10F473FL+nlr1XBESpYeb/z4N7adt6wQIO3EEtaByLXx+yIj12j12TeYWIemaFwW1PQAHBv5t7WPeDlKFvdTshHz9X7RPrbT/oUdYvnTB2jknxBCCCGlCRX8IwDyv9SrlQwYMKJtUp/+dZH/t+gLtWDE+D/7bqC8nweGNssvxOVeQPCvF4382w5w3dQK2aXOtp1PRGLacTSu6M9vm/zLaQR7a23OKWZZoMXc3djyTmub55OSdiRIRyPliidKg4qkjFx8sfUS4gVLFcotS6g3mq2C/6eZelQqY10kT+583Ah6lSBv/rEt58Sp3QaTWVS4TBiUXU3MsDr2TckSa/aKxT3N1IuD/yy96L6yVcBQ2sFgS47BJGqvMOHE2dUqhDLzAtaC2pFjMOG174/BK28EPFinRWq2ATkGM249yUTV4PzrzkC8ukSG4L4RjtRLg1dhZw8X5EtrHfi6q0X1KoQJEMJ7LyvvvfLQOFbIc8HOq9hxIRE7LiTi1ufdrDo00nONuJOUhQoBHrL3PWC5lnKB/o3H1vcWIO4QK+crLnYofd1ybC2DKKxroDeZsXyv9QoQttL+bdUMoeD/Jfff/wK5uYBWW/C+hBBCSAlAI/8EgGDkX5D2LzcKtuNCIk7czh/tF47QSzsLpGn5CplAShT8Ozjy76mR77N6mqnH7kuPsGjXVX6b0cxiwIojdgv5ZeQa8c3ua04FnELCgNNoIwVeem2+2nEF6/6+h4ep+Wn13HGt5vznbffNK+LHBUeHrj1B87m7rOaMm8ws30HBzf0f0rQCvPMC1LRs8bXVG82izhFh54at+dNCmXZSsXNNZlHaf2q2OO0/NVu+gKGtaQTSDqQco1l0POGItzBQLGhJOAD465+HeO2HY7jwIA31Z+/AsB+OFfic7RcScezWU+y+9AiA5fNTp7wvAODE7WTRdZUW/BPe48LgOEcyn/+WoLOFS7OXdhC42wnmRcF/3jXx0DoW/CdJCm9KO1T0RjPafLkHey4/sjm1JsNG8N9JMurOZUJcfZTfKaBSij+Tjoz828qyEaX9m1jZz7vNkX8by19S2v9Lrl07IDbW8jchhBBSClDwTwA4Nuf/y22X8MaPf4u2pQmK5NkrBgbIV/TOEa03L64yzz1HWjldGuhUDhSPgsutpb4q/rbdtm27kOBw8P/Oz6cxdeM/yDGY8M+9FPH8akkKPUd6LeWqwctNUVh/4i5/jYLzlhfkUuVf++EYHqbmYETc3zCazBi16jg+++uC6Fzce+mmVqJr7VAAwCPJGvR6Eytaao2b85+tNzk0xSDTzsh/rsEsGuFOzRKn/dsa+beVdSJcxQCwdJiIgn8zi083XQDLsuLq8w5kAYxbcxL7rzxG10UHoDeacfDakwKfI+1U0CgVaJKXeXL8VjJyBG1jwIjulQxB8C+3PCXnmiAY5p4vDYLtFUhcuPMKbj7JxJEbSVi+zzLabasDTUotCb65a/2vumVF25ftuW7zPcvINdoMnoW4TiThVBPpSL+tkX+DyQyzmcXcLRfxy/G7svsIq/0bTGb4eVjXPcgxmGQ/v8J6BkI08k8IIYSQ0oSCfwJAMvKvtJ7zf/puCpbssQQO0TWDEF0zCIB49NLeGuCA/Lz7bJtp/5bjfnvgJlrP24P/HskP3qUpy34eGiwcWM/uuQuSkmVwamL42mN3Mea/J/Cvbw5hw4l7/PZco1k20JQbMZWSG/kXdo6UyZu3z6XRC0dT9115jJ0XH+HbAzdFxxYGhVxHwMWH4nnU0pF/rhNGbvk3ubn4GXbm/OcaxSP/KVYj//LBv617yVcSsGUbTMg1ic//3cGbuPEkUxT8ZzmQLi6noHnpwlUVAMtIdaOKfgCAv28/FWWJsBDPc8/IzX/twvZJA1xh8M+9N9J95OppcE7eSUGPxQcxcMURvoPJXqaAkLRTgXtfdO7izoNMvdHme2Yr7V+Kmz4ifL3C3w8pWXqbI/+ZuUZsOZeA/+y7YbO+h3Dk32Bi4a6xvpfvPc1Cw9k7MGX9GdF2uQKmAI38E0IIIaR0oeCfAMgPTtUqJj/tX/BFmys01rSSP74b1hgRgV4AxAXbCgr+5QLefVce4/0NZ2Ays5L15S2BwBdbLwEApv1+jh9llQY67moletYrh2HNLfUFbM35LYjJyUrxe/Iqr2+/kJ92b2vkMFfSJm7k31urwustK+Y9N2/k38ZwIrc8m9xI+x1BJgE3cqxgxFXgufe1oIJ/3PvwKG+qhJdWhXphvpjWrSZCfNyszp1lb+TfaLJb7d/myL+Ne8nPUzzyz7LiYnkcs5kVrwlvp4OCUzHAw2pbQcs/SkfG1UoFapfzAWB5T4Sv3WBiRWnlaYKOM2HdCGmAe00wN95oI/gP0Vm/L0LSugyOj/xLg3/L+T00KtG9lZlrhN7GfZuZa+SfF1MrGIHe8vOjuWt1W1CtP9tggtnMYuPJe6g3awe+3HpZ9rnpOUYcu5kk+xhH2CmQkqWXnQr0IDUHaTlGrD9xT5TVYSuriUb+X3J79wLbtln+JoQQQkoBCv4JgPwidcI5/8IA7EqCJRW3fgXLqKbO3RKEpefYT/sXV5GXD+jW/X3Pat4416kQ5u/Obzv/wDJiLe1E4DoDuMr2heXI6GRBbKb9Syr4P8xbQm/PlHaoGarjtwO2Rxl98655cqYer68Uz0dPENQO4DpONCoFGMY6+Odw8+cNkjoF3HvBjfxXCfLC7+NaYlTrygiVCf7tFfzTG82i7JCULL3oWtgKrm2n/Vunas/edMFqW67RLAqoswwFrx4h12mUnCWe8y5N85e+1yqlAv4eGigVDFgWGPCfI4Ljm2SzWwDJyL80+BeN/JvzXk/+PmW8tBjTLsL2C5Nhb86/8HMgzPTYei6BD9A1SoWo/kKmXr7TC7AUBeSmkgxvUZG/j632y+GC//zOqSfpuWg9bw8mrzvDH0tOpt4oqhVQEKOZxbcHbtrd57Eg88VWwVAT69hUIfKCevVVoHNny9+EEEKeSbt27TBx4kRXN+OFR8E/AZAfuKuVCj7VVxiAXc6bh1s9xDLir5MpHif35V+0lJ+dzIBUyQgwFwhkC4qfcXOBpQX3uBRme6nPz0uu0SQ7P1h4Le8nZ4NlLdMXAjw1/PXOMZjBsqzNUUZPreWarzl2h8864AgLB3IdJ9KUdGnKfs1Qb75twg6HeVsv44utl/iRUeFIbXk/69Fxu3P+jWZRx86lhHTRffA007mCf8Lg3zvvepy6k2K1X47BJAqQHRn5l1tDPkUS/AvbxTDWBeI0SgYKBYOAvKr8CWn570uupDihsONMGPznSEb1U7Ots2u4bePaR+DI1A4Frv4gZa/av1zNCAAY/d8TWH30DgDL7wlhAc9svclmh01mrpHPWFApFTbP/TRTj3FrTooyU7ZfSMT9FOvMDqvnZuhx5Ib9kX9nXUu07nSRorR/QgghL7sePXqgc+fOso8dOHAADMPgn3/+eebzxMXFgWEYMAwDhUKB0NBQDBgwAHfu3BHt165dOzAMg88//9zqGN26dQPDMJgxYwa/7ebNmxg8eDDKli0LNzc3lC9fHj179sSlS5f4ffz8/KBUKvnzc39+/vnnZ35dzxsF/wRA/pdbjXDOf96XeZZl+cC7Wt7SZd5ueSP/gnnLzhT0k5Ku956eY0kPF8475+bsSkdo3dVcUTvX3845Bhtz/gWB0a28kc0K/h5gGIbvtMg1mmzOVwbyg3+5lRD+PPOA/zdXhFE60q+VXJ8aeRkHepPZaqrBsr3X+WvPTTcAxJkYnEw78+ktc/7zH7+fkm2V1p2eY+A7GqasP4MPf/3Hdtq/oOBf/XA/2KrRmG0widP+HZjzL7ePdFqCsF0sa52hoFJYrrHwmnFyjeIAOcNG2r+04J8Qd38cuW4JdGuX9YFKKc7wcISHRoU/xrXEKw3Lo3udUNFjwjZK0/757SpGFBBn6o02l8nLzDXxBf+UCsZmvYFfT97DX/88lH2sIEN/OFbkgbhwuoXBxsEp7Z8QQsjLbuTIkdixYwfu3btn9djKlSvRqFEj1KlTp0jOpdPp8PDhQ9y/fx+//vorLl++jH79+lntFxYWhri4ONG2+/fvY9euXQgNzf/eYzAY0KlTJ6SmpmLjxo24fPkyfvnlF0RFRSElJUX0/O+//x4PHz4U/enVq1eRvK7nybGJn+SFx8/5VzJ8kMhtS84y8EEQN9efK/glWupP5ptwSpaeHzm2V3FduvxcRq4RiWniqvTcsmNWy5ypS9DIv605/4JtZ+9Zpi9w6f7CkX9b8/0B8Ev1FYQbFZYWa5OO/JfNS+HP1psgFzvKjfyHyYz820v7zzWY+Mf9PTV4mqnH4eviEdrPt1zC6qN38FHXGlh/wvo/DiFunXvA8vrK+3mI6h1wcgxmUfDPzR2XW24S4LIfrK+9dLUD6Vr20ntanXfNA721gCSOzZV0DNlM+9fb/pwYjGbceJyBG08yoVYyaFW1jM197fHUqFA3zBd1w3zx5bZLoseEbbTVp6BRKkSdBCwrzj4RSs81wsRnFjE26w08yZDPApEjXTbR3pQdtZIpcCWSad1q4tO/Loq2XU0U1lqwlfbvQGMJIYSQQmJZFtnGgjPgioO7yt2hwYXu3bsjMDAQcXFxmDZtGr89IyMD69evx5dffomkpCSMHz8e+/fvR3JyMiIiIvDRRx9h0KBBTrWJYRiEhIQAAEJDQzFy5EhMmDABaWlp0Ol0ojatW7cOhw4dQsuWLQEAq1atQkxMjChT4Pz587h+/Tp27dqF8HBL7bDw8HD+OUK+vr78uUszCv4JABvV/vO+3HMjwH4eaj7A5kb+02RSkoU6fb0f8/vXRZ8G5W1+gQas536bWeDGE3Fhuqd5S3VJRxjd8kYSpWnurmBrzr+weOKZeykAgLrlLYXhuGuaYzTZXRLN0SJtXCE5afAv/TmAKyCoN1plCQD577so+Pd3LO3fW6tCeq4RGblG/j5qWy0Qv526j+O3nor25VLJ52y+ZHUcKemc/0plPG0E/+K0f8DSAXApIR0fbTyLad1ronXVQOQYTNAoFbIp/4Bc2r/4/ZFmoajzOhfkitrZS/uXdlTYYjCzOJTXedK4oj//OQQsnWD2niskHH33kNxXXPC/8eQ9/HxMftk8jUph1VlyL9n6fQDyCv7l7atSKGyO/Mstf2mLh1ppN+NEyNdDI1vcjzOjRy10iQq1Cv5P3E7m/22rU47S/gkhhBSnbGM2mq5p6pJzHx18FB5q6+99UiqVCq+99hri4uLw8ccf8x0G69evh8lkwqBBg5CRkYGGDRvigw8+gE6nw19//YWhQ4ciIiICTZo0KVT7Hj16hN9++w1KpRJKpWSqq0aDIUOGYOXKlXwgHxcXh3nz5olS/gMDA6FQKLBhwwZMnDjR6jgvItfnSZPnjmUtI2VGk5kfMeO+3KpV+QX/0rINOHIjCY/SLF+c/QWjrjou7T8v0Dx28ym+PyhfQIsr1iUcfauTF/hyuLR/nZuKT+W+JinglZShh9nMWs2zzh/5L/h29tIWb3+XJfiXmfNvyp9CceZuCgCgbpgvgPzgP7eAkX8vB0f+89P+Jb8IJQE+F0hn5BpliwzyI/9e+e+7XNq/3Mg/VxBSmDbPzUu3VeHfEcKRfwCoHCg/192S9i9uV6beiL7LDuNyYjomrD2FlCw9Ovx7L/r9J95mICkt+Ccd6Zd2fHBp8rbS/nON8iP/wvNznVtlZYorGk1mXMubghMl+QytecPxLweeWmHwL75PLEUaDZi87oyoZoGQ3HQAaZYER7jUn8rOyL8wc2B0W/sFDN1ljqFWMgjWWV934VQROW5qJdxkOg4vPEzjs49sZS3RyD8hhBACjBgxAtevX8e+ffv4bStXrkTfvn3h4+ODcuXK4b333kO9evVQuXJlvP322+jcuTPWrVvn1HlSU1Ph5eUFT09PBAcHY8+ePRg3bhw8Pa2/D44YMQLr1q1DZmYm9u/fj9TUVHTv3l20T7ly5bBo0SJMnz4dfn5+6NChA2bPno0bN25YHW/IkCHw8vIS/ZHWGygNaOT/JfPR7+fx6wklHvrcxroT9+HtZpn7K5rznxckLtp9DYt2X+PT0wMEAQ2Xgp6WYwDLsuj/n/gCz819ge5WJxQfd62JFp/v5h8TBqxaE4tsg4kfCdSqFMg1mpGUqZedD80F/+4OpP17aVV209Sfla2l/rgR34S0HCRl6qFSMFZp/7lGk83siO9ea1Tg2uzhAR64nZTFX0urtH/Bz25qBf8eCpdiE+LSsIWj2MHeckv9Wb8n3LGF19rf07pSv7OkleIr501DAYBG4X4o46XF1vMJyDWYrObwC9uZnGVA63l7kJ5jxIPUHCTbKDxoNeffaD/4V+Ut/adRWqfJZelN4uUsBc8VdlRwwX90rWBk5prw68n8qRBL917n/x0heO2AZSWOnZPbInr+PhREGIBLg3G90Wx3pBywnkIC2F65QXgPqOzM+eeMax+BDjWCsHzfdZv76NxVonogABDgqc3ryBBv93FXW00TEHJTK63qYXD2XX6M/o3DbHbKUfBPCCGkOLmr3HF08FGXndtRNWrUQIsWLfDDDz+gXbt2uHbtGg4cOIBZs2YBAEwmE+bMmYN169bh/v370Ov1yM3NhYdHwZkFQt7e3jh58iQMBgO2bNmC1atX47PPPpPdt27duqhatSo2bNiAPXv2YOjQoVCprEPfcePG4bXXXsPevXtx5MgRrF+/HnPmzMGff/6JTp068ft99dVXiImJET23bNmyTrW/JKDg/yWjYBiYweDMvVTczEurz9SbRHP+NZKUl4sPLXPUywhGgLmRXena5fZw86VbRARYpW/zwb+SgVGtQLbBxC+1VTnQCxcfpiEpM1d2OTZnqv17uamANIeaWyh/nnmAHRcSrbZzQSOXRRHoreXbKxz5lxthnNGjFqJrBeN0XsaALVznB5dFIR3pF76vbmolX0AwM1e+w4ILAIWj2AoFg061gnHqTjICvd1w8WGa1TKNgPVSkBqlgt/2LKTHqBiQ/5+GRpWfUi6d8w8Auy49Ev0sHHnnppi4qRWie0wa/Es7SaQdSVxQnCtzPaWFGm1V++fa7aFRYea/IvFmm8rYePIe/rNf3AstDf4By+fXEcLPijQYt4z820+pV8tME7ElM9fIT8lRKWxX++f4eWgKnMLzVpvK+HrHVVFmQhlvDeRmzbiplXBXK20WfXRTK2Q7MwBgz+VHluDfxnQcCv4JIYQUJ4ZhHEq9LwlGjhyJt99+G0uWLMHKlSsRERGBtm3bAgC+/PJLLFy4EAsWLEBUVBQ8PT0xceJE6PWO1/sBAIVCgSpVqgAAatasievXr2PMmDH46aefZPcfMWIElixZggsXLuDYsWOy+wCWToUePXqgR48e+PTTTxEbG4tPP/1UFPyHhITw5y7NKO3/JcMt0fdAsHxWcqaeX55OLRj5lxKO3HpqlPDM+xJfZ8Z2h84trCsgHZVOFVSo51Jwn+aNPFfISzV/mqGXndMsDaLtKe60f7nAH8gP/p/mpZELOz/4gn9Gk+wIIzev217blQqGP87avHna9kb+3dX571+m3ih7Xu5aS1PYVwxtiMMfduSXs5NN+89rMzfQrVEpHC5YaI/0GMIChGqlgp/6kS0oNMiZvemCzePeyKvsXt7PAxM6VuU7FZKz9Mg1mvAw1fJ5kXaS2Br571Gn4J5gW2n/3HV3U1uq+FcP8Za9tyNkpjzYqs4v5SPoRBFOAQAsGToFj/w7vrqAmc1/TSolw3c62WtbQdkBYf4eiJ/aAUsGN+C3BXppZZ+nVdnvcNCqlVAoGNHno2klfwDAgatPYDDJT+UBaM4/IYQQwunfvz8UCgXWrFmDH3/8ESNGjODn/x86dAg9e/bEq6++irp166Jy5cq4cuXKM5/zww8/xC+//IKTJ0/KPj548GCcPXsWtWvXRq1atRw6JsMwqFGjBjIzMwveuRSi4P8lwwVP1wXF9J5m6sUF/2wE/wGe+UEgwzBoVyMIgP0q/kLcF2iNZI1wIL9InVqp4FNwuTXgK+QVmcvUm6wKsFmeYzmWI3P+iyIABYA21QKhsrXOnADXtlyjGd/uv4HXVx4HIO5I4QI7W1kUXJvtBf/uaiW0kgBROnqqFaX9C0f+jTbXMffQKK2CNYaxBErca5Mr+MetBsHRqBR8xxOnnK/j6WQc4TVgAJQVHCM9x5BfPNFgQkLeHPJPe9Uu8N64kbe2vKdGicmdqmHGvyIBWKYHvL3mFJrP3Y1z91Ot7vUMyZQHLviuXc4Hu95ta/ecGTbS/rlAWTiNRW5E39fDehqFqoCgvHKgJ07+XydJR5D4fTlyIwnvrT9j9zi2fkdISYsEqxRMgdNz/Dw0Be7jpVWBYRhUDc7PfijjpZW9TlqV0m5nAtfZKDxno4p+CPDUICPXiL9vJfOfjxCdZdoL1/FCI/+EEEKIhZeXFwYMGICpU6fi4cOHGD58OP9Y1apVsWPHDhw+fBgXL17EW2+9hcRE+QEzZ4SFhaF3796YPn267ON+fn54+PAhdu3aJfv46dOn0bNnT2zYsAEXLlzAtWvX8P333+OHH35Az549RfumpKQgISFB9Kc0dhBQ8P+SyZ/nnR+0rD9xF4/SLYGSRmU9Ks8Rpv0DQJfazi13IexgkOJG/oVZAdzSfsE6N/5L/f3k7Ly25HdEcAGYIyP/jgb/nWoFyxYP4/SuXxZXPu1iFdAKTYquhnc6VgVgGfn/bHN+NXFfQREyYUCblLeigSg4ywtcpCO0Qu4apdXrtz/nPz+oN5hYmynRclXrOdz7mCkz51/nJk7P1ygVosr0gPX95Ah7rykxLVcQ/JtxL+9eqeDvIeokEOJWXLjxxDLyz11rLjMjJUuP7XnZHEv3XrM757+MlxYxkfmfCbm0fKEMwcj/lcQM3EnKymu7SdQWAFBJPjOvNqsge0xb6eucsj7uVrUXpAHzl9suF1hJX61U4OOuNe3uAwBeknoCKqXC7n0MWD4bBQX/3L1bMSA/+8FkZqGQWZJIrWTgobb9OeU6hjwF19tNpUT9Cn4AgOuPM/jMmM/7RmHzhNYY3rJS3jntNpO86O7dsxSTkFnbmhBCXkYjR45EcnIyYmNjRfPhp02bhgYNGiA2Nhbt2rVDSEgIevXqVSTnnDRpEv766y+baf2+vr6yBQEBoHz58qhYsSJmzpyJpk2bokGDBli4cCFmzpyJjz/+2Oq1hYaGiv4sXry4SF7D80Rz/l8y3jKjx/89kl+p0jLn31bavzgQjK4ZjOaVAxB/I0l2f6HZmy7wQQ03Ojm3TxSmbjwLQFjwTwGl2fI4t7Sfl1bFL9fFzfH11CqRF6/xtQTkKnaPax+BJXvyC4c5kvb/Rd8o9KpfDu+t/wf/O/OAP86OC4m4krf2t3teqrDOXc1nLUipVQzUirxlEyVRgjAA06gU8HFXIzXbwI9We2lVeGq0dH5w74e9pf7c1coCl/aTFvwTBjsp2fJzruSq1nP44D9v1FqpYPiCdtK5+WoVY9Xx4kgBwO3vtMTlR1nYf+UxKvh72F1vNjEth78Hsg0m3M1beq68nzvK+rjzo/tClcp44sy9VMHIv6WN+cF//rz8zWcTrOaUc6/9g841MKad/Qr1UtIaAO//egY/v9mcrzngJhr5z3/vqgR54dNeUbLHlHYSSDnSQeYItVKBN9pURp8G5TB/xxV+uUYpz7wlHzlKBSN6Lb3rl0OnWsEYuzo/Xc/XQ1NgO7nfY8J7OiXbIBv8MwzDLwcqhzuXh+B3g1atgL8nt2KFnl+C012tRK2yOpy9nwIAoNifEEIIyde8eXOwMhV2/f398fvvv9t97t69e+0+Pnz4cFE2AadZs2aicxZ0nNOnT/P/LlOmDBYuXGh3fwBITk6GTqeDQlH6x81L/ysgTilouTi7af+SkVo3tRJr32zm0Hm/P3gTZ+6lAsgPZgc1qYChzcIBCIrUCUb+ufm0nloVHzg+yZuL7KZSYnTbCFQu44m+DcoDgFXF7q8H1MWbrcUBmXREWkrnpsKAxhWgVSlFaf3NKgdgRo9I/mduqTEuDViOcOWEHEmtAmnBQ+7acssiCucoc8dQKBhRwC7UqVawVcAkXYdd2KnjrlZCJbjWqTaWaQu0G/xbrg83gi1sszQjQm7kX9qZJKdSGU/0qFsWX/ari7fzsihsMZpZuGssr+dhajafzVDW1x1lfeXfp4p5yw9y+3Kj7b55gZ+0xsTW8wmin//h7mknCuBxpB1CR248tZxTb53JIhydtzeFoaCCf3Lp70EyKzgUhHu9AV5au/PppaP80vZ90bcO6lfwFW3z9VDbzD7KP27+/dUtKhQAMLJVJatpBoBl6oGHnc4E7joLP1talZL/jCZnGfiRf65zRZn3nz+N/BNCCCGkNKHg/yVTUNq7veC/MHO0bZ2DwwUjfNq/irEK4r20Kn6kj1sBwE2jxIddamD3e+3gk5dCr1UpRF/+hfUDOCEya6cLaQTZA0pB8C+9LlxasnDOsZRKUERMuiyZdO3xMpJAWKNUoF6YL0J93FCrrI7f7i+TKv9Jj1p4L6Y63CTv21PJ8nXSOf9AfiaEreyFMt62R+e595HrZBBmJkhH/jUqS4FBYZmEwqT9y5mQ1ykwpl0E/7quPbJkaATlrapgK+2/UhlxGhj3Gry1KtH7XxBbn5nZvWo7fAwuy0Juzr9K0NNsrxK+uoAeaXeZjoMKAR6Y37+uw+0EJB1JdjJSvCQdPtJraqkFId7H110NhYKx28kh7HD4ekA97JvSDi2rlJEf+Qcj2r+8n/he4M7jIXgdWpWCr6mQnKUXLYUK5Hdi0Jx/QgghhJQmlPb/kvHW2h/51ijFy14FeWsxvGVFlPN1R5i//FIjwnRvR4hHMfOWp8s28ueXpnZ7aJR8xsLjdEtAKw10gbz0XpWSD57USuslvAqahy0MkFXS4F8pE/wHeds8lt5k5jsmHqbmiB7z85Qf+Ree79cxLWBmWVFnSVQ5H9x9mi3a9/W8+cfSjg5ph4Nc54WnVsXXVpAT6GW7s0S63JswwJJOL9EoGTAMA283Nd/RI70GzuLuuHc6VkXnyBBUD/HGrycsc2+5+f5coBdg41zSDi2PvJFqhmHg6662e22EtDbS7Yc2C0ffBuVQa/o2AED/RuVhMgO/nsyfI/xmm8pYsf8GX/QvRy74F3xm7I2KKxSM1edR+LOtufR9GpTH/848wJ7Lj+2+To7wnrQ38u8lHflXKCDNBhQ+v0aINz+67q5Wyi7tCUD0O0KjUiA8b+6/XH+NghFnPMzpHYXbSZn4vz/OA8ifLiTMUtCoFHwH3caT9/nt3PvAdcaYWcc7iMgLaOZMIDUV8PEBPvnE1a0hhBBCCkQj/y+Zgkf+xUte1Q3zxdh2VdCzXjmbzxHO3d7yTusC26CWCUKFAbs0uPHUqvgRam7JNVsBh3C0kFtVQNiXUFlmeTQh4bmFo5QaSRYBF0zYG/lPzTbwo5pcMMqRzneXBv8qJWM1PxoA6pb3Ff08o0f+siXSmgdPMqQj/4K0Zn600/7carsj/5JIy0MQPLmplaL7iPu38P6T68ApDKWCQa2yOigV1nO7Q/OCe7kik25qhVXFfOH1EBZlDPDU2E3tt/eYh0aF5pUDAADj21e12pe7F7IMJrAsKyj4J76XOQWlxAs71/47sil+HdOC/9ne3Hdhu4a3qGj1eK1Qney+dtP+BaPpDGPpnIiuFQxvrQptqgXmbWcwvEVFNK8cgNWjmgqO63zftPycf3EbQ3zcRKuN8HP+RSP/StnVFLhry/1uoJH/l9y33wJff235mxBCCCkFaOT/JVNg8K9SiFK2qwfbHtnmLBncAKNWHcf/da/lUEE98Qi6OJBRq6xH6720KnjlZSycf5AGAKhsYwTf8kU+f+UAAKKRxoKmLmhsjPyrlIwo6OKC/2p2rk9qtsFqnjvH110S/EvS/m0VbhNOATgzPYbPLACsi7n1ri9ea1742riOgILeL/tz/sVtTBBkN2jzVo3gquPnB/9qANl524qm+JyQtEOBm8fdJSoUC3ddRXiABz+33l2ttJp+IXxfLHO+LYUAde5qmFnWqto/p6A5/ytfb4yMXCPKeGmhkcx754J/lrXUHnicV9fCR/A5FI/8279uaoUCOXml6Hw91KKijbYyFCyvIf+4fRuUxz/3UnDyTgoAYMGAemhfIwitvtgNsOJVIOwV5xPWGOGmJPi4q3F8WrToc84trSj0RutKOHrzKW4lZeHiwzSb5xCSKwipYMQdgMHeblAy1pkUwpF/rUoh+7uSG/GntH9CCCGElEYU/L9kCgr21EoFmlcOwAedayDER4sedcra3R8AmlTyx5lPYsAwDG4+KXi9S2EgIy1AppWpOeChVVqlD9cUjEIK2SqSln9u+0GaeOQ//99qpUKUCcBlLAR5a1E50FO2knyYn4fVevccaeHFMpIl9aSj6py6Yb42jyFs+4ddaliN3mpEr83yd0q2fKE/W+0StVHyPlUL9saTDMvKD1q1ElqVEumwpLJzHQXCgEqjUkDB5Bd2LArSQJR7n3zc1Tj0QQekZBvQYPYO/vw+ktoEwgwM4ci/zk0FvdGMZBuFEQtaYs9Nnb8Uo7TTxNddDYaxBP8XH6YhU2+CVqUQLWMnnPNfUEeDWqUAcvP3FdZWsLWko/Q16NxVonYqFQx83NU4+EEHGExm0e8R+2n/+fupZKb72DO8ZSUMb1kJryw7XOC+HLmPDcOIa1ro3FWikX/u38IsBa1aITsthU/7V4qLkhJCCCGElAaU9v+SEQawfh5qqzR4Ll17TLsI9K5fvsBgmcONuJX3c7c5v5ojLvinsnpMmtbspVVZBbo1Q+VH3IXPlQanjhCOqgqDFWktAm56AcMw2DaxDeb2yV967ccRTTC2XQSGtagou7rA2HYRVoXmpMG+WWaZFMCyWsHe99ph/5T2VsXThO/VgEZhVgGWMLjj0qO5wngA8O9+1kXfHB35j64ZjNeaV+R/5kb+pefWSYJ/1TMsmSLXPSLtTBKmuSsU4uwNBcNApVSI6hMIR8mFad86d7XdjjNnqv1L70tPrYrvpDh605KVUCNUJ3o/NSrH5vwD1rUqhMdJtdPZI7ydvN3UotfEdaT5uKutln+0FfwrFYyoKr8zBRSFpJ0l9lY0sLXUX5rgdTMMIxr55wiX+tMoFaLOH+F2IP8a08g/IYQQQkoTCv5fYqE+7lg6pIFoW0GBRUHUSgUOT+2APe+1E80RFpIrnMc/X8VAK9imYCz7eAkKFaoUDKoE2Uv7tz6PPa2rlsl/jo05/yolg7I+buhYIwjd6oSK5gerlQr45o0gB3hq0KZaIN7vXANuaqVV1ftBTSrg/c41rNog3S/bRqEzwLI8XYUA6+KLBsG6Y3JLOgqDJi5I4ka+fdzVskFcoL2Rf8H1mRJbXZTl4KaWBP95/xZOk9Dk1TWQmhRdDS0i/PF6Nduj1LZI6x5Y/Sy4P7gzc8v6AeKRf+GUAJ27WlTToLKk88aZ4F96XwZ4afj76fgtS/AfWVb82RFV+7dTBR8AMnPzR7ml0xoycuVXdQDESw96u4lH/u110rir5TtF1EpG1GEiV3fBEdLOEnudCKPbWZb2/Ffd/IwlBQOkS1azaFzR3+q5oqX+1EqrqTlAfgcbF/zTyD8hhBBCShMK/l9iZby1VoGII+m4BdGqlKhUxhOb32mNczNjMa59hOhxtZ3gX6NUioJGT40KDMOIgtmKZTxtzntuHhEgex7Lz9ZBw4VZsahT3ie/PYIAQDg6qM4b+f9+eGMsGSzuMAHylxCsJQnapMvbyY0mAkBMrWAMahLG/5xrcD7wFc5Hlwu0hJkL3D+Xv9oQDcP9sPaNZlajpt5ald37QZg67e+psVqOTiMz8t+5dgi/jWXFo9ScMH93rBreCPUC5COrGiGWDoQ+DayLUEqXh5NmAggDR+56CLMzhJkOFQRp9zo3tSgtvHf9cvioa34njjPBv3SJwwBPLT/ffG9etf3aZX1E+zgz5z9TkNrPZS+MbhsBrUqBtztUtfm8HME9p1YqRJ8XpZ3Rdlsj/2qlQhT8F3bkX1ojwV5HRL0wX5ye3gkLB9bjtzFg8GabygCAbnVCAViWN9w5uQ1OTIvm9xOO/GtV8kueStP+aeSfEEIIIaWJy4P/+/fv49VXX0VAQADc3d0RFRWFv//+m3+cZVlMnz4doaGhcHd3R3R0NK5evSo6xtOnTzFkyBDodDr4+vpi5MiRyMjIEO3zzz//oHXr1nBzc0NYWBjmzZtn1Zb169ejRo0acHNzQ1RUFDZv3lw8L9rFfDSWb6y965e1ChKfdeRfykurQufIUNE2YVAhDYbbVg8UtYEbbRWmZofbWHIQAN7tVA0jWlZCTK1gq+wAuerhHhoVNEpBhXd3+eC8oCyCemG+WPNGU3wlWS+dYRjRqL6t46uUCsztU4f/ObsQwb9w5L8gXMdG84gA/DqmBWqV1VlNNbA36g9YjzBLl6MTZnBwgVRUufyg1lOrkg0qCxoh3ji2BTa93QqxkSFWj9ma8+8o4TxvrkI/YHk90pUAhJ0GjmaZANbX1c9DLbo3Azw1ok4SQHxNHP2MCu//D7vUwJlPYqw6p4Sky+oJz6m2N/JvI/jXKBWitH9bdSwKIu2UUtnpiAAsHR7Cjq4gby26RoViz3vtsGBAPX57lSBvBAg6e0Qj/zauMXcdKO2fEEIIIaWRS4P/5ORktGzZEmq1Glu2bMGFCxfw1Vdfwc/Pj99n3rx5WLRoEZYvX46jR4/C09MTsbGxyMnJryw+ZMgQnD9/Hjt27MCmTZuwf/9+vPnmm/zjaWlpiImJQXh4OE6cOIEvv/wSM2bMwIoVK/h9Dh8+jEGDBmHkyJE4deoUevXqhV69euHcuXPP52I8R5Nrm7B0UD30qlfO6ktuUQf/gHWAL0zjrRWqw9QuNVArVIdZPSPRtlqgaGSTCx6EI4hhdoJ/lVKB6T1qYcVrjaxGGrngTboGvVown1pYAE4YDBcUcDAMgxYRZRDk7Wb1mLDIXYCdOfRChQn+c21UopejkAnEjJIcZuncbilhKrW0NoRWpZRN+2cYBr+NbYH/614LLauUkR35t1XvgOOhUaF2OR/Zyu7Sefn2gn/u6SbB6xYGvBGCehjXHmWIju3joRHdK858bgIF94il00QhCjxHtKpktRSkOPh3rEOjVz1xsc6CsnpyJPecsEPD3v3vLpmmw7GM/AsyaQr4DNkifZvl7hk5y4Y0QM96ZTGqtWXUv1IZT7sdS8IOGO5+/XVMc9E+an7kn9L+CSGEEM7w4cPBMAwYhoFarUZwcDA6deqEH374AWaz499PASAuLg6+vr5F0q527dph4sSJRXKsF4VLg/8vvvgCYWFhWLlyJZo0aYJKlSohJiYGERGWNHGWZbFgwQJMmzYNPXv2RJ06dfDjjz/iwYMH+P333wEAFy9exNatW/Hdd9+hadOmaNWqFRYvXoyff/4ZDx48AACsXr0aer0eP/zwAyIjIzFw4EBMmDAB8+fP59uycOFCdO7cGVOmTEHNmjUxe/ZsNGjQAN98881zvy7FzVcLdKoVBIZhrFJbHQ0snOEjCf5FhecUDN5qG4HN77TmC8YJ5zRzAZcw7d9e8G8PN0K5aHB9AMAnPWpZtUfYUSH8Xl/Y+cqAOK28UhnH2m6vKrstQ5uHAwC61LYeEZeSK4xmlGQOFDTyn5YjLh5nFkRCWrU4oBVe4/oV/DCyVSUoFfJz/guI/e3ylAT/9ubHc5dA2umR/ziDjjWCAAADGoeJ5vz7uqtFGR3OpP0HCa4rF+QLR8/l7m9hwFvQnP+f32yGCR2qYHTbCLv7SUk7j0Qj/w6m/auUCnTPS60f2z5CVKvDXvaAM+QyeOR0iQrFwoH1bWYmWB9XOPJv+XfDcH+MbFWJ387dr2pK+yeEEEJEOnfujIcPH+LWrVvYsmUL2rdvj3feeQfdu3eH0Wi75hB5vly61N+ff/6J2NhY9OvXD/v27UO5cuUwduxYvPHGGwCAmzdvIiEhAdHR+fMyfXx80LRpU8THx2PgwIGIj4+Hr68vGjVqxO8THR0NhUKBo0ePonfv3oiPj0ebNm2g0eSPpsXGxuKLL75AcnIy/Pz8EB8fj8mTJ4vaFxsby3cySOXm5iI3N5f/OS3Nsg61wWCAwWB/+TRX4trG/c2w4iBTxZiLvP1uCvE3ZNZkhAG2ewEFA/Hw81DDYDDATfD9vaxO41Qb3dQK5BjMaBkRAIPBgFaV/XB2eke4qZUwGAxQMvnt89Io+GMbjYJrYzbCYChstfL855Xz0TrUdr3R+fehgq8WJz/uAC+tsuDnstbHz9GLfzGHFHCdhXG7wWBArmBfBWsSBf9KBrLHqhbkhcS0XNE2vcFodZ86ioElEOfqH2gUdo7BWh4TdnpI9/26XxSuPMpAnXI6nLmbzG/30jBgBOsNMDLX0xY/N2EhPQYGgwHugoC+jIfK6ljCz6mKYe2eq2GYDg3DdGDNJhjMjnciCd9/g8EApeBzy5ptvz6NYD+90Yx5fSIxunVFVAv2wsWEdP4xhY17oCDCTqUATw3m94sqlt+xCsHvJCVM/DncBb+QuC8vrMlyXc1s4V4TeTEoW7cGkpKAgACYSuB9UNjfo4Q8L3SPihkMBrAsC7PZzI+WsywLNjvbJe1h3N1lsyzlsCwLjUaDoCDLoEloaCjq1auHJk2a8BkAo0aNAgB8/fXXiIuLw40bN+Dv74/u3bvjiy++gJeXF/bu3YvXX3/dcv68c0+fPh2ffPIJfvrpJyxevBiXL1+Gp6cn2rdvj6+//po/p7222co++PXXXzFjxgxcu3YNoaGhGD9+vCgeXLZsGRYsWIC7d+/Cx8cHrVq1wvr16wEAGzZswOzZs3Ht2jV4eHigfv36+O233+Dp6Sl7rmdlNpvBspbvgEqleGDDmc+QS4P/GzduYNmyZZg8eTI++ugjHD9+HBMmTIBGo8GwYcOQkJAAAAgODhY9Lzg4mH8sISHB6k1XqVTw9/cX7VOpUiWrY3CP+fn5ISEhwe55pObOnYuZM2dabd++fTs8PAo3Mv087dhhWevcMnKVfxscOrAPFx3LTHdKGTclnuRYPsTbtm61SuUVuviYAWC5qfUpj7B582Y8yclv561zf2PzTcfPPTkSOJPEIMp8A5s337B6/FJi/vluXj6PzUmWqR7XbynAJcds2bLF8RNK3HusBFdb/sjenXZfu/C9KJ6aE5bjGxKvYfNmce2MM4Lr0C3MhArZ17F583WbR6rDAH97KtEmxIzNmzfjSmr+87ds2YLkR/nX7/bN61bnA4BoHZAZoECbUDO23VPgZjoD093T2PHQ8jh3nzpDDSX0edf7n1MnoL8pHZ61XIPsrCxs3rwZFdQK3IIC3mrW5jW//w9w/27+6zt19FBehoLlWAf27oaXfDkHGyzPS01Lx+bNm0XX6uLJeDy+IN77UXb+c65cuoDNyeedOZlDGnozuJSoRB1/y/t5/05+m47GH8J9O/+X+WqUSNFbrvmObVsBANcBpOrz252ZmVGoezoxMb8d/xeVhfv/HML9f5w+TIFupgNcW/fs3AEumePO/fz3nWs/9/vIxBbuHiUviIED8/9dgmsE0T1KSjq6Ry1UKhVCQkKQkZEBvV4PADBnZyOxfQeXtCd4z24o3N0d2tdgMMBoNPKDoZxGjRqhdu3aWL9+Pfr37w8A0Ov1mDNnDsLDw3Hr1i289957mDRpEr766ivUrl0bc+fOxZw5c3D8+HEAgKenJ9LS0pCeno4PPvgAVatWxePHj/Hxxx9j6NChfDAux2g0Qq/XW7ULAE6fPo2BAwfiww8/RO/evXHs2DG899578PDwwODBg3Hq1Cm88847WL58OZo0aYKUlBTEx8cjLS0NCQkJGDJkCGbOnInu3bsjPT0d8fHxSE1NhcnkfPauI/R6PbKzs7F//36rTIqsrCyHj+PS4N9sNqNRo0aYM2cOAKB+/fo4d+4cli9fjmHDhrmyaQWaOnWqqGcoLS0NYWFhiImJgU5nu6iWqxkMBuzYsQOdOnWCWm2JVt49up1Pte7SqaPD89KdsT3jH/x11tKR0q1bV7v7Ks8n4qdrZwAAjSOromuHCDzJyMXsU/sAAAP/FWN3zXVn5Zy6j19uWIKpts0b80v/ndlyGXh4GwDQtav9Ntsz7dRuAJYPaUGv/Z347fy/n+WctlRtmIHzD9PQs26odW/u2QT8csMSVS14s4tDx3tV8O+OBhN+W3gIFfzd0bVrY5zbdgWHEm8BACJrVEPXtpVljzE47++xLAu90QxtXkaG9D511JeXDiAz2dJL3qZlMzQK9xM9zl1jT09PdO3aCm1zjVh7/B5iI4MQ5me74+7OvhvYcu8aAKB3107IMZjx6WnLPRkb00lUA6AgXBs88tqw/Zd/gCeWz8eAHrGiYokAcDc5C5+dPggAaFivDrrWt17p4Fl1YVkMeZyJigEeUCsVOLvtCvYl3AIAtGvTWrRMo1T1xpkY9N0x1Cnvg65d81fD0BvNmH5iJwBAqXFH165tnG7XX6mn8c/TRwAK/vw8i3/upWLBuaMAgB7duvCfj+Sjd/DnnUsA8j+TD1NzMPvUfphYFOoeJeR5eJbfo4Q8D3SPiuXk5ODu3bvw8vKCm5ulPpBZpUKii9qj8/aGwsEBTbVaDZVKJRsD1apVC2fPnuUf++CDD/jHateujZycHIwdOxbffvstACAoKAgKhQJVq4pXKBo7dqzoZy4bXKFQwMtLfglwlUoFjUYj264VK1agQ4cOmD17NgCgQYMGuHnzJpYsWYLRo0cjKSkJnp6eeOWVVwAAkZGRaNWqFQDg2rVrMBqNGDRoEMLDLdNumzdvbnWOopSTkwN3d3e0adOGvz84cp0btrg0+A8NDUWtWrVE22rWrIlff/0VABASYpm7nJiYiNDQ/IrxiYmJqFevHr/Po0ePRMcwGo14+vQp//yQkBAkJoo/OtzPBe3DPS6l1Wqh1VoHyWq1ulT8AhO2UzjH2svDDWob63Y/i/oV/Pjgv6Dr4+mWPz2jrJ8H1Go1Qv3UmBhdFZ4aFfy8HOuFdJS7Nv98Ad7ufPsYwRzlZ3pPBde3oON0rBGEXZceoXXVMsVyH9Uq74da5f1kH+tWtxwOXn+KxhX9C3VutVqN/e+3h1JhKfji65n/+XDTOPa50EiWVi/M50nYMeTtrrX5fIXCUpTGV63GmPa2l8DjmAUlUrh7sEfeevJldIXL9jGzlteYIajx4OVhXTRSeI96aDXF9jumVrn887gJfg9oNfbPWaOsLw592AFuKqWomKTwKak5xkK1W8EU0eewAHUr+KNWqA7BOq1oilh5//wvFNz5tZr8tP/S8jufvLzoHiUlHd2jFiaTCQzDQKFQQJH3HZTx9ET1kydc0h5n0v65Yn8KG/V9hI/t3LkTc+fOxaVLl5CWlgaj0YicnBzk5OTAw8OD3096rBMnTmDGjBk4c+YMkpOT+VT+e/fuWcWTts4tdOnSJfTs2VP0WKtWrbBw4UKwLIvY2FiEh4ejatWq6NChA7p3746+ffvyKf4dO3ZE3bp1ERsbi5iYGLzyyiuiovVFTaFQ8AUVpZ8XZz4/Li3417JlS1y+fFm07cqVK3wPSqVKlRASEoJdu3bxj6elpeHo0aN870rz5s2RkpKCEyfyPxi7d++G2WxG06ZN+X32798vmg+xY8cOVK9enX+TmjdvLjoPt09x9+KUNMVR7R8A+jUKQ5C3lh9Vd7QNwbr8QGhidDW80UZ+9PhZaGwsPVhQ1XlHLRpUHzo3FZYNaVDgvvMH1MOc3lFYNLB+kZzbGSqlAl/2q4v+jcOe6RjcfxTCVQ6cKYj3rITntVfh3tkKDsKlFLn/5BYPqo/Fg5x/r6JrWqYYccXk0nPsz9USrm3/vK6lowX/OB4alewqEhzh6hAlkUqpwF8TWuGH4Y1F2zvUCMLgphXwWe/a/LbCVf8ghBBCnMMwDBQeHi7542jgX5CLFy/y069v3bqF7t27o06dOvj1119x4sQJLFmyBAD4qQ5yMjMzERsbC51Oh9WrV+P48eP47bffCnzes/D29sbJkyexevVqBAcHY8aMGahbty5SUlKgVCqxY8cObNmyBbVq1cLixYtRvXp13LzpxLxkF3Fp8D9p0iQcOXIEc+bMwbVr17BmzRqsWLEC48aNA2C54SdOnIhPP/0Uf/75J86ePYvXXnsNZcuWRa9evQBYMgU6d+6MN954A8eOHcOhQ4cwfvx4DBw4EGXLWkblBg8eDI1Gg5EjR+L8+fP45ZdfsHDhQlHa/jvvvIOtW7fiq6++wqVLlzBjxgz8/fffGD9+/HO/Lq7k6DJazvJxV+PQhx0Q93qTAvcVVjMXBv/PgzB1u4hif7SvEYQzn8SgS1Rogfv6uKsxuGkF0XrzpZWrgn9hxX83mcr4IXn3VIca9gvESBVVZxAALB5UHxtGN8eQppaOzoJW2RCulqAsov+MCyJcAlO6lOOLiuvUEVIoGMzpHcW/V5Ydn3PDSMnUoQMQGWn5mxBCiJXdu3fj7Nmz6Nu3LwDL6L3ZbMZXX32FZs2aoVq1avzqbByNRmM1b/7SpUtISkrC559/jtatW6NGjRpWmd/OqlmzJg4dOiTadujQIVSrVo0vqKdSqRAdHY1Zs2bh9OnTuHXrFnbv3g3A8p2hZcuWmDlzJk6dOgWNRsN3SJRkLk37b9y4MX777TdMnToVs2bNQqVKlbBgwQIMGTKE3+f9999HZmYm3nzzTaSkpKBVq1bYunWraK7D6tWrMX78eHTs2BEKhQJ9+/bFokWL+Md9fHywfft2jBs3Dg0bNkSZMmUwffp0vPnmm/w+LVq0wJo1azBt2jR89NFHqFq1Kn7//XfUrp0/2vMyKKpePjmOLpcnbEOIT/EH/9mC9c293Yp+5B8o3utaUnkLl3l7jsGjcMk2d5mR/9/GtcDuS4/Qt0F5p447rEVFrD12Bz3rPft8e3eNEo0q+vM/z+oZibGrT+LtjvLTD1SCkXe55RGLg7DDobg6BUs7lnoBXm5XrgD37wOpqa5uCSGEuFxubi4SEhJgMpmQmJiIrVu3Yu7cuejevTtee+01AECVKlVgMBiwePFi9OjRA4cOHcLy5ctFx6lYsSIyMjKwa9cu1K1bFx4eHqhQoQI0Gg0WL16M0aNH49y5c/xc/YI8fvwYp0+fFm0LDQ3Fu+++i8aNG2P27NkYMGAA4uPj8c0332Dp0qUAgE2bNuHGjRto1aoVVCoVDhw4ALPZjOrVq+Po0aPYtWsXYmJiEBQUhKNHj+Lx48eoWbPms1/IYubS4B8Aunfvju7du9t8nGEYzJo1C7NmzbK5j7+/P9asWWP3PHXq1MGBAwfs7tOvXz/069fPfoNJsTMJlvby8yj+OWCZufnBvzCwKsLY/6Xk5aKRf2GKvNwa76E+7uJRXAcF69zw97ROxRJ8Vw32xo7JbW0+Lgz+7aXWFyVhwP8swb9GqYDeZHtpz4KUxH4zhoJ+QgghRGTr1q0IDQ2FSqWCn58f6tati0WLFmHYsGH8vPq6deti/vz5+OKLLzB16lS0adMGc+fO5TsHAMuA7OjRozFgwAAkJSXhk08+wYwZMxAXF4ePPvoIixYtQoMGDfDvf/8b//rXvwps15o1a6zixNmzZ2PatGlYt24dpk+fjtmzZyM0NBSzZs3C8OHDAQC+vr7YuHEjZsyYgZycHFStWhVr165FZGQkLl68iP3792PBggVIS0tDeHg4vvrqK3Tp4ljBbFdyefBPiFSDCn7oVCsY1YO9n8uIeUSgfIXQohz5fxmJ0v6f48i/MFB1KyCd3lnPa9RdSq14/mn/ws/es6T969zVeJKRWxRNIoQQQkgJFBcXh7i4OIf2nTRpEiZNmiTaNnToUNHPy5Ytw7Jly0TbBg0ahEGDBom2sQV8V9+7d6/dx/v27ctPSZBq1aoV9u7dC7PZjLS0NOh0Or4To2bNmti6davdY5dUFPyTEkepYPDta42e2/maRwRgwYB6qBos7gQwU+z/THSCKRTFVUhSjnBk/HmNkhc30Wt6Ti9JeJ5nGfmPiQzGmqN34P8C1LHgCPtfCvriQQghhBBSUlDwTwiAXjLrpr/doQq2nHuIAY0KX/3+ZSYc+X+eqdvPa2TcVcr6Fu1SlzaJRv4Lf02ndauJcH8PdK4tv2xqQSoHehb63MXlxb7DCCGEEPKiouCfEBvK+rrjRDHN8X4ZCKvu642Fn/PtLOUzBKol2c9vNsPTTD0qlnk+wbDwKqptrNvrCA+NCm+1jSj088e1r4LMXBO6FLLzoLjRwD8hhBBCSgsK/gmxgwL/whNW+M95jsH/i1qZvlnlgOd6PgVTMqZPeGhUmPGvSJedX46wHgLF/oQQQggpLV6OxZsJIS5VKeD5pW4HeGqf27leZC/47IlnQpeGEEIIIaURjfwTQorNprdb4e7TLESV93lu5xzZuhJO3ElG1xKaJl5aUIDrGCr4RwghhJDSgoJ/QkixqV3OB7XLPb/AHwC8tCr8OKLJcz3ni0hBQ/820aUhAIDp04GMDMBLfrlYQgghpKSh4J8QQog1CnAdQuP+L7E333R1CwghhBCn0Jx/QgghVir4e7i6CSUWI+gZoax/QgghhJQWFPwTQgix0qxyAGb1jMSaUU1d3ZSSh7IiCCGEEIfFxcXB19e32I6/d+9eMAyDlJSUYjvHi4KCf4JJ0dUAABOjq7q4JYSQkuS15hXRokoZVzejRKOB/5fYw4fAvXuWvwkh5CU2fPhwMAwDhmGg0WhQpUoVzJo1C0aj8bmcv0WLFnj48CF8fIq+ztStW7fg5+eH06dPF/mxXYHm/BNM6FgFveuXQ5i/u6ubQgghJZ6o4B/l/b+8GjcG7t8HypWzdAIQQshLrHPnzli5ciVyc3OxefNmjBs3Dmq1GlOnTi32c2s0GoSE0CpPjqCRfwKGYVAhwAMMlbAmhJAC0W9KQgghzwPLsjDkmlzyx9mlbLVaLUJCQhAeHo4xY8YgOjoaf/75p2ifbdu2oWbNmvDy8kLnzp3xMC9zav/+/VCr1UhISBDtP3HiRLRu3RoAcPv2bfTo0QN+fn7w9PREZGQkNm/eDEA+7f/QoUNo164dPDw84Ofnh9jYWCQnJwMANmzYgKioKLi7uyMgIADR0dHIzMx06vVycnNzMWHCBAQFBcHNzQ2tWrXC8ePH+ceTk5MxZMgQBAYGwt3dHVWrVsXKlSsBAHq9HuPHj0doaCjc3NwQHh6OuXPnFqodjqKRf0IIIaSQaNyfEEJIcTHqzVjxzj6XnPvNhW2h1ioL/Xx3d3ckJSXxP2dlZeHf//43fvrpJygUCrz66qt47733sHr1arRp0waVK1fGTz/9hClTpgAADAYDVq9ejXnz5gEAxo0bB71ej/3798PT0xMXLlyAl42lVk+fPo2OHTtixIgRWLhwIVQqFfbs2QOTyYSHDx9i0KBBmDdvHnr37o309HQcOHDA6c4Ozvvvv49ff/0Vq1atQnh4OObNm4fY2Fhcu3YN/v7++L//+z9cuHABW7ZsQZkyZXDt2jVkZ2cDABYtWoQ///wT69atQ4UKFXD37l3cvXu3UO1wFAX/hBBCiBMoS4oQQgiRx7Isdu3ahW3btuHtt9/mtxsMBixfvhwREREAgPHjx2PWrFn84yNHjsTKlSv54P9///sfcnJy0L9/fwDAnTt30LdvX0RFRQEAKleubLMN8+bNQ6NGjbB06VJ+W2RkJADg5MmTMBqN6NOnD8LDwwGAP6azMjMzsWzZMsTFxaFLly4AgG+//RY7duzA999/jylTpuDOnTuoX78+GjVqBACoWLEi//w7d+6gatWqaNWqFRiG4dtTnCj4J4QQQgqJpvwTQggpLiqNAm8ubOuycztj06ZN8PLygsFggNlsxuDBgzFjxgz+cQ8PDz7wB4DQ0FA8evSI/3n48OGYNm0ajhw5gmbNmiEuLg79+/eHp6cnAGDChAkYM2YMtm/fjujoaPTt2xd16tSRbcvp06fRr18/2cfq1q2Ljh07IioqCrGxsYiJicErr7wCPz8/p14vAFy/fh0GgwEtW7bkt6nVajRp0gQXL14EAIwZMwZ9+/bFyZMnERMTg169eqFFixb8a+7UqROqV6+Ozp07o3v37oiJiXG6Hc6gOf+EEEKIE0T1/ijxnxBCSDFhGAZqrdIlf5zNcmvfvj1Onz6Nq1evIjs7G6tWreIDd8ASFEtfmzDVPigoCD169MDKlSuRmJiILVu2YMSIEfzjo0aNwo0bNzB06FCcPXsWjRo1wuLFi2Xb4u5uu4i5UqnEjh07sGXLFtSqVQuLFy9G9erVcfPmTader6O6dOmC27dvY9KkSXjw4AE6duyI9957DwDQoEED3Lx5E7Nnz0Z2djb69++PV155pVjawaHgnxBCCHECZf0TQgghYp6enqhSpQoqVKgAlapwyeWjRo3CL7/8ghUrViAiIkI0og4AYWFhGD16NDZu3Ih3330X3377rexx6tSpg127dtk8D8MwaNmyJWbOnIlTp05Bo9Hgt99+c7q9ERER0Gg0OHToEL/NYDDg+PHjqFWrFr8tMDAQw4YNw3//+18sWLAAK1as4B/T6XQYMGAAvv32W/zyyy/49ddf8fTpU6fb4ihK+yeEEEIKidL+CSGEkKIRGxsLnU6HTz/9VFQPALBU/u/SpQuqVauG5ORk7NmzBzVr1pQ9ztSpUxEVFYWxY8di9OjR0Gg02LNnD/r164fr169j165diImJQVBQEI4ePYrHjx/bPBbn8uXLUCjE4+aRkZEYM2YMpkyZAn9/f1SoUAHz5s1DVlYWRo4cCQCYPn06GjZsiMjISOTm5mLTpk38uebPn4/Q0FDUr18fCoUC69evR0hICHx9fQt5BQtGwT8hhBDiBIYW+yOEEEKKnEKhwPDhwzFnzhy89tprosdMJhPGjRuHe/fuQafToXPnzvj6669lj1OtWjVs374dH330EZo0aQJ3d3c0bdoUgwYNgk6nw/79+7FgwQKkpaUhPDwcX331FV+wz5bBgwdbbbt79y4+//xzmM1mDB06FOnp6WjUqBG2bdvG1xDQaDSYOnUqbt26BXd3d7Ru3Ro///wzAMDb2xvz5s3D1atXoVQq0bhxY2zevNmqk6EoMWxh1zUgImlpafDx8UFqaip0Op2rm2OTwWDA5s2b0bVrV6u5N4SUFHSfkpIsx2BCjf/bCgA4Na0D/Lxszy0kL7Dy5YH794Fy5YB791zdGiv0e5SUdHSPiuXk5ODmzZuoVKkS3NzcXN0clxk5ciQeP36MP//809VNAQCYzWakpaVBp9MVa1BeEHv3hzNxKI38E0IIIYVE3ecvsV27AKMRKOTcVkIIIflSU1Nx9uxZrFmzpsQE/i8i+h+LEEIIIcRZ1au7ugWEEPLC6NmzJ44dO4bRo0ejU6dOrm7OC4uCf0IIIaTQaOifEEIIeVZ79+51dRNeCrTUHyGEEOIE4VJ/lPZPCCGEkNKCRv4JIYQQJ1C1fwIAWLMGyMoCPDwAmSrQhBBSGFSLncgpqvuCgn9CCCGkkOgr2kvs/ffzq/1T8E8IeUbcigdZWVlwd6dVZIhYVlYWADzzyhgU/BNCCCFOYGjgnxBCSBFTKpXw9fXFo0ePAAAeHh5g6D8clzObzdDr9cjJyXHJUn8syyIrKwuPHj2Cr68vlErlMx2Pgn9CCCHECcKvYpSdSQghpKiEhIQAAN8BQFyPZVlkZ2fD3d3dpZ0xvr6+/P3xLCj4J4QQQgqJpcR/QgghRYRhGISGhiIoKAgGg8HVzSEADAYD9u/fjzZt2jxzyn1hqdXqZx7x51DwTwghhDiB0jAJIYQUJ6VSWWTBHnk2SqUSRqMRbm5uLgv+ixIt9UcIIYQUEqX9E0IIIaS0oOCfEEIIcYJozr/LWkEIIYQQ4hwK/gkhhBAnUNY/IYQQQkojCv4JIYSQwqK8f0IIIYSUElTwjxBCCHECFfwjAABuyaUiWHqJEEIIeR4o+CeEEEIKicb9X2J//+3qFhBCCCFOobR/QgghpJAo658QQgghpQUF/4QQQoiTKPOfEEIIIaUNBf+EEEJIIdHAPyGEEEJKC5rzTwghhDiJgSXwZynv/+X11lvA06eAvz/wn/+4ujWEEEJIgSj4J4QQQpzEMAxN+H/Z/fUXcP8+UK6cq1tCCCGEOITS/gkhhJBCovCfEEIIIaUFBf+EEEKIk6jeHyGEEEJKGwr+CSGEkEKizH9CCCGElBYU/BNCCCFOoqX+CCGEEFLaUPBPCCGEEEIIIYS84Cj4J4QQQgqJlvojhBBCSGnh0uB/xowZYBhG9KdGjRr84zk5ORg3bhwCAgLg5eWFvn37IjExUXSMO3fuoFu3bvDw8EBQUBCmTJkCo9Eo2mfv3r1o0KABtFotqlSpgri4OKu2LFmyBBUrVoSbmxuaNm2KY8eOFctrJoQQUvoxlPdPCCGEkFLG5SP/kZGRePjwIf/n4MGD/GOTJk3C//73P6xfvx779u3DgwcP0KdPH/5xk8mEbt26Qa/X4/Dhw1i1ahXi4uIwffp0fp+bN2+iW7duaN++PU6fPo2JEydi1KhR2LZtG7/PL7/8gsmTJ+OTTz7ByZMnUbduXcTGxuLRo0fP5yIQQggpVbjQn8b9CSGEEFJaqFzeAJUKISEhVttTU1Px/fffY82aNejQoQMAYOXKlahZsyaOHDmCZs2aYfv27bhw4QJ27tyJ4OBg1KtXD7Nnz8YHH3yAGTNmQKPRYPny5ahUqRK++uorAEDNmjVx8OBBfP3114iNjQUAzJ8/H2+88QZef/11AMDy5cvx119/4YcffsCHH374nK4EIYSQ0oay/l9igwYBycmAn5+rW0IIIYQ4xOXB/9WrV1G2bFm4ubmhefPmmDt3LipUqIATJ07AYDAgOjqa37dGjRqoUKEC4uPj0axZM8THxyMqKgrBwcH8PrGxsRgzZgzOnz+P+vXrIz4+XnQMbp+JEycCAPR6PU6cOIGpU6fyjysUCkRHRyM+Pt5mu3Nzc5Gbm8v/nJaWBgAwGAwwGAzPdE2KE9e2ktxGQug+JSUdN/JvNJbs3/mkGM2Zk//vEngP0O9RUtLRPUpKg9JwnzrTNpcG/02bNkVcXByqV6+Ohw8fYubMmWjdujXOnTuHhIQEaDQa+Pr6ip4THByMhIQEAEBCQoIo8Oce5x6zt09aWhqys7ORnJwMk8kku8+lS5dstn3u3LmYOXOm1fbt27fDw8PDsQvgQjt27HB1EwgpEN2npKQymZUAGBw4cBAX3FzdGkJso9+jpKSje5SUBiX5Ps3KynJ4X5cG/126dOH/XadOHTRt2hTh4eFYt24d3N3dXdiygk2dOhWTJ0/mf05LS0NYWBhiYmKg0+lc2DL7DAYDduzYgU6dOkGtVru6OYTIovuUlHQf/r0TBrMZLVu1QuWgkvs7n7y86PcoKenoHiWlQWm4T7kMdEe4PO1fyNfXF9WqVcO1a9fQqVMn6PV6pKSkiEb/ExMT+RoBISEhVlX5udUAhPtIVwhITEyETqeDu7s7lEollEql7D5ytQg4Wq0WWq3WartarS6xN4ZQaWknebnRfUpKKq7av0qlonuUlGj0e5SUdHSPktKgJN+nzrTL5dX+hTIyMnD9+nWEhoaiYcOGUKvV2LVrF//45cuXcefOHTRv3hwA0Lx5c5w9e1ZUlX/Hjh3Q6XSoVasWv4/wGNw+3DE0Gg0aNmwo2sdsNmPXrl38PoQQQogcqvf3EqtRA9DpLH8TQgghpYBLg//33nsP+/btw61bt3D48GH07t0bSqUSgwYNgo+PD0aOHInJkydjz549OHHiBF5//XU0b94czZo1AwDExMSgVq1aGDp0KM6cOYNt27Zh2rRpGDduHD8qP3r0aNy4cQPvv/8+Ll26hKVLl2LdunWYNGkS347Jkyfj22+/xapVq3Dx4kWMGTMGmZmZfPV/QgghRIgpeBfyosvIANLTLX8TQgghpYBL0/7v3buHQYMGISkpCYGBgWjVqhWOHDmCwMBAAMDXX38NhUKBvn37Ijc3F7GxsVi6dCn/fKVSiU2bNmHMmDFo3rw5PD09MWzYMMyaNYvfp1KlSvjrr78wadIkLFy4EOXLl8d3333HL/MHAAMGDMDjx48xffp0JCQkoF69eti6datVEUBCCCEEQH70T0P/hBBCCCklXBr8//zzz3Yfd3Nzw5IlS7BkyRKb+4SHh2Pz5s12j9OuXTucOnXK7j7jx4/H+PHj7e5DCCGECLEU/RNCCCGklChRc/4JIYSQ0oChxH9CCCGElDIU/BNCCCGFxNLAPyGEEEJKCQr+CSGEECflrfRHwT8hhBBCSg0K/gkhhBAnUdI/IYQQQkobCv4JIYSQQqKBf0IIIYSUFhT8E0IIIU5iaOifEEIIIaWMS5f6I4QQQkojrto/S5P+X17LlwPZ2YC7u6tbQgghhDiEgn9CCCGkkCj0f4l17+7qFhBCCCFOobR/QgghxEmU9k8IIYSQ0oaCf0IIIaSwaOifEEIIIaUEpf0TQgghhDjrxAlArwc0GqBhQ1e3hhBCCCkQBf+EEEKIk7i0f5aG/l9ePXsC9+8D5coB9+65ujWEEEJIgSjtnxBCCCkkKvZPCCGEkNKCgn9CCCHESdxSf4QQQgghpQUF/4QQQoiT8tP+CSGEEEJKBwr+CSGEkEKitH9CCCGElBYU/BNCCCFOoqR/QgghhJQ2FPwTQgghhUTV/gkhhBBSWlDwTwghhDiJYWjsnxBCCCGlCwX/hBBCiJO40J/m/BNCCCGktKDgnxBCCCGEEEIIecGpXN0AQgghpNShrH9y8aIl9YOmgBBCCCklKPgnhBBCnERp/wTe3q5uASGEEOIUSvsnhBBCComq/RNCCCGktKDgnxBCCHESVfsnhBBCSGlDaf+EEEJIIVHa/0ts/nwgLQ3Q6YDJk13dGkIIIaRAFPwTQgghTqJxf4L584H794Fy5Sj4J4QQUipQ2j8hhBDiJC7rnwb+CSGEEFJaUPBPCCGEFBJLef+EEEIIKSUo+CeEEEKcRGn/hBBCCCltKPgnhBBCnJWX90/j/oQQQggpLSj4J4QQQgghhBBCXnAU/BNCCCFO4tP+aeifEEIIIaUEBf+EEEJIIVHsTwghhJDSgoJ/QgghxEkMVfwjhBBCSCmjcnUDCCGEkNKGyUv8p6X+XmINGgBhYUBgoKtbQgghhDiEgn9CCCGkkCj0f4n9+aerW0AIIYQ4hdL+CSGEECdR2j8hhBBCShsK/gkhhJBCoqx/QgghhJQWFPwTQgghTqKBf0IIIYSUNjTnnxBCCHESl/bP0qz/l9e//gU8fmwp+Efz/wkhhJQCFPwTQgghhURp/y+xkyeB+/eBcuVc3RJCCCHEIZT2TwghhDiNEv8JIYQQUrpQ8E8IIYQ4iar9E0IIIaS0oeCfEEIIKSRK+yeEEEJIaUHBPyGEEOIkGvgnhBBCSGlDwT8hhBBSSFTtnxBCCCGlBQX/hBBCiJNozj8hhBBCShsK/gkhhBAnMXmJ/zTnnxBCCCGlRYkJ/j///HMwDIOJEyfy23JycjBu3DgEBATAy8sLffv2RWJiouh5d+7cQbdu3eDh4YGgoCBMmTIFRqNRtM/evXvRoEEDaLVaVKlSBXFxcVbnX7JkCSpWrAg3Nzc0bdoUx44dK46XSQgh5AVCsT8hhBBCSosSEfwfP34c//nPf1CnTh3R9kmTJuF///sf1q9fj3379uHBgwfo06cP/7jJZEK3bt2g1+tx+PBhrFq1CnFxcZg+fTq/z82bN9GtWze0b98ep0+fxsSJEzFq1Chs27aN3+eXX/6/vfsOj6pM+zj+nZreQxIgoShI7whELIhIBNaKnWURXV0UVGTXgutiWxfFtYuga8FdG+JrBQQiKIiG3qsgvSQBQnqZdt4/hgyEhBJEZib8PtcFyZzzzJn7zDwzk/s893nOZEaNGsXjjz/OsmXL6NChAxkZGeTm5v7+Oy8iIkFHZf/CqFHw+OPenyIiIkHA78l/cXExgwYN4j//+Q9xcXG+5QUFBbzzzju8+OKL9O7dmy5duvDee+/x888/s2DBAgBmzZrFunXr+OCDD+jYsSP9+vXj6aefZvz48TgcDgAmTpxI06ZNeeGFF2jVqhUjRozg+uuv56WXXvI91osvvsidd97J0KFDad26NRMnTiQ8PJx33333zD4ZIiISFCpzf5X9n8VGjYInnlDyLyIiQcPq7wCGDx/OgAED6NOnD//85z99y5cuXYrT6aRPnz6+ZS1btqRRo0ZkZWXRo0cPsrKyaNeuHcnJyb42GRkZ3H333axdu5ZOnTqRlZVVZRuVbSpPL3A4HCxdupTRo0f71pvNZvr06UNWVtYx466oqKCiosJ3u7CwEACn04nT6Ty1J+MMqIwtkGMUUT+VQFc5y7/L5VI/lYCkz1EJdOqjEgyCoZ/WJja/Jv+ffPIJy5YtY/HixdXWZWdnY7fbiY2NrbI8OTmZ7OxsX5sjE//K9ZXrjtemsLCQsrIyDh48iNvtrrHNhg0bjhn72LFjefLJJ6stnzVrFuHh4ce8X6DIzMz0dwgiJ6R+KoGquMgCmFi2bBmlWzT8L4FLn6MS6NRHJRgEcj8tLS096bZ+S/537tzJ/fffT2ZmJqGhof4K45SNHj2aUUeU+hUWFpKWlkbfvn2Jjo72Y2TH53Q6yczM5PLLL8dms/k7HJEaqZ9KoHtjy89QWkynTp24tFWKv8MRfygq8p73YTJBVJS/o6lGn6MS6NRHJRgEQz+trEA/GX5L/pcuXUpubi6dO3f2LXO73cybN4/XX3+dmTNn4nA4yM/PrzL6n5OTQ0qK9w+tlJSUarPyV14N4Mg2R18hICcnh+joaMLCwrBYLFgslhrbVG6jJiEhIYSEhFRbbrPZArZjHClY4pSzm/qpBCrzoRn/rFar+ujZqn172L0bGjaEXbv8Hc0x6XNUAp36qASDQO6ntYnLbxP+XXbZZaxevZoVK1b4/nXt2pVBgwb5frfZbMyePdt3n40bN7Jjxw7S09MBSE9PZ/Xq1VVm5c/MzCQ6OprWrVv72hy5jco2lduw2+106dKlShuPx8Ps2bN9bURERI5UOdu/Cv5FREQkWPht5D8qKoq2bdtWWRYREUFCQoJv+R133MGoUaOIj48nOjqae++9l/T0dHr06AFA3759ad26NYMHD2bcuHFkZ2fz2GOPMXz4cN+o/LBhw3j99dd56KGHuP3225kzZw6ffvop06ZN8z3uqFGjGDJkCF27dqVbt268/PLLlJSUMHTo0DP0bIiISDAyNN2/iIiIBAm/z/Z/PC+99BJms5mBAwdSUVFBRkYGb7zxhm+9xWJh6tSp3H333aSnpxMREcGQIUN46qmnfG2aNm3KtGnTeOCBB3jllVdITU3l7bffJiMjw9fmpptuYt++fYwZM4bs7Gw6duzIjBkzqk0CKCIiAodH/kVERESCRUAl/z/88EOV26GhoYwfP57x48cf8z6NGzdm+vTpx91ur169WL58+XHbjBgxghEjRpx0rCIicvYy4c3+Ne4vIiIiwcJv5/yLiIiIiIiIyJmh5F9ERKSWfBP+aehfREREgoSSfxERkVOk3F9ERESChZJ/ERGRWtJ8fyIiIhJslPyLiIjUlq/sX2P/IiIiEhwCarZ/ERGRoKLc/+z11VfgcIDd7u9IREREToqSfxERkVoyqfBfunTxdwQiIiK1orJ/ERGRWvLN9u/fMEREREROmpJ/ERERERERkTpOZf8iIiK1VFn0r/n+zmJTp0JZGYSFwR/+4O9oRERETkjJv4iIyCkyVPh/9ho2DHbvhoYNYdcuf0cjIiJyQir7FxERqSWTSRP+iYiISHA5peR/586d7DriKPeiRYsYOXIkb7311mkLTEREJFCp7F9ERESCzSkl/7feeivff/89ANnZ2Vx++eUsWrSIv//97zz11FOnNUAREZFApdxfREREgsUpJf9r1qyhW7duAHz66ae0bduWn3/+mQ8//JBJkyadzvhEREQCjqr+RUREJNicUvLvdDoJCQkB4LvvvuOqq64CoGXLluzdu/f0RSciIhLADNX9i4iISJA4peS/TZs2TJw4kR9//JHMzEyuuOIKAPbs2UNCQsJpDVBEREREREREfptTSv6fe+453nzzTXr16sUtt9xChw4dAPj66699pwOIiIjUVZrtX0RERIKN9VTu1KtXL/bv309hYSFxcXG+5XfddRfh4eGnLTgREZFApqp/ERERCRanNPJfVlZGRUWFL/Hfvn07L7/8Mhs3biQpKem0BigiIhJoNO4vREZCVJT3p4iISBA4peT/6quv5r///S8A+fn5dO/enRdeeIFrrrmGCRMmnNYARUREAk1l1b8G/s9iGzZAYaH3p4iISBA4peR/2bJlXHTRRQB89tlnJCcns337dv773//y6quvntYARUREREREROS3OaXkv7S0lKioKABmzZrFddddh9lspkePHmzfvv20BigiIhJoKsv+dak/ERERCRanlPw3a9aML7/8kp07dzJz5kz69u0LQG5uLtHR0ac1QBERkUBTOdu/Un8REREJFqeU/I8ZM4a//e1vNGnShG7dupGeng54qwA6dep0WgMUERERCTgPPgh//rP3p4iISBA4pUv9XX/99Vx44YXs3buXDh06+JZfdtllXHvttactOBERkUB0uOzfr2GIP338MezeDQ0bwvPP+zsaERGREzql5B8gJSWFlJQUdu3aBUBqairdunU7bYGJiIgEOuX+IiIiEixOqezf4/Hw1FNPERMTQ+PGjWncuDGxsbE8/fTTeDye0x2jiIhIYDGduImIiIhIIDmlkf+///3vvPPOOzz77LP07NkTgPnz5/PEE09QXl7OM888c1qDFBERCSSmyuxfdf8iIiISJE4p+X///fd5++23ueqqq3zL2rdvT8OGDbnnnnuU/IuIiIiIiIgEkFMq+8/Ly6Nly5bVlrds2ZK8vLzfHJSIiEggM1UO/Ps3DBEREZGTdkrJf4cOHXj99derLX/99ddp3779bw5KREQkGKjqX0RERILFKZX9jxs3jgEDBvDdd9+Rnp4OQFZWFjt37mT69OmnNUAREZFAo/n+REREJNic0sj/JZdcwi+//MK1115Lfn4++fn5XHfddaxdu5b//e9/pztGERGRgHK47F9D/yIiIhIcTmnkH6BBgwbVJvZbuXIl77zzDm+99dZvDkxERCTQqez/LDZgAOTlQXy8vyMRERE5Kaec/IuIiJytTCr8lzff9HcEIiIitXJKZf8iIiJnNc32LyIiIkFGyb+IiIiIiIhIHVersv/rrrvuuOvz8/N/SywiIiJBobLoX+f8i4iISLCoVfIfExNzwvV/+tOfflNAIiIiwUPZ/1mra1fIzoaUFFiyxN/RiIiInFCtkv/33nvv94pDREQkaJg0359kZ8Pu3f6OQkRE5KTpnH8REZFaqpztX2X/IiIiEiyU/IuIiIiIiIjUcUr+RUREasmkS/2JiIhIkFHyLyIiUkua7V9ERESCjZJ/ERERERERkTpOyb+IiEgtmQ7V/Rsq/BcREZEgoeRfRETkFKnsX0RERIKFkn8RERERERGROs6vyf+ECRNo37490dHRREdHk56ezrfffutbX15ezvDhw0lISCAyMpKBAweSk5NTZRs7duxgwIABhIeHk5SUxIMPPojL5arS5ocffqBz586EhITQrFkzJk2aVC2W8ePH06RJE0JDQ+nevTuLFi36XfZZRESCn2b7F8aNg//8x/tTREQkCPg1+U9NTeXZZ59l6dKlLFmyhN69e3P11Vezdu1aAB544AG++eYbpkyZwty5c9mzZw/XXXed7/5ut5sBAwbgcDj4+eefef/995k0aRJjxozxtdm6dSsDBgzg0ksvZcWKFYwcOZI///nPzJw509dm8uTJjBo1iscff5xly5bRoUMHMjIyyM3NPXNPhoiIiASPW2+FP//Z+1NERCQI+DX5v/LKK+nfvz/NmzfnvPPO45lnniEyMpIFCxZQUFDAO++8w4svvkjv3r3p0qUL7733Hj///DMLFiwAYNasWaxbt44PPviAjh070q9fP55++mnGjx+Pw+EAYOLEiTRt2pQXXniBVq1aMWLECK6//npeeuklXxwvvvgid955J0OHDqV169ZMnDiR8PBw3n33Xb88LyIiEtgqL/Wnk/5FREQkWFj9HUAlt9vNlClTKCkpIT09naVLl+J0OunTp4+vTcuWLWnUqBFZWVn06NGDrKws2rVrR3Jysq9NRkYGd999N2vXrqVTp05kZWVV2UZlm5EjRwLgcDhYunQpo0eP9q03m8306dOHrKysY8ZbUVFBRUWF73ZhYSEATqcTp9P5m56L31NlbIEco4j6qQQ641DS73K71U8lIOlzVAKd+qgEg2Dop7WJze/J/+rVq0lPT6e8vJzIyEi++OILWrduzYoVK7Db7cTGxlZpn5ycTHZ2NgDZ2dlVEv/K9ZXrjtemsLCQsrIyDh48iNvtrrHNhg0bjhn32LFjefLJJ6stnzVrFuHh4Se3836UmZnp7xBETkj9VAJVbo4ZMLNhwwam56/3dzjiB5G7d2NyuzEsFoobNvR3OMekz1EJdOqjEgwCuZ+WlpaedFu/J/8tWrRgxYoVFBQU8NlnnzFkyBDmzp3r77BOaPTo0YwaNcp3u7CwkLS0NPr27Ut0dLQfIzs+p9NJZmYml19+OTabzd/hiNRI/VQC3fSC5ZC3jxYtWtK/Z1N/hyN+YG3aFNPu3RgNG+LautXf4VSjz1EJdOqjEgyCoZ9WVqCfDL8n/3a7nWbNmgHQpUsXFi9ezCuvvMJNN92Ew+EgPz+/yuh/Tk4OKSkpAKSkpFSblb/yagBHtjn6CgE5OTlER0cTFhaGxWLBYrHU2KZyGzUJCQkhJCSk2nKbzRawHeNIwRKnnN3UTyVQmU3eKXPMFov66FnOBAHdB/Q5KoFOfVSCQSD309rE5dcJ/2ri8XioqKigS5cu2Gw2Zs+e7Vu3ceNGduzYQXp6OgDp6emsXr26yqz8mZmZREdH07p1a1+bI7dR2aZyG3a7nS5dulRp4/F4mD17tq+NiIjIkUymE7cRERERCSR+HfkfPXo0/fr1o1GjRhQVFfHRRx/xww8/MHPmTGJiYrjjjjsYNWoU8fHxREdHc++995Kenk6PHj0A6Nu3L61bt2bw4MGMGzeO7OxsHnvsMYYPH+4blR82bBivv/46Dz30ELfffjtz5szh008/Zdq0ab44Ro0axZAhQ+jatSvdunXj5ZdfpqSkhKFDh/rleRERkcBmOjTfv6HZ/kVERCRI+DX5z83N5U9/+hN79+4lJiaG9u3bM3PmTC6//HIAXnrpJcxmMwMHDqSiooKMjAzeeOMN3/0tFgtTp07l7rvvJj09nYiICIYMGcJTTz3la9O0aVOmTZvGAw88wCuvvEJqaipvv/02GRkZvjY33XQT+/btY8yYMWRnZ9OxY0dmzJhRbRJAERERERERkWDk1+T/nXfeOe760NBQxo8fz/jx44/ZpnHjxkyfPv242+nVqxfLly8/bpsRI0YwYsSI47YREREBODTwj8b9RUREJFgE3Dn/IiIiga7ylH9V/YuIiEiwUPIvIiIiIiIiUscp+RcREaklzfYvIiIiwUbJv4iIyCnSbP8iIiISLPw64Z+IiEgwMqGh/7Pe4sXgdoPF4u9IREREToqSfxERkVoyabZ/qV/f3xGIiIjUisr+RUREREREROo4Jf8iIiK1pEv9iYiISLBR2b+IiEgtHS77V/Z/1nrrLSguhshIuOsuf0cjIiJyQkr+RURERGrrqadg925o2FDJv4iIBAWV/YuIiNTWoaF/lf2LiIhIsFDyLyIiIiIiIlLHKfkXERGpJU34JyIiIsFGyb+IiEgtVU74JyIiIhIslPyLiIiIiIiI1HFK/kVERGrJROWEf6r7FxERkeCg5F9ERKSWKsv+lfqLiIhIsFDyLyIiIiIiIlLHWf0dgIiISLDRbP/CeedBTAwkJ/s7EhERkZOi5F9ERESktubM8XcEIiIitaKyfxERkVrSOf8iIiISbJT8i4iI1Jpm+xcREZHgouRfREREREREpI7TOf8iIiK1pLJ/YdAg2L8fEhPhww/9HY2IiMgJKfkXERE5Vcr+z15z58Lu3dCwob8jEREROSkq+xcREakl04mbiIiIiAQUJf8iIiK1dLjsX0P/IiIiEhyU/IuIiIiIiIjUcUr+RUREasnku9SfnwMREREROUlK/kVERGpJs/2LiIhIsFHyLyIiIiIiIlLHKfkXERGppcrZ/lX2LyIiIsFCyb+IiIiIiIhIHWf1dwAiIiJB59BJ/7rU31nszjuhoABiYvwdiYiIyElR8i8iIlJLlWX/yv3PYo8/7u8IREREakVl/yIiIiIiIiJ1nJJ/ERGRWtKl/kRERCTYKPkXERGpJc32LyIiIsFGyb+IiIhIbaWmektAUlP9HYmIiMhJUfIvIiJSSybN9i8iIiJBRsm/iIiIiIiISB2n5F9ERKSWdM6/iIiIBBsl/yIiIrWk2f5FREQk2Cj5FxEREREREanjlPyLiIicIkN1/yIiIhIklPyLiIjUUuVs/yIiIiLBQsm/iIiIiIiISB2n5F9ERKSWNNu/iIiIBBurvwMQERERCToffAAVFRAS4u9IREREToqSfxERkVrSpf6EXr38HYGIiEit+LXsf+zYsZx//vlERUWRlJTENddcw8aNG6u0KS8vZ/jw4SQkJBAZGcnAgQPJycmp0mbHjh0MGDCA8PBwkpKSePDBB3G5XFXa/PDDD3Tu3JmQkBCaNWvGpEmTqsUzfvx4mjRpQmhoKN27d2fRokWnfZ9FRCT4mQ4V/mu2fxEREQkWfk3+586dy/Dhw1mwYAGZmZk4nU769u1LSUmJr80DDzzAN998w5QpU5g7dy579uzhuuuu8613u90MGDAAh8PBzz//zPvvv8+kSZMYM2aMr83WrVsZMGAAl156KStWrGDkyJH8+c9/ZubMmb42kydPZtSoUTz++OMsW7aMDh06kJGRQW5u7pl5MkRERERERER+J34t+58xY0aV25MmTSIpKYmlS5dy8cUXU1BQwDvvvMNHH31E7969AXjvvfdo1aoVCxYsoEePHsyaNYt169bx3XffkZycTMeOHXn66ad5+OGHeeKJJ7Db7UycOJGmTZvywgsvANCqVSvmz5/PSy+9REZGBgAvvvgid955J0OHDgVg4sSJTJs2jXfffZdHHnmkWuwVFRVUVFT4bhcWFgLgdDpxOp2n/8k6TSpjC+QYRdRPJdB5PG4A3B6P+ulZyjR3ru+cf+OSS/wdTjX6HJVApz4qwSAY+mltYguoc/4LCgoAiI+PB2Dp0qU4nU769Onja9OyZUsaNWpEVlYWPXr0ICsri3bt2pGcnOxrk5GRwd13383atWvp1KkTWVlZVbZR2WbkyJEAOBwOli5dyujRo33rzWYzffr0ISsrq8ZYx44dy5NPPllt+axZswgPDz+1J+AMyszM9HcIIiekfiqBatsOM2Bmx/YdTJ++zd/hiB/0veMOwg4coCwhgVnvvOPvcI5Jn6MS6NRHJRgEcj8tLS096bYBk/x7PB5GjhxJz549adu2LQDZ2dnY7XZiY2OrtE1OTiY7O9vX5sjEv3J95brjtSksLKSsrIyDBw/idrtrbLNhw4Ya4x09ejSjRo3y3S4sLCQtLY2+ffsSHR1dy70/c5xOJ5mZmVx++eXYbDZ/hyNSI/VTCXRrZ26E3dtJa9SI/v1b+zsc8QNraCgAoaGh9O/f38/RVKfPUQl06qMSDIKhn1ZWoJ+MgEn+hw8fzpo1a5g/f76/QzkpISEhhNRweR+bzRawHeNIwRKnnN3UTyVQWSzeKXNMZrP66FnOBAHdB/Q5KoFOfVSCQSD309rE5dcJ/yqNGDGCqVOn8v3335OamupbnpKSgsPhID8/v0r7nJwcUlJSfG2Onv2/8vaJ2kRHRxMWFkZiYiIWi6XGNpXbEBEREREREQlWfk3+DcNgxIgRfPHFF8yZM4emTZtWWd+lSxdsNhuzZ8/2Ldu4cSM7duwgPT0dgPT0dFavXl1lVv7MzEyio6Np3bq1r82R26hsU7kNu91Oly5dqrTxeDzMnj3b10ZERKRS5aX+0KX+REREJEj4tex/+PDhfPTRR3z11VdERUX5ztGPiYkhLCyMmJgY7rjjDkaNGkV8fDzR0dHce++9pKen06NHDwD69u1L69atGTx4MOPGjSM7O5vHHnuM4cOH+8ryhw0bxuuvv85DDz3E7bffzpw5c/j000+ZNm2aL5ZRo0YxZMgQunbtSrdu3Xj55ZcpKSnxzf4vIiJSyVSZ+/s3DBEREZGT5tfkf8KECQD06tWryvL33nuP2267DYCXXnoJs9nMwIEDqaioICMjgzfeeMPX1mKxMHXqVO6++27S09OJiIhgyJAhPPXUU742TZs2Zdq0aTzwwAO88sorpKam8vbbb/su8wdw0003sW/fPsaMGUN2djYdO3ZkxowZ1SYBFBEREREREQk2fk3+jZMolwwNDWX8+PGMHz/+mG0aN27M9OnTj7udXr16sXz58uO2GTFiBCNGjDhhTCIicnY7NPCvqn8REREJGgEx4Z+IiEgwqSz7FxEREQkWSv5FREROkaGz/kVERCRI+LXsX0REJBhVzvavsv+z2K5d/o5ARESkVjTyLyIiIiIiIlLHKfkXERGpLV3qT0RERIKMkn8REZFa0mz/IiIiEmx0zr+IiIhIbT35JBQUQEwMPP64v6MRERE5ISX/IiIitWTyXetPQ/9nrf/8B3bvhoYNlfyLiEhQUNm/iIhILZlO3EREREQkoCj5FxEROUU6519ERESChZJ/ERGRWjJptn8REREJMkr+RUREREREROo4Jf8iIiK1pEv9iYiISLBR8i8iIlJLlbP9Gyr8FxERkSCh5F9ERERERESkjlPyLyIicopU9i8iIiLBwurvAERERESCziWXwP79kJjo70hEREROipJ/ERGRWtKl/oQPP/R3BCIiIrWisn8REZFaMmm6fxEREQkySv5FRERERERE6jgl/yIiIrVk4tCl/jTwLyIiIkFCyb+IiEgt+cr+5ezVuze0aeP9KSIiEgQ04Z+IiMgp0sD/WeyXX2D3bigo8HckIiIiJ0Uj/yIiIrWk+f5EREQk2Cj5FxEREREREanjlPyLiIjUkunQSf+GCv9FREQkSCj5FxEROUUq+xcREZFgoeRfREREREREpI5T8i8iIlJLlZf608C/iIiIBAsl/yIiIrVkOnETERERkYCi5F9ERORUaehfREREgoTV3wGIiIgEG832L4wZA8XFEBnp70hEREROipJ/ERERkdq66y5/RyAiIlIrKvsXERGppcpz/nWpPxEREQkWSv5FRERqSbP9i4iISLBR2b+IiIhIbe3dC243WCxQv76/oxERETkhjfyLiIjU0uGyf439n7XOPx/S0rw/RUREgoCSfxERkdqqrPsXERERCRJK/kVERE6Rxv1FREQkWCj5FxERqSXN9i8iIiLBRsm/iIiIiIiISB2n5F9ERKSWdMq/iIiIBBsl/yIiIrVkQtm/iIiIBBcl/yIiIqdIl/oTERGRYKHkX0REpJYqy/6V+stZ5dNPIS/P31GIiMgpUvIvIiJSSyr6l7PO2rXwxBMwaBDk5/s7GhEROQVK/kVERE6Rqv7PYrNnw5o13p9ng5Yt4dFHoaQE/vhHOHjQ3xGJiEgtWf0dgIiISLA5XPav7P+s1aKFvyM4cwwDLBa45RYwm2H8eBg8GP73P4iL83d0IiJykjTyLyIiIiLHZjKBx+M9AHDTTXD33d5z/wcPVgWAiEgQ8WvyP2/ePK688koaNGiAyWTiyy+/rLLeMAzGjBlD/fr1CQsLo0+fPmzatKlKm7y8PAYNGkR0dDSxsbHccccdFBcXV2mzatUqLrroIkJDQ0lLS2PcuHHVYpkyZQotW7YkNDSUdu3aMX369NO+vyIiUld4h/5V9i91XmUnN5mgvNx7AODWW2HkSNi/XwcARESCiF+T/5KSEjp06MD48eNrXD9u3DheffVVJk6cyMKFC4mIiCAjI4Py8nJfm0GDBrF27VoyMzOZOnUq8+bN46677vKtLywspG/fvjRu3JilS5fy/PPP88QTT/DWW2/52vz888/ccsst3HHHHSxfvpxrrrmGa665hjVr1vx+Oy8iIkHLpBn/5KOP4O23vT/rKsPwdvaZM2HoUOjVCx57DJYvhxtvhAce8Cb+gwdrEkARkSDg1+S/X79+/POf/+Taa6+tts4wDF5++WUee+wxrr76atq3b89///tf9uzZ46sQWL9+PTNmzODtt9+me/fuXHjhhbz22mt88skn7NmzB4APP/wQh8PBu+++S5s2bbj55pu57777ePHFF32P9corr3DFFVfw4IMP0qpVK55++mk6d+7M66+/fkaeBxERCU4a+D+LPfQQ3Hmn92ddZTLBV1/BwIGQkAB//jNMngz33ANbtsD118Pw4VBYCFddBQUF/o5YRESOI2An/Nu6dSvZ2dn06dPHtywmJobu3buTlZXFzTffTFZWFrGxsXTt2tXXpk+fPpjNZhYuXMi1115LVlYWF198MXa73dcmIyOD5557joMHDxIXF0dWVhajRo2q8vgZGRnVTkM4UkVFBRUVFb7bhYWFADidTpxO52/d/d9NZWyBHKOI+qkEOo/b7f3p8aifnqWseE/+MABXAPaBU/oc9Xi8E/pVlvrv24flX//CePppPCNGgNuNdfRoPH/4A57UVG/7gQMxlZVhnjwZ94EDEB7+O+yN1EX6rpdgEAz9tDaxBWzyn52dDUBycnKV5cnJyb512dnZJCUlVVlvtVqJj4+v0qZp06bVtlG5Li4ujuzs7OM+Tk3Gjh3Lk08+WW35rFmzCA+CL77MzEx/hyByQuqnEqjW5ZoAC/v379ccMWepvuXlhAHl5eXMCuA+cLKfo42++w6PzcbuCy7AsNkAsBYXc0FeHgvi47G++y4XPvoou7t0YWWvXvDttySuXs3B5s1xx8djHToU1+rVsHr177g3Uhfpu16CQSD309LS0pNuG7DJf6AbPXp0lWqBwsJC0tLS6Nu3L9HR0X6M7PicTieZmZlcfvnl2A59uYsEGvVTCXQlS3bArxtISEikf/+uJ76D1DnW0FAAQkND6d+/v5+jqa5Wn6MeD5Znn8VUXEyH7t0x+vUDux127cLqdHK5243l3//GuPZaGr7+Og0tFti8GcukSXjOPx+jd+8zs1NSp+i7XoJBMPTTygr0kxGwyX9KSgoAOTk51K9f37c8JyeHjh07+trk5uZWuZ/L5SIvL893/5SUFHJycqq0qbx9ojaV62sSEhJCSEhIteU2my1gO8aRgiVOObupn0qgslq8X58mk0l99CxngoDuAyf8HK2c1O+HH+D667E+/7z39lVXQdOmcOutWO+6CzIyML399uHJoj74ALZtw9ymDQTw/kvg03e9BINA7qe1icuvE/4dT9OmTUlJSWH27Nm+ZYWFhSxcuJD09HQA0tPTyc/PZ+nSpb42c+bMwePx0L17d1+befPmVTkXIjMzkxYtWhAXF+drc+TjVLapfBwRERGROslkAocDQkLgvfe8P19/HaZOBbfbO8nfDTfAsmXeqxu8/Tbcey+89hpMmgSpqf7eAxEROUl+HfkvLi5m8+bNvttbt25lxYoVxMfH06hRI0aOHMk///lPmjdvTtOmTfnHP/5BgwYNuOaaawBo1aoVV1xxBXfeeScTJ07E6XQyYsQIbr75Zho0aADArbfeypNPPskdd9zBww8/zJo1a3jllVd46aWXfI97//33c8kll/DCCy8wYMAAPvnkE5YsWVLlcoAiIiKVKi/1p9n+JegZhrfE/5NPvDP7WyyweDE8+CBYrXDNNfD445CWBmPGQIMG3oT/p5+gXTt/Ry8iIrXg1+R/yZIlXHrppb7blefQDxkyhEmTJvHQQw9RUlLCXXfdRX5+PhdeeCEzZswg9NB5duC9lN+IESO47LLLMJvNDBw4kFdffdW3PiYmhlmzZjF8+HC6dOlCYmIiY8aM4a677vK1ueCCC/joo4947LHHePTRR2nevDlffvklbdu2PQPPgoiIBBuTvwMQOV1MJliwAO64A8aPh+7dvTP233ILPPKId/0f/gDjxnkPCCQkQEUFhIX5O3IREaklvyb/vXr1wjCOPW5iMpl46qmneOqpp47ZJj4+no8++ui4j9O+fXt+/PHH47a54YYbuOGGG44fsIiIyBGO8xUmEjzWrfOe3z9wIERFeZfNnQsXXQQjR4LLBf37Q7163nVHDMKIiEjwCNhz/kVERALWobp/Q4X/Z6+UFGjY0PszWFUevXI4oLz8cFJfWuqdxO/ddyEnB554AmbOPHw/k2pfRESCkZJ/ERGRWlLqIyxZArt2eX8Gq8okfsAAb5L/yCPe2+Hh3p+lpXDxxdC4MRy60pKIiASvgL3Un4iISMDTwL8Ek8rL+q1dC5s2QUyMdwK/Fi28s/ffcw94PN6J/dxu+Pprb2XDhAk6x19EpA5Q8i8iIlJLqnqWoGQywf/9nzfJj4+HkhIwm+GVV+C227wz/d93H3z+ufcKAHl5kJmpxF9+sz35ZUxbtZfB6Y0JtVn8HY7IWUvJv4iIyCnSwL8EleXLvbP6P/cc3HgjbNkCH3wA110HX3wBgwfD5ZfDDz94L/PXpYt3IsBjMAwDk46E/W42ZBfy5/eX8Le+LbimU0N/h/ObXD3+J/YVVVDmdHPfZc1/t8fZW1BGTJiNcPvpTXF+3VdMYkQIMeG207rdEzEMA5fHwGap+Uzt7QdKiAyxkhAZcloft7jCxSP/t4orOzQgo03gzWuybMdBHv9qLSFWM6/e0okGsTpAebJ0zr+IiEgtVaY7x7tijdRxf/kL3HCD92egcbur3HRVOPh65R5KVq2FVq28o/xxcd7kfswYGDECHnoItm71lvnffDNcf321xH9zbjEvZf5CudPNroOldP/XbB6cshK35/D7IL/UgcPl8d0uKHNysMRRZTtvzv2V+z5eTpnDjWEY7C0o872XiitclDurxp9TWE5uUfnpeGZ+E8MwyPr1AAWlzpO+T1G5kx837avyWeH2GGzMLsLp9vD3L1bzyP+tqvIcVvrHl2vYdbCMkZNXUO50k1/qoLDcyfMzN7ByZz4AeSUOftq8ny37imv1eVTudDNjzd4qr9WJjJ2+npvfyiKvxMFT36xj6fY837q1ewpYvaugxvvtzCtlX1EFAN9vzK2ybuYuEz3HzWVTTtFJx3Esm3OLuHjc99z70XLfsoVbDjB/0/6Tuv/2AyVs219SbfnS7Xlc9sJc7vtkeQ33qp2jX6PiChcvzNp4zP3/+5dr6PjkLLbWENfOvFIuef4HbnprAYZhkF9a9X2WW1TOi7M2klfiIKewnOyCmt9DO/NKuX7Cz8xYs9cX43+ztjF11V7+8r+leGromwAut4ei8pN/L5xOHy/cwerdBSzZfpD/W7rLLzEEK438i4iI1JJGO4Vp02D3bu+M/4HEMLzl+6tW0ej/vuSWPYks2Z4PwN8Ld3Dn6tWQnc2emCSiQ61ExsXB9dfj/Hgyq5ZtxmqNY9gHS2nbMIa7e51L2wYxANitZm5+K4v9xQ7yShzUiwoht6iCKUt30SQxguGXNmPN7gJufDOLrk3i+e/t3fh40Q6e+HotZpOJcde358oODShzuBn77QYAmiZG0DghnFGfruSJK1tjAGO/3UBKdCgf/rk7afHhlDpcXPjcHOwWM8vH9MVuNR+1uwaLtx2keVIkB0oqaJYUdcKnKL/UwYbsInqck4BhGMz9ZR/tU2OJj7DX2L7M4cbl8fDd+hwemLyS3i2TePe28wFvEm01m7AeY2T2H1+u4csVe/jXte24pVsaU5bu4sdN+/lm5Z4q7c6pF8FdF59bZVlhmcv3+01vZrExpwi3x8DpNpi/+QBf3nMB14z/iR15pQD0aZXEa7d0Jsxu4bOluzCb4OqODVm7p4BPl+xkZJ/zSDw0QjzykxXMWJuN3WrmgnMTePWWTqzYkc+oT1cwuEcT7rusWZXPuZzCct6ctwWAGyb+zK/7Snj3p610TItl2CXncv8ny6lweRhxaTP+dEFjlm0/SEabFEwmE58v2+3bTl6Jg1e+20R8pJ2MVolM32kBKrjprQW8MagzPc5JOOHrV5O1ewr4x5drcLoNZm/I9R6gMcHgdxfhcHkY3a8lf7mk6vObV+LguW83kBwdwq3dG3P1+J9wuDx8N+qSKiPJL3+3CYC5v+zD4zEwm2v+/He4PBgYhFirn9bg9hi8mLmRDxfu4Mmr2nBVhwY8M209b8/fCsC3a7KZdt+FON0G4TYLX6/cQ7OkSD5auAOA1+Zs4sUbO1bZZua6HMB7UO7vX67ho4U7eG/o+cSG2QizWxj5yQo2ZBexfGc+a3Z7D8zMf7g3ESFV07+np65jyfaDLNl+kF//1Z9rxv/E6t2HD+Qs23GQrk3iAdhfXMHjX69lUPdGvP3jVhZvzeOL4T1plhSJYRjklTj4bOku9hVV8EtuMS63h86N4qhwubm/z3nM+2UfydGhdGkcV+NzOGttNnM25HJ9l1TyShys3VPI5a2TaZYUSX6pk+gwK6M/X81XKw6/fxZuzePeI7bh8Ri4jarVEu/M38rbP27h9Vs7Exli5Zb/LOD6Lqm0TIlixppsHrj8PFbuzKdTozhapJz4MySYKfkXERE5RRr3l4BjMkF+PtaePelQXs4lm8tYcvFgAD4vj+LOli3Z9dIbXG+0J7FZI74afiHORk3Y47Hx78+WsHWjmezCcvYWlPuSixbJUUy770L2F3tHFv+3YHuVh/x82S7uvuRc/vHVGkodbn7avJ/Ccif/mraeikMjyw9+tpJLWyax6tCINcD7WdvIPzSK/sQ36zCZvMcuduSV0veleXRqFMvVHRvgdBs43W7u/XgZTRMjydpygGeuaUuD2DDm/pLLA5NX+rb5wR3dSYoOoajcSce0OCxmE6UOF7dPWkxaXDjjrm/Pn95dxKpdBbx/ezfySip4YPJKLmyWyAd/7g54E5x7P1pOj3MS2JFXylcrdhNqs1Bc4U3G52zIZdTkFczbtJ/8UgdtGkTz+T09+XHTPqau2stf+57H41+tZfO+Yrbs847YPvrFarbnlfDm3C01vmzPfruBmDAbV3VoyGtzNlHqcFN4xKjqyqNG1VfuzGf7gVJf4g/w3fpc/j1rI0PSm/C3Kd7nZNSnh5+bnMIK/vOnrng8BjPWZgPehPWHjfsY/uEynG4P+4sdvPTdL5ybFMEf2jfwndpx5MGKX/cdHoVesTOfYR8s9d1+/fvNfLF8N7vzy3j++vbc0DWNbw+NKANsP1DKS9/9AsDSrfV9y/NKHPzx7YWseLwvkSEnTk+KK1w8/c06Lm+dTI9zE7j69Z9wHTFCPX/zfmwWk6+y4dXZm7itZxP+8eUavt+4jz6tkgmxmpm8ZCcA01bv9fXF1+Zs4sGMlny/IZc+rZJZt6fQt93d+WXUjwll2uq9RIfZuLRFEgBOt4eBE34mu7Cc6fddxOrd+XRvmuBLtN+Zv4Xx3/8KwCeLdmK3mH2JP3gT+D+/v4QVO/IZckETXv9+c5X9XbEznxsnZnFJi3oMv7QZABuzD1cLVB4k+OunK8kvdXDkYP2PR1Q+LN1+kIvPqwd437eLtx1k1qH3OcAXy3dXSfwBJi/eyaJteezMK8Ph8jBt1V6mrTr8mj76+Wo6NYrlfwu2U+qoWrUD8POvBwDYlFvMDxv3AfCXi8/h40U76N+uPu1TY4kOs+L2GNz/yQrvc7R4p+/+/83aRsuUaBZty+PyVsm+vltpyfY8HC4PdquZ79bl8LfPVlLh9PD1CO9BCfAe4AAYOOFn0uLDyCtx8Na8w+/FyucgKsTKswPb88PGXJZuP8j/3X0Bkfa6dbDfZKhm8bQoLCwkJiaGgoICoqOj/R3OMTmdTqZPn07//v2x2c7seUsiJ0v9VALdV8t2cv+nq+jWJI5Ph13g73DEH1JTD4/87wqwstOyMtxDhrA8aw2tc7bweZvePJYxHIAN7rns/t9nTDunG1+16cX9t/Sky6dvY3z8CdcO/jf7Ir0jfA1iQtlzRJnwxD92qZLkHe3W7o18CQjAqMvP48XMX4gKsRIdZmN3fhldG8cRG27ju/W5x9xObcQeOv86/4gy/MoDCJXqx4TS45wEvli+++i7c12nhqzcle9LZhf/vQ9Ot4cnv1nLzLU51dofz/2XNee1OZs4RoX0cYXbLTUmTTWJsFsoOdT23t7NeG3OZtqnxjC0ZxMemLySVvWjGdi5If+ctr7G+9/QJZXLWiUf97UE73N7baeGTF21l4axYWzdX0JBWe1KvNs2jOaVmztx2QtzsZpNRIRYT7iN67ukck+vczmnXiQ/bd7PwVIHf2jfgIJSJ1NX76F/2/qM+XpttcqJo93UNY0wu4VJP2/zLQu1mSl3ntxpDnarucZTIi45rx57C8r4JacYi9nEV8N7sv1AKY9+sbravvVtnUy/diks3JLHVyv2UOY8udf4RCYNPZ8VO/OZsmQXu/PLanXfm7qmAfDtmr0UlrtO0Nrr6PfU7yUtPoyded79CbWZsVnMFJ0gxuhQK4XlLv7v7gvo0jiOoe8t4vtDBxguap7Ir7nF1IsKqXbw7GS9cEMHrmqfHPB/k9YmD1Xyf5oo+Rc5fdRPJdAp+ZeATv4B55gx/Drhv7yRfgNPZU5kWssL+XvGCPq3S6HV+Oe5cNsK2uZsZktKUxqWF3DTVY+xNvlwWfRTV7dhzFdrfbe7No5jyfaDVR7jwYwWLNhyoMrIYpjNUiXJuaFLKvVjQnl1TtWRzKs7NqhSultp1OXn8da8Lb5R9t/TOYkR7MgrrTJiXJOnr2nLuG83UOHycEmLer6KiKSoELo0juPbNdnHvX9NmiSEc22nVDZkF/LCjR14fuZG3vtp23HvM7JPc4Zdci43vZlVJZm5pVsj7u3djAuenXPM+9osJpzu3/Ynf72oEN+5+5WOTNhq0q1pPIu25nHxefU4v3EcL2T+Uq3NP69uzWNfrauy7L3bzufO/y7B5TGY+MfOfLZ0N9+tP/kDMinRoYSHWHyVF5WOPHgC0DA2DKvFxPYD3gqKyBBrtb4XYjWTFB1y3P08GYmRIewvrvr8nZMYQXZhebWDPyFWs69qxp86psWy4ohqneMZ84fW3Nq9EeNmbOTdn7b6ll3ZoQH9Xvmx2r4fy+y/XkKj+HAsJhM78kr5w2vzj/l50KdVEt+tz+W2C5qwIbuQBVvyamwHcH6TODLapLByVwH7iyrI2uKtSOjUKJYOqbFEh1qrfU4lRobw+bDuLP9pTkD/TVqbPFRl/yIiIrVUeSqsjp6L37nd3nP8j1L20GiK3/2MBoX7+W7Yo1z/2uMYmHiM4Uy/eDDnPDic/345j/3lHjYlNCI7OpEbu6ayYEse13RswAXnVj33+sjEf0C7+vzr2nbEhNu4uHk9fs1dQnZhOTd0SSM5JpRXZ2/ytb22c0OaJEQwddVeYsJtpMWFExdu4+F+Lasl/2E2C9d0bMj/Fmw/5eTfajbxr+va8dBnq47ZpkvjOJZuP8iWGiZRO9qVHRowuEdj+rZOxgQkRYcy4Ydf+W/WNt4c3IWGsWGs3JlfpUoCwG4xM/uvl7BubyFdGsfxr2nr+fxQ9UGH1Biev6ED5yUfPrf44Sta+pL/FslR3N+nOX/9dCVXdqhP75ZJzFiTzdCeTQm1WWhVP7pK8t+mQTT1Y0JrTFwB1jyZQWSIlRlrshn+0TLcHoNeLer5SrC7N43njUGdeWveFtqnxjL8o2XVttEhLZZnrmnLp0t28t8s72kfG56+gi37Suj/6o8AtGsYw+rdBdgsJv7YozHv/bSNRVu9yVj/tilc27khs9blsL+4gr1HPF8DOzVgfXYxHx5ROTJ00mLf78M+qB5PpX9d244t+4pZn11IqcNNk4QIpq/eS3ahd/shVjMXNU/0VZu8e9v5lDrdDH3Pu/2/9j2Pxgnh3PfxCipcbl69uRN/nbKSMJvF1z/GXd+e9XuLmDj3V9/j1o8JrbIPFzVPxO0xfCXuNbmuc8MqpeYAr9zciR825lY7KPJgRgt6Nkvko4U7KHe6mXKMSe3ap8aQU1jOwM6pfLc+h19yiokNt/mqYTqkxrBqdwFPXd2Wf3y5pkq86ecmcMeFTdmwt4iGcWE89sWaaiX1r97ciT4vzsXhPnwg4sgDfNd1asj67CKaJ0UytGcTTCYTrRscTj5b1Y+mXlQIbw7uwj++XENUqJXYcNsxK2uiQqyckxjhm2+iSWIE4wd15oVZG1l1qM+3TIliY04Rwy45lwi7he/W51ap8ACICbNVq8S47YKmDGh/+FSTfUUVlDvdpMWHA95TNxrEhpG5LofZG7z9ZX9xBX1ens9jHWoMNygp+RcREREJRpWT+61bB59+6p3FPzERIiNxVjj4sUknGhbmcvW/32GO1cwNLz2GYTKx+tF/0f+a9hQ0aMyjX6z2be4vl5zLuOs7HNp0zYe23hrchb5HXPqrXWoM8x/ujctjYLea2ZxbzMeLdmAY8NAVLbjg3EQA5vytV7VtJUeHkFPoHQ384W+9SIi0ExVq45zECN8Ic3yEnbwjrhaw+Zl+NPv7t1W2ExNmY8lj3pL94goXSVGhADz02SoSI+00ig/n3zd0YM6GXLo2iadDagwXjfueXQe9I7n/vqEDeSUVTF21l9H9WnFuUgShNgsb9hbRPjXmUKyhvse7u9e53N3rcJXE/Id7c86j0wHv6RLPXNuO6DArafHhvsTiyg4NfMn/B3/uTlRo1RHEUJuFJ69qwyuzNzF2YDs6N4qjd8skQqxmTCYTV7Q9nLT8Kb0JW/aXsHhbHuE2C5ecVw+TyUSI1Uzl4OqiRy9j1roczq0X6TuH/oq2KcwceTEAzZIiafLINADOqRdJQmQIo/u3qva6//OatvyxR2Pf7XPqRVBU7qJDasyhAxFRtKofzb6iCt4e0pX3f97GwEPVHrPW5vjK0vu2SSHEauGLey7AZDLxp3cX8tNmb6JstZh952YfyW4xEx5i8SWy9WNC6ZAaS7vUGJ6fuZGU6FBu6ZZWbQLW/cUVvmqUfm1TyGiTwnfrc0mIsNOtaTwuj0FGm2Riw+xc26khJpOJ+Q9fiscAi9lE1ujLADhY4mB3fhltG8bQuVEpi7flMbRnEy45rx7zftnvO0jy3aiLaZYURZnDzYDXfiQ2zMazA9sz8YdfKXG4fInuhc0SqyT/D1/RkrYNo2lZP4oOabGYTLB4ax6lDje392yK2Wzi6WvaMveXfb7k/6auab55CsJsFl64oQPNDx1E6tM6mQVbDhBms/DkN95Kisl/SSe/1ElKTChv/7jFV+Ew4Y9dfP2iQ1osAK/d2om9+eWkxITy8P+tolX9KBolhPPMtW15Z/5WNhyaY2DS0PP5+dcDvPfTVob1OrfKQSyA85IPv5YtD02e16VxHNPvvwjwztfQZeF2erdMos+L8wDvAagl2w8y5IIm1V7PS86rxyXn1eO12Zv4dV8xzw5sT3GFi9gwGwu31jzSP3PkxazYme+9kkJu8aHnJ6lKm3pRVS+PaLOYublbI246P435m/cz+J1F3sdvnkiM/finmQQTlf2fJir7Fzl91E8l0H29fCf3TV5F18axfHZ3T3+HI/4QKGX/Bw9Cixawfz/cdBOUl8Mjj7D3vDbcMHoy3753L5FffIZxRT9+fXEC5zz5MOaB12Oa9B5Ot4cLn5tDTmEF13RswMs3d6qy6XV7CsktKuf9n7fx/cZ9RIZYWfl4XyzHmO28tp76Zh3v/rSVcLuFdU9d4Vu+bX8Jf5uykhG9m3F+k3hKKlz8N2s7bRvGcEXbFL5asZufNx/gus4NGfLeIvq3rc+LN3Wstv39xRUkRNhrvDrHip3eme2jQ218+pf0alcRqK135m/l6anrmDCoM/3a1a+23uHycN/Hy0mNC+OxP7T+TY9VyTAMHG6Pb3b5/1u6i9Gfr+blmzvSv4YYjjZjTTafLd3FCzd0qHL9+g8Xbmf2+lxeurHjSV3XvtThwu0xqh3Q2JPvvUxhl8ZxPHxFyyrrsgvKeeyLVbQ0Z3P/Lf34bsN+7v7Qm0w/mNGCST9v45WbO1LmcHPH+0tomhjB1Hsv9E2gt3DLAZKiQ2maGFEtnu835HL/J8spd3mY8pd02qfGMHXVXro2iaN+zOm5Hnypw8Udk5bQsn4Uj1/Zpsq6I68IUOFyc9kLc/F4DOb8rRdLth3kjR828+x17WmUEH5Sj+Vwebjzv0tIjAzhhRs7+C6FWepw13iFinKnm9Gfr+aCcxO44dA5/uC9WsGQdxcxoF19xg/qXOt9/t+C7YTbLAzskgrgmwzyaG6PwcjJK4gLt/HU1W2Pu81/z9zI1gMlvHxTR5xuD+H22o1LF5U76fRUZpVTd+LCbSwf0xeADdmFDHl3EXdfci639Wx6rM1UU+Zw02rMDAA+H9adnSt/Cui/SXXOvx8o+Rc5fdRPJdB9s2IX936yUsn/2SxQkv+CAnj5ZRg7Fm68EZo3hwkTKOl5EY9UNKbFwV2MaB8Hr70GDgf8738wZgysWAFJSazdW8jXK/dwT69mxITV/HmbV+Lg+ZkbuapDA9LPPbVLsdWkwuVmwg+/0rd1SpVS4dooKHMSGWI95QMSx0pgTmU7hWWuk0qWf09uj3HaDs783o78rncZZoa8t4gOqTH8fUDVgyOrduWTGhd+zEsx1sTjMXzVKP5WWO7EMDjm++tM+nVfMfVjQmudZAeyWWuzKXG46JAay+Nfr+W+y5pz/qFLE/4WP27aR7nTQ6/m8QH/N6nO+RcREfkdBcef1nJWiImBkSO9pwA8/TTMmgUDB1I25QsefvFVkosPwKZEeOopSEiAwYO9BwlivOXsbRrE0KZBzHEfIj7Cztjr2p320EOsFkb2Oe83beO3JlSnI/Gv3I6/E38gaBL/o4XZLXz6l/Qa17VPja319sxmE/YAeS6iQ/3fLyqdW6/66RXB7sjTkP53R/fTtt2Lmnsvieh01u4qF4FOyb+IiMgpUu3cWeyWW7wl93Fx/o7Em8j/9a/ekv++feHTT9n353u46WAzBv0yj4f/0teb+AOEhnr/iYjIWUfJv4iISC1ptn/h+efP/GNWzuzv8YD5qHLmqCh47DFv57zxRiL+/RqFoY35Iv0qHr4648zHKiIiAcf/J8KIiIiIyPG9/z4MHw4VFd7E31PDNcAjI+Hvf4eHHybtb/dy1bq5WGwa5xERES8l/yIiIrVkOnTWv+bMlTPC5YLVq2HJEu9kfSc6APDoo+wedj8vf/Nv+qz98czHKyIiAUnJv4iISC2dpjnCRE6O1QpPPglXXglZWTB69AkPAGy//R5euvBWdjU458zHKyIiAUnJv4iIyCnSuP9ZrGVLiI72/vy9uVwQEQE33QTt2sHUqd6Z/R2OYx4AKA+L4LULbia7YZPfPz4REQkKOhFMRESklioH/lX1fxYrLoaiIu/P35vVCpMnw+uvew84FBfDm296k/+nn4aQkGqTADpdHjCZsFk0ziMiIl76RhAREaktlf3LmbR6NQwbBkOGwP/+B1u2eKsAvv/eOwdADRUADrf3dyX/IiJSSd8IIiIip8hQ4b+cCTt2eCfy69cP4uMhNBSeeQa6dIG33/b+XjkHwCEOt7dv2pX8i4jIIfpGEBERqSVT5Yx/yv3l91R5XklsLNjt3oMAAG43xMTA2LHekv+334annqpyV6dv5F9lKiIi4qXkX0RERCRQHDmRROVBptatwWKB55+Hgwe9v4P33P9OnbynAwwbVmUzTpX9i4jIUTThn4iISC35JvzzaxRS5xiGN+H/4QeYPdt7bn///jBoEHz1FaSnwx13wD33QJMm8O67UF4Of/0rJCRU2ZRTZf8iInIUJf8iIiK1ZFIltfweTCb4/HNvgt+vH6SkeEf1MzPhrbfgxx/hllvgrrvA6fRO8Pf119USfwCHS2X/IiJSlZJ/ERGRU6RL/clptXUrjB4Nzz3nTfDBe0m/+vW9l/tr1w5+/hm2bYP8fGjWDBo0qHFTlWX/dqtG/kVExEvJv4iISC0dLvtX9i+nkcMBcXHexH/TJrj0Um/J/9ix3vUrV0KHDtC+/Qk3pXP+RUTkaEr+RUREasmkun+ZOBHKyiAs7NS3UXmOv8vlHdk/cAB274affvKW+/fvDxMmeNsuWgTPPuv9d955J9x05Tn/KvsXEZFKSv5FREROkcr+z2J/+MNv34bJBAsWwN13Q1YWXHCBd1K/Sy6BgQO95/lX+vJLyMnxXuLvJBw+518j/yIi4qXkX0REpJY0liqnTeXIf2YmXHkl3Hwz7NkD+/Z5R/uLiuDbb+E///FO+JecfFKbVdm/iIgcTcm/iIjIKdLIv/xmbdt6E/r33/cm/9dd57183yefwIUXQosW3tH+efNO6lz/Sg6V/YuIyFGU/IuIiNTWoXxKuf9ZbOlS7wR9djt06XJy96k8x9/tBovFuywiAp5/Hnr3hk8/hRtvhFtv9f5bt857YMBigdjYWoWn2f5FRORo+kYQERGpJZMK/+Xqq73n6F999cnfx2SCWbO8if3kyYeXt2gB/fp5R/ddLvB4E3dat4aEhFon/qCyfxERqU7fCCIiIqdKdf9SW7Gx3nP6n38ezj8fZs70VgLcfrv3vP4NG8Bs/s19q3LCP7vK/kVE5BAl/yIiIrVkUtm/nKpu3WDaNHj7bWjSBP72N+jb13uef3o6/Otf3ksI/sbLSR6+1J/+1BMRES+d8y8iIlJLGkuVk1J5jv/SpbB8uff3Cy6AVq2gY0eYMgXmzPGO/t96KxQXQ4cO3tL/30hl/yIicjQl/yIiIqdIVf9nL5fHwAp4jGOUUVYm/p9/DvfeC/Xreyf3e+QR+Oor70EA8E7017s3/PGP3uU33ABRUb85Pk34JyIiR9M3goiISC39xors311BqRPXoeTPnzye6kdHjDpyxORgqQOAgjJnzQ1MJvjxR/jLX+CJJ2DJEnjhBThwAPr0gRkzvO08Hu+/du3g0Ue9k/+dBk5d6k9ERI6ikX8REZFT5DYMlm7P493522gQG0q/dvUZ9r+lnJccRUy4jfxSBy1TonkwowXLdhzkuW838Gj/VrRLjWHMV2tJjg4hr8TBql0FZLRJIf3cBMJsFto2jOFgiYMl2w+ybX8JbsPgus4N+XzZblqkRFFY5uQP7RtgMR9O7N7/eRvLdxwko00K932ynCva1ue1WzpVifeblXt49tsN/LXveTRLimRTTjE7D5byh/YNOCcxgr9OWcmyHQdJjg5lb0EZQy9oiscw+HzZbro2ieOxAa1POJI8c202DpeHMJuFuz9cysg+5zH80mYYh7bz7IwNpMWFEWa3cKDYQYfUWJonR3LJefWwW800ig/nrXlbKHd6+FN6Y+Zv3s/UVXt4pF8rIkOsjP58FZe3Tuam8xuxaGseO/JK2ZtfRnykna37StiUW8zBUgc2i5k2DaK5qHk9NuwtZN6mfTxxVRvaNIjxxTpnQw6bcoq5rFUy93+ynEtbJHFpy3p0bhRHYbmLUZNXkBYfToXLQ6nDxW0XNGH2+lyaJEbQ81ByXe50e5N3s5lpK3bz8BdreG5gewY0j8U1K5PFGTfyitGWu+cs55KhA/EMGcL23CIaX30N5sxZbDivI1mb9tGjWT0iQ6w8P3Mjt/VsQudGcVWe148W7uDdn7ZydYcGJMeEEhVixeUxiAmzMXnJTgDuuugcOqTFMn31XpbuyAdU9i8iIoeZjLpyCN7PCgsLiYmJoaCggOjoaH+Hc0xOp5Pp06fTv39/bDabv8MRqZH6qQS6+b/k8Md3l5x0e7vFjOOIkfiWKVFsyC46Zvs+rZKZ98u+Kvc5Wrjdws3nNyKnsJxpq/fW2KZZUiT1Y0IBSI0L4+NFO48ZX7OkSNbtLTzufvRtnUyvFkkcKK5gX3EFhWVOQqwWDpQ4AAO3x+D7jfuq3e+KNikcKKlg8baDx93+8aREh5IYZWfNbm+MoTYz5c7aVTdYzCaaJIRjMZsoc7rZmVdWY7sOabGUVLjYnFt8zG1ljR9C/eID7I2I56PPfyYmexe/fvQla5LPZXX95rRIjqJ1zma2785jY2JjPvz0H5g6dGDCLQ+xb+Ycpnz4MJhg8I1P81OTjphMVU8jyWiTzIXNElmxswC3x8OXK/accP+sZhNXdmjAF8t3+5a9+6fO9G5d/+SfJJEzRN/1EgyCoZ/WJg/VyL+IiEgttW0QTZNIg23F3pF3u8WMgeErtQZo0yCaazs1ZOy3G6ol8Ucn/rd0S+OTxTt9yd9363NOGEOpw827P209bpvNucXHTWDtFjNhdgsFZU5f4t+vbQqNEsL5z7wtHF21P2tdDrPWnTi2o81Ym+19PKsZj8fA5TEY2DmV7ufEM27GBvYXO6rdJzHSXmV5dmE52YXlvts1Jf7n1otgcI/GfLc+l/mb91db7/YY/Lqv5ITxrtyZX21Z18ZxLNlew8ELk4lZk7/jjS/HkprYiL1RCQBszCliI8nQMJkLDvyKyeNhVMNL+XVtNs1DIpje8kJ2RdcjO9Lb/uihmJlrc5i5tvpz3SI5isQoOzmFFb7Xtm3DaGLD7MzfvL9K4g+QEGk/4f6KiMjZQcm/iIhILUWEWLm/rZvQc7qy/WA5N3VNo6DMyTcr99A8OZK8EicZbZJJiAyhT6tkFm/LY+WufLo2jmfLvmIsZjPnN41j8uKd9G2dwoD29bmsZTJ7CsromBbLx4t2sCOvlNsuaEr9mFDC7BY25RTTpkE0v+QU8eu+YjZkF2E1m0iODiUq1Mr+Yge7D3pHsh++oiVfr9xNqM2Cy2NQVO6kzOGhW9M4KlwePlu6i3t6NaNxQjix4Ta+WrGH/FIn7RrGcGHzRAD+1rcFHyzYzvq9hTx0RUs2Zhfx3k/b8BgG9SJDSIyyExtmp7jCRVy4DZPJRG5ROb1bJrEjr5SFW/K4skMDYsJsZK7LwTAMbu3emJSYUEodLsLt3j9BBnZOZU9+GRPn/krDuDBW7swnzGbh2YHt+SWniHC7hXqRocxal83aPYW0T40h1GYh1GYmJsxOdkE54SEWGsSE0TghnFCbhSEXNGHK0l2kRIfSIDaMlJhQwmwWpizZicPtoVm9SMxmEy2SoygqdzHsg6U0SQynb+sUVu7K58dN+wm3W4iPsOP2GNzarRH92tU/dDCliEc+X+2b9yHU5OHzyY/y82UD+THjZpo3TqV0ZwE9zk3A4zFo0yCajC1uTO9spmeTWFo0SeHe72eRlBzCF6OfpFuJwVsXNeXxr9ayO7+MTo1iqXB6CLGZ+WnzfvYXO2ieFEnbhjE8fEVL6kWF+PrhvqIK5v6yj35tU7Bbzbz/8zbW7S1kx4FSrulYn41rV9O6fuBWI4qIyJmlsv/TRGX/IqeP+qkEOvXRs5vHY2AqLsJUXg7DhkGDBvD664cbOJ2QkwMlJYcn8PvDH2D6dDj/fFi3DubP917W7zgq/0QzncIMk+qjEujURyUYBEM/Vdm/iIiIyO/EbDZBdDSEh8P+/d5L9VWaOdM7k/+770JCApxzDnz3HUyZAv/7H5SWwoAB0Lz5CR/nVJJ+ERGRY1Hyf5Zx5eZiKSzEMAzcRUWYrFZMFgsmu52KzZsp37ARk91GSPPm2Bs39l2CyFNWhutAHkZFOYbbA24XptAwbMlJmEJCMFksuIuLce7ciaesDFuDBliiozE8HiwxMd7tWK1UbNoEBrgPHsRks2JNTsYSFYUlJgZ3fj7l69djiY/HnpaGOTwcw+3GffAgnrIyLNHRmKOiMBwOKjb/iiUmGqOiAufebEw2K+bIKG/s55xDxa9bMBwV3thMJuxNm1KycCGuffsIbdUaW/0UHFu3Yk1OxnXgAPbUVExhYVT8sgnDUYG7oACTzY69UZo3rnXrCDn3XCzx8ZhDQzGcTkxh4XhKSrAmJuAuKMAcHo4lKgpPeTmu3FxcublgsWJLTsLepAkAhsuFp7gYd1ERnqIiDI+BUVYKZjOh7dphsljwlJRQumwZlogIrMnJGE4nju07MIeGYE1Kwt64Ma6DB73PeUEB1vh4PA4HnuJirImJh16LEhy/bsbjcGCUV2AOC8UUGoYlNhZ7akMcu3ZRvmYttrRUwjt3BouF0oULMdnt2Bs3xhIdDYaB68ABXPv2effXYsEUEoI5LAzDMHDl7sPepDHWevVw5eZSvmYN1qQkrAkJOHbtxhwZQWiLFhhOJxUbN3qfiwb1scTH49qzB9f+/djS0rDExuLatx9zRIT3OSovw2O1Evbrr1T88guesDAMpwsMD44tWzBcLmwNGuAuKCDk3HMxR0ZiVFRQsXUbtvopmCOjsCUnYbjdOPdm48rNxZba0PsGMAxMZjOW+HjchYXe/m+3Y7LbweXCdeAAhsOB4XJ7+2dSEiabDU9pqbetzYanpITyDRsIa9MGU1iY93U2DNwFBZStXIUttSHWhAQssbEYLhcA7gMHCGnRAsPlonzdOuxNmmBLSsKZm4unpARbcjKmsDBMJhOe0lJcBw7gPnAAc1QUppBQjPIyTDYb1pQU3PkF4HLiOpiPKycbw+XGXZCPvVFjLHFxuA8eJKxzJxzbtgFgttsxx8RgslpxHziAu7AQ1/4DGC6nt++UV2BvlIZRUYE1ORmT1YqnogLD4cQSG4P7wAFMNhuG04k1MZGytWu9z4fFgi0lBY/DgS05GXN0NOVr1uDYvoPwbufjKSrCU15OSNOmYDZTtmoV7ryDuAsLCO/SBUzeGchd2XuxpaVhrVcPT1kZRlkZ7qIiMAxsqal4CgqwJCTgLizEEhuLY/NmXAcPYomMxBwVhSUqClNICJ6SEjylpVji4nDu2oXhcGBLTcWxbRvmiEhCzmuOc+dOKn7dQmiL8zDHxODOy8NTUoJj2zYscXGEdeyIp7QUo6ICc2QklshITHY7hsdD+apVYLFiTaqHc+dO3GZz9ZO05exTWgr79sGqVbBxI3z+Obz/PrRtC08/DZGR8Mwz8OCD8PzzcNdd/o5YRETOYkr+zzIH35vEuR98wK/PPgdut2+5OTIST/GxJ4U6HUwhIRgVFTWsMGEKC8MoKzv8x7TNhiUqCnd+vvfAQSWLpUrcNTqZNkczm70JTk3xnQaW2FgMpxNPyYknmjrjanq+rFY4lLj+FuaoKIyKCgzHEZN5Hb3tY7xeacDOt/5zSo9bmSziPM71t49I3Ezh4VX735FNDyW+J7Odk2Y2ew94HTw8eZg5OhpcLjylpbXfXgA45mdI5chlkCbKJrsdLBZv/zhKcteu3hFcOXtFR8P48ZCRAbNmQV6eN8m/7DJo1sz7GTR5svcAgYiIiJ8p+T/K+PHjef7558nOzqZDhw689tprdOvWzd9hnTaeoiIMkwnTUcmWp7gYbDbC2rXDcDgoX7++WkJmjozEHBYGFgsmsxl3cTGewuNfFupIRkUFmM2Yw8Kw1quHu6QY94E88HgwDiU81uRkjPJy3AUFuPPyvHc0mTCFhnr/+D4UkzkmxvfHuL1pUwy3C09RsXe0sTJ5stkwh4aC2+0dqQwLI+Scc3Bs21YlCbfExuLOz8eo8FYKYBiYw8JwFxR4H95u9yZqRUXeJNBkwmy34ykrwxweXnNCbzJhDg+HQyP57vz8qqvDw7FEREDlaHJhYZU21gb1McrKcRcXY7LZsNWvj8lsxrl7d5Xk0GS3exNrsxlzaGiVdbbGjTBHRGAOC8coL8dTXo5zzx6MsjLMMTHekefsbDxFNVxurDI5t9mwJiZittsxXC4Mh8M7MmoYWKKicOUcnona3uxcHNt3gGFgT03FlZfn6x+WxERMZjOuffu82zaZvKPvBw54X9MakmhXRAR2s9nXb0xWKyHNm+Pavx/nzp1Y6iVilJb5nn9LvUSMklI85eXe7QJYLN7XLi8PzGbvP5er2mNV9r/K18NkNnv3t6Li2Ik/HN6O2TuKHdqyJZ6KCtz5+d7qEbMZw+GoeuDr0MEPX+J/6PaR7yVTaCjWQ9UJhtvtHY2vfJ0OPRfm8HAsCQmHRsgb4vh1i7cfH4q7Muk22e2+xzbZ7ZgjIrClpnqf4+xszJGROHbtqvF5qeLQa2QKC8OWlIRhGDh37/Ym/QUFeIqLff3atW8fpvBwzCEhvv00hYZiS0nBEhND+bp1YDJ5qzhSUnzVJYDvwJ9xqJrlaOaoKGz16x+uoCku9sZttWIOC8NTVOQ92GYYeAoKsDdp4n098vO9nyWV7xnD8FWy2M85B9eB/Ti37zh8MPJQn6g8cGWy2XzVAubISDyFhcQsWeKtLGnY8NjPm9RdL74IhYXeAwBbtkBuLjRuDImJh9tYLBATA2lph99fKuUXERE/UfJ/hMmTJzNq1CgmTpxI9+7defnll8nIyGDjxo0kJSX5O7zTIvmfT7Ps/K706d6d0Hr1fH9gV2zeTGi7dljj4gDwlJbiqajwnm9YWe5tr365IMPp9JZJO52YIyO9f9yYzd6Eu7zcW45bVARmM+68PF+Zv+/+Hg/uysTBbsd66Hl2bNuGUV6OJSEBa3y8N/lxOHAfzMdktWCJj/duwOPBZLEc3p7LhSsnB3NMjK+MHJcLZ04O1qQkbxLr8eDavx9rQgJGeTnmiAgqtmwFw4O9aVNMhxI5gPKNv2BLTsISG+t9uENJlDkkBMMwvGXaZWXekcFDyYIpNBQ8Hm/yD7iLS3Du3oU5NBRzdLS3lPioCUMMw/AmqIbhLQNPS/P+gXj0/h16vszh4b7kxXA6vQmryYRr/37vAQyTGUtkRLXXy1NejrugEFtykm97rn37ABOW2BhfkuguKPCWPcfEVHk+jubMycFkNmMKDfVWahxKxCxRURhOJ+UbfwEThLZujclkwnA4cB044E1u4+J8p0jYUlLwlJXhqajAGh9PRV4eM3/6iX4DBlSZXMVkMmF4PHhKSnz9yPB4MFwuX/90F5dQsX4d1nr1sKWmYrJW/ZjzlJXh3L0be6NGGIBRXo5r/wEs0VFY4uN9+2sYhvegTFEx1sQEb58+lDRWHjDylJZhjgj3niYBVV4r32vmdHoPAhUWehPUiAhcuftw5x/EVr++9wBSeTnOXd4+YkmshzkivNq5vq59+3AXFWNv2uS45wEbTqe3v8fHHx61rqjAU1bme39Xu8+hAzCu7GzMUVHe/mUyUb52HdaEeO/zYrd7+0VEhO+5rnwPuA4e9J4GktoQU3g47v37vfexWHDl5eHcs5eQZud6D8ZVPieHDsaYDh0E8JSXYw4J8b03DLcbo7zcezrO5s1Y69XDKC/3nppwRJ80PB7ve8Bux2Qy4S4u9h6kNAw85eVYIiO9768DBw4dDAvDcLvxlJVjttu8z5GvP+diDg3BEhPjjamkxHuQobgEW8OGWCIjfPu85dprqVi/gbJlywhT8n92evFF2L0bGjaEUaO8Cf6RHA5v6f9PP3lL/5X0i4iIn2m2/yN0796d888/n9cPzdjr8XhIS0vj3nvv5ZFHHjnufYNhtn/DMPh13jzWLFxFo7Q0zMdJ6kT8yePxsGPnTvVTCVjF81dRsWYr1tR6hJzT4PRu/HR/K5+oogNAeWmtdXppDCFFBVRExbD8gaeqrEtctZiIPTtIWLucDbcOo7R+6ul98BP1ERN43B4KduwhzHWSL6/J+5+pcvNHXmnAbDpU/XPowQ0wDI/3dm0rGkymmuPRwZEgdezOaBjG4T5yZLPKl9pkoryigtCwUG8/q9IHjriP4fsPDKPaI5qObFYTzxF99Dd0M1PlNnwhGsd/3KMd+X7x3fHI2EzezZ/se+GIbVWLocbHwrsT5t/xvXYq31+mY0xuWsN31/E2f+oTpB7/fma7mf1XdWPAUQNSgUSz/Z8Ch8PB0qVLGT16tG+Z2WymT58+ZGVlVWtfUVFBxRHnhxceKtl1Op04j1cm7EdlrjJe+e4nWu7rwe4t/o5G5ETaqJ9KALsAWlzg/TXn+C2lbmrrsRECOD02lua09S2Pzd9J6qLVlNqj+H7AKxw0N/ZfHwlv76cHFhGpG6zOYuKpCNj8DqhVbEr+D9m/fz9ut5vk5OQqy5OTk9mwYUO19mPHjuXJJ5+stnzWrFmEHyr3DjQOw8H+iN1sda32dygiwu8/2Kqyrt+PyYC0/Qb23/i3gG+UtaYVJ3KyL/DxtnXENo4Zi9TMcPp+WspW+RYXhcB3PQfisVhx2AuqrPstTqqvHNXAZYGi8KqLjSPam45qf6wRVNMRbSvv7xuUPWp7xnH6W+U2jt6X4BjzD44oA40BYDrG620c+t04avnR9692xxM8HjW8LX7jB9yJHrrWmz7qfVT5PBz5fJz0gx8vhiMe58jHCKTPet8+1/ItdsLPkNO0ky5bOfGcQ2Zm5unZ4O+gtBaTRSv5P0WjR49m1KhRvtuFhYWkpaXRt2/fgC777927N3PmzKF3794BW7oi4nQ61U8loKmPSvh3z0NpAeGxEdw68XZ/h1NNZR+9RX1UApQ+RyUYOJ1OfpzzI5dffnnA9tPCWkzAruT/kMTERCwWCzk5VWvzcnJySElJqdY+JCSEkJCQasttNlvAdgyAaFM0dpOd6LDogI5Tzm5Oq1P9VAKa+qhUnpdrNpmIDgu8g/7qoxLo1EclGDitTkwmU0DneLWJSzNpHWK32+nSpQuzZ8/2LfN4PMyePZv09HQ/RiYiIiIiIiLy22jk/wijRo1iyJAhdO3alW7duvHyyy9TUlLC0KFD/R2aiIiIiIiIyClT8n+Em266iX379jFmzBiys7Pp2LEjM2bMqDYJoIiIiIiIiEgwUfJ/lBEjRjBixAh/hyEiIiKBrHNnSEuDevX8HYmIiMhJUfIvIiIiUltff+3vCERERGpFE/6JiIiIiIiI1HFK/kVERERERETqOCX/IiIiIiIiInWczvkXERERqa2rroJ9+7wT/un8fxERCQJK/kVERERqa9ky2L0bGjb0dyQiIiInRWX/IiIiIiIiInWckn8RERERERGROk7Jv4iIiIiIiEgdp+RfREREREREpI5T8i8iIiIiIiJSxyn5FxEREREREanjlPyLiIiIiIiI1HFWfwdQVxiGAUBhYaGfIzk+p9NJaWkphYWF2Gw2f4cjUiP1Uwl06qOCx3P4ZwB+96uPSqBTH5VgEAz9tDL/rMxHj0fJ/2lSVFQEQFpamp8jERERkTNm716IifF3FCIicpYrKioi5gTfRybjZA4RyAl5PB727NlDVFQUJpPJ3+EcU2FhIWlpaezcuZPo6Gh/hyNSI/VTCXTqoxLo1Ecl0KmPSjAIhn5qGAZFRUU0aNAAs/n4Z/Vr5P80MZvNpKam+juMkxYdHR2wHVikkvqpBDr1UQl06qMS6NRHJRgEej890Yh/JU34JyIiIiIiIlLHKfkXERERERERqeOU/J9lQkJCePzxxwkJCfF3KCLHpH4qgU59VAKd+qgEOvVRCQZ1rZ9qwj8RERERERGROk4j/yIiIiIiIiJ1nJJ/ERERERERkTpOyb+IiIiIiIhIHafkX0RERERERKSOU/J/lhk/fjxNmjQhNDSU7t27s2jRIn+HJGeJsWPHcv755xMVFUVSUhLXXHMNGzdurNKmvLyc4cOHk5CQQGRkJAMHDiQnJ6dKmx07djBgwADCw8NJSkriwQcfxOVyncldkbPEs88+i8lkYuTIkb5l6qPib7t37+aPf/wjCQkJhIWF0a5dO5YsWeJbbxgGY8aMoX79+oSFhdGnTx82bdpUZRt5eXkMGjSI6OhoYmNjueOOOyguLj7TuyJ1kNvt5h//+AdNmzYlLCyMc889l6effpoj5xdXH5Uzbd68eVx55ZU0aNAAk8nEl19+WWX96eqTq1at4qKLLiI0NJS0tDTGjRv3e+9arSn5P4tMnjyZUaNG8fjjj7Ns2TI6dOhARkYGubm5/g5NzgJz585l+PDhLFiwgMzMTJxOJ3379qWkpMTX5oEHHuCbb75hypQpzJ07lz179nDdddf51rvdbgYMGIDD4eDnn3/m/fffZ9KkSYwZM8YfuyR12OLFi3nzzTdp3759leXqo+JPBw8epGfPnthsNr799lvWrVvHCy+8QFxcnK/NuHHjePXVV5k4cSILFy4kIiKCjIwMysvLfW0GDRrE2rVryczMZOrUqcybN4+77rrLH7skdcxzzz3HhAkTeP3111m/fj3PPfcc48aN47XXXvO1UR+VM62kpIQOHTowfvz4Gtefjj5ZWFhI3759ady4MUuXLuX555/niSee4K233vrd969WDDlrdOvWzRg+fLjvttvtNho0aGCMHTvWj1HJ2So3N9cAjLlz5xqGYRj5+fmGzWYzpkyZ4muzfv16AzCysrIMwzCM6dOnG2az2cjOzva1mTBhghEdHW1UVFSc2R2QOquoqMho3ry5kZmZaVxyySXG/fffbxiG+qj438MPP2xceOGFx1zv8XiMlJQU4/nnn/cty8/PN0JCQoyPP/7YMAzDWLdunQEYixcv9rX59ttvDZPJZOzevfv3C17OCgMGDDBuv/32Ksuuu+46Y9CgQYZhqI+K/wHGF1984bt9uvrkG2+8YcTFxVX5rn/44YeNFi1a/M57VDsa+T9LOBwOli5dSp8+fXzLzGYzffr0ISsry4+RydmqoKAAgPj4eACWLl2K0+ms0kdbtmxJo0aNfH00KyuLdu3akZyc7GuTkZFBYWEha9euPYPRS102fPhwBgwYUKUvgvqo+N/XX39N165dueGGG0hKSqJTp0785z//8a3funUr2dnZVfpoTEwM3bt3r9JHY2Nj6dq1q69Nnz59MJvNLFy48MztjNRJF1xwAbNnz+aXX34BYOXKlcyfP59+/foB6qMSeE5Xn8zKyuLiiy/Gbrf72mRkZLBx40YOHjx4hvbmxKz+DkDOjP379+N2u6v8QQqQnJzMhg0b/BSVnK08Hg8jR46kZ8+etG3bFoDs7GzsdjuxsbFV2iYnJ5Odne1rU1Mfrlwn8lt98sknLFu2jMWLF1dbpz4q/rZlyxYmTJjAqFGjePTRR1m8eDH33XcfdrudIUOG+PpYTX3wyD6alJRUZb3VaiU+Pl59VH6zRx55hMLCQlq2bInFYsHtdvPMM88waNAgAPVRCTinq09mZ2fTtGnTatuoXHfk6Vn+pORfRM644cOHs2bNGubPn+/vUER8du7cyf33309mZiahoaH+DkekGo/HQ9euXfnXv/4FQKdOnVizZg0TJ05kyJAhfo5OBD799FM+/PBDPvroI9q0acOKFSsYOXIkDRo0UB8VCQAq+z9LJCYmYrFYqs1KnZOTQ0pKip+ikrPRiBEjmDp1Kt9//z2pqam+5SkpKTgcDvLz86u0P7KPpqSk1NiHK9eJ/BZLly4lNzeXzp07Y7VasVqtzJ07l1dffRWr1UpycrL6qPhV/fr1ad26dZVlrVq1YseOHcDhPna87/qUlJRqE/26XC7y8vLUR+U3e/DBB3nkkUe4+eabadeuHYMHD+aBBx5g7NixgPqoBJ7T1SeD5ftfyf9Zwm6306VLF2bPnu1b5vF4mD17Nunp6X6MTM4WhmEwYsQIvvjiC+bMmVOtNKpLly7YbLYqfXTjxo3s2LHD10fT09NZvXp1lQ/gzMxMoqOjq/1BLFJbl112GatXr2bFihW+f127dmXQoEG+39VHxZ969uxZ7RKpv/zyC40bNwagadOmpKSkVOmjhYWFLFy4sEofzc/PZ+nSpb42c+bMwePx0L179zOwF1KXlZaWYjZXTS8sFgsejwdQH5XAc7r6ZHp6OvPmzcPpdPraZGZm0qJFi4Ap+Qc02//Z5JNPPjFCQkKMSZMmGevWrTPuuusuIzY2tsqs1CK/l7vvvtuIiYkxfvjhB2Pv3r2+f6Wlpb42w4YNMxo1amTMmTPHWLJkiZGenm6kp6f71rtcLqNt27ZG3759jRUrVhgzZsww6tWrZ4wePdofuyRngSNn+zcM9VHxr0WLFhlWq9V45plnjE2bNhkffvihER4ebnzwwQe+Ns8++6wRGxtrfPXVV8aqVauMq6++2mjatKlRVlbma3PFFVcYnTp1MhYuXGjMnz/faN68uXHLLbf4Y5ekjhkyZIjRsGFDY+rUqcbWrVuNzz//3EhMTDQeeughXxv1UTnTioqKjOXLlxvLly83AOPFF180li9fbmzfvt0wjNPTJ/Pz843k5GRj8ODBxpo1a4xPPvnECA8PN958880zvr/Ho+T/LPPaa68ZjRo1Mux2u9GtWzdjwYIF/g5JzhJAjf/ee+89X5uysjLjnnvuMeLi4ozw8HDj2muvNfbu3VtlO9u2bTP69etnhIWFGYmJicZf//pXw+l0nuG9kbPF0cm/+qj42zfffGO0bdvWCAkJMVq2bGm89dZbVdZ7PB7jH//4h5GcnGyEhIQYl112mbFx48YqbQ4cOGDccsstRmRkpBEdHW0MHTrUKCoqOpO7IXVUYWGhcf/99xuNGjUyQkNDjXPOOcf4+9//XuXyZ+qjcqZ9//33Nf4NOmTIEMMwTl+fXLlypXHhhRcaISEhRsOGDY1nn332TO3iSTMZhmH4p+ZARERERERERM4EnfMvIiIiIiIiUscp+RcRERERERGp45T8i4iIiIiIiNRxSv5FRERERERE6jgl/yIiIiIiIiJ1nJJ/ERERERERkTpOyb+IiIiIiIhIHafkX0RERERERKSOU/IvIiIiQclkMvHll1/6OwwREZGgoORfREREau22227DZDJV+3fFFVf4OzQRERGpgdXfAYiIiEhwuuKKK3jvvfeqLAsJCfFTNCIiInI8GvkXERGRUxISEkJKSkqVf3FxcYC3JH/ChAn069ePsLAwzjnnHD777LMq91+9ejW9e/cmLCyMhIQE7rrrLoqLi6u0effdd2nTpg0hISHUr1+fESNGVFm/f/9+rr32WsLDw2nevDlff/3177vTIiIiQUrJv4iIiPwu/vGPfzBw4EBWrlzJoEGDuPnmm1m/fj0AJSUlZGRkEBcXx+LFi5kyZQrfffddleR+woQJDB8+nLvuuovVq1fz9ddf06xZsyqP8eSTT3LjjTeyatUq+vfvz6BBg8jLyzuj+ykiIhIMTIZhGP4OQkRERILLbbfdxgcffEBoaGiV5Y8++iiPPvooJpOJYcOGMWHCBN+6Hj160LlzZ9544w3+85//8PDDD7Nz504iIiIAmD59OldeeSV79uwhOTmZhg0bMnToUP75z3/WGIPJZOKxxx7j6aefBrwHFCIjI/n2228194CIiMhRdM6/iIiInJJLL720SnIPEB8f7/s9PT29yrr09HRWrFgBwPr16+nQoYMv8Qfo2bMnHo+HjRs3YjKZ2LNnD5dddtlxY2jfvr3v94iICKKjo8nNzT3VXRIREamzlPyLiIjIKYmIiKhWhn+6hIWFnVQ7m81W5bbJZMLj8fweIYmIiAQ1nfMvIiIiv4sFCxZUu92qVSsAWrVqxeBIPYgAAAGZSURBVMqVKykpKfGt/+mnnzCbzbRo0YKoqCiaNGnC7Nmzz2jMIiIidZVG/kVEROSUVFRUkJ2dXWWZ1WolMTERgClTptC1a1cuvPBCPvzwQxYtWsQ777wDwKBBg3j88ccZMmQITzzxBPv27ePee+9l8ODBJCcnA/DEE08wbNgwkpKS6NevH0VFRfz000/ce++9Z3ZHRURE6gAl/yIiInJKZsyYQf369assa9GiBRs2bAC8M/F/8skn3HPPPdSvX5+PP/6Y1q1bAxAeHs7MmTO5//77Of/88wkPD2fgwIG8+OKLvm0NGTKE8vJyXnrpJf72t7+RmJjI9ddff+Z2UEREpA7RbP8iIiJy2plMJr744guuueYaf4ciIiIi6Jx/ERERERERkTpPyb+IiIiIiIhIHadz/kVEROS001mFIiIigUUj/yIiIiIiIiJ1nJJ/ERERERERkTpOyb+IiIiIiIhIHafkX0RERERERKSOU/IvIiIiIiIiUscp+RcRERERERGp45T8i4iIiIiIiNRxSv5FRERERERE6rj/B+nL2/TvmIMYAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA+wAAAIjCAYAAACZEJFdAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xd4U+XbwPFv0r0XndCWWWYBZe8hQ1BkKFsFFUEpoC+KP3AgiHsgKnUPcAAKAqLsvfeGMgqlQAvde6fJef9Ie2hoC0VGWnN/rourzZn3OXlScp9naRRFURBCCCGEEEIIIUSlojV3AEIIIYQQQgghhChNEnYhhBBCCCGEEKISkoRdCCGEEEIIIYSohCRhF0IIIYQQQgghKiFJ2IUQQgghhBBCiEpIEnYhhBBCCCGEEKISkoRdCCGEEEIIIYSohCRhF0IIIYQQQgghKiFJ2IUQQgghhBBCiEpIEnYhhKgiRo8eTc2aNf/VvjNmzECj0dzZgKq4LVu2oNFo2LJli7qsovc4OjoajUbDvHnz7mhMNWvWZPTo0Xf0mFXZvHnz0Gg0REdHmzuUCrkbn7Oq9tmtau+ZEEJUdpKwCyHEbdJoNBX6VzIxtDQGg4GPP/6YevXq4eDgQJ06dXj++efJysqq0P5NmzYlKCgIRVHK3aZDhw74+vpSWFh4p8K+K3bt2sWMGTNIS0szdyiq4iRLo9GwY8eOUusVRSEwMBCNRsPDDz/8r87x5Zdf3vEHHHfS+PHj0Wq1pKSkmCxPSUlBq9ViZ2dHXl6eybqoqCg0Gg2vvvrqvQzVLAoKCvjss8+47777cHV1xd3dncaNGzN27FhOnz5t1tiuXr3K1KlT6datGy4uLjf9e7tr1y46duyIo6Mjfn5+TJo0qcy/Rfn5+fzvf/8jICAABwcH2rRpw/r16+/ilQghRGmSsAshxG365ZdfTP717NmzzOUNGza8rfN89913nDlz5l/t+/rrr5Obm3tb578dn332GVOmTKFJkyZ89tlnDBs2jLVr15KUlFSh/UeOHMnly5fZvn17meujo6PZvXs3Q4cOxdra+l/HeTv3uKJ27drFzJkzy0zYz5w5w3fffXdXz38j9vb2LFiwoNTyrVu3EhMTg52d3b8+9r9J2J944glyc3MJDg7+1+etqI4dO6IoCjt37jRZvmvXLrRaLTqdjgMHDpisK962Y8eOgPk/Z3fTo48+yksvvUSTJk14//33mTlzJp07d2b16tXs2bNH3e5evmfFzpw5wwcffEBsbCyhoaE33PbIkSM88MAD5OTkMHv2bMaMGcO3337L4MGDS207evRoZs+ezciRI/nss8+wsrKib9++ZT7UEkKIu+Xff6sRQggBwOOPP27yes+ePaxfv77U8uvl5OTg6OhY4fPY2Nj8q/gArK2tbyuRvV2LFi2icePGLF26VG3eO2vWLAwGQ4X2HzFiBNOmTWPBggV07ty51PqFCxeiKAojR468rThv5x7fCbeTEN8Jffv2ZfHixXz++ecm5WXBggW0aNGiwg9Ybld2djZOTk5YWVlhZWV1T85ZnHTv2LGDfv36qct37txJ06ZNyc3NZceOHep2xdtqtVrat28PmP9zdrfs37+ff/75h3feeadUa4K5c+eaPHy6l+9ZsRYtWpCcnIynpydLliwpM/ku9uqrr+Lh4cGWLVtwdXUFjF1Rnn32WdatW0evXr0A2LdvH4sWLeKjjz7i5ZdfBuDJJ5+kSZMmvPLKK+zatevuX5gQQiA17EIIcU907dqVJk2acPDgQTp37oyjo6P6xfevv/7ioYceIiAgADs7O+rUqcOsWbPQ6/Umx7i+f3VxP+qPP/6Yb7/9ljp16mBnZ0erVq3Yv3+/yb5l9YPVaDRMmDCB5cuX06RJE+zs7GjcuDFr1qwpFf+WLVto2bIl9vb21KlTh2+++eaW+tZqtVoMBoPJ9lqttsLJTWBgIJ07d2bJkiXodLpS6xcsWECdOnVo06YNFy9eZPz48dSvXx8HBwe8vLwYPHhwhfrUltWHPS0tjdGjR+Pm5oa7uzujRo0qs3b82LFjjB49mtq1a2Nvb4+fnx9PP/00ycnJ6jYzZsxgypQpANSqVUtthl4cW1l92KOiohg8eDCenp44OjrStm1bVq5cabJNcX/8P/74g3feeYcaNWpgb2/PAw88wLlz52563cWGDx9OcnKySbPfgoIClixZwogRI8rcx2AwMGfOHBo3boy9vT2+vr6MGzeO1NRUdZuaNWty8uRJtm7dql5z165dgWvN8bdu3cr48ePx8fGhRo0aJuuuf+9Wr15Nly5dcHFxwdXVlVatWpm0DIiMjOTRRx/Fz88Pe3t7atSowbBhw0hPTy/32oOCgggMDCxVw75z5046dOhA+/bty1zXuHFj3N3dgdv/nO3YsYNWrVqZfM7KUlhYyKxZs9TPfM2aNXn11VfJz89Xt5k8eTJeXl4m3UgmTpyIRqPh888/V5fFx8ej0Wj46quvyr0358+fB4zdTq5nZWWFl5eX+vr696z4npT1r2RZr0g5Ko+Liwuenp433S4jI0N9mFqcrIMxEXd2duaPP/5Qly1ZsgQrKyvGjh2rLrO3t+eZZ55h9+7dXL58+abnE0KIO+G/9xhYCCEqqeTkZPr06cOwYcN4/PHH8fX1BYxfcJ2dnZk8eTLOzs5s2rSJ6dOnk5GRwUcffXTT4y5YsIDMzEzGjRuHRqPhww8/ZNCgQURFRd20xnjHjh0sXbqU8ePH4+Liwueff86jjz7KpUuX1C/hhw8f5sEHH8Tf35+ZM2ei1+t566238Pb2rvC1P/XUU4wbN45vvvmGcePGVXi/kkaOHMnYsWNZu3atST/q48ePc+LECaZPnw4YawN37drFsGHDqFGjBtHR0Xz11Vd07dqViIiIW2rVoCgK/fv3Z8eOHTz33HM0bNiQZcuWMWrUqFLbrl+/nqioKJ566in8/Pw4efIk3377LSdPnmTPnj1oNBoGDRrE2bNnWbhwIZ9++inVqlUDKPdexsfH0759e3Jycpg0aRJeXl7Mnz+fRx55hCVLljBw4ECT7d9//320Wi0vv/wy6enpfPjhh4wcOZK9e/dW6Hpr1qxJu3btWLhwIX369AGMyXF6ejrDhg0zSfSKjRs3jnnz5vHUU08xadIkLly4wNy5czl8+DA7d+7ExsaGOXPmMHHiRJydnXnttdcA1PJfbPz48Xh7ezN9+nSys7PLjXHevHk8/fTTNG7cmGnTpuHu7s7hw4dZs2YNI0aMoKCggN69e5Ofn8/EiRPx8/MjNjaWf/75h7S0NNzc3Mo9dseOHVm6dCn5+fnY2dlRUFDA/v37ef7558nJyeGVV15BURQ0Gg2pqalERETw3HPP3fS+VuRzdvz4cXr16oW3tzczZsygsLCQN998s9R9AhgzZgzz58/nscce46WXXmLv3r289957nDp1imXLlgHQqVMnPv30U06ePEmTJk0A2L59O1qtlu3btzNp0iR1GVBmy5Vixc3bf/vtNzp06HBLrQgGDRpE3bp1TZYdPHiQOXPm4OPjoy6rSDm6XcePH6ewsJCWLVuaLLe1taV58+YcPnxYXXb48GFCQkJMEnuA1q1bA8am9YGBgbcdkxBC3JQihBDijgoLC1Ou//PapUsXBVC+/vrrUtvn5OSUWjZu3DjF0dFRycvLU5eNGjVKCQ4OVl9fuHBBARQvLy8lJSVFXf7XX38pgPL333+ry958881SMQGKra2tcu7cOXXZ0aNHFUD54osv1GX9+vVTHB0dldjYWHVZZGSkYm1tXeqY5Zk6dapia2urWFlZKUuXLq3QPtdLSUlR7OzslOHDh5c6NqCcOXNGUZSy7+fu3bsVQPn555/VZZs3b1YAZfPmzeqy6+/x8uXLFUD58MMP1WWFhYVKp06dFED56aef1OVlnXfhwoUKoGzbtk1d9tFHHymAcuHChVLbBwcHK6NGjVJfv/jiiwqgbN++XV2WmZmp1KpVS6lZs6ai1+tNrqVhw4ZKfn6+uu1nn32mAMrx48dLnaukn376SQGU/fv3K3PnzlVcXFzU6xk8eLDSrVs3Nb6HHnpI3W/79u0KoPz2228mx1uzZk2p5Y0bN1a6dOlS7rk7duyoFBYWlrmu+F6lpaUpLi4uSps2bZTc3FyTbQ0Gg6IoinL48GEFUBYvXnzDay5LeHi4yf0uLjcXL15UIiIiFEA5efKkoiiK8s8//5S6xtv5nA0YMECxt7dXLl68qC6LiIhQrKysTI555MgRBVDGjBljcp6XX35ZAZRNmzYpiqIoCQkJCqB8+eWXiqIY751Wq1UGDx6s+Pr6qvtNmjRJ8fT0VO9fWQwGg/o3zNfXVxk+fLgSHh5uEmux69+z6yUmJipBQUFKaGiokpWVpSjKrZWjm1m8eHGpz/X160p+HosNHjxY8fPzU183btxY6d69e6ntTp48We7fciGEuBukSbwQQtwjdnZ2PPXUU6WWOzg4qL9nZmaSlJREp06dyMnJqdDoy0OHDsXDw0N93alTJ8DYlPpmevToQZ06ddTXTZs2xdXVVd1Xr9ezYcMGBgwYQEBAgLpd3bp11RrYm/n888+ZPXs2O3fuZPjw4QwbNox169aZbGNnZ8cbb7xxw+N4eHjQt29fVqxYodbAKorCokWLaNmyJSEhIYDp/dTpdCQnJ1O3bl3c3d05dOhQhWIutmrVKqytrXn++efVZVZWVkycOLHUtiXPm5eXR1JSEm3btgW45fOWPH/r1q1N+k07OzszduxYoqOjiYiIMNn+qaeewtbWVn19K2Wh2JAhQ8jNzeWff/4hMzOTf/75p9zm8IsXL8bNzY2ePXuSlJSk/mvRogXOzs5s3ry5wud99tlnb9r3ef369WRmZjJ16lTs7e1N1hU3RS+uQV+7di05OTkVPj+Y9mMHY5P36tWrExQURIMGDfD09FSbxV8/4NyNVORztnbtWgYMGEBQUJC6XcOGDendu7fJsVatWgUYm7yX9NJLLwGo3SW8vb1p0KAB27ZtU+O1srJiypQpxMfHExkZCRhr2Dt27HjD7i0ajYa1a9fy9ttv4+HhwcKFCwkLCyM4OJihQ4dWeMYDvV7P8OHDyczMZNmyZTg5OQF3thzdSPGAgGWNFWFvb28yYGBubm6525U8lhBC3G2SsAshxD1SvXp1k2Sq2MmTJxk4cCBubm64urri7e2tDlh3oz63xUp+wQfU5L0ifT+v37d4/+J9ExISyM3NLdWkFShz2fVyc3N58803GTNmDC1btuSnn36ie/fuDBw4UE2KIiMjKSgooE2bNjc93siRI8nOzuavv/4CjCN4R0dHmww2l5uby/Tp0wkMDMTOzo5q1arh7e1NWlpahe5nSRcvXsTf3x9nZ2eT5fXr1y+1bUpKCi+88AK+vr44ODjg7e1NrVq1gIq9j+Wdv6xzFc84cPHiRZPlt1MWinl7e9OjRw8WLFjA0qVL0ev1PPbYY2VuGxkZSXp6Oj4+Pnh7e5v8y8rKIiEhocLnLb5XN1Lcl7q4iXd5x5k8eTLff/891apVo3fv3oSHh1foPWjSpAnu7u4mSXlxv22NRkO7du1M1gUGBpb5GbrezT5niYmJ5ObmUq9evVLbXf/+X7x4Ea1WW+rz5+fnh7u7u0mZ6NSpk9rkffv27bRs2ZKWLVvi6enJ9u3bycjI4OjRo+qDnRuxs7Pjtdde49SpU1y5coWFCxfStm1b/vjjDyZMmHDT/cE4iv6mTZvUMSeK3clydCPFD9VK9vUvlpeXZ/LQzcHBodztSh5LCCHuNunDLoQQ90hZX/DS0tLo0qULrq6uvPXWW9SpUwd7e3sOHTrE//73vwqNol5eraRygznL78S+FXHq1CnS0tLUmmZra2uWLFlC9+7deeihh9i8eTMLFy7Ex8dHnQ7vRh5++GHc3NxYsGABI0aMYMGCBVhZWTFs2DB1m4kTJ/LTTz/x4osv0q5dO9zc3NBoNAwbNqzCo9L/G0OGDGHXrl1MmTKF5s2b4+zsjMFg4MEHH7yr5y3pTr2fI0aM4NlnnyUuLo4+ffqog6pdz2Aw4OPjw2+//Vbm+lsZ5+BOJkCffPIJo0eP5q+//mLdunVMmjSJ9957jz179qgD2pVFq9XSrl07du3apU7xVnJU9Pbt2/Pjjz+qfdsHDBhQoXjuxuesIgM+duzYke+++46oqCi2b99Op06d0Gg0dOzYke3btxMQEIDBYKhQwl6Sv78/w4YN49FHH6Vx48b88ccfzJs374Z925cvX84HH3zArFmzePDBB03W3clydLO4wThv+/WuXr1q0orI39+f2NjYMrcDTLYVQoi7SRJ2IYQwoy1btpCcnMzSpUtNBn26cOGCGaO6xsfHB3t7+zJHGq/I6OPFSUXJEZWdnJxYtWoVHTt2pHfv3uTl5fH2229XaEozOzs7HnvsMX7++Wfi4+NZvHgx3bt3x8/PT91myZIljBo1ik8++URdlpeXV+FmuyUFBwezceNGsrKyTGrZr5+rPTU1lY0bNzJz5kx18DtAbXZcUkVH1i8+f1nzwhd3lbhbc10PHDiQcePGsWfPHn7//fdyt6tTpw4bNmygQ4cON024b+W6b3Q+gBMnTty0hUdoaCihoaG8/vrr7Nq1iw4dOvD111/z9ttv33C/jh07snr1alasWEFCQoLJyOjt27fntddeY9WqVeTm5laoOXxFeHt74+DgUGZ5uf79Dw4OxmAwEBkZqba0AOMAhWlpaSZlojgRX79+Pfv372fq1KmAcYC5r776ioCAAJycnGjRosW/itvGxoamTZsSGRlJUlKSyeewpLNnzzJq1CgGDBhQalo4uLVydDuaNGmCtbU1Bw4cYMiQIerygoICjhw5YrKsefPmbN68mYyMDJOB54oHcGzevPldi1MIIUqSJvFCCGFGxTVvJWvaCgoK+PLLL80VkgkrKyt69OjB8uXLuXLlirr83LlzrF69+qb7h4aG4uvry9y5c02atXp5efHTTz+RlJREbm6uybzXNzNy5Eh0Oh3jxo0jMTGx1NzrVlZWpWouv/jii1LT5FVE3759KSwsNJnySq/X88UXX5Q6J5SuMZ0zZ06pYxb3263IA4S+ffuyb98+du/erS7Lzs7m22+/pWbNmjRq1Kiil3JLnJ2d+eqrr5gxY8YN35shQ4ag1+uZNWtWqXWFhYUm1+jk5PSvHpqU1KtXL1xcXHjvvffUpsnFiu99RkYGhYWFJutCQ0PRarVlNnG+XnES/sEHH+Do6GiSmLVu3Rpra2s+/PBDk21vl5WVFb1792b58uVcunRJXX7q1CnWrl1rsm3fvn2B0mVr9uzZADz00EPqslq1alG9enU+/fRTdDqd+vChU6dOnD9/niVLltC2bdubjvoeGRlpElextLQ0du/ejYeHR7m14FlZWQwcOJDq1aszf/78Mh/c3Eo5uh1ubm706NGDX3/9lczMTHX5L7/8QlZWlsn87Y899hh6vZ5vv/1WXZafn89PP/1EmzZtZIR4IcQ9IzXsQghhRu3bt8fDw4NRo0YxadIkNBoNv/zyyx1rkn4nzJgxg3Xr1tGhQweef/559Ho9c+fOpUmTJhw5cuSG+1pbWzN37lyGDh1KaGgo48aNIzg4mFOnTvHjjz8SGhpKTEwM/fv3Z+fOnaWmUCpLly5dqFGjBn/99RcODg4MGjTIZP3DDz/ML7/8gpubG40aNWL37t1s2LDBZK7oiurXrx8dOnRg6tSpREdH06hRI5YuXVqqP7SrqyudO3fmww8/RKfTUb16ddatW1dmS4ni2szXXnuNYcOGYWNjQ79+/dREvqSpU6eqU6xNmjQJT09P5s+fz4ULF/jzzz/Rau/ec/eypq67XpcuXRg3bhzvvfceR44coVevXtjY2BAZGcnixYv57LPP1P7vLVq04KuvvuLtt9+mbt26+Pj40L1791uKydXVlU8//ZQxY8bQqlUrRowYgYeHB0ePHiUnJ4f58+ezadMmJkyYwODBgwkJCaGwsJBffvkFKysrHn300Zueo3Xr1tja2rJ79266du1qksw6OjrSrFkzdu/ejbu7+w370t+qmTNnsmbNGjp16sT48eMpLCzkiy++oHHjxhw7dkzdrlmzZowaNYpvv/1W7VKzb98+5s+fz4ABA+jWrZvJcTt16sSiRYsIDQ1VxzS4//77cXJy4uzZs+UOKFjS0aNHGTFiBH369KFTp054enoSGxvL/PnzuXLlCnPmzCm32f/MmTOJiIjg9ddfV8eeKFanTh3atWt3S+WoPMUtJ06ePAkYk/DicTJef/11dbt33nmH9u3b06VLF8aOHUtMTAyffPIJvXr1Mmmq36ZNGwYPHsy0adNISEigbt26zJ8/n+joaH744Yeb3jMhhLhjzDQ6vRBC/GeVN61b48aNy9x+586dStu2bRUHBwclICBAeeWVV5S1a9fedMqx4mndPvroo1LHBJQ333xTfV3edFNhYWGl9r1+ajFFUZSNGzcq9913n2Jra6vUqVNH+f7775WXXnpJsbe3L+cumNq2bZvSu3dvxdXVVbGzs1OaNGmivPfee0pOTo6yevVqRavVKr169VJ0Ol2FjjdlyhQFUIYMGVJqXWpqqvLUU08p1apVU5ydnZXevXsrp0+fLnVdFZnWTVEUJTk5WXniiScUV1dXxc3NTXniiSfUqcNKTusWExOjDBw4UHF3d1fc3NyUwYMHK1euXCn1XiiKosyaNUupXr26otVqTabAKuvenz9/XnnssccUd3d3xd7eXmndurXyzz//mGxTfC3XT2VWXEZKxlmWktO63cj107oV+/bbb5UWLVooDg4OiouLixIaGqq88sorypUrV9Rt4uLilIceekhxcXFRAHWKtxudu7wpwlasWKG0b99ecXBwUFxdXZXWrVsrCxcuVBRFUaKiopSnn35aqVOnjmJvb694enoq3bp1UzZs2HDDayupXbt2CqC8+uqrpdZNmjRJAZQ+ffqUWne7n7OtW7cqLVq0UGxtbZXatWsrX3/9dZnH1Ol0ysyZM5VatWopNjY2SmBgoDJt2jSTaSCLFU9V9/zzz5ss79GjhwIoGzduLPc+FIuPj1fef/99pUuXLoq/v79ibW2teHh4KN27d1eWLFlisu3179moUaMUoMx/119/RcpReco7R1lfdbdv3660b99esbe3V7y9vZWwsDAlIyOj1Ha5ubnKyy+/rPj5+Sl2dnZKq1atlDVr1tw0FiGEuJM0ilKJqnGEEEJUGQMGDODkyZNl9rsVQgghhBC3T/qwCyGEuKnr5xyOjIxk1apVdO3a1TwBCSGEEEJYAKlhF0IIcVP+/v6MHj2a2rVrc/HiRb766ivy8/M5fPhwmXNHCyGEEEKI2yeDzgkhhLipBx98kIULFxIXF4ednR3t2rXj3XfflWRdCCGEEOIukhp2IYQQQgghhBCiEpI+7EIIIYQQQgghRCUkCbsQQgghhBBCCFEJWXwfdoPBwJUrV3BxcUGj0Zg7HCGEEEIIIYQQ/3GKopCZmUlAQABabfn16BafsF+5coXAwEBzhyGEEEIIIYQQwsJcvnyZGjVqlLv+P5Gw16xZE1dXV7RaLR4eHmzevLnC+7q4uADGG+Xq6nq3QrxtOp2OdevW0atXL2xsbMwdjhClSBkVlZ2UUUGDBnD1Kvj7w+nT5o6mTFJORWUnZVRUdlWljGZkZBAYGKjmo+X5TyTsALt27cLZ2fmW9ytuBu/q6lrpE3ZHR0dcXV0rdcETlkvKqKjspIwKipscarVQSf/Pl3IqKjspo6Kyq2pl9GbdsmXQOSGEEEIIIYQQohIye8K+bds2+vXrR0BAABqNhuXLl5faJjw8nJo1a2Jvb0+bNm3Yt2+fyXqNRkOXLl1o1aoVv/322z2KXAghhBBCCCGEuHvM3iQ+OzubZs2a8fTTTzNo0KBS63///XcmT57M119/TZs2bZgzZw69e/fmzJkz+Pj4ALBjxw6qV6/O1atX6dGjB6GhoTRt2rTM8+Xn55Ofn6++zsjIAIxNJ3Q63V24wjujOLbKHKOwbFJGRWUnZVRYAxpAAQoraTmQcioqOymjorKrKmW0ovFpFEVR7nIsFabRaFi2bBkDBgxQl7Vp04ZWrVoxd+5cwDgNW2BgIBMnTmTq1KmljjFlyhQaN27M6NGjyzzHjBkzmDlzZqnlCxYswNHR8Y5chxBCCCEqH7uUFDQGA4pWS76np7nDEUIIYcFycnIYMWIE6enpNxxLzew17DdSUFDAwYMHmTZtmrpMq9XSo0cPdu/eDRhr6A0GAy4uLmRlZbFp0yaGDBlS7jGnTZvG5MmT1dfFo/P16tWr0g86t379enr27FklBk8QlkfKqKjspIyKqkDKqajspIyKyq6qlNHilt43U6kT9qSkJPR6Pb6+vibLfX19OV00HUt8fDwDBw4EQK/X8+yzz9KqVatyj2lnZ4ednV2p5TY2NpX6DS1WVeIUlkvKqKjspIyKqkDKqajspIyKyq6yl9GKxlapE/aKqF27NkePHjV3GEIIIYQQQgghxB1VqRP2atWqYWVlRXx8vMny+Ph4/Pz8buvY4eHhhIeHo9frb+s4QgghhKgivv0WsrLA2RnGjjV3NEIIIcRNmX1atxuxtbWlRYsWbNy4UV1mMBjYuHEj7dq1u61jh4WFERERwf79+283TCGEEEJUBW+9BS+9ZPwphBBCVAFmr2HPysri3Llz6usLFy5w5MgRPD09CQoKYvLkyYwaNYqWLVvSunVr5syZQ3Z2Nk899ZQZoxZCCCGEEEIIIe4usyfsBw4coFu3burr4hHcR40axbx58xg6dCiJiYlMnz6duLg4mjdvzpo1a0oNRCeEEEIIIYQQQvyXmD1h79q1KzebCn7ChAlMmDDhHkUkhBBCCCGEEEKYX6Xuw343hYeH06hRoxtOASeEEEIIIYQQQpiLxSbsMuicEEIIIYQQQojKzGITdiGEEEIIIYQQojKThF0IIYQQQgghhKiEJGEX/1kXk7OJz8gzdxhCCCGEEEII8a9Iwi4AOHU1g/xCvbnDKOVicjbP/3qQE7Hpt7RffEYeXT7awqAvd910FoKqZN7OC4z4bg/pOToAdHoDB6JT0Bsq9zWeicvkckpOpStnOr2Byb8fIXzzuXt63jydnjUn4kjIrBwPlM4lZHLqakaZ605eSaf5W+v4blvUPY7KSFEUft1zkYMXU02W5xfq/1OfbXGPhIRAo0bGn0IIIUQVYLEJ+39llPj4jDxGfLeH91adYt+FlH/1BXbZ4Rj6fLad91efvum2eTo9huuSw4TMPDLydLd0zkK9oVTtd3qOjkX7LpGnu5bQvb/6NKtPxPHwFztQFIX0HB1Z+YWljncuIZP7Z62n3xc7ePiL7fy48wIAsWm5xKblViimhIw8xv1ygGWHY/jzYEyp6yxJpzfw99ErJGflq8tOxKaz7mScei+WHorhnZURXEjKrtD5wXhfjsWkMWHBIb7fHsXpuGtJVEGhgRl/R7DrfDI/744G4IPVp3ns6918t71iydT1931DRDxP/LCXgxdTKhzjjaTlw9V00/d1+l8n6D1nG50+3Eyfz7bz2rIT6ro/DlxmT1TyHTn3zaw6fpWtZxNNlu2NSmHp4Vg+WnuGcwmZ5e67JyqZn3ZeuO0EMSu/kLAFh2j59gae+/UgXT7cwvGYdFYdv8rhS6kU6g1El1NeCvUGwjefI3zzOdadjONqesXKNRjL5pTFR7mckkOh3mCyLiEjjx6zt/HoV7tKfY4vJmcT9tsh0nJ0vLPqlHr9UYlZJGbmcysSMvJu+mBJb1BQFOO/hKJyuvZkPK8vP8GjX+1St0vJLqDLh1vo+MFmjlxOK/d4WfmFXE7JASA6KZsxPx/ifNnPJYSl2LQJTp40/hRCCCGqALPPw24uYWFhhIWFkZGRgZubm7nD+deWH45l1/lkdp1P5pttUXw+/D4eaRZgsk12fiGxablcTskhLiOPbvV9CHB3UNdP/+skAD/tjOap9rUI8nI02X/poRg2nIonM6+QHeeSGHhfdWYPaQ5ATGoOvT7dhqLAu4OaMPC+GuXG+teRWPQGhfp+Ljz1034SMvP5vx4h9G7ii6+LPa//dYKVx66y5Uwikx6oRx0fJ87EX0uilh+J5e1/TuHtYseHjzUlwN0BLydbPt0QyecbIwHjF3mAE7HXvpV3/GAzv41pQ9vaXlhpNerylOwCtp1NpF+zAKy0Gt76J4K1J+NZezIeMCblw1oHmVxDclY+abk6vtgYyfIjV3ioqT/hI+7nYnI2j8zdgUGB3o19eXtAKFOWHENvUFi07zKrXuhEoKfxvuYUGB84ONoaP356g4KVVsPlFOO9zC16YPHPsasAjGoXzLOda5sk/qfjjPfl+x3GBxPvrz7Nc13qAMYaydUn4th8OoE2tb2ITsqmZjUnWgZ7MPaXA5yNz+KPce2w0moY8/OBolisyNWdIzY1hw8fa0aLYA/mbDjLz7sv8t2TLWkR7MHBi6lM/uMIL/WqX6qMAeTr9Hx83IqPTu7ih1EtmbcrmvuC3Pl590WT7ZYcjOGt/o05eSWDV5YcA+DkzN442Rnvx9X0XIZ9u4euId7M7N+knNJ0TWaeDhsrLTZWWpKy8vF1tS+1TXRSNuN/OwRAxFu90Wo0XEnL5VhsmrrNzL8jmDv8ftwcbYhNy6X/3B10b+DD+4OaMuzbPQAEeznStrYXC/ddZkDzALyc7dT9U7ML+G57FHHpebz6UENc7K3540AM7Wp7UtfHBYBfdl9kZdH7CpCr0zPi+z1k5hnLxKTudfl807lSn+Nf9lxk1j8RFBReS7Z9XOx4e0AT4jLyeLxNMNoSZTshI4+EzHyaVDf+bZv+1wkOXUpj8cEYfF3t+GNcO2p4OGKl1fDllvMA5BToWXHkCusi4nm+Sx2aVHfl4c93kFniAdmhS6lYa7UM/mY31ZxsWT+5i/q+5emM+zep7kajAFeT+//XkVheWHSEBn4utKzpQaFe4a3+TbC1vvbM+O+jV3hzxUnq+Thja61le2QSs/o3Jjbt2gOgxMx8dHoD768+TVxRQv+/JcdY+3+dS73nAGN/PsDuqGQ+fqwZs9efJTYtl6O2Vkwsc2shhBBCiMpHo1h4m8LihD09PR1XV9eb72AmcWnZ/LlqA2MH90WPFnsbKwDG/XJATTAB+jUL4MNHm2JrrSU5K59Xlx1nw6kEk2M18HNh5aROWGk1pOfquH/WepOar/cGhdK+jhfBXk4oikKtaatKxVPb24m5w+/naEwa05YeB8DZzpotU7qSW6Dn2Z8PkKvT88ID9Rh0fw0uJefQ+aPNAPi62hGfYVo7V9PLkejknFLLrqTlUVBUI2hrrTVJWKy0GqY+2IB3Vp2q0D10srXipV71+W3vRZrWcOdqei57olKY1qcB47rUocfsrZxLyFK3b1fbi4Vj2wJgMCj8fewKry8/oSZXxU699SCrT1xl8h9Hyz13+zpejOtShz/2X2bLmQSc7KyZ2L0um04nsDsqmflPtebgpVQ+XHPG5B5HJRqTdHdHG7yd7Ygsiq+asx37Xn2A2q9ee29eebA+z3Wuw6cbzvLFptJNvN0dbUgrakof1q0O+TqDmvCX5OFow19hHdX3y9HWigXPtuWLjZFsPG0sS5tf7kpSVj4Gg0IDP1eWHo5Bi8Kbf1fsvajp5cj9QR4sPRyrLvvw0aYMaRXIjBUnmbcrGoBtU7oR5OXIxeRsrLQafF3tyS80EJeeSx1vZ84lZNFv7g4MBijQG9Bo4Ivh9/FwU9MHCn8ejOGlxcb3Z9HYtiw7FMvvBy6XisvR1ooWwR5sj0xSly18ti3DvzMm7C/2qEdGbiE/7rxAy2APljzfXt3utWXH+W3vJQCa1nDDWqvh0KU0Wtfy5LcxbTifmMXgr3abJMA3UtfHmTcebsTYnw+QX2i44bafDWtO/+bVMRgUTsVl8OrS4xyNSad1TU8Gt6zBlKIHIyXV83Hml2fa0OezbaTmmNasV3O25cuRLRjyze4bnvfFHvVoV9uLpYdi2Xk+iZjUXHxd7Zj+cGNWn7iKjZUWa62Gk1cyiLiuyf20Pg0wKLDtbCL7o1MoLKP23cHGihA/F44W1aJbazVlbvdYixrsiUpmQre6DLivOvY2VuQX6qn/+poy446c1QsbG5sbXpsQ5qLT6Vi1ahV9+/aVcioqJSmjorKrKmW0onmoJOxVIGHffT6ZMT/vx0lTyEP31+S3fZdpXdOTziHV+G77BRIz83m5VwgfrzsLgKeTLTZWGrLz9WrzcTcHG6o523K+KAH0dbUjLUdXbiKg0UBY17p0qleNoUW1i9frEuKNl7MtSw9dS7qe7VSLDacS1NpgrQZ+HdOGg9GpfLL+rMn+Dzf1Z+uZxAonMHdTfV8Xk9p8MCbFqyZ15P3Vp1lf1MKgmLOdtXpvX+oZwtqIOJNa/WKj29dkwd5L6kOHimhX24uvHr8fV3sbPlp3hq+KakDBeD+L85XXH2rI2ytNE+QHG/ux5mSc+tpaq6GOt3Opa6vn48yVtFyyC8ruT35/kDuHLqWpr69/WHKrujfwYdPphBtuU6uaE78805pH5u5UW0oA2FlryyynT3eoRU5BIYv2mybeTrZW/D6uHZ9vjMTL2Y4gT0fOxGWw/MgVALW1QElT+zTgz4Mx6gOR8jzQwEd9aAEwuEUN3hkYioJC63c2kp5b8a4ho9oFE5uWx4ZT8TffGOMDqgVj2jDsuz2U9Ve7W31vNp9JLL3iBko+FLpep3rV1AcXTrZW5ZYVcwr2cuTidQ/6XOytmdwzhCBPR56Zf6DUPhoUjr7RA1en0i0xhKgMqsoXTWG5pIyKyq6qlFFJ2CuoKiTsmXk6On+4uVQtWDFrrYaDb/SkzbsbyNOZJjaeTrb8NqYNDf2N1/bHgctqM+Rb9XjbIOp6OzPj74hS6x5rUYMlB2PU18521jTyd2VftLFvtI2VBp1eIcjTkUspOdwX5M6y8R1QFIUpS46Z7Hu9FsEexKXnVbgvejGNhjITm1vham9NRolEfVL3uoztUgc7ay2z1581SabBNKHuEuLN58Pv44M1p1lQVPN6PQ9Hm1Lv69YpXQn2clJfP/bVLg4UJZhzR9zHznNJLNxXuna4pAnd6vJSrxDydAYcbK3459gVPl57hg51q6m1wGXp3zyAv4oSW4CXe4Ww6nicSe1oWS0kSvJysiW5RMI9uWcImXk6vtturM1f8GwbPtsQyd4LxrJRXtJ4K+9flxBvDkSn/Kuk0tXemj2vPoCNlZbnfz1U4QS65LkNisL2yCR8Xe2Y9EA9ftoZTTVnW/ZEmY4N0KOhD//XMwRXext8Xe0xKApzNkTy9dbz5Rz9mk8GN+PRFjU4eDEFTyc7Xlh0mGMxFR+MsV1tLz4ffh9dP9p8w/vkYm9t8nDqqQ41ebVvQ+q9tlpd5mBjRbCXo9o1A4wPp5Ky8tWuHNd7pFkANas58efBGDJydWTmF2Kt1dCypkep+wTGhyg3Glfjq5H3k5arU1v4QNllM8DNnqz8QhxtrdVm9Muea8t9Nb3KPbb4Dxs5EpKSoFo1+O03c0dTpqryRVNYLimjorKrKmW0onmoxfZhr0pc7G0Y37U276w6U+b6JtXdcHOwYUTrYObtukCtak40reGOBni6Yy01WQcY0jKQ+4M82HshmZXHrpKYmU+tak5k5hWy+waDfw1vHcTbA0IxGBSCqzkxY8VJtWbL1krLCw/UM0m6x3WuzZPta9LqnQ0UFBrQ6RUa+ruyYEwbVp+Io0Nd45dljUZD/+YB6r6dQ7x5rnNtEjLzefH3I4AxYdAbFF5YdEQ9frMabhwtkax8OrQZHepUIz1XR89PtwHwUKg/z3SsxYqjV/hpZzQzH2nMw039AZi9/ixLD8Wq/cUdbKzI1ekZeF91ZvZvzLsrT7Fo/2WTZH1yzxAmPVBPfT2hW102nornbPy1Wtl5T7Vm57kkHm8brPZZn9KrPjGpuew8l0R9XxeCi5qDN/B3oX2datQp0ay9d2Nfk2QdjAltccLeq5EfjfxdWXIwBp2+7Gz2oVB/JvcMQaPR4GBr7DrxcNMAHm4agE5vMEnYS9bKejja0L2Bj0nC3r95dbo18OGhz3cAxpr3/+sZwhM/7Ct13trVnLiYnMUfY1uTmqvH392ByPhMOtfzZl90Ct9tv4CHow3t61SjdU1P5u2KxtZay5PtavLUT/vUOJztrFk/uTNuDjbM2RDJhlPx2FtbqQ8NZg9pxtX0PD5aa/w8tK7lybynWqEoEJmQRb8vdlSoRYOrvTWfDb+PEF8XdTyB755sQYHewKJ9l8nV6dl6JhFvFzu8nG35dc9F9Z439HclPiOPlOwCk4HswrrVZWSbYEa2CQag3Xsb1UH45j/dms71qqHRaEzimNqnAfV8nNUm+w39XWke6IaTrTXf77hAXR9n/pnYUe0G0yLYE4CvH2/BjzsuqN0agr0cSc/Vqd0eAHo09CVXV8iZuCxGtg3C28WO1S90xtpKw6SFh9Vy1bqWJ/uKHqAsG9+B/nN3qEl9fV8XbKy0fDnyfn7dc5HxXevi42qH3qDw+vIT+LjY8WrfhgR6OnIuIYvTcZk08ndlxdFr5Qjg+a51aOjvyuSeIeTp9Cw+GEOTAFeaB7oz4MtdnIxN5/G2wWp3iNHta/LdtiiSswuY1qcB2QV61py4ytn4LL4aeT99Qv3Jzi9UE/Za1ZzYOLkLr/xp+gCwS31vXundACc7a578YQ97LqRyJj5TEnZLtXUrxMZC9ermjkQIIYSoEItN2MPDwwkPD0evr3zNPMvyRJsgzp2O4NEH2tGqdjU+33iOTzcYm5h3re8NwPR+jXjj4YalEoLr1fVxpq6Ps5pUgHGgsrQcHRoNZOYVotMbiEzIYsrio2TkFdKt6BxarYZu9X3we9yePw5cRoOGLvW9CfR05L4gdw5fSsNaq+HJdjVxc7Dhfw824J2VETzRNpjXHmqErbWWEW1MB3LrVM+bH0a15FJKDg+F+uPjak92fiEd6nrRyN9V7Y8cn5FHRm4hA++vjreLHS3fNj4MaOTvqg5251NiwLHq7g7cF+RB0xruDGsVRIivs3pv3hkYytsDmnA5JZeIqxn0auRLXqEeBxsrNBoN/9czhNUn4sgt0PPuoFD6hvqpSV0xJztr/ny+PTvPJfH68hPYWGlpU9uTziHeJtt5ONny89Ot1cHlrvf14/ezLTKJV/s2xNmu9EcyrFtddp1PZkSbIGyttdT2dmbFhI5oNFDH25mP157hx50XqOHhSIivMx8PbmYyAFlJNlZanutSh70Xkvl82H3U8HDgt72XeHfVKb57sqXJwx2AGh4OaDSOrH2xM5tOJ9CjoQ91fZwZ3jqQq+l5THqgHi/9cZTnu9ShR4Nq/LV6HUGejtQpeppZvWhww7a1vVj4bFt1QENrKy1jOtVWzzOyTbCasLev44W/m3G/V/s25NW+DTkek06/uTuw0mro1ywADbDvQgoxqTl8Mfw+NBoNGg3U93Phj+fa8fOuaJoFuvN422C6fbyFSyk5vNwrhH7NAvjzYAyfbzrHm/0a062+j8n1ajQa7KytGNW+JoA6kB9A65qejF9wCC8nW+Y91QoHWyvavbuR7AI9fUP9GNu5Ds0D3U2O51j0wASMNfHl6dcsgLMJmfi52vN422BsrLQoikKX+t7UquakJuslBbg78PrDjcjI03E2Pov5T7fG3kar9ttuX8eL70e1LLVf8Xsw8YF6vLjoMDkFet54qBF/H7tCsJcjdX2cmdC9Hh+sMdZu1/VxBqBvqD99Q/1NjvVniT78xdtumNyF/EK9ScL+Yo96JmXL3saKJ9pe+/vz81OtSc7OJ9jLiUBPR9rV9sLexopFY9uSmJlP+7rVAHi+Sx3OJWQRWsM4mJ6TnbH5++z1Z/nfgw3QajXM6t+Ehv6uzN8VzaWUHB5s4o+Hky0A1T0c4ELqLY9wL4QQQghhLtIkvgo0iYfSTTuOXE5jQPhOAJaNb899QR535byZeTpi03Jp4Hfze3MgOoUF+y7x4gMhJiPN5xbo1ZreO2ntyTiWHYplZv/GJiODf7ExktUn4vjlmdYmo3jfqviMPDSYPgQoT1pOARo0uDne+2Y3iqKoTd/vhPDN5/ho7RmGtKzBh481q/B+t9P8yGBQeGD2Vi4kZfPTU61KJdIA2yMT8XW1J8TX5ZaOfSYuk8OXUhncMlB9YJKWU4Cbg81NH25dLyoxCw9HWzUB3B+dwtX0PPo19S/zWPujUxgz/wAzH2nMgPvuTY1ezakrARh0/7XZHMqjKAp6g4K1lekMnwaDwtSlx7iSlsePo1uZjOZeUa8uO862s4ksD+tAtdv4HN6MoiikZBeU+qwX6g1EJ+eoDxwApiw+wuKDsUzuUZdJPerftZhEJVajxrUa9pjyu2KZU1Vpyiksl5RRUdlVlTIqTeL/45oEuHJfkDtWGg1Na7jftfO42NvQwK9iBb1lTU9a1vQstfxuJOsAvRv70buxX6nlEx+ox8QSTdf/rbKmByuPu6PtbZ/v3yrZ9P1OGN+1Ds1quNM08N5Nd6jValjwbBvOxGXStYxkHYwtMf6N+n4u1PczTfL/7ftV29vZ5HWrMsr79euPvtnrX53r3/r2iRb8vPsi0/o0vOm2Go0Ga6vSDxq0Ws0tPawpy7sDQ29r/4rSaDRlPpizttKaJOtgHGMCbn9sCyGEEEKIe0US9irK2krLsvEdzB2G+A/SaDR0rFftnp/X381BbQov/r1ejf3oVcaDLAFgzNgNkrELIYQQooq49XaOQgghRBWk1rCbNwwhhBBCiAqThF0IIYRF0KhN4iVlF0IIIUTVIAm7EEIIi6AtytglXxdCCCFEVWGxCXt4eDiNGjWiVatW5g5FCCHEPVA8vJ5BEnYhhBBCVBEWO+hcWFgYYWFh6nD6Qggh/tuKp95TpBe75Xr2WUhPB/l/XwghRBVhsQm7EEIIy6KRad3Em2+aOwIhhBDillhsk3ghhBCWRfqwCyGEEKKqkYRdCCGERbjWh10ydiGEEEJUDZKwCyGEsAgamYddCCGEEFWMJOxCCCEsgjronNSwW64aNYxPbmrUMHckQgghRIVIwi6EEMIiFDeJl3xdCCGEEFWFJOxCCCEsQvGgc9KHXQghhBBVhcUm7OHh4TRq1IhWrVqZOxQhhBD3gPRhF0IIIURVY7EJe1hYGBEREezfv9/coQghhLgHihN2g2TsQgghhKgiLDZhF0IIYVk0xb3YpUm8EEIIIaoISdiFEEJYBK3UsAshhBCiipGEXQghhEW41oddMnYhhBBCVA2SsAshhLAIGnWUeDMHIoQQQghRQZKwCyGEsAgyD7sQQgghqhprcwcghBBC3AvF87ArkrFbrl9/hfx8sLMzdyRCCCFEhUjCLoQQwiLIPOyCrl3NHYEQQghxS6RJvBBCCIugVfuwS8ouhBBCiKpBEnYhhBAWRfJ1IYQQQlQV0iReCCGERdAWPaKWPuwWbMuWa33YpXm8EEKIKsBiE/bw8HDCw8PR6/XmDkUIIcQ9oKF40DkzByLM5/HHITYWqleHmBhzRyOEEELclMU2iQ8LCyMiIoL9+/ebOxQhhBD3gLZo0DmZh10IIYQQVYXFJuxCCCEsi6Z4WjcZJ14IIYQQVYQk7EIIISyCRmrYhRBCCFHFSMIuhBDCImiKf5GEXQghhBBVhCTsQgghLIJWmsQLIYQQooqRhF0IIYRFkCbxQgghhKhqJGEXQghhEdRB52ReNyGEEEJUEZKwCyGEsAjFfdilhl0IIYQQVYUk7EIIISxCcR92IYQQQoiqwtrcAQghhBD3wrU+7FLFbrFiYswdgRBCCHFLpIZdCCGERdAWJeySrwshhBCiqpCEXQghhIUwZuxSwy6EEEKIqkISdiGEEBZBrWE3bxhCCCGEEBUmfdiFEEJYBI3aJF5Sdos1cyakp4ObG7z5prmjEUIIIW5KEnYhhBAWQavOw27mQIT5fPcdxMZC9eqSsAshhKgSpEm8EEIIiyDzsAshhBCiqrHYhD08PJxGjRrRqlUrc4cihBDiHtAU17BLL3YhhBBCVBEWm7CHhYURERHB/v37zR2KEEKIe0Aj07oJIYQQooqx2IRdCCGEZbnWh10ydiGEEEJUDZKwCyGEsAjSh10IIYQQVY0k7EIIISyCRuZhF0IIIUQVIwm7EEIIi6CRJvFCCCGEqGIkYRdCCGERtDLonBBCCCGqGGtzByCEEELcC5qiXuwGydgtV5cukJQE1aqZOxIhhBCiQiRhF0IIYRG00odd/PabuSMQQgghbok0iRdCCGEZihJ2qWEXQgghRFUhCbsQQgiLoJVh4oUQQghRxUjCLoQQwiJo1Bp288YhhBBCCFFRkrALIYSwCMU17IpUsVuu7t2hcWPjTyGEEKIKkEHnhBBCWISiCnYMBrOGIczp7FmIjYX0dHNHIoQQQlSI1LALIYSwCBq1hl0IIYQQomqQhF0IIYRFUMeck1HihRBCCFFFSMIuhBDCIqjzsEu+LoQQQogqQhJ2IYQQFkFT1Itd5mEXQgghRFUhCbsQQgiLINOwCyGEEKKqkYRdCCGERZA+7EIIIYSoaiRhF0IIYRHUedglXxdCCCFEFSEJuxBCCIugzsMuCbsQQgghqghrcwdwp+Tk5NCwYUMGDx7Mxx9/bO5whBBCVDLX5mGXjN1iTZ8OWVng7GzuSIQQQogK+c8k7O+88w5t27Y1dxhCCCEqqeI+7FLDbsHGjjV3BEIIIcQt+U80iY+MjOT06dP06dPH3KEIIYSopIqbxEsndiGEEEJUFWZP2Ldt20a/fv0ICAhAo9GwfPnyUtuEh4dTs2ZN7O3tadOmDfv27TNZ//LLL/Pee+/do4iFEEJURcWDzkkNuxBCCCGqCrMn7NnZ2TRr1ozw8PAy1//+++9MnjyZN998k0OHDtGsWTN69+5NQkICAH/99RchISGEhITcy7CFEEJUMTIPu+DqVYiJMf4UQgghqgCz92Hv06fPDZuyz549m2effZannnoKgK+//pqVK1fy448/MnXqVPbs2cOiRYtYvHgxWVlZ6HQ6XF1dmT59epnHy8/PJz8/X32dkZEBgE6nQ6fT3cEru7OKY6vMMQrLJmVUVHYGvd7406BIObVQ1q1aoYmNRalencILF8wdTpnkb6mo7KSMisquqpTRisanUZTK05lPo9GwbNkyBgwYAEBBQQGOjo4sWbJEXQYwatQo0tLS+Ouvv0z2nzdvHidOnLjhKPEzZsxg5syZpZYvWLAAR0fHO3IdQgghKp+YbPjomDVuNgpvtdSbOxxhBr2eeQaH5GRyvbxY98MP5g5HCCGEBcvJyWHEiBGkp6fj6upa7nZmr2G/kaSkJPR6Pb6+vibLfX19OX369L865rRp05g8ebL6OiMjg8DAQHr16nXDG2VuOp2O9evX07NnT2xsbMwdjhClSBkVld3xy6lwbD82dnb07dvV3OEIM7C2twfA3t6evn37mjmassnfUlHZSRkVlV1VKaPFLb1vplIn7Ldq9OjRN93Gzs4OOzu7UsttbGwq9RtarKrEKSyXlFFRWdnYWJf4XcqoJdNQ+cuA/C0VlZ2UUVHZVfYyWtHYzD7o3I1Uq1YNKysr4uPjTZbHx8fj5+dnpqiEEEJURVp1HvZK0xNMCCGEEOKGKnXCbmtrS4sWLdi4caO6zGAwsHHjRtq1a3dbxw4PD6dRo0a0atXqdsMUQghRBWiKZmKXfF0IIYQQVYXZm8RnZWVx7tw59fWFCxc4cuQInp6eBAUFMXnyZEaNGkXLli1p3bo1c+bMITs7Wx01/t8KCwsjLCyMjIwM3NzcbvcyhBBCVHLqtG6SsAshhBCiijB7wn7gwAG6deumvi4eEG7UqFHMmzePoUOHkpiYyPTp04mLi6N58+asWbOm1EB0QgghxI1oijJ2RWZiF0IIIUQVYfaEvWvXrtxsZrkJEyYwYcKEexSREEKI/6JrfdjNG4cQQlQVF5Ozcbazxsu59IDNQoh7o1L3YRdCCCHuFGkSL4S4F77eep7Xlh1HX8WfDp5LyKLnp9sY+f3eu3oend5Aeq7ujh83T6cnLj3vjh/3dhUUGriSlntXjr0jMom5myIrbdn7eut5xsw/wMpjV80dSpVisQm7DDonhBCWRW0SLxm75dq4EU6cMP6s5BRF4cjlNLZHJt7SPmUt+357FFvPGo/zwZrTdPloM4cvparbFBQauJCUrb7W6Q0cj0lHpzeoy2JSc3jih71sOm2cuSchM4+EjLwy9y9etj86pVIkDheTs9kTlXxL++yNSiYqMctk2YWkbM4lZHLwYgpDv9nNwYsppfbL0+l5f/Vpftt7iZ3nkjiXkEmh3sCaE1eZs+EsBoNCod7AjsgkNkTEk5SVf0tx7buQwvGYdJNlxccsy/nELPp9sYO/jsSy7mQcH689o26bnqvjn2NXSMwsO4ZfdkdTUGjgdFymSYIZl5HHe0eseH/NmVuKvTyvLj1Oq3c2EHHFOCd1anYBP++OJiHz5sl2nk7PxlPxJmW12P/9foQOH2xSj/tvGQxKqeOvOXGVH3ZcKPMzdzE5m2Yz1/HuqlNlHu+91ado//4mdp1LIiEzj8w804cVf+y/zD/HrqA3KOw+n0xBYdnv7afrzxL22yHydHoA0nIKePyHvXy87izzdkWXez2XU3LKLS9305W0XN5ffZoNp+KZtvRYhWLQGxQMRX9DCvUG9VotjdmbxJuLDDonhBCWpaiCXXqwW7L69c0dQYWsi9Hw4pvr1dYgO/7XjRoejiRm5nMsJo3uDXzQaDQkZubz/Y4oRrQOYsqSY0RcyWDAfQGM6ViblJwCmtdwZ+f5JN5eaUwcjkzvyVdbzgMw8MtdHJ3eCxd7a56Zv5/tkUn8Ma4d1T0cGPL1bmLTcmno78r8p1vh42LPL7svsj0yiaOX01jzYmcGfrmTrLxCFo5ty3O/HORKeh4j2wTx9oAmaDQaPlxzmu93XOB/Dzbg+a510BsUrIr7pQCHL6WSnqvDoCjU9XYhyMux1H1QFIXM/EJc7W1QFIU/D8XSpLorDfxcOXgxlajELB5rUaPUwziNRkNSVj6nr2bSpLorQ7/ZQ1xGHkuea0c9Hxcy83UkZxXQtIabum8xnd7A6auZDP12D+6ONux/rQcHolNZfjiWPw/FUFjiAcSjX+1m19TuBLg7qMvOxGWqv7+3+jSnrpomi61reXI+MZs3lp8AwMZKw9sDmjC0VRBn4jL5bONZxneti6OtFXM3n6OujzPju9YFICoxiyHf7AbA28WOCd3qMrJNEA9/sYP0XB1zR9xHi2BPk/NN/uMox2PTeWHREXXZ3M3n6NHQF2c7K5YfuYKdtZbVL3Ri85lEmge60yLYg+z8Qv48FKvu8/nGSGJSc5nxSCPCN58jLlfDDzsvkpGn5+Xe9fF1tS/1/t1Meq6O77dHsfhgDADvrznNz0+35rONkczbFc0Hq0+zbnIXqrs7kFugR6MBexsrvtgYye6oZPo1C+BKWi5fbDrH8NZBvDcoVD12TGoOq0/EAfD3sSs0CnAtM4biB2MN/V2xt7EqtX7jqXimLT2Og60V6/+vCzvOJTJt6XHiM4wPOVoEe6A3KNT3c+HwpVT2R6dy6moG6bk6vt0WxeSeIWg1Gmytter5ftoZDcBzvx5Eb1AI9nLi+1EtWXY4luaB7rzy5zEAHmkWz4qjV3i5VwgTutcziSslu4DPNkYC0KORDzq9witLjqnrlx6K4ZmOtdTXH689w9n4TIa1DuTpeQd4pFkAnw+/j8w8HVGJ2Zy6msHK41ex0mo4eSUDfzd7qjnb0a+ZP/V8XFhx9ArPdamDBrCy0uBqf23+8Mj4TD5YcwZ/N3ua1nDjyOU0bKy0vPJgfY5cTqOutzOrjl9lxt8R6j4ZeYUcjUmnRbAHYHzwcjY+k9rezjjbGdPTuPQ8+ny2jSbV3fhxdCvG/XKQfRdS+OaJFnyw5jQd6lbD29mOlOwCJnSvi72NFQmZeXg42pb5XldlFpuwCyGEsCzaoi/mMg+7qMyOxaSz8rJp4nA8Jh13R1uGfrObqKRs3urfmCfb1eTLLef4aWc032yNUrf9dc8lft1zCYDpDzciv0TtXPO31pscd21EHPmFBrZHJgGw6XQCGg3EFtWmnrqawTdbo3jj4UZsK9omI6+QMfMPqAnLyO/3kplXCMBvey/x295LdA7xZluJGv2Vx69wPiGbNrU96dvEn16NfRn45S41jppejnw27D42nk6gcYArTaq74Wxnzffbo5i7+RwfDGpKZEIm322/gL+bPVumdOXRr4z7eznb0r2BcSDi//15jNXH4+hYrxrrI+JNkmuAsAWHSM4qUJfP6t+Yh5oGMPL7vXSt7429tRVzNp5VH5Sk5ejoMXsrF5Nzyn2/Hv1qF58Obc7Bi6ks2HsJd8dricz1yTrA0cvpHL2cpr7W6RVeXXaCTvW8eeKHvSRk5rPqeJzJPg39XelW34dVx681I07MzOfNFSdZFxHH6aKHBBMXHGbn1O7kFxqIS8/D3dHG5FwlbTgVr/6eX2ig+ydbjffTyZadU7uz7HAsWfmF6jaL9l8GoP/cnRSUqBldfDAGDydbXu3bsNx7VCwlu4AfdkSx7WwSYd3q8OehWNZHXIvj9NUMDAZFXZZdoGfZoRjsbaz4YM1pNBoNz3WuzeebjLNL7TqfrCbCi/ZfYnjrQNaejCPYy4mD0ddakMRn5KEoCosPxrD5dAIv965PHW9nAMI3n+PjdWcZ07EW9XydycrX83SHmmg0Gi4l5xC24BB5OuP1vrMygl/3XjJpNfL68uOciM2gewMf9l1IMblnAA/O2UZiZj5/PNeOxgFunC/RaiOj6HMTcTWDod/u5nKKaTP5FUevAPDNtigmdK9Hnk7PySvpvLniJCdir5Wt1cfjWFfiPgKcvJLBN1vP8+22KMZ1qc3czcZ7VrzdiqNXsNZqWHsyjuyC0rXWxa0udpxLwtPRlriMPJYfjiWnQE9BoYFuDbzJyC1kyoP1eWbeflJzSndp2HEuiXMJWTQOcOVkGa0ctp1NpEWwB1n5hfT4ZCtxGXl0DvHm68fvJzoph61nE0nN0bE9Molen25TW/EUd9E4VqKlya97L9KuthdrT8Yxqn1NXn0wpNT5qjKNYuFtA4tr2NPT03F1LfvpW2Wg0+lYtWoVffv2xcbG5uY7CHGPSRkVlV10YgZdP9mOnbWWM2/3MXc4QpTp4zWnmLslymRZWLc6pOboWLDXmIi72luz/X/d6f3pNuIyrjUbblLd1eSLPECfJn5qTWMxK60GvUHBWqvB2kqjJiSta3qSkJlHdHIODzf155+ifqYtgz04cNGYAGk0ZY8DEejpUCrhKE99XxfOxGeaLNNqTAeEdHe0Ia2MJADgnYFNeG2ZsYb6oab+PNaiBiuOXGHZ4dgyt7+R+4PcOXQp7Zb3q+Zsi6uDDVGJ2TffmGv3zdnOWk3qFo1ty6x/Ijh5JYPpDzfirX8iyt2/d2NfDl5Mu2kTensbrfp+/lsfPBrKz7svcvJKBq1rerIvunTT/+v1DfXj3YGhxKTmMmdDJE93rEmbWl689fdJnIpqTL8sat1xI58Na27SGuBuqO/rQrcGPqw7GUdUUun375mOtVhzIk59cHUn+LjYUdvbicj4LJKzC25pX40GuhQ9BLsbPUy8nGxLxVTH24nzFSzbxTrVq0Zsai5+bvbsOl9+F5Tiz3+rmh78OqYNq4/H8eLvR9T1NlYadPrbu9DIWb2qxHfSiuahFtuHXQghhGWRGnbBggXw/ffGn5VUWV9UwzefZ+G+S+rrjLxCZv0TYZKsAzzdoRYejqZfTndf13f7xR71WDGhA1oNFBoU8nQGaldzAmBfdArRyTnYWmt5Z0Ao1ZyNTUuLk/UWwR480iygVHzV3R14tlPtCl/j9ck6lJ69obxkHVCTdYCVx67y1E/7SyXrz3SsxSeDm1HN2Y7nu9ahX1HctlZaNr7UBb+iJtzXJ+t21loCPR24rqW8cV9rLWte7MQLD9Rj0di2/BXWgQZ+LmXG6OVkvHf1fJy58F5fFj7bFsCkBja0uhv3BxmbBJeVrD/Y2I+u9b0BWHuydH/3xgGuONlaodEYkxygVLJupdXwWIsaJsuGtQosM+Zi//vzOCevZGCl1TB35H042pZuKt7a28AbDzVQX686Hke/uTsY/t0eNpyKZ/SP+/liUyTzd1/kyy3nK5Ssw7X31uG65umTe5rWmL7c69rrIE9H7KxLpzQTutUt8xxn4jP5euv5MpN1gB92XDBJ1u8Pci+1zVv9G5e578g2QWUuT8jMZ09Uyi0n61ZaDYoCW85cS9a9XexoV9sLF3trQnydS+0T4uuMtozyW5YeDX058HoPIt7qrS4zfka68v2TLSsc51MdavLLM23Y9HJXFjzblrcHNCl322GtjeXvWEw6Q77ebZKsg+nfQCuthpWTOjJ3xH3M6t+YoS0DCXCzZ+B91Xl3YCjbX+lGs0D3Uud44sf9xN7a84ZKzWKbxIeHhxMeHo5eb5mDFwghhBAW55VXIDYWqleHESPMHU2Z9EUPlB5p6k+zIA9mFSVyigID76vOI80DeOqn/Swp6vcb4uvMK70bsPdCMg83DWDu5nMmzVPTcnRYaTVM69OA3o39CPQ09hX/5omW/L7/MjW9HAnrVpfun2xR9xvftQ5ujjbMHtKcTacTaFrDDSuthna1vdhxLom/jlxRjx9a3Y3nu9Yh0KN0H/SSXOyt1abzJT16fw3OxmfSr5k/LYI91abuAE62VjjaWZOSXcBLvULYfT5Zbb4PxmQ4MiELjcbYOiCnQE92fiFOdta80KMervY2PFqUrBYUGqhVzYn6vi7U8Xbmuydb0m/ujlLxfPBoU/qG+qsJT93XVgPGWs4VEzrQwM/Yh77Y+G51mbTwMACH3ujJiiOx7ItO4cPHmrHrXBIhvi5oNBqaVHfDzlqrdlHwc7XHyc6aZoHu/LLnImCsHfd3c+BCUjbzn25NlxBjsv7VlvN8vz2KTvWqAbC86P4vfq4dmXmFxKXn8dPOC+pyMLa2aFXTkwcb+9Ei2IONp+JJzdHx2bDm9G9enX3RKUQlZuNib42Pix1X0vJYHtaBsAWHOJdgbLbduqYnPi72/BXWgQ/XnqGRv6vab7q2i0L/Zv7MWnlaPWfJFhYFegNzNkSWWRZ6NvLli+H38ffRK1R3d2D7uSRc7W34YM1p9YHGqw81VPv5P9IsgEkP1ONEbDrrIuJ5oIEPE7rXo4GfK38cuMyo9jXJzNOx+XQiQV6OLNx3iYdC/XmpVwi/7b2oluuXeobwyfqzahyj29ekV2NfXllyjJjUXDwcbcgp0Jt0IwEY1jrI5MHOmI61GNE6iJXHrrI/OkVNpLuEePP2gCYMbx2EvY2WHrO3lXn9AJ8ObcbpuEy61fdh+Hd7aBF0rRVLaHU33B1taF3Tk/xCg9qcvWWwB68/3Ig63k642NtgMCik5BQwaeFh7G2s2HQ6AYChrYKwsdIw/a+TJufs0dBX7QqxdHx7Vhy5wgsP1EOj0eBoey0lVIpGeunRyJf5T7dmXVEz82fm7ye3QE9SVumHDt3q+5i8frxtMNWcbVl2OJa1J+Op6+NMnyZ+RCVmM7JNMJ9tjCQtR8fREs3au9X35tClNJMZA7rV96FxgBuNA8ofb+z3sW3JyNMx659T/F3UjWDPhVQ8Ayv41KIKsNiEXQadE0IIyyLzsIuqoLh/rK+rHaPaBbPrXBJ7opLp0ciX9waFYmetpUNdL3aeM9acP3p/DXo08qVHI2M/7vcHNWXYt7vRaDTqsR5pFsCY62rAezbypWfRPgCzhzRn7ck4WtX0ZND91QHoHOJN56KksVi7Ol7q7x8+1pQhLY21ZSVH0X7lwfpsPJXAidh08gsNfP14CxRF4fnfDhHk6UhcRh4FhQY1eSymKApjO9cmT6cnyNORDnWrUdPLiZScAqq7O9CmlpeasPds5MtXI+9n8cEYano5mcRVFltrrUktbWgNN34a3Yqn5u0HjAPy2Vlb4XBdbXKIrzNn47N4vksdk0S92EOh/lxKzqa2tzOeTraM7lCL0R2Mg331auynbudsZ838p1sTm5rLrvPJPNzUH4DmJWoHfx/bjvp+LiRk5JsMwvd81zo837UOYJy2a/mRK2g14GhrjaOtNb6u9nRv6MvyI1ewtdby46hWtK/jhbZENeva/+vMrnPJ9GtqbGnw/ZMteXPFSV7sUY9a1ZzJKSikhocjXwy/j4c+345BMTZzB6jn68J3T7Y0vmdFCXs9NwU3h9JNja21Gt4dFKoOgFbPx5mpfRpQ3cOB9Sfj+XZbFC/2qIe9jRWDi8pO+7rVuJySwwdrTqv36tH7q3MgOoWd55KY0ts4WOTM/o1pXcuTYa2Ntdglyz3Ag02M9zSsRM3696NaEZeeR99QPzQaDatOxHHqagZPdajJm/2MteTfPNGC+bui+b+eIWTmFZKZV8izPx8gJbuAIE9Hkxr2RWPb0jzQHWsrLb+Pa4eiKOTq9Gw8lUCPhr7qwxm41qx8WKtAXOyNXSEup+Qyok0QfUP91WNunNyFAHcHBoTv5HRcJmM711ZbhFxNz1UT9oea+puUF61WQzVnOxYUtd6Iz8hje2QSA++rjlYDPi72rDlxleVHjAPXPdu5Np9vjOS+QA/uD/JQW3cUuy/IncOX0tTPNBgfQhQ/OFo1qRMGxdjv/9ttUbQM9qBziDdxGXm0L+Pz92ATf7o18GH54Vh6NvLD0+naYHD3Bbqz+YzpDBhPtqvJT0/5kKfT0+CNNQClWoeUxd7GyjgY4fD7OBGbrvZ17+z33/nPXvqwSx92Ie4IKaOisotNyaLDh1vRaiDqvYfMHY4whxo1rtWwx8SYO5oyTV9+nJ/3XOL5zrX4X99GZW5zIjadQV/uItDTgVUvdMLO2jTJ1BsUcgoK6fXpNuIz8lgxoaOaRNwJD32+nfOJWWx+uSv+btdGSD9UNPJ7cW1bRp6O6KRsmtZwR1EUtkcm0TjAlc1nEtl6NpEPH21aKkG+mf3RKfxz9ArPda1jcu5/Q29QeGflKbxd7NSE+HoJGXmsOn6V4W2CSt3nOyV88zlc7K15sl3NCm2/+XQCtao5UbOoKwMYH3b8deQKLYI91FYU/9a5hCy2nU3kiXbB2Fhpr1uXSVJGLvEnd9O3b1/avr+F5OwCxnaujbezHUFejvRu7MfsdWdYdSKOrx9vQV2f0s22y7L0UAxbziTSq7EvDxc9WLh+doHbFZWYxabTCTzRLviG7+eJ2HTeXhnB6w81okl1N7aeTaSas+0Na3qvdzU9lyUHYni6Yy21H/+NxKXncepqBt0amNZWf74xkj1RyXz9RAuT0dkroqDQQHRyNvV8nEvNiHC9pKx8/j56paiFQPn3Jr9Qz58HY+nd2BcvZ7tbiqfYgr2XeHXZcR5vG6QOkrn/tR54uxiPt/FUPNHJOeoAgBW17HAM//f7UV7rWx+f1JOV/jtpRfNQSdglYRfijpAyKiq7q6lZtPvAOBJy9PuSsFukKpCwv7b0GL/tu0xY19pMebD8kbcvp+Tgam+Dm2P5f2/jM/JIysq/pSSjIjLydOTk6/Fzu/WpvETVV/L/+4up+Ww4Fc/THWqpI7YLcTOKonA1PY8AdweOxaSRW6CnTe0bt5Kp6HHTc3U42WiqxHfSiuahFtskXgghhGUp+YxeUZRbemovxL1S3Ifd6iblsyK1qL6u9v9qfuybcbW3ueWaPvHfVNfHucI16EIU02g0BLgbW8g0reF+R4/r7miLTlf+oJVVkTwKE0IIYRFKJuiW3bZMVGaGon7nd7IZsBBCiKpLEnYhhBAWoWSFpeTrorIqlIRdCCFECRabsIeHh9OoUSNatWpl7lCEEELcA1qTGnZJ2UXlJDXsQgghSrLYhD0sLIyIiAj2799v7lCEEELcAyXTH5naTVRWUsMuhBCiJBl0TgghhEUw6cMujeItk5+f6c9KyFDU+kPydSGEECAJuxBCCAth0odd8nXLdOCAuSO4KX1RDbu1ZOxCCCGw4CbxQgghLItWEnZRBRQn7FpJ2IUQQiAJuxBCCAuhKdGL3SAZu6ikiudhlxp2IYQQIAm7EEIIC6GVad1EFaDWsGskYRdCCGHBfdjDw8MJDw9Hr9ebOxQhhBD3gkZq2C3euHGQkgKenvDNN+aOpkx6g/Gn1LALIYQAC07Yw8LCCAsLIyMjAzc3N3OHI4QQ4i6TPuyClSshNhaqVzd3JOVSR4mXhF0IIQTSJF4IIYSFKJn+KJKxi0pKnYddmsQLIYRAEnYhhBAWomSfYMnXRWVlKE7YpYZdCCEEkrALIYSwECUrLKUPu6isCiVhF0IIUYIk7EIIISyCpmQNuxnjEOJGih8mScIuhBACJGEXQghhQTRFqbrUsIvKSi817EIIIUqQhF0IIYTFUFMgyddFJSXzsAshhChJEnYhhBCWoygHMkjCLiqp4oRd5mEXQggBFpywh4eH06hRI1q1amXuUIQQQtwjxSmQIlXsopJSa9gt9huaEEKIkqzNHYC5hIWFERYWRkZGBm5ubuYORwghxD1QnLBLDbuFGj4cUlPBw8PckZRLrxTXsEvGLoQQwoITdiGEEJZHowEUUGTQOcv00UfmjuCmDGofdjMHIoQQolKQx7dCCCEshtokXvJ1UUkVGqSGXQghxDXyv4EQQgiLIQm7qOyKu2tIvi6EEAIkYRdCCGFJ1FHiJWMXlVOhwQCAlUzrJoQQAknYhRBCWJDi//QkXbdQDRqAq6vxZyVVlK9jJZ3YhRBCIAm7EEIICyQ17BYqKwsyM40/K6niUeIlYRdCCAGSsAshhLAgxa2MJV8XlVXxPOySsAshhABJ2IUQQliQa4POScYuKidJ2IUQQpQkCbsQQgiLoSbsZo1CiPLp1XnYJWEXQgghCbsQQghLIqPEi0quuA+7tdSwCyGEwIIT9vDwcBo1akSrVq3MHYoQQoh7ROZhF5WZwaCoZVMrCbsQQggsOGEPCwsjIiKC/fv3mzsUIYQQ90hxCiQ17KIy0pcol1LDLoQQAiw4YRdCCGF5pIZdVGbF/ddB+rALIYQwkoRdCCGExZBp3URlVjJht5JvaEIIIQBrcwcghBBC3GuKjBNvmb7+GnJzwcHB3JGUqWSTeCutZOxCCCEkYRdCCGFBtOoo8eaNQ5jJww+bO4IbMpSsYZcW8UIIIZAm8UIIISyQIm3iRSVUaNIkXjJ2IYQQkrALIYSwINdGiTdrGEKUqbiGXYOCRgadE0IIgTSJF0IIYUGupUCSsVukgwehoABsbaFFC3NHU0pxDbtUrgshhCgmCbsQQgiLoZE+7Jatf3+IjYXq1SEmxtzRlKKXhF0IIcR1pEm8EEIIiyHzsIvKzFBUMOXLmRBCiGLyf4IQQgiLca2GXTJ2UflIk3ghhBDXk4RdCCGExZF8XVRGBknYhRBCXEcSdiGEEBbjWpN4ydhF5SM17EIIIa4nCbsQQgiLoSbsZo1CmEvx+56VX2jWOMqjDjpn5jiEEEJUHvJ/ghBCCIshfdgtm05vACAzr7BStrIoLpcyBbsQQohiFpuwh4eH06hRI1q1amXuUIQQQtwjMkq8ZSv5vufpDOYLpBzFTeKtJGEXQghRxGIT9rCwMCIiIti/f7+5QxFCCHGPSA27KJaeqzN3CKUUDzonNexCCCGKWWzCLoQQwnJJum6ZDCXe+MqYsEsNuxBCiOtZmzsAIYQQ4l650SjxeoOClRmH5953IYVDl1IZ26k2WjPGkZZTQEGhAR9Xe7PFcLf8s2gDM1ecRAHm5V1L2AuL+rZbW5m3HkOtYTdrFEIIISoTqWEXQghhMcrrwz5t6THavLuBxMx80nN1pOdcS+a2nElg9/nkov0U0nIKMBgUvt8exerjV287prj0PAwGhSHf7Ob91adZFxFvsl5RFKYtPcbMv0+Wuf/llBzG/XKAtSfjCN98jsm/H6FQb+BEbDq7ziWpA61VhKIo9PlsO23f20hGiYR20sLDPPDJFhIz81mw9xJn4zNveqz4jDx1NPbs/EIuJGXf8LznEjKZ/McRziVkqssW7L3E6biMcvdbffwq05Yeo6Dw2jVeScvln2NX1OS3pBStHVl2jmTbOarvcaHeQO8523j4ix0m+6TlFJCQkXfT67wRnd7A//1+hB93XChz/aXkHPVhAcDWs4mA1LALIYS4RmrYhRBCWIzivsF5OgNhCw4RlZjNhG51WbjvMgCt3tmARmNM6N/q35iHmwYw+ifjWCenZz3IpxvO8u22KAbeV52lh2IB2PRSF2p7O5ucJyoxiw/XnOHl3iE42lpTqFcI8nIsFc+u80mM+G4vg+6vri67PrE9l5Clxvd/PUNwtbchMTMfJzsrHG2tWXwwhrUn41l78lqi7+9uT/jm8wBM6V2fsG51b3hfFEXhbHwWHk42XE03JqmHL6XRJcSbK2m5rDh6Rb0/ADZWGr55ogVdQnzUVgknYtP5eut5XupVH2uthp6fbuX+IA8WPNuWp+btZ9+FFFa/0ImG/q5k5xdyPjGLzacT6Vrfm6fn7Sc5uwCApYdieaRZAG1qe/LashMARL//kEm8By+mUs/Xmed/OwRAAz9XRrWvCcCA8J0kZObTMjiarPxCuoR407uJH7/svoijrZV6jOIm8ZdTczmfaLznCZn5+LnZczY+k6Hf7KbQoLDjle64OdqQp9OTlJVPDY/S72PYgkNcScvlj3HtsClRS7/1TCLLDsey7HAsw1oH4mhrrd7vD9ac4eut53m8bRBvDwjl5JV0vtkWBUgfdiGEENdIwi6EEMJiFOdBO84lsvKYsXb8w7WnTbYprn3/59hVGge4qcujk7P5ZqsxoSpO1gFe/P0I9wd5MLF7XTwcbflo3Rm+2mJMlg9fTiUnX09mfiGOtlY807EWL/Wqr+77werTpY6XlW/at/pU3LXa7JXHrnI+IYvvd1zAydaKdZO7cKaMGujf98eov++JSr5pwj5vVzQz/46gX7MAdVlkfCZdQrzZeDqh1PY6vcLT8w7gYmfNusmd8Xdz4O2VEeyJSmHV8atM7F6PPJ2BXeeTycovZN+FlKK4LtO9gQ+jftqn3udPN5wtdfwVR6+oDwnAWGse4O4AwKrjVxn/2yF6N/ZV1y89HEvnEG9qVXMiITMfgAMXUwE4E5+pJsIlFSfssam56rKr6bn4udnz6tLjpBbVwO+9kEyvxn68+ddJfj9wmSXPtSPQ05Hnfj2In6s9/9czRC1Lx2LSaBHsqR4vPvNaDf0jc3dSq5oTDfxciEnNZdlh43v+655LjO1Uh2Mx6eq2hZVvAHshhBBmIgm7EEIIi1Fcc3n4Upq67GJyjsk27o42pOXoOBmbblLbXTKhKulYTDrHYtI5dTWDpzvWUpN1gPiMfPX3nAI9X2w6Z5KwF+hLN9u+nJJr8vpk7LXzTlt6XP09u0DP5tMJRCZklTpGUta18568koGiKGhuUG078+8IAP4ukSSfvGJ8EHB9s/8uId5q0+3M/ELavbeJAc0D1HtlUODn3dHq9ptKJPzZ+YWELThU5rR6rWt5qon99XZEJjGkVSAAH6wxPuQo2aLg6OU0en+6jYi3epfat+S5ntm3DJeCHDJtHcnoUQ+Ay6nX3v+49Dx0eoPJe70/OoVejf34/cBl9fy9G/upZajkvY5Lv/Y7QHSJ8nMuIYtzCVmsv67LA8DvBy7h7mCrvk68vZb4Qggh/kOkD7sQQgiLUcPRmL2djiu/D/bQVoE42FipCXGxksksQE0vR5NB6vZeSOH01Zv37S7uO/3b3ouculq6dvxSSg46vYHVx6+SkafjxJWyHxQA7D6frCaFm17qwrgutUttk5JdQNwN+mIX9zO/3tHLacZ+8EX994s907FWqW2XH7li8nAitcQYAH8dvtZ6YPHBGDLzSp9vaMtAvnuiZbkxLj8Sy9azifx5MIbLKTllblOgN3Dkclq5xwAYs385L+5cyJj9y681iS9xvGnLjrP0UAwFJfqV77uQQmLmtWvLzCvkRGzJhD5V/T062bQ7w4Wka8f2K2MQv3o+zuoxYko8ONAr0iZeCCGEkdSwCyGEsBitfAxsj7/2rNrF3rpUAtm0ujuNA1I5cDGVlSVql7dHJpls16S6G2m5OtJKJKdlNe++3vtrTtMy2EPtn329I5fTqPfaagAa+LkQdYPB2orjc7W3plY1Jx69v4babL+k4zHpeDjaFiXMOgr1CqfjMohKzCalqO/49aKSsnn25wMA9Gzkq9Yot67lyceDm/Hy4qM3vVagzCb112tdyxM3R5tSyxv6u3Lqaga7zieXenBQkXMNblGDxQdjytw2PVdHnk7PpRIJe1qOjv/9aWzFEOjpwOWUXI7HprPlzLXjno7LLPeBz0drzwAQ4G7PkoMxHC+qqf/wsaYMuq86Or3C9L9OqDE907EWU5ce5+jlNOyspQ5FCCFEaZKwCyGEsBhBTtC0uivHYo012y/1DGFGUXPwYvX9nHmgoa/aB7o8/m72FJbRpL2Yp5MtGbk6dW7tYgv3XWLhvksViresxNDNwYavH2/ByO/3qPOKt6vjhUajoXY1Jxxtrcgp0APwSLMAVhy9wthfDtLAz+WGLQtKKk6Uiwege+GBevi42IEG7G2seKxFDeMDgq92mezn62rHiNbB7I9OoUuIN7PXnyVXp7/huXo28uWhpv6llr/WtyFPd6xFyOur0RddaA0PB9rV9uKhpv7qYIAlleyOAFDP15nPhjXnhUVHSm279FAsK45cKfX+FOtY15szcRkcupTGlCXHbngNJRUn7SW1reWFtZUWaysY3DJQTdj7NQvggzWnSc3RlXogJISoevR6PTqd7uYbirtKp9NhbW1NXl4eev2N/w+6m2xsbLCysrr5hjchCbsQQgiLodHAt0/czytLT1LN2ZZR7Wuy41wyG07F07W+N6HV3ajr40Idb2daBHsw5JvdALjYWZOZX8ij99fgz0PGZMvd0Zbp/RrxypJjDGlZg/UR8WpT8DUvdiLEx8VYi1uo50paHp9vjFT7fpenQ10v9kalEODuoNb8Vnd34KGm/nxbNHDanmkP4GBrxdsDQll94ip1vJ154QFjf2xrKy2DW9Rg/u6LAIR1q6sO3nY6LhMHGys61atGQmY+l1JyaF/Hi3Un4wnxc+bJdjV5pSgxndGvET/tjGbNyTg61atGk+pu14dKfT+XUsusNBpeKOobDtA4wJUR3+8F4Lkudegb6oervQ1bzyby5oqTzH+6NV1CvNXtp/ZpwOz1Z1kwpg0taxoHb3t7QBPe/OskHw1uSv/mxtH0cwtMv4DZ22jJ05Ueqc3Zzob+zavTvYEPoTPWlVpfVrLubGddNLp8NRr6u3CoxHgHz3Wpw6GLqeyLNva1r1XN6YbT1QFUc7aluoeD+rpVTQ8mdKuLn5s9TnbWdGvgYzLooIejDQ8FSCd2IaoSRVGIi4sjLS3N3KEIjO+Hn58fly9fvuH4LfeCu7s7fn5+txWHJOxCCCEsipeTLT8/3Vp9/c0TLbickkPNak7qMo1GQ+taniwY04Yr6Xk81qIGBYUGbK21VHO2ZX1EPCNaB+HuaENDP1dC/JwZ1jqIsT8fxNPJhhAfF7RaDR5OxoHE/N0ceHtAE9746wTNarijAGM61WLz6QQ+2xBJVFI2TWu48eszbdTz5+n0bI9Mon0dL66m5/Lttii8XexwKJqabESbIEa0CSp1fVMebEBiVj5tanlR38+FoS0DWRcRxxNtg3m8XTA+Lsa+1MUD0aVmF+Bsb42NlZZAD0dOx2XQupYnoTXc6HDQi16N/cq8j8521rzWtyEpOQXYaDV8vukcA0tMTwfQvm41tr/Sjd1RyfRvHoCdtTH2YC9HnmwXXOoLzHNd6vB0h1rYlmgePrx1EENaBpqMF+BQYnq2Xo18GdwyUG2+X1KX+saHAS72Nnhc1+TezlpL61qeeLvYYWulZdF+46ByB9/oQWR8Fo0DXMkp0HPoYipX0/N4sIkfo9rVpNCg8Mm6M4TWcGP18Tg1Yfd2saOutzP7o1NMHgS8P6ipSewajYaXe5eYKeDRprSt5cUrfx7Dw9GG7VO6sHHdmjLvuRCicipO1n18fHB0dDR7kmjpDAYDWVlZODs7o9Wap7uRoijk5OSQkGDsUuXvX7olWUVpFKWssVotR0ZGBm5ubqSnp+Pq6mrucMql0+lYtWoVffv2xcamdD8/IcxNyqio7O5FGc0v1KPVaEzm4r6Z1OwCFuy7xIjWQWqCX5aTV9LxdbWnmrPdnQj1jotKzCLQ0/GWrv12/LH/Ml9sjuTbJ1pSz8eZzzedo4GfC1qNhhbBHuTp9AR6Xpsz/flfDzL9+d74ZyVjCKhOQfRF7G2uJf5/HYmlVjUnmtZwr3AMF5OzWXY4lqY13OhW3weNRsO7q07x7bYoJnSrS7cG3ibTvN3IpeQcNBrwc7GRv6WiUpP/703p9XrOnj2Lj48PXl5e5g5HYEzYMzIycHV1NVvCXiw5OZmEhARCQkJKNY+vaB4qNexCCCHEHVJcg3wrPJxsbzpPOmAyJ3xlVNvb+Z6eb0irQHWqN4DJPUNuuP2nQ5tj/YodZIG2qC9+ScXN7W9FsJcTL/YwPe+0Pg0Y0jKQOt5Ot1TLFuRlfLgg/V+FqFqKP7OOjo432VJYouJyodPp/nV/dknYhRBCCPGfZ29jZczU7zKNRkNdn3v78EIIYX7SDF6U5U6UC5lDRAghhBBCCCGEqIQkYRdCCCGEZbj/fmjb1vhTCCHEHVWzZk3mzJlj7jD+c6RJvBBCCCEsw4oV5o5ACCHM7mbNtN98801mzJhxy8fdv38/Tk5ON9/wBrp27Urz5s0l8S9BEnYhhBBCCCGEsBBXr15Vf//999+ZPn06Z86cUZc5O18bh0NRFPR6PdbWN08bvb2972ygApAm8UIIIYQQQghhMfz8/NR/bm5uaDQa9fXp06dxcXFh9erVtGjRAjs7O3bs2MH58+fp378/vr6+ODs706pVKzZs2GBy3OubxGs0Gr7//nsGDhyIo6Mj9erVY8VttnT6888/ady4MXZ2dtSsWZNPPvnEZP2XX35J/fr18fPzw9/fn8cee0xdt2TJEkJDQ3FwcMDLy4sePXqQnZ19W/HcC1LDLoQQQgghhBB3gKIo5Or0Zjm3g43VHRutfurUqXz88cfUrl0bDw8PLl++TN++fXnnnXews7Pj559/pl+/fpw5c4agoKByjzNz5kw+/PBDPvroI7744gtGjhzJxYsX8fT0vOWYDh48yJAhQ5gxYwZDhw5l165djB8/Hi8vL0aPHs2BAweYNGkS8+fPJzQ0FJ1Ox86dOwFjq4Lhw4fz4YcfMnDgQDIzM9m+fTuKovzre3SvSMIuhBBCCMvwyCOQmAje3tKfXQhxV+Tq9DSavtYs5454qzeOtncmvXvrrbfo2bOn+trT05NmzZqpr2fNmsWyZctYsWIFEyZMKPc4o0ePZvjw4QC8++67fP755+zbt48HH3zwlmOaPXs2DzzwAG+88QYAISEhRERE8NFHHzF69GguXbqEk5MTDz/8MIqi4OrqSosWLQBjwl5YWMigQYMIDg4GIDQ09JZjMIcq3yQ+LS2Nli1b0rx5c5o0acJ3331n7pCEEEIIURkdOgR79hh/CiGEKFfLli1NXmdlZfHyyy/TsGFD3N3dcXZ25tSpU1y6dOmGx2natKn6u5OTE66uriQkJPyrmE6dOkWHDh1MlnXo0IHIyEj0ej09e/YkODiYunXrMm7cOH777TdycnIAaNasGQ888AChoaEMHjyY7777jtTU1H8Vx71W5WvYXVxc2LZtG46OjmRnZ9OkSRMGDRqEl5eXuUMTQgghhBBCWBAHGysi3upttnPfKdeP9v7yyy+zfv16Pv74Y+rWrYuDgwOPPfYYBQUFNzyOjY2NyWuNRoPBYLhjcZbk4uLCoUOH2LRpE//88w8zZszgrbfeYv/+/bi7u7N+/Xp27drFunXr+OKLL3jttdfYu3cvtWrVuivx3ClVPmG3srLC0dERgPz8fBRFqRJ9EYQQQgghhBD/LRqN5o41S69Mdu7cyejRoxk4cCBgrHGPjo6+pzE0bNhQ7ZNeMq6QkBCsrIwPK6ytrenRowetW7fmnXfewdPTk02bNjFo0CA0Gg0dOnSgQ4cOTJ8+neDgYJYtW8bkyZPv6XXcKrM3id+2bRv9+vUjICAAjUbD8uXLS20THh5OzZo1sbe3p02bNuzbt89kfVpaGs2aNaNGjRpMmTKFatWq3aPohRBCCCGEEOK/rV69eixdupQjR45w9OhRRowYcddqyhMTEzly5IjJv/j4eF566SU2btzIrFmzOHv2LPPnz2fu3Lm8/PLLAPzzzz98/vnnHDlyhEuXLvHzzz9jMBioX78+e/fu5d133+XAgQNcunSJpUuXkpiYSMOGDe/KNdxJZn/8k52dTbNmzXj66acZNGhQqfW///47kydP5uuvv6ZNmzbMmTOH3r17c+bMGXx8fABwd3fn6NGjxMfHM2jQIB577DF8fX3LPF9+fj75+fnq64yMDAB0Oh06ne4uXOGdURxbZY5RWDYpo6KykzIqrAENoACFlbQcSDkVlZ2UUVM6nQ5FUTAYDHctgb2bimMu62fJ6/n4448ZM2YM7du3p1q1arzyyitkZGSo117s+tdl3Zeb3asFCxawYMECk2VvvfUWr732GosWLWLGjBnMmjULf39/Zs6cyZNPPonBYMDV1ZWlS5cyY8YM8vLyqFevHr/99hsNGzbk1KlTbN26lTlz5pCRkUFwcDAff/wxvXv3vqvvm8FgQFEUdDqd2gqgWEU/QxqlErUf12g0LFu2jAEDBqjL2rRpQ6tWrZg7dy5gvOjAwEAmTpzI1KlTSx1j/PjxdO/e3WTOvZJmzJjBzJkzSy1fsGCB2rReCCGEEP89vZ55BofkZHK9vFj3ww/mDkcI8R9gbW2Nn58fgYGB2NramjscUckUFBRw+fJl4uLiKCwsNFmXk5PDiBEjSE9Px9XVtdxjmL2G/UYKCgo4ePAg06ZNU5dptVp69OjB7t27AYiPj8fR0REXFxfS09PZtm0bzz//fLnHnDZtmkk/hYyMDAIDA+nVq9cNb5S56XQ61q9fT8+ePUsN3iBEZSBlVFR2UkaFtb09APb29vTt29fM0ZRNyqmo7KSMmsrLy+Py5cs4OztjX/Q3RpiXoihkZmbi4uJyx+al/7fy8vJwcHCgc+fOpcpHcUvvm6nUCXtSUhJ6vb5U83ZfX19Onz4NwMWLFxk7dqw62NzEiRNvOKeenZ0ddnZ2pZbb2NhUiT86VSVOYbmkjIrKTsqo0FB65OLKRsqpqOykjBrp9Xo0Gg1arRat1uzDgwmuNesvfl/MSavVotFoyvy8VPTzU6kT9opo3bo1R44cMXcYQgghhBBCCCHEHVWpE/Zq1aphZWVFfHy8yfL4+Hj8/PzMFJUQQgghqqTJkyEjAypxFzghhBCipEqdsNva2tKiRQs2btyoDkRnMBjYuHEjEyZMuK1jh4eHEx4ejl6vvwORCiGEEKLSq+Rz7QohhBDXM3vCnpWVxblz59TXFy5c4MiRI3h6ehIUFMTkyZMZNWoULVu2pHXr1syZM4fs7Gyeeuqp2zpvWFgYYWFhZGRk4ObmdruXIYQQQgghhBBC3FFmT9gPHDhAt27d1NfFI7iPGjWKefPmMXToUBITE5k+fTpxcXE0b96cNWvWlDvPuhBCCCGEEEII8V9g9oS9a9eu3Gwq+AkTJtx2E3ghhBBCWLjMTFAU0GjAxcXc0QghhBA3JXMPCCGEEMIyNGwIbm7Gn0IIIUQVYLEJe3h4OI0aNaJVq1bmDkUIIYQQQgghqpSuXbvy4osvmjuM/zyLTdjDwsKIiIhg//795g5FCCGEEEIIIe6Jfv368eCDD5a5bvv27Wg0Go4dO3bb55k3bx7u7u63fRxLZ7EJuxBCCCGEEEJYmmeeeYb169cTExNTat1PP/1Ey5Ytadq0qRkiE2WRhF0IIYQQQgghLMTDDz+Mt7c38+bNM1melZXF4sWLeeaZZ0hOTmb48OFUr14dR0dHQkNDWbhw4R2N49KlS/Tv3x9nZ2dcXV0ZMmQI8fHx6vqjR4/SrVs3XFxccHV1pUWLFhw4cACAixcv0q9fPzw8PHBycqJx48asWrXqjsZXWUjCLio1Q34+GatWYcjNveX9Lj7+BFf+97+7FJl55J06ReqiRTedWUFUXObGjeSeOGnuMMzqRuVJn5nJ1TdnkHvkyL0L6Dq6q1fRZ2Wb7fxCCCFEhSkKFGSb518Fvx9aW1vz5JNPMm/ePJPvAIsXL0av1zN8+HDy8vJo0aIFK1eu5MSJE4wdO5YnnniCffv23ZHbZDAY6N+/PykpKWzdupX169cTFRXF0KFD1W1GjhxJjRo12L9/PwcPHmTq1KnY2NgAxu7N+fn5bNu2jePHj/PBBx/g7Ox8R2KrbMw+rZu5hIeHEx4ejl6vN3cot0XR6Uj7808cW7bErm7df3WMvLNniZ04Ce8XJuHat+8djrDiFEXBkJmJlauruixl/s8kzp6NU4cOBP3wfbn7GrKzif/4YxyaNaMwIRGbgAByDhyAA+D76qtYubnd/Px6PRmrVuPUvh0aOzusbvKhN+Tno7WzK3e9LjaWwqQk7END0Wgr/mzMkJND1rbtODRvhrWvLxqNxhifonBh4CAAtE7OuPV7mMwtW4h7YzrVZ3+C478YQLEwNZXMDRtw6dEDaw+PW96/IrK27yD+vfeg6LPm9dxzuA8cAIAuIQGtgwNW92B6pcKUFDTW1iblKz8ykpgw45SRIfv3lRuHPiMDfUYGtjVq3FYMiqKQuXYd+WfPkPT1Nzh16ED1jz8CQGNnh9be/ob75587h6I3YFe7Flhbq2XjZvRZWeQePIhTx45orKxMYyoo4OITT6LodNT8fRGaov8IiyV88glpv/9O2u+/0/D0qVu42jujIDqa8/0ewalVS4J+/NEYs05H3LvvorWzp9qEsJt+VsF4nRnL/8JKV3C3QxZCCGHJdDnwboB5zv3qFbB1qtCmTz/9NB999BFbt26la9eugLE5/KOPPoqbmxtubm68/PLL6vYTJ05k7dq1/PHHH7Ru3fq2Q924cSPHjx/nwoULBAYGAvDzzz/TuHFj9u/fT6tWrbh06RJTpkyhQYMGANSrV0/d/9KlSzz66KOEhoYCULt27duOqbKy2Br2/8qgc+krVxI3YyZRD/fjbIeO5J0q/wu1oijo09JK1aZd+d9UCi5eJHbyS2XuVxAdTcb69WSsWcPl8WGk//23us6Qk8OlZ8dyecIECi5evGGsuoQEChMTMWRnE/fWW0SPGEnuyWs1m6kLF3K2dRsyVq9Wl6UtWgRA9s6dxgcLL71M/EcfmRw399gxLj/3PGkLF3F16jQSZ8/mSok/MPEffEhhUlKZ98OQl6e+Tpn/M1emTCGyQ0fOtm1H7vHj5V5LxurVnGnRkqSvvwaMtZDnH+zDqSahJH3zLYaCAqKHDSd66DAujRqNoeDmSYIhL4/YV17hzP0tiH3xRc517UbUQw+Tf/48AAXnzqnbZu/eDUDMc89TmJhIzMRJJscqTE0la+tWDPn55J06hS4+AaWggIQ5c4geMRLd1avo4hM41607cW9MJ+nLr0hdvJiEzz7DkG2syczeu4+rM2ZQmJqqHjPhk9kUXLpUZvyKohDw88/EjnsOQ04O6StXkh8VxeXnnqMgKoqCixcpuHiRq9OmoRgM6K5e5fyDfbg4apRJmVQKCkj4ZDaZmzff9J5VlD4zk6h+jxA9ZCiKwaAuzzl4UP09Zd589XdDQQFxb72llsXLY8cR1aevWsZ1cXFl1krnnjxJ5qbN6rrCpCSUwkJ1fdbmLcS++CJJX34FBgPZ27cT99YszvXqzaVnxpC5aTNRAwaSd/asyXHzTp0idvJLRPUfwIX+/Tkd2pTYSS+gz8pGV6LpWHkSZ8/m8rjnON24CXGz3kYp8aAybekyco8eJS8igryICNKWLKEwNRWlsJCLo0aTtuh3dVtdfAKG7GwuDBlK7JRXSt0DfUYGhvz8UufPj4wkeuTjJH4xl9zjx8navr3UNrqrV7k6/U1SFy4kdfFi49+HEydJW7YcdDqyd+1WW9pkbd9O2sJFpMybx9Vpr5Z/3XPDuTB4CAXR0VydMZOE6dPx/vufm94vIYQQ4r+uQYMGtG/fnh+LHoafO3eO7du388wzzwCg1+uZNWsWoaGheHp64uzszNq1a7lUzvfAW3Xq1CkCAwPVZB2gUaNGuLu7c6oon5k8eTJjxoyhR48evP/++5wv+k4MMGnSJN5++206dOjAm2++eUcGyausLLaGvSrSlPjiXyy3RMKhT04mbdky/Irml1UUhYwVK0hfuRJdTCy62FiU/HzchwzB/62Z6n66Eon2hUGPUmPuF9gEXHsyGDv5JfIiItTXWZs2UZiQiPvQoWTv2kl20ZfvggvR1P5rOQBJ33yDkpeH+9Bh2Naojj4rmwv9B6DodDi0uJ/srdsAuDhsODY1auDYqhVpf/xhPN//TSZhzhxcevRA0enU8159/Q3yij6MaQsXYd+0KT4vTSZ6yLWmM2VJX7qU9BUrcHuoL9UmTiR96VLsQkIouHCBxC/mEvTjDzi1bUvq74uu7VRYSNLX3xAYPvfaopQUrr4xnYILFyiIigIgcc5neI0dS87+AxRERxuXffopqb8vojAxEYCc/ftJ+vxzPJ96itzjx8netg3FYMDn//6PrC1byNq5E5+XXiZ7504yVlx7GAJQEBVF1EMP49q3Dxrba7X5edc9TNCnpZF/7hx2deuSvXcfsZMmoU9PV9drnZxwuP9+9b3KWLUaxaBHKXpgkfrLL+q2OXv3EfTTj8SEhWHIyqIg6gKB339H8rffkfLTT6TMm0fIwQPG2l1r45+QwsRECpKTcT4ZQS4Q2amzmviXJeHDj7D280XJySE/4hTne/aixtwvsG/QgNTf/yD5u+8AqH/wAFqn8p8U67OyuDJ1KhqtFdbe3uSdOU3A229jW7OmyXY5+w+gT05Gn5xMwfnz5Bw5Quqvv5nUaCeFh5N35jReo0eTtngJ6X/9ReqChTg0a6Y2B89Yuw4bf3+uTJlCtUkT8R4/Xt0/5ZdfiX/nHQB8pkxB6+xM3MyZuPXvT8B776IYDCR99VWpa8hYuRIwfpZjij7PFx7pT7Xx4/F44nESZ39K2uLFpfbLXL+ezPXrwcqKWov/wL5RI3Vd3NvvkL19O76vTsOubl1SF1zrc5b622+kLlpEtXFjqTZhAikl3vvoocMAcN60Ge8XXyBn716Tc14cORJd0QA1eceO4dq7F45t25J/5gyZGzeR8tNPOHXsSMC775Bz8BBaZ2cUXQHpy5aTe/AguQcPkhQeDkDwLz9j7eND1s6d5J86TfqKFSjXJftX/vc/k+tK+Ogj9GlpZKy69lAvc/NmMjdtJufAATxHjcLG1wcw/v1Lmmv8/J7v+xAUPahxNWPTfiGEEBbAxtFY022uc9+CZ555hokTJxIeHs5PP/1EnTp16NKlCwAfffQRn332GXPmzCE0NBQnJydefPFFCipQCXWnzJgxgxEjRrBy5UpWr17Nm2++yaJFixg4cCBjxoyhd+/erFy5knXr1vHee+/xySefMHHixHsW372iUSy8M2xGRgZubm6kp6fjWqKpbGVSEB3N1ffeIz4pmfqT/4/MPxbjMWIE1j7exE6aRH7kOeybNiXv2DFsAgNxbNECKzdXClNTSyWAxTxGjMDKw4PcY8fUJK4kx5YtCfjkY7ROTpxtWXZTa8+nnwaDgZQSA1b4zXqL3AMHSP9rBQDW3t7UWvon2bt2ceV/U2//ZlzH2ttbTYz/LZvAQNwGDiDp8y9MltuFhFBr+TKytmxFd+UKiXPmYMjKKrV/zT9+J2vLFmOtaRnH1l2+jMbGBmtvb3RXyv4Dbh8aapKEB/30IzaBQVx65ml0F8t+kllr2VK1iTwA1tZU//hjrr7xBobMzBtes0ufBzFkZJK9c2eZ6z1HPUnK/J/V13b16lGYloo+0dhSwalDB/IvRKHkF2AXUo+c3XtueD5rHx8KExKuW2gNJR5COXXuROBXX3Hh0cfIP33aeN4GDXAfMpi0JUvQ2thiU706+rQ0dAnx+L36KnkRESR8/InJYe0aNaTWokVkrF2HtY8PNgH+pC35k+RvvgHAb8YM4mbMMNnHtm4dCs6dpyx29euTf+YMAG4DB5K5aROGoochNX9fhEOzZiiKQtTD/Sg4X/YxnDp2JC8iAn1KyrXjhoRgyMlRE+CyWLm7o09Lu3acTp3K/Lw6tW+Pa79+FFyMJmPVanQVfPrtPmyoSQ16SV7jxqn3rDw21aujdXZW748at5cX+uTkCsVwp7n264fXs2PQOjpyvkfPUusVrZa6hw5ie5PuB+I/qkYNiI2F6tXhBp89c9LpdKxatYq+ffuqfTWFqEykjJrKy8vjwoUL1KpVC/sq9n9LVlYW/v7+fPzxx7z99ts8//zzvPqqseVav3798PHx4YcffgCMfc4bNGhAo0aNWL58OWCch7158+bMmTOnzOPPmzePF198kbQS32WKrV+/nj59+pg0iY+IiFCbxLds2bLUPsOHDyc7O5sVK1aUWjdt2jRWrlzJsWPHMBgMZGRk4OrqivYWuqXeDTcqHxXNQyVhrwIJe35kJFH9B6g1RGWp/fcKovo9UuY6r+efw6lNG2wCAkgKD1eT6Yqw8vRUkwxrX18cW7ZUawQBtK6uGDIysAsJIf/sWbQuLqWSRcdWrShMSTEmM1otGAxY+/hQZ91acg8d4tLTz9wwBpsaNShMSUHJyalw3HeK+/BhpC28VvNuW6cObv36YdegPul/LjXWcN5ArRV/Ef/WLGN/+goKXvAbjvffDxhrCc917UZhUbNnjyeeIP/0aXJu0pXD4b778Jsxg/zISJw7dyJuxgwyVq1W369y92vWjNyjR9XXdg0bUnDhgloT/284tmyJfePGpMw3Njl3GzCAzHXrMJTxfjp360ZWRZvCazTlDq6isbVFuYUnwHW3bkWfnMSlZ8feMNG08vBAX9RFoFiNL8PRXb1K/Ky30djYYN+0qUnLl5K0zs54Pv0UWnsHXHo8gCE3l5hJk8p9KFOSx+OP4zv1fyR8MhvboEDiZr5V4esD44MH/7dnET14yC3tZxMcRPVPZhP92GMmyzWOjiafSWsfH7SuLuU++LAPDUVja1vuvbmec48HyNqwsdz1rg8/jE1AAMnffluh45UU9M/fOP3LMT9EFXfwIBQUgK0ttGhh7mjKJMmQqOykjJqqygk7wJgxY1i6dCkZGRlcunSJgKJWtpMnT2bJkiUsWrQIDw8PZs+ezR9//EG3bt1uKWGfOHEi26+rbLCzs6NBgwbcf//9uLi4MGfOHAoLCxk/fjzOzs5s2bKF3NxcpkyZwmOPPUatWrWIiYlh1KhRPProo3zwwQe8+OKL9OnTh5CQEFJTUxk/fjzBwcH8/vvv/7mE3WL7sFcldvXq4dK/f7nrbQID+X/27jq+qvoN4PjnxrqTBQtyo7tRkAZFBRUEDBQbE+uHgdgKBoJgoIIBioCi0iENkpKj2Vh3563z++Nsd7tswMgN97xfL17jnnviueeee+95vunQpAlO7drZLNc4OuL/v5fxf+YZXLp2xT40lIC33sLv2Wdw7tq1WscuS9Zd+/al0ZrV1HtlIu63DrU+X5b8+b/8kvq4NFn3uO1WGpQ2jy/ctQvDqVNoHBwI+eorXLp3x/+ll9A6OuLSvTsBFZrno9HgOepufB5/zLrIffBgPO+447xxOrVtS9CUD9Va/1KuvXrhPa78sWOb1gS8+Sb1XnnFpsl/Rfbh4QS+8zaOLVsC2CTrTh070PCPJfg+9ihuvXvj+9STaM764Pk+9SR2wcEET/uUpv9sx7FpU/xffAGNk5PNenbBwXiNHkXTXbYjber9/HCqMO+lRqPBITKifP+PP2bzmqpiV78+9Wd+jmNEUzxuuRmduztBH31Eg99/o9GqlbbrnnUe3AYNsnkc+NZbNt0n7OrXx/8CI+8HTJuG13334v/ii3jcMZygjz/GPjzM+nzQB+8TvngRTu3a4dylCxH7/sUhQn2N+evXg0ZD8CcfE/TRR9g3aFBp/w5Nm2IfFmZN1nU+PjTd8Q+h331LYGmT9Oom6x53DCfwnbexq+ePY/PmNFi0kKApH+LUoQM6T0/QaNA4OqItHdTs7GQdIP6J8aS8/Q4A7kOGEPbjDzRc+hdNtmxGHxBgXc+1b18arVqJ3xNP4PPgA9iHhuIYEUGj5cvxf/FFm31qXVygwk1Q4Dtv4//8BDR6PfVefgmvUaOoP2tmpevvfFxvvBGnVq0IeGMSge++gz4w0PpcxTgDP3jfZjuvu0fh1LIFXmPGgFaLa+/e+L/4IgGvvQaAxsmJ+p/PoMmmjfg/+yygFvSdzefBBwj56ksa/LGE8F8X4NylC75PPEHE/n3WdeyCg63/L9s/gEOTxtg3alS+r4cfIvijqXjeWf69oPP0JHzBLzhUGJAGwH3IYII//YTQ77/HobS7UMXxIEQd06EDdOtWa5N1IYS41saNG0dWVhYDBw60JusAr732Gu3bt2fgwIH07t2bgIAAbr/99ovef35+Pu3atbP5N3ToUDQaDX/88QdeXl7ceOON9OvXj4YNG7JggdryT6fTkZGRwX333UfTpk0ZMWIEgwcP5s031ftSs9nM+PHjadasGYMGDaJp06bMmjXripyTWkepoz7//HOlWbNmStOmTRVAycnJqemQzqswMVHZdctQJfG995SS6Gjl+E03KVERkUpURKSS+MYbiqIoijE9XclZtUopiYtTFEVRLBbLefdpzMxUDImJisVoVIyZmUrmz78ouev+VjLmzlXSvvpaOTVsmPUY6d9+Z7Nt5s8/K8dv7KUc79VbSXrzLcVisShH2rW3rl909JiiKIoS9/QzSlREpHJy0GCl8MDBKuMwFxYqCS+9rESPHqPkb92qKIqilMTFKUdat1FO3nyzYi4sVMyFhUr0iJHKiQEDlNTpM5SCPXutx4qKiFQsJSXW11y2LOF/ExVFURRDcrKSOn2GYsrKsh7TlJev5G3YoGQt/k2Jn/C8UhIXp567mBhFURSlYOdO637OPDhOyf7zT8WUm1sp9vzt/yjRI0Za1614jIoMKSlKzoqVijk/XzHl5dk8l/jGG8qxLl2VnJWrFENCQqVt87ZsUaIiIpXYRx61vsa0L75QUj76WMn+a6lycsBA6/GPduqsFB8/XmUMZaJH3q1eN69PUopPnlQSXnpZiYqIVNK++lopiYuzPa9Go2Ixm5W0r75WTg+/Q8latEgxFxUpJ/r2U4526KikffmVuq/XXleyN29WNrz4olJS+l5UZDEYlKQ331JyVq+uMqb07+ZYjxkz5h7rcnNxsVJ09JiSs2KF9XljZqZiTE1VjnXpqkQ1a67kbd5SfhyLRUmZNk050radEnP/WMWYmVn+Pj7wgJL+zTfWxykffXze86QoimLMyFAsBoP6Pk2erG4b2UzJ27JFKT51qvy8d+ykJE+Zopjz8222P9qhY/m5PM/n0ZCQoJzo20+JHj1GMcTHKxaLRTHl5CiJr09SspcuPed2FotFOTlwkHKkZSul6OhRpSQu3nq86DFjFHNBgWJISVEKdu1SzGe9L2lffGFdN3PBAuXETX2UM488olgsFiXuqaetz+Vt3Kgey2RSjJmZtnEnp1R6zcXHjyuGlBSb6+j0HXcqFpPpnK8jf/t2Jf2bbxRjWpoSffcoJeOnnxRFUZSMn35SEidPVsyFhYqiKErRkSNKykcfKeaCAuu2p0eMUKIiIpXUz6Zbj3962HDrsTPmzbOuG/vcBPW9/+qrc8YiRE0zGAzKkiVLFEPpd48QtY1co7aKioqUqKgopaioqKZDEaXMZrOSlZWlmM3mmg7lvNdHTk5OtfLQOpuwl6nuiappZ3855m3caL0hzV279qoc02KxKDmrVyupn0233jCfT/o33ypH2rVXsv/8y7rMnJ+vJruXcH7PTgYsZrNN0pP83nvKkVatlfxt22y2ix4zRomKiFQK9+276GNWlL1kiZL5y4LzJhplcteuVXLXrbvkY1ku8IVSeOBglQUGiqIohsREJWfZMjWRPSuhqoopJ8daqKMoimLKzVXyNm2yntvoUaPVQpYBA8+9j6wsxZCcoiiKmkSZi4sv6wfclJdXXjj0zTeVnrcYDEripDeUjB9+tC4riYtXio4cqXJ/FrPZek4z589Xzjw4TjGkpFhfb+bPvyimvPwqtz0Xi8GgpH35lZKzapV1WcqnnypxTz19zus744cfbQqPrgZjaqq1oElRlPKCj3vvO+92FrNZyd/+j5K7Zo31XJVdA8aMDOV4r97KkXbtFWNGxiXFdfzGXkpURKSS/88OxVxcfEn7qA5jZqaSMW9epWMUnzqlpM6cabM8fuJEJSoiUkmeNeuqxSPE5ZJkSNR2co3akoS99vmvJezSh/066MMOlfsLmfPyON5JnQOx6e7d6FyrN+fi1aaYzZXmeb5qx1IUlKIitM62I2Kac3IwZWTg8B+ej/FqMmVkkPbZdDzvvMOmef6FXG6fttxVq8nftJGAV1+t9J5erxSzmcKdO3Fq3x6tg8OFN7gCUj/6iIxvvyP85/k4tW17yfsx5+ZiKSrCrl69S9remJyMKSUFpzZtLjmGKy3h1VfJXfwb3k89Sb3x42s6HFETli6FoiJwcoJbbqnpaKok/YNFbSfXqK3rvQ/7f9F/rQ+7TOt2ndK5udHwrz9Bq601yTpwzZJ1UPt3a6pI7HQeHug8PK5ZHP81eh8fm37r14r7wAG4DxxwzY97NWl0Oly6dbumx/R79ll8HnpI7YN/GXTu7uguoxDTLiAAuwp942sFTemPdp0upq7jHnus1o8SL4QQQlQkCft17OzBlYQQQqPXX3ay/p+l0ah/zzPjhhBCCCFEbXJJbQTi4uKIr1AyvXPnTp599lm+voTpdYQQQohrQluasEsVuxBCCCGuE5eUsI8ePZr1pXMlJycn079/f3bu3Mmrr77KW29d3NzAQgghxLWgQU3YFalhF0IIIcR14pIS9kOHDtG5szrg2a+//krLli3Ztm0b8+bNY+7cuVcyPiGEEOLK0EofdiGEEEJcXy4pYTcajTiUjni8du1abr31VgAiIyNJSkq6ctFdRTNnzqR58+Z06tSppkMRQghxLUgfdiGEEEJcZy4pYW/RogVffvklmzdvZs2aNQwaNAiAxMREfHx8rmiAV8v48eOJiopi165dNR2KEEKIa0H6sAshhBDiOnNJCfuHH37IV199Re/evRk1ahRtSufZ/fPPP61N5YUQQojapawPuyTsQgghxOXq3bs3zz77bE2H8Z93SQl77969SU9PJz09ne+++866/JFHHuHLL7+8YsEJIYQQV0xZk3hFEnYhhBB119ChQ60tpM+2efNmNBoNBw4cuOzjzJ07F41Gg0ajQavVEhgYyMiRI4mNjbVZr3fv3mg0Gj744INK+7j55pvRaDRMnjzZuiw6OprRo0cTFBSEo6Mj9evX57bbbuPo0aPWdby8vNDpdNbjl/375ZdfLvt1XWuXlLAXFRVRUlKCl5cXAGfOnGHatGkcO3YMf3//KxqgEEIIcSVoyprEK9KHvc5ydQU3N/WvEELUUePGjWPNmjU203SXmTNnDh07dqR169ZX5Fju7u4kJSWRkJDA4sWLOXbsGHfddVel9UJCQioNXp6QkMC6desIDAy0LjMajfTv35+cnBx+++03jh07xoIFC2jVqhXZ2dk223/77bckJSXZ/Lv99tuvyOu6li4pYb/tttv44YcfAMjOzqZLly58/PHH3H777XzxxRdXNEAhhBDiipAadnH0KOTmqn+FEKKOuuWWW/Dz86uUIOfn57Nw4ULGjRtHRkYGo0aNIjg4GGdnZ1q1asXPP/980cfSaDQEBAQQGBhI9+7dGTduHDt37iQ3N7dSTOnp6WzdutW67Pvvv2fAgAE2FcKHDx/m1KlTzJo1i65duxIWFkaPHj1455136Nq1q80+PT09CQgIsPnn6Oh40a+hpl1Swr53715uuOEGABYtWkS9evU4c+YMP/zwA9OnT7+iAQohhBBXhKb0J0/6sAshhLhKFEWh0FhYI/+UahZI6/V67rvvPubOnWuzzcKFCzGbzYwaNYri4mI6dOjAsmXLOHToEI888gj33nsvO3fuvORzk5qayu+//45Op0On09k8Z29vz5gxY5gzZ4512dy5c3nwwQdt1vPz80Or1bJo0SLMZvMlx3I90V/KRoWFhbi5uQGwevVqhg8fjlarpWvXrpw5c+aKBiiEEEJcEaU17NW9oRFCCCEuVpGpiC7zu9TIsXeM3oGznXO11n3wwQeZOnUqGzdupHfv3oDaHP6OO+7Aw8MDDw8PXnjhBev6Tz31FKtWreLXX3+9qEHGc3JycHV1VQsyCgsBePrpp3FxcakyphtuuIHPPvuMPXv2kJOTwy233GLTfz04OJjp06fz0ksv8eabb9KxY0duuukmxowZQ8OGDW32N2bMmEoFA1FRUYSGhlY7/trgkmrYGzduzJIlS4iLi2PVqlUMGDAAUEtN3N3dr2iAQgghxJUgfdiFEEIIVWRkJN27d7cOIH7y5Ek2b97MuHHjADCbzbz99tu0atUKb29vXF1dWbVqVaUB4y7Ezc2Nffv2sXv3bj7++GPat2/Pu+++W+W6bdq0oUmTJixatIjvvvuOe++9F72+cv3y+PHjSU5OZt68eXTr1o2FCxfSokUL1qxZY7Pexx9/zL59+2z+BQUFXVT8tcEl1bBPmjSJ0aNH89xzz9GnTx+6desGqLXt7dq1u6IBXi0zZ85k5syZdaYphRBC1HnSh128+CJkZYGXF0ydWtPRCCH+g5z0TuwYvaPGjn0xxo0bx1NPPcXMmTOZM2cOjRo1olevXgBMnTqVzz77jGnTptGqVStcXFx49tlnMRgMF3UMrVZL48aNAWjWrBmnTp3i8ccf58cff6xy/QcffJCZM2cSFRV13ub3bm5uDB06lKFDh/LOO+8wcOBA3nnnHfr3729dJyAgwHrs69klJex33nknPXv2JCkpyToHO0Dfvn0ZNmzYFQvuaho/fjzjx48nNzcXDw+Pmg5HCCHE1SZ92MXPP0NCAgQHS8IuhLgqNBpNtZul17QRI0bwzDPPMH/+fH744Qcef/xxNKWF21u3buW2227jnnvuAcBisXD8+HGaN29+Wcf83//+R6NGjXjuuedo3759pedHjx7NCy+8QJs2bap9LI1GQ2RkJNu2bbus2GqrS0rYAetIe2XTAdSvX/+i+jMIIYQQ15TUsAshhBBWrq6ujBw5kokTJ5Kbm8vYsWOtz5U1Td+2bRteXl588sknpKSkXHbCHhISwrBhw5g0aRJLly6t9LyXlxdJSUnY2dlVuf2+fft44403uPfee2nevDn29vZs3LiR7777jpdfftlm3ezsbJKTk22Wubm5Vdl/vja7pD7sFouFt956Cw8PD8LCwggLC8PT05O3334bi0X6BgohhKiFtGWDzsnvlBBCCAFqs/isrCwGDhxo07/7tddeo3379gwcOJDevXsTEBBwxeYwf+6551i2bNk5m7x7enqeM6muX78+4eHhvPnmm3Tp0oX27dvz2Wef8eabb/Lqq69Wem2BgYE2/2bMmHFFXsO1dEk17K+++irffvstH3zwAT169ABgy5YtTJ48meLi4nMOJCCEEELUFI3UsAshhBA2unXrVuXsKd7e3ixZsuS8227YsOG8z48dO9am1r5M165dbY55of3s27fP+n9fX18+++yz864PkJWVhbu7O1rtJdVP1yqXlLB///33fPPNN9x6663WZa1btyY4OJgnnnhCEnYhhBC1j/RhF0IIIcR15pKKHDIzM4mMjKy0PDIykszMzMsOSgghhLjipIZdCCGEENeZS0rY27Rpw+eff15p+eeff07r1q0vOyghhBDiipN52IUQQghxnbmkJvFTpkzh5ptvZu3atdY52Ldv305cXBzLly+/ogEKIYQQV4SmbNC5Go5DCCGEEKKaLqmGvVevXhw/fpxhw4aRnZ1NdnY2w4cP5/Dhw/z4449XOkYhhBDismnK+rBLxi6EEEKI68Qlz8MeFBRUaXC5/fv38+233/L1119fdmBCCCHEFVXaIh6ZfrTuuvlmyMwEb++ajkQIIYSolktO2K93M2fOZObMmZjN5poORQghxLVgndpFatjrrK++qukIhBBCiIty/U9Md4nGjx9PVFQUu3btqulQhBBCXAtlo8TLtG5CCCGEuE7U2YRdCCFE3VLWh12RPuxCCCGEuE5cVJP44cOHn/f57Ozsy4lFCCGEuHqkD7sQQgghrjMXVcPu4eFx3n9hYWHcd999VytWIYQQ4tJJH3bRsSPUr6/+FUKIOmzs2LFoNBo0Gg12dnbUq1eP/v37891332G5yILtuXPn4unpeUXi6t27N88+++wV2dd/xUXVsM+ZM+dqxSGEEEJcZdKHvc5LToaEhJqOQgghaoVBgwYxZ84czGYzKSkprFy5kmeeeYZFixbx559/otfX2fHJaxXpwy6EEKJu0Mo87EIIIUQZBwcHAgICCA4Opn379rzyyiv88ccfrFixgrlz51rX++STT2jVqhUuLi6EhITwxBNPkJ+fD8CGDRt44IEHyMnJsdbYT548GYAff/yRjh074ubmRkBAAKNHjyY1NfWyYl68eDEtWrTAwcGB8PBwPv74Y5vnZ82aRUREBAEBAQQGBnLnnXdan1u0aBGtWrXCyckJHx8f+vXrR0FBwWXFcy1IsYkQQog6oWyQeEWRPuxCCCGuDkVRUIqKauTYGicnNGU/dpeoT58+tGnTht9++42HHnoIAK1Wy/Tp02nQoAGnT5/miSee4KWXXmLWrFl0796dadOmMWnSJI4dOwaAq6srAEajkbfffpuIiAhSU1OZMGECY8eOZfny5ZcU2549exgxYgSTJ09m5MiRbNu2jSeeeAIfHx/Gjh3L7t27efrpp/n+++9p1aoVRqORrVu3ApCUlMSoUaOYMmUKw4YNIy8vj82bN18XA9FKwi6EEKJukBp2IYQQV5lSVMSx9h1q5NgRe/egcXa+7P1ERkZy4MAB6+OKfcrDw8N55513eOyxx5g1axb29vZ4eHig0WgICAiw2c+DDz5o/X/Dhg2ZPn06nTp1Ij8/35rUX4xPPvmEvn378vrrrwPQtGlToqKimDp1KmPHjiU2NhYXFxduueUWFEXB3d2dDh3U9yIpKQmTycTw4cMJCwsDoFWrVhcdQ02QJvFCCCHqCOnDLoQQQlyIoig2NfVr166lb9++BAcH4+bmxr333ktGRgaFhYXn3c+ePXsYOnQooaGhuLm50atXLwBiY2MvKa4jR47Qo0cPm2U9evTgxIkTmM1m+vfvT1hYGI0bN+bRRx9l3rx51hjbtGlD3759adWqFXfddRezZ88mKyvrkuK41qSGXQghRN2gtbaJr9k4hBBC/GdpnJyI2Lunxo59JRw5coQGDRoAEBMTwy233MLjjz/Ou+++i7e3N1u2bGHcuHEYDAacz1GjX1BQwMCBAxk4cCDz5s3Dz8+P2NhYBg4ciMFguCJxns3NzY29e/fy999/s3TpUiZPnsxbb73Frl278PT0ZM2aNWzbto3Vq1czY8YMXn31VXbs2GF9rbWVJOxCCCHqhtLaAunDLoQQ4mrRaDRXpFl6Tfn77785ePAgzz33HKDWklssFj7++GO0pV3Lfv31V5tt7O3tMZvNNsuOHj1KRkYGH3zwASEhIQDs3r37smJr1qyZtU96ma1bt9K0aVN0Oh0Aer2efv360blzZ2sBw99//83w4cPRaDT06NGDHj16MGnSJMLCwvj999+ZMGHCZcV1tUnCLoQQok7QSB92IYQQwqqkpITk5GSbad3ef/99brnlFu677z4AGjdujNFoZMaMGQwdOpStW7fy5Zdf2uwnPDyc/Px81q1bR5s2bXB2diY0NBR7e3tmzJjBY489xqFDh3j77berFVdaWhr79u2zWRYYGMjzzz9Pp06dePvttxk5ciTbt2/n888/Z9asWQAsXbqU06dP07NnT/R6PZs3b8ZisRAREcGOHTtYt24dAwYMwN/fnx07dpCWlkazZs0u/0ReZdKHXQghRB0hfdiFEEKIMitXriQwMJDw8HAGDRrE+vXrmT59On/88Ye1xrpNmzZ88sknfPjhh7Rs2ZJ58+bx/vvv2+yne/fuPPbYY4wcORI/Pz+mTJmCn58fc+fOZeHChTRv3pwPPviAjz76qFpxzZ8/n3bt2tn8mz17Nu3bt+fXX3/ll19+oWXLlkyaNIm33nqLsWPHAuDp6clvv/1Gv3796Nq1K19//TU///wzLVq0wN3dnU2bNjFkyBCaNm3Ka6+9xscff8zgwYOv6Dm9GjTK9TCW/VWUm5uLh4cHOTk5uLu713Q452Q0Glm+fDlDhgzBzs6upsMRohK5RkVtl7FoEamvvY5zz56EfTO7psMRNWH+fCgsBGdnGD26pqOpknyXitpOrlFbxcXFREdH06BBAxwdHWs6HAFYLBZyc3Nxd3e3NuOvKee7Pqqbh0qTeCGEEHWDdSJ26cNeZ9XSJF0IIYQ4F2kSL4QQok7QaNSfvLrdrkwIIYQQ15M6m7DPnDmT5s2b06lTp5oORQghxLVQVsNukRp2IYQQQlwf6mzCPn78eKKioti1a1dNhyKEEOJakHnYxbFjcPiw+lcIIYS4DkgfdiGEEHWD9GEXfftCQgIEB0N8fE1HI4QQQlxQna1hF0IIUcdoyuZhr9kwhBBCCCGqSxJ2IYQQdYKmtIZdkT7sQgghhLhOSMIuhBCibihtES992IUQQghxvZCEXQghRN2gLWsSLwm7EEIIIa4PkrALIYSoGzQySrwQQghRHXPnzsXT0/Oq7X/Dhg1oNBqys7Ov2jH+KyRhF0IIUTeUDjonfdiFEELUdWPHjkWj0aDRaLC3t6dx48a89dZbmEyma3L87t27k5SUhIeHxxXfd0xMDF5eXuzbt++K77smyLRuQggh6oSyCnYZJl4IIYSAQYMGMWfOHEpKSli+fDnjx4/Hzs6OiRMnXvVj29vbExAQcNWP818gNexCCCHqhrI+7BZJ2IUQQggHBwcCAgIICwvj8ccfp1+/fvz5558266xatYpmzZrh6urKoEGDSEpKAmDTpk3Y2dmRnJxss/6zzz7LDTfcAMCZM2cYOnQoXl5euLi40KJFC5YvXw5U3SR+69at9O7dG2dnZ7y8vBg4cCBZWVkALFq0iFatWuHk5ISPjw/9+vWjoKDgkl53SUkJTz/9NP7+/jg6OtKzZ0927dplfT4rK4sxY8bg5+eHk5MTTZo0Yc6cOQAYDAaefPJJAgMDcXR0JCwsjPfff/+S4qguqWEXQghRN0gfdiGEEFeZoiiYDDXT9Upvr7VOYXopnJycyMjIsD4uLCzko48+4scff0Sr1XLPPffwwgsvMG/ePG688UYaNmzIjz/+yIsvvgiA0Whk3rx5TJkyBYDx48djMBjYtGkTLi4uREVF4erqWuWx9+3bR9++fXnwwQf57LPP0Ov1rF+/HrPZTFJSEqNGjWLKlCkMGzaMvLw8Nm/ejHKJv+cvvfQSixcv5vvvvycsLIwpU6YwcOBATp48ibe3N6+//jpRUVGsWLECX19fTp48SVFREQDTp0/nzz//5NdffyU0NJS4uDji4uIuKY7qkoRdCCFE3VB2EyN92OuuXbvAbAadrqYjEUL8R5kMFr5+ZmONHPuRz3ph53Dx32+KorBu3TpWrVrFU089ZV1uNBr58ssvadSoEQBPPvkkb731lvX5cePGMWfOHGvC/tdff1FcXMyIESMAiI2N5Y477qBVq1YANGzY8JwxTJkyhY4dOzJr1izrshYtWgCwd+9eTCYTw4cPJywsDMC6z4tVUFDAF198wdy5cxk8eDAAs2fPZs2aNXz77be8+OKLxMbG0q5dOzp27AhAeHi4dfvY2FiaNGlCz5490Wg01niuJmkSL4QQom6w1jpIDXudFRgI9eurf4UQoo5bunQprq6uODo6MnjwYEaOHMnkyZOtzzs7O1uTdYDAwEBSU1Otj8eOHcvJkyf5559/AHVk+REjRuDi4gLA008/zTvvvEOPHj144403OHDgwDljKathr0qbNm3o27cvrVq14q677mL27NnWpvIX69SpUxiNRnr06GFdZmdnR+fOnTly5AgAjz/+OL/88gtt27blpZdeYtu2bTaved++fURERPD000+zevXqS4rjYkgNuxBCiDpBoy0bJV4SdiGEEFeH3l7LI5/1qrFjX4ybbrqJL774Ant7e4KCgtDrbVNDOzs7m8cajcamGbq/vz9Dhw5lzpw5NGjQgBUrVrBhwwbr8w899BADBw5k2bJlrF69mvfff5+PP/7Ypha/jJOT0znj1Ol0rFmzhm3btrF69WpmzJjBq6++yo4dO2jQoMFFvebqGDx4MGfOnGH58uWsWbOGvn37Mn78eD766CPat29PdHQ0K1asYO3atYwYMYJ+/fqxaNGiKx5HGalhF0IIUUdIH3YhhBBXl0ajwc5BVyP/Lrb/uouLC40bNyY0NLRSsl5dDz30EAsWLODrr7+mUaNGNjXXACEhITz22GP89ttvPP/888yePbvK/bRu3Zp169ad8zgajYYePXrw5ptv8u+//2Jvb8/vv/9+0fE2atQIe3t7tm7dal1mNBrZtWsXzZs3ty7z8/Pj/vvv56effmLatGl8/fXX1ufc3d0ZOXIks2fPZsGCBSxevJjMzMyLjqW6pIZdCCFE3aCVPux13tdfQ34+uLrCI4/UdDRCCHHdGzhwIO7u7rzzzjs2/dtBHTF+8ODBNG3alKysLNavX0+zZs2q3M/EiRNp1aoVTzzxBI899hj29vasX7+eu+66i1OnTrFu3ToGDBiAv78/O3bsIC0t7Zz7KnPs2DG0Wtv66RYtWvD444/z4osv4u3tTWhoKFOmTKGwsJBx48YBMGnSJDp06ECLFi0oKSlh6dKl1mN98sknBAYG0q5dO7RaLQsXLiQgIABPT89LPIMXJgm7EEKIukH6sIu33oKEBAgOloRdCCGuAK1Wy9ixY3nvvfe47777bJ4zm82MHz+e+Ph43N3dGTRoEJ9++mmV+2natCmrV6/mlVdeoXPnzjg5OdGlSxdGjRqFu7s7mzZtYtq0aeTm5hIWFsbHH39sHTTuXEaPHl1pWVxcHB988AEWi4V7772XvLw8OnbsyKpVq/Dy8gLUOeInTpxITEwMTk5O3HDDDfzyyy8AuLm5MWXKFE6cOIFOp6NTp04sX768UsHAlaRRLnU8/P+I3NxcPDw8yMnJwd3dvabDOSej0cjy5csZMmRIpf4kQtQGco2K2i5n2zYSHxyHXcOGNF6+rKbDETWhfv3yhD0+vqajqZJ8l4raTq5RW8XFxURHR9OgQQMcHR1rOpwaMW7cONLS0irN4V5TLBYLubm5uLu7X9VEujrOd31UNw+VGnYhhBB1gkb6sAshhBBXTE5ODgcPHmT+/Pm1Jln/L5KEXQghRN1Q1oddkT7sQgghxOW67bbb2LlzJ4899hj9+/ev6XD+syRhF0IIUTeU9WGXCnYhhBDislWcwk1cPTKtmxBCiLqhrB+bjBIvhBBCiOvEdZ+wx8XF0bt3b5o3b07r1q1ZuHBhTYckhBCiVlJr2Ov4WKtCCCGuAvltEVW5EtfFdd8kXq/XM23aNNq2bUtycjIdOnRgyJAhuLi41HRoQgghahGNVgadE0IIcWWVjZRfWFiIk5NTDUcjapvCwkKAy5pR4bpP2AMDAwkMDAQgICAAX19fMjMzJWEXQghhSyMJuxBCiCtLp9Ph6elJamoqAM7OzmjKfm9EjbBYLBgMBoqLi2tsWjdFUSgsLCQ1NRVPT090Ot0l76vGE/ZNmzYxdepU9uzZQ1JSEr///ju33367zTozZ85k6tSpJCcn06ZNG2bMmEHnzp0r7WvPnj2YzWZCQkKuUfRCCCGuG2U/2pKw111Nm4KHB9SrV9ORCCH+QwICAgCsSbuoWYqiUFRUhJOTU40Xnnh6elqvj0tV4wl7QUEBbdq04cEHH2T48OGVnl+wYAETJkzgyy+/pEuXLkybNo2BAwdy7Ngx/P39retlZmZy3333MXv27PMer6SkhJKSEuvj3NxcAIxGI0aj8Qq9qiuvLLbaHKOo2+QaFbWdyWQGQLFY5Dqtq1atKv9/Lb0G5LtU1HZyjVbN19cXLy8vTCaT9GevYSaTiW3bttG9e3f0+ppJdzUaDXq9Hp1Oh8lkqnKd6n6GNEotuqI0Gk2lGvYuXbrQqVMnPv/8c0Bt4hASEsJTTz3F//73P0BNwvv378/DDz/Mvffee95jTJ48mTfffLPS8vnz5+Ps7HzlXowQQohaxSEhgbDpMzC6uxP96is1HY4QQggh6rDCwkJGjx5NTk4O7u7u51yvVifsBoMBZ2dnFi1aZJPE33///WRnZ/PHH3+gKAqjR48mIiKCyZMnX/AYVdWwh4SEkJ6eft4TVdOMRiNr1qyhf//+lzVogRBXi1yjorYrOHSIpFGj0fn60mD93zUdjhBVku9SUdvJNSpqu+vlGs3NzcXX1/eCCXuNN4k/n/T0dMxmM/XO6mtWr149jh49CsDWrVtZsGABrVu3ZsmSJQD8+OOPtGrVqsp9Ojg44ODgUGm5nZ1drX5Dy1wvcYq6S65RUVvp7eyt/5drVNR28l0qaju5RkVtV9uv0erGVqsT9uro2bMnFoulpsMQQghR25WNOyO/GXXXmDGQng6+vjBvXk1HI4QQQlxQrU7YfX190el0pKSk2CxPSUm57NH2Zs6cycyZMzGbzZe1HyGEENcJ69QutaYnmLjWNm6EhAQIDq7pSIQQQohqqZmJ6arJ3t6eDh06sG7dOusyi8XCunXr6Nat22Xte/z48URFRbFr167LDVMIIcR1oGxqF8UiCbsQQgghrg81XsOen5/PyZMnrY+jo6PZt28f3t7ehIaGMmHCBO6//346duxI586dmTZtGgUFBTzwwAM1GLUQQojrjkbmYRfioikK1PA8xkIIUZfVeMK+e/dubrrpJuvjCRMmAOpI8HPnzmXkyJGkpaUxadIkkpOTadu2LStXrqw0EJ0QQghxXtKHXYiLU1ICBQXg7V3TkQghRJ1V4wl77969udDMck8++SRPPvnkNYpICCHEf5K2VvcCE6J2SU6GBx4ANzf49deajua6UnLqFJa8PJzatq3pUC6LpbCQ+GefxTGyGf4TnqvpcEQ1mPMLsOTmYBcUVNOhVMkQH0/JiRM4tW2L3surpsO5btTZu5eZM2fSvHlzOnXqVNOhCCGEuBasfdilhl1cHxSL5YKVGtVhTE3FnJ8PQOHef0n/ejaW4uLzb+TrCzfcAFFRsGEDitlM7sqVmHNzLzue6505v4Ccv5ZiKSqq9JxisXD65luIuXsUhpgYm20M8fHXMMrSeIxGsn5ZgDE5GVNWlk1MF5Lzxx8UbNpMxrffYiksLN+n2Yzn5i0U/fvvFYkxd/VqEl56yXqNAhe+Pqshe8kS4h57HHNe3mXv62zm3FyMqalVPqcYDKR+9BGF5xgnK2/9emIffgRTWlqVz5uysqr1OSs+epT8LVttliU89xynBg6i+MiRC25/rVkKCogeNpz4x58g7uFHajqc60qdTdhl0DkhhKhbNNKHXVwnHBITSfvgA6JvH8apgYOqTAwBFEXBcOYMiqKQvWQJKR9OqXSjbkxK4tSgwcQ98iiKxULCC8+T9sknxD38iLXwKvv3JZy5fyym9HQUi4W06TOIHjGSbI0GpXFjmDKF7IWLSHj2OeIeexzFYiFp8mSSXp+kJieffcaZe+6lcM8e63ELtm3j1OAhFO3ff87YFZPpos6LpaTEWoChGAxYCgqqtV3iy//j1KDBmDIyqrW+OTubU0NuJuGFF9XjFhRQcjqauMceJ2b0GM6MGkXiiy+S8OxzlQpUjAkJ1v9n/bqQmDH3kPnTPKKHDePU4CEY4uMp2r+fk/36c7znDcQ/+xzG0tmQFKOR/K1breelcNcuSqKjy19/YSGnh95KzOgxpM2aZX0u84cfSJs+A0tJSaXXkjbjc5InTybh+ReIH/8kp4bcTNLkyeStX0/24t841qkzif+biGI0Ys7ORjEa1VgUhaz5P5eeEDP5W7eSu3IlislE7sKF+C9dSsLYByiJjr6sQiVjSgoJTz9D7p9/kT5zFoAaV7v2JL3+epXXSMnJk+QsW4YpLY28DRs4fsMN5FUYoBrUayXpfxPJ37CB7IWLLjk+c14eqZ99RvoXXwBgSk8nc/58Tt9+O6cHD7FJui0FBRiTksj8aR4Z33zLmXvvq/LcxD/+BAWbN5P8zrskvPAi6V9+iaIomDIzMWVlcfqWoUTfcSd5f6/n1MBBFGzfXmkfitFI9LDhxD30EEUHD1Fy+jRx45+kYPNmFKOR1E8+tVm/6PBh8jZswJiaypkHHiB35Srb81VQQO7KlRTs2Ena9Onkb9xI0YEDmNLTsRgMlJyO5lwUi4W8v9dTfOQIitlM0aHDFB87ZrNOSXQ0ye++h6W08KT40CGMSUnnPfeKyUTK1KnkLF0GQM7SZSS9+SaWwkKyfv2V4uPHMefmYohPOO9+/gtqvEm8EEIIcU2U9WFXpIZd1F7mvDzCPptOToVlRfv349K1K6kff0zOkj8Imf01jpGR5CxeTNJrr+M+ZAi5K1eCxULmnDk4RESAohD47rsUHdiPUlhI0d69RA+/A1OiepNcuGsXhbt3o/P0JGnSJDAayfnjT7TubqTPUhOnpKgo7Pv2xXnDepTShKVo717SZ84i+5cFarw5OeStXg1A7AMP4nXPPTi1bEHChOcBiBl5N/W/mEXR/v04RjbDbUB/NFotsffdT+GuXWgcHPAYPoyASZMwpaSg9/FBY2enxrh3L1k/zcP/5Zex5OUSc/co3Pr2IejDD4l77DGKDh2m4e+/YVc6TV/+xo0UbP8H95tvJuPbbyk+eBDnLl3I+eMPANK/+BJzVhbmrCxMmZkEffA+DhERZM2fj1Pr1qAoZC1YQMHmLZhSUzGcPo3bgP7k/L6E/PXrK71X+Rs3kjz5Teq9+gqm1FRyly5F4+BofT7zu+/Uc1ahIKNwxw6KDh7EWFrbnrdyJcWHDtFw6V+kfTadzDlz0Lq7o5hMKIWF6Dw9abRyBTpPT/LWrqXkxIny92H6DBos+Z2U994H1BrXkFkzrcdSLBYyvv66UgzZvyywvn8AOUuWoBhKyFu7DucuXQid/TVFe/ZYjwWQ8NTTAHjffx+5q9eoCy0WTg8eQsDkyXjdPbLS+alK0cGDFB89iufw4eT89RdJ/5tofa5g2zY1vsWLQVHIXrgI544dcerQgfQvvkCj1eF17z3E3nsf5pwcdH6+OIQ3wJyWTtLkybh07YopLQ2tmxsF2/+x7rcsMTTn5lJ04CAuXbug0aspUNH+/SRNegO/Z57BuXMnLIWF2Pn7q+fPaCT2/rEUR0UB4D5kCGkzZ5L751/l7/G8eeSvW4f3/feTvWgxRfv22bzerB9/whAbi/9zz6J1ccGcnW19Lm9VedJsSk0la/7POLVpgzkjA3NGBvFPPAFAypSpNPz9N/WUFxWR88cfFB89ai18zl36F7mr12CqkAAX/fsvxoQEshYuxOPW24gZeTeYTDh16EDRnj0Ubv8H+98Wkzl/PsWlCT+lhTUV2TdogFPbtuT8/jv+L76gFtAYDHjdPQpjUiLu/fsTP2EC+WvVAhONnZ210MfvuefI/m0xHjffQuZPP2E5q9VA/pYteN11F4qikPbJp+Rv2YLn8OF433uPen7WriPzW/UzZExIIO1TtRCi8J8dGCoUZKHREDR1Kg5NGpM2fQaed96BY8+elV7L9UyjXIm2Vtex3NxcPDw8yMnJwd3dvabDOSej0cjy5csZMmQIdqU/ZELUJnKNitqu8MwZzgwchMbRkch9V6Ypp7jO1K9fPg97DTRPro7Ub74l46OPbJb5/+9l7OrVI+E5dWBep3btCJs/j5g776L48GHrehpnZ5QKTZf1AQG43tSb7J9/qfJYHsOGUXLsmHUfboMGYcnPp2jTJixaLWg02AGhoSGYt24lNigYi0532a/Rd/x40mfOtFnm2LIlxYcOoXVzw6FxY5y7diHjiy+r3D588SJi7rgTAJ+HH8ZvwnMYExI41a//RcXhEBmJ7+OPk/DMMwDo/fzO2Uz5fLTOzmqCbTBccF3PESMoPnSI4qgovMaMIXfFCsyZmYR+9y2xD46rOs6mTQn6aCop779PYYVEtCp+zzyNYjJTuHs39qGhZC9ceNGvJ3zhr2TOmUvu8uVo3dystaLn4tC0Kf4vvYRLj+5gNJK/bRsuXbqgdXKi6NBhtA72WIqLyfj6a/LWrFU30unAbLbdkUZDw+XLOH3LUOtzGmdntPb2NonuOeOIiKDk5El1W63WOsCo1s0N/+efJ/XTT7Hk5OD79FN4jx5Nzp9/kfLee9bttW5uYLEQ9tOP5G/chCE2lpzffqv+iTsPz1F349yhI4YzMaTP+PyittXY21N/xnRyV64i5/ffKz2v9fDAkpNTxZZXn2Ob1hTvP3BJ27oNGkT9aZ9SfOQI0cOGA+pnyfuhceSv34DGzo6ivXsvad+NDx64Lu5Jq5uHSsIuCbsQV4Rco6K2K4yN5cyAgWjs7Yk8UHUzXfEf9+abkJMDHh7wxhs1HU2Vkj/6iKxvvrVZ5tqnD0UHD2BOS7cuC/70ExJffc0mQfcdP56MOXNsljk0b0ZJVIVm8no9/g89ROqXFZLh0uTJubAQv/R0FA04tmjBmbQ0ShwccSkowD8tlQI/P3I6dKTk6FGb+DQODniNHk3mnDlX6CycX1ktofX4Tk4oZ3UbcIiIwJKXhzExEZ2vL+b08nNX6ZxcBL9nniZz/nx8H3sMu4AAEp6bcMFEvf6smZizc0h65RWb5Y3/XkfKlKnkrVx5SbFcKo2jI8p5+og7d+xI0cGDKCUl1P98BvFPPlVpnaKQENyMRkzJydZl9V6ZSOGeveStWoVz587Ue2Ui0XfeBRfR9cH1ppuqbM1wNruwUIxnYtUHVSX/gNbdvVKt7pWi8/DAXENJ8oVoHBxQqugeURWtuztB76uFFvHjSwf4trOjwcJfSf/iS5tWAOfj1r8fAZMnU3LqFI7NW5Dw7LMUbNlS5bqed48k+5cF6P398XnkEVLeeecC++6PpSAfY1KyTc26feNGuPXvT+Z3cyq93sDPZ7AxP58hN99cq+9Jq5uH1tk+7EIIIeoWTdmgc3W7nLpue+MN+OSTWpusAyhmtVZQ5+eHztcXgPy//8aclo59WBje4x4EUBPFwkK0bm649u2L1t0dj2G3o3V0tNlfWWKq8/Eh4I1JNBt6C97/7sW9Sxd1uacnwR9NxTMnh+DEBAqdnMjv2AldZCQhRUW4FRdj6dmTkiZN8NRo8GrfrlLM7oMH49Kt62W9bocmTfB5/LFqrVsxWQcqJetotQS++y4NVyyn/qxZNFq5kgZ//IHezw/X3r1psHgxvqXNjc/mOXIkIV9/RcjXX1H/8xk2z3mPexCfRx+l6ebNeI8Zg1vfvng/+ID1+eDpn6ndEbRaQmbPxuXGG6j32mu49emDS5fOlY6lDwzEqVWrKuPwuO1WmmzdQqM1q3GIjATURExT4f1tvP5v/F9+Gc8RI3Dt27fSPjROTngMG0ajtWusy7zGjKHx3+sqrQvg+9STaOzsKNy9G6WkBLuQEFz79qXe669Z4y1T0Lw5wXO+s9k+5b33rQle4c6dRN8+rOpkXavF79lncWrbFsfWrdF6eOBcej2WJeset91mXd2+USMiDx5A4+BgXRb+yy+4lk4L7fPAWFz79bUOLApgHx5Oo1W2BSE6D49K58dtwIAqz4XNeTlrpiqHJo1ptGqlTTxlGm/cgNfoUfg8/PB59+nQtCk6Ly/r51nr4mJ9TuvmBno9Ds2a4TZwoM12nnfdSf0vZtFozWqCP/2EkG++qTQDilu/frj0uvG8x/e+/340Dg7Un/Ypbn374tqnj/U5jUaDY2QkQe+9q54fnQ6fxx497/5ce/VC7+ODS+fO6FxdCP74I1zOapaudXZG4+yMz4MPgl6PKTX1gsm6fYMGBE//jNDvvqPRiuU02bKZkNlf03jjBhotXYr/M88QMvtrPO+6Sy24KZX05FP4XuOCsKupzvZhnzlzJjNnzsRcRYmcEEKI/6CymzlJ2EVtZlHvS9xuuRm/Rx7h5E19UIqL1ZqwKR/i0LgxOX/8aa0xdu3Vi6CpU8BiQaPT4TniLjK+/Mpmlw7NmtFg8SI0Wi1kZqL5+WeCp06l3kdT0bq7o3VwwM7dnQyTiaL+/an/2TSYMQO71FTqf/EFPPoobN8Or72G2969lNWp+jzyCG4DB+AYGWk7qrWdnU1/WM+7R6J1dCJz7lybuPyefQbv+++n5NRpHJo0BrOZnN+XYEpORu/nh8fw4bh0707hrl34PPwQOb8vIXnyZPUQISHUnzGdpNdex75hA/yffx5zRgYaRycs+Xk4tWyhnsc+alKni2hK440b1HMA+D39FFp3N1I/+BCA+p/PwJiYiMfwO9C5unA2586dqffii5WWe99zDzl//oljZDPcBwzArU8fzDk56H18cL2hPGHRBwXheded5Cz5A8VoxHPU3Wpi1Kpl+b4efBD3wYPJW7MG38cfQ+vkBD4+hP34A4U7duDUoQO5f/5JyvsfqPsMCMDngbEApH7yKfmlA6+59u1LvYkTsQsOshZUBk2dQtaCBfg88gh6b2+Cpk4h8cWX8Ht+AnaBQRjj4/F55GG0Li7Wc+LWty8ajQbvMWNwHzgQxWzhZK9eABSFhaI/x9RhdqGhGGNjrY8dW7TAqX178taswZScTODbb+F5xx34VkgCC3ft4syOHdbHPo89Sv6WLZgzMvB7cjwaOzuCPniftOkzCHznbfReXoR8MQtTZiY6Dw80Op06QJ1OR8HWbThGRqD38sJ77FiKjx0l4NVXsQsJ4VibtmqMwcFq0q3XkzF3LqlTphL86acUHzyAKSPTpum5U7u21v+79LoR/2eeQefpSfgvP1N8+DBaZ2eyfl2I31NPYlevHgGTJgGQMXt26ZuvV2vkSwc9dL91KMFTpqjdKCwWUMD1xhuJHau+lx633473mNHovL0pPnTIWgjiff/91Jv4P2ss9iEhADRcuhStsxNF+/aT+cMP+L/4Ami1JL3yqrWW2y4oiJBvvuHMvffi0qM79Sb+D///vWy9PjQVCjvsSverdXGh/vTPUCwWNFotjpGRWIqKyfrpJ2s3Grv69TFnZ+Pau7fNNaDz8CD0m9mUnDxJ4iuv4vPwQ7h06oSluBi7gAAcIyJsuvMABL77LlpXV4wJCaROmaKei9tus4lN7+uL6w032Gzn0rkzLp074/fM05waNBhL6WwDeW3a8J+h1HE5OTkKoOTk5NR0KOdlMBiUJUuWKAaDoaZDEaJKco2K2q4wPl6JiohUopq3qOlQhDinhLffVqIiIpXEKVMVRVGUkjNnlOylSxVDUpJ1nYx589RrOSJSKYmJsdneYjAomT//ouT+/bd1nazFvymK0Vi+Up8+inL77Ypy9Kj6ODZWUcLDFUt+vqIsXKgogYGK0q6doqxaVWHHFkX57DPF4u+vxAYFK1ERkUr+tm02x05+7z0l7smnFEtJiVJ8/LiSt2WLkvj6JMWUl6+YsrKUuCefUnJWr1bOPPSwEtWylVJ86lSl12/KyVFM57gns5SUKAkTX1GOtGmr5KxYeTGntUrGzEzlWM+eyqmhtyoWi6XKdXJWrlJO3nyzUnzixGUfr4wpL896PHNRkXKiX3/l9PA7FEtJyQW3NRcUKLFPjFcy58+3WW5MT1dixtyjZP76a7XjKImLVywmk80yi8WipM2apZwccrNSEh1daZu0L75U4idOVJYsXqwYDAbrNXZy4CDl5ICBSuxjjyuWkhLl1K23KVERkUrmz7+Ux5iWpuQsW6ZYzOZK+7UYDErMmHuUqIhIJXb8eEVRFKXo6LEr8j5XlPnLAuXk4CFK8fHjNsvNxcU2jzO+/16JiohUMn78SbEYDErc088oKdOmXdSxcpYtU4736q0U7t+vmAsLFXNJiVJy5oxiOcd9UvZfS5XYJ8Yrpuxs6zKLxaKcHjZciWreQik6cuSijq8oilJ88qSS9PY7iiE5Wd3fWe93RXkbNignb775gscpPn5cOT1suJK7fr1iKSlRTLm5Fx1X0rvvKlERkcqJPn2t11DR0WPW5+MnPK+cHHKzYsrKurj9vv2Out/BQ66Le9Lq5qHSh136sAtxRcg1Kmq7oqQkYm7qAxoNzY5E1XQ4QlQp8c23yPn5Z7weeYSACc9VuY5iMpH+xZc4NGmC+6CBVa4DkPXLAkpOnKDeiy+UN6UuLIS//oKXX4YXXoCHHwa9Htq1Uwfic3WFF19Ua9Xt7SEzE/btgz594MgR+PZbCps1o0hR8B43zqb2q7oUgwFzQQF6L6+L3hbUecA1V2DwO1BH5dfo9Wptdg1RLBa1hYT++mj4WvH3Pn/RIjK+m0PI11/h0KCBdR1TZiaG6GicO3S4qH2b8wvQOjtZW0LUFEVRMMTEYB8efknX+JVkysrCnJmJQ6NGNRrHlWRKSyNz/ny87rqLzB9+xJybS+A7b1/2+27KyiJ74SJcb7uVVf/8U+vvSaubh14f3wxCCCHEZZMm8XXedTBKvGIu7fOrO/eNq0avx++pJ8/5fBmbqbYKC2HcONi4Efr2VV//999Dz57Qti0MHw5ffAF//qk+LrN4MWzaBK1bQ7Nm8NFHOAPOl/TqSuO3t0dvb3/p21+hZB1A5+Z2xfZ1qTRabaV+yNcLr1Gj8Bo1qtJyvbc3em/vi95fVd0RaoJGo7EpgKhJei+vSy7cqq30fn74l87OUO9/L1+5/Xp54fvIwxirmKLuenZ9fjsIIYQQF0mjLa8lqeONy0RtVjronEZ75ZJSAD7/HPbsgc2b4e234fff4eBB+OUXtb/5LbdAWBiMH68m6bt3q7Xsr78OHTqAj8+VjUcIIUS1SA27EEKIuqFis0ZFsX0sRC2hlM4djf4SEvaybSvW1prN6r/Nm9Xa9EaN1Os/PBxeegnmzoWBA+Gmm+DHH9Um8qWDZuHtDWvXQsuWZx9JCCHENVJnE3YZJV4IIeqYikmMxXLdNkEV/3GlTeIvui9nxWs6IUGdbz4yUp3qSKeDrCwoG9W7pAQcHdV56b/4AubMgcaNISICVq1Sp+JKSoKmTa/gCxNCCHEp6uzdyvjx44mKimLXrl01HYoQQohr4qwadiFqobJ52NGdo06lrBbdukHptazVgsEADz4I7dvD7bfD0KHw66/q8yNGwE8/QV6emqybTGork0aN1H7ty5er6zk5gZubJOtCCFFL1NmEXQghRB0jfdjF9aC05Z/mXIPOldWil87NbO3aYTTCk0/CoUPwxx8wdara7/zRR+HYMRg9Gho0gPvuU5N1vR5OnFCbxnt4qMvkcyGEELVOnW0SL4QQom7RnN2HXYhaSCnrqneuQeeKiqB/f9i2DRYtUkd3VxS1Cftvv8HXX0PXruq6nTpBWpqatG/YAF9+qda6d+4MrVqpterPPAOzZ6u16kIIIWodqWEXQghRN5zdh12I2qj02tTodVUXLBmN4OmpDgj37LPqMo1G3c7LS+2vXqZePXj6aXU0+G3b1EHnVqxQa9vz8+Gjj+C11yRZF0KIWkwSdiGEEHWP1LCLWso6D7tWazuTQVnNu7MzZGfD/fer/c1ffFFdbjCAuzvs26f2Uwc1ea9XT23yXlysLuvaFV54QZ267f77r8VLEkIIcRkkYRdCCFE3VKhhVyySsItaqmwedp1eHb29Rw84ebK8kEmvV2vKjx+Hl1+GTz8tH9G9Z09YvRr+/rt8f9nZarIfEnLtX4sQQojLVmf7sMu0bkIIUcfYzLsuCXud9NNP6pRmDg41Hcm5WdT7Em16Grz7rpqMT56sjvw+YYKauIeFqX3TO3eGNm3gkUfgr7/g1VfhgQfU9TZsUNf7+GMYMACCg2v0ZQkhhLg0dbaGXaZ1E0KIukUjfdhF794wcKD6t5ZSTGrCrvj6qf3LtVp1gLjPP1dHfrdY1Nry7duhZUuYOBGWLVMTdH9/mDULHn9cHRn+hx/UWvhvv1Wb0gshhLju1NmEXQghRB0mfdhFbZGSov4ta/FXVpjk5Ai33aYm62fOwNy5amJ+773qMo1G7a9+883qes88o24XFqb2Uf/tN9izR53qTQghxHVLEnYhhBB1g00fdqlhFzUsM1MdrX3CBPVx6ejuZYPOabRaCAqCV15Rp2Pz8YFvvoGcHOjVS53erbCwfOC5gwdhxozy/Ts6ntUNRAghxPVIEnYhhBB1gyQvYsMGWLVK/VvTvL0hNFStPV+6VF1msVgHnUNXOszQoEEwZAiMGweNG6uju/fpA/b26vRuoPZjnzpV/SuEEOI/pc4OOieEEKKOqZiwSw173XTPPZCQoA7AFh9fc3GUDXz3xBMQFQXff6/2q3d1La9h15XWqbi5qX3Ze/eG776DBx+ETz5Rl5fNue7iAs8/XyMvRQghxNUlNexCCCHqBE3FhF36sIuaVDZKfVIStGgB0dGwYIG6rHTQObS68vU7dFAHknvjDTAa1Zp1nU6uYyGEqAMkYRdCCFFnKKVJu/RhFzVq+3YIDISXXoK9e+HAAfjxR4iNRVEUUBQ0+goJu52dOnhcVlZ5n3eQbh5CCFEHSJN4IYQQdYdGo9ZKSsWkqCkWC7z/vjq93OzZYDCof2fOVP+azep1qj2rTqVhQ/jiC3XwOSGEEHVGnU3YZ86cycyZMzGXTaMihBDiv6+sRlKRGnZxlZlMoK/iNistDU6cgMceU2vO7ezgqafg+HFYswZ7jRYDoNGclbBrNOqUbkIIIeqUOtskfvz48URFRbFr166aDkUIIcQ1Yq1Yl76/4morS9Z//x3WrFEHlwO1hj0/H7y81Mdms9of/e674cgR3BIS1OV2erlOhRBC1N0adiGEEHWQtYZdEiFxlf3zD9x/v5qQ+/ioteqff67Ovd6rl9oE/tZby6dma9oUXFxwSUvFw9NLHXRO+qgLIUSdV2dr2IUQQtRB1kHnJGEXV1BZ97qygqDiYpg8We2nfvIk7NgB992njvS+dSu89x7s2weffaaOEA+wciV06UKBjw8GOzvbQeeEEELUWVLDLoQQou6w1lhKwi6uoLL50PPywN0d/v4bzpxRk3CjESZNUudQv/NOaNQIAgJg2jS1xn3OHHXZjh0wfz6pGZmYU1MrDzonhBCiTpJfAyGEEHWPTOsmriSjEYYOVWvQQU3aFUVN0hs2VBP4P/5QHwcEqAPSPf44LFoEzz0HnTqpfdxvvdVaW6/RSZ2KEEIIqWEXQghRhyjSh71ui4+//H2UDRJXkaKo86rb2anP29uDk5M6+vtXX8HIkepzoNaoa7Vq//ZmzdR/FXdVVpikkzoVIYQQUsMuhBCiLrH2YZcadnGJdDrIzoay0dxBTdD9/GD7dvX5yEjo0gUaN4auXcuT9R07YO5cdQA6k6nq/Zcu12ilD7sQQghJ2IUQQtQl1hr2mg1DXEeqKtwZNAhGjIDFi8uX3Xqr2m/9xAm1Sfx990FYGHTsqPZdHzFCHR2+Qwd4882q52inQmGSDDonhBACaRIvhBCiLrEm7FLDLi6gLHEuG/zNYFBr0gF++EEdMO6xx8DFRU3gHR3V/uqnTkGTJtC9uzoH+/TpkJGh1sofOKBO33be45b2YZdB54QQQiAJuxBCiDpE+rDXcW++CTk54OEBb7xRvlxRbOc8V5TyRP3AAZgyRX2+e3e46y416X77bXBwgCeegPHjYcIESEqCtDR1u7IE/7nnLipExVQ6Rdw5auCFEELULVJ8K4QQos6RPux11OzZ8Omn6l8or0XXaNTB4ozG8scmE7zyCvTsqQ4g5+AAP/0EDz2kruPhAVOnqrXsM2eqU7d16ABr1qjPl9XGX6zSmKSGXQghBNThhH3mzJk0b96cTp061XQoQgghrhXpwy4qKkuKv/pKTbzXrStP4rdtU/ujr1ihJvjffAPt28Off8Jff5XvY8IEdfuvv1aX5+RAbu4lhaMoinVaN2TQOSGEENThhH38+PFERUWxa9eumg5FCCHEtSJ92EVFa9ZAgwZqf3R3d7WG3WBQn2vbVh04rkcPWLsWWrZUE/obb4QnnyxPrDUa6N9fHf39ySfhww/VfV2KCi0/NDLonBBCCKQPuxBCiLpE+rCLMhs3qrXjjz4Kzz6rXhsODuXPu7vD0KHqVG1PPaXOpf7SS7B3r5q0f/45PPOMmmTrdDB4sPrvcpQVAkB57b8QQog6TX4NhBBC1Bllabr0YRcsXw6BgWpTeEdH22S9om+/hUaN4NVXwdlZHVhOq1UHk0tKKp9j/QpQKiTsGhl0TgghBFLDLoQQoi6RPuyizIED4O0Nnp7q45Ur4ehRiI1VE/mbb4bmzdXnYmLUxDwnBzZtghkz1H7qbm5XNCTFXKEgSWrYhRBCIAm7EEKIukT6sIsyzz2nzp+ekqIm5A4OEBqqPi4oUGvWjx6FRx6BhQuhTRuIj4eICHjhBQgLu/IxWSrUsOukD7sQQghJ2IUQQtQl0oddlBkwAP74A7ZsgYED1X7p9eqpzd83bYK774ZfflH/bt6s/vP1VfuyXyUVm8QjCbsQQggkYRdCCFGHKGUJu/RhF6AOKjd0aOXlFos6WnzZaO+tW6v/rraKfdilSbwQQggkYRdCCFEHKVLDXjf16gXp6WpN+bkUFsKyZeqc6x07XrvYKK9hVyRZF0IIUUoSdiGEEHWHNImv2+bNq3p5dDRs2wb5+fDRR2p/9u++A3//axufJOxCCCHOIgm7EEKIukMSdlGVQ4fUkd91OnVu9SefrJEwrNMNll2nQggh6jxJ2IUQQtQd0oddVGXoUGjcGJo0gZqc/9xkAqSGXQghRDlJ2IUQQtQZZfXq0oddVNKsWU1HUF7DLgm7EEKIUvKLIIQQou6w1rBLwl4n9ekDLVqof2sj6cMuhBDiLFLDLoQQou6w9g2WhL1OOn4cEhJQsrOpjb3ErfOwS8IuhBCilPwiCCGEqDukD3udVtYVwpSeXsORVM06rZsMOieEEKKUJOxCCCHqDusg8VLDXhcpRqP6H4ulvL94bVJWw66T2zMhhBCqOvuLMHPmTJo3b06nTp1qOhQhhBDXiCJ92EUpS15eTYdQSXkNe529PRNCCHGWOvuLMH78eKKioti1a1dNhyKEEOKakT7sdVqFlhXmWpiwI6PECyGEOIv8IgghhKg7pA973VYxYc/JqcFAqqaYZJR4IYQQtuQXQQghRN1RmrCf3YfdEJ9A/tatNRGRVdrMmUTfcSfm3NxKz5nz87EUFJxzW0thofq3uPi861VH3t9/k/PHHzbLDPEJFB06fFn7rY5L6VduHVm9Oirs31LhPBf88w8FO3de9LGrw5yfj6W4uHorW8pGiZdB54QQQqgkYRdCCFFnVOzDbs4vwBATg2KxcPqWW4gb9xC5a9aQteBXsn79FXNeHoqikPT6JFI++BAAY0ICWb8swJSWRvSIkSQ8/8JlDWBnMRjI/OFHDDExpM/4nOLDh8n58y/bdYqLOTVoMKdvH1ZlQpu/eQvHOnYiYcIEzowew4k+fTEmJpL66TSSJk/GEJ9Q/fNjMhH/xHgSX/4fhjNn1GUWC6eHDCHmzjvJXbGCM/feR9rMmeUDuJ21PagFIlm//krh3r0AFO7eTfrXs8+ZkBfu3Uva9Bkc69CRnL/+wlJQgDk7mzNjHyBz3rxzxpv81luc6NETY0qqdVnuqtXEPf4Epqwsio8fx5SVhaIo1n9lzLlqk3hzXh6xYx8g9r77bRLrnGXLyFq4sFrnzZiYSMmpU5WWm9LSONmrN/FPPFHpOXNODhnffIMhLg5Qz13Ciy+p/5cadiGEEKVkHnYhhBB1jiklmZO9emEpKMDrnntQShO1hKeetq5TcvQo3mPHkl2atPk8+ghn7r0PY2IiyZMnA1B84ABugwbi3r+/zf5z16wh4amnCf70ExSTGUtBPo7Nm2MXEoLey8u6XsY335A+fQYp75VvqxQX2eyrOCoKc3o6ZtTpyDRaLbmrVqH388N9wADyN24Ei4Xc5Sus2yS/+x7569apDzQaAt9447znw5ydTeaPP+F64w3lxz12DPuwMIoPH0YxGNTz89wEAAp37cKYlIT7gAG43ngjANmLF5P85lsETfkQjaMjyZPUY0YeieLMPfcCYBdQD/chQ8hdsQJjfDy5a9bg0q0bmd9+Zz1u4osvoXV2xrVPHwr/+YfCf/7Ba9QoNKVJrKWggLTp03EfPJis+T8DkPXjD/i/8IIa4zPPAHCiW3cAHCIj8Rg6lNRPP6VJ6esAMOeqTeKNCeUFGqaUFOzDwshauJDk1ycB4NyxIw4NGlB87BgFmzfjPXYsGr0eQ1wcOi8vtC4unOzTF4Am27ai9/a27q/gn3+wFBRQsG07xsREtK6u6NzdsRQXc+aBByiJOkLumjWE//ILeX//jblsujkZdE4IIUQpSdiFEELUHaU17AU7dlqbjmcvWFDlqoW79+DWr5/1sSE6BmNiYqX1Ep56mmRfX+p/+gn2jRsTM/JujLGx6nOlCW4Zp/btCZ9fXmOcv+7vSvszpqbaPC45ftzmWMaUFEzJyQDo58/HcLpyza41WQeKD0dV+fpsXsOECRRs2076V1+VH/foMRgwgPz166vcJmfRYnIWLSboww9w7tpVrXU3GEh49jm87ru3/PUklJ+zon37KTl1moyKx4k6UmnflsJCcpcuLX8NUUdwatkCgLTp08n8/gcyv//B+nzGN99i36ABnnfcUWlfJUePknr0qLqtmztaF1csWi32pU3ijfHx5bEmJWMfFkbGV19blxXu3o1DgwZED78DzGYUkwnnTp04M+YeNE5OBLz2Wvmxjh1D362b9bEprXy+97Kk/mzF+w9QsHkz5qxs6zLdZXZrEEII8d8hRbhCCCHqDMVOLacu2Ly5fNlZTbudOnYAoOTUKYqPlSfLhefp42xOT+fM/WPJX7fOmqxXpWjvXptm2VU1Kzee1YS9YsJdtH+/NVlXY9pByYmTALj07FnlMUuOHbM2VT+Xgm3b1f9UWK/4yBEUs5mcpcts1vV+8EGbx4kv/4/o4Xeg0dtZl2X98KP1/3mrVlr/b8rKtEnWyzhERhI8bdp54tsGqE3tc5Yvr3KdpFdfq7L/f0XZnp5kenuT7elpbRJvqJCwm1KSMefl2STxRbv3qP3kS/vK56/fQEHpeAdKURHps2ZZ1z27QKesW8GF5K1Zgyk9zfrYLju7WtsJIYT475OEXQghRJ1REBEBcN6B2Vx79ULn6wtmM6kffmhdnrPUtm+5U5s2aBwdyxdYLBTu23fBGEyJiVhKSoh99FGb2vMyxvh4jImJJL72GkWHDlN8+NyDveWv34ApTU30gqdNo/6XX1RaRykpoeT06crLzWaMiYk2TcIrKtq/n5wlf1QqgPC45WbsgoNtlpkzMqosqFCAlGXrrZPo5a1YWWkdANeePXDtc1OVzwGkffIJSW++ScIzz2KuUGt9NmvBQyldhebpZ7M2ia9QQJL48v+Ie+xxm/UKd+/GcKb8tRXt30/2kiXWxxXPX8E/O7AUF6MYjZgyMzHElifsvuPH4zV6lM2+Xfupte6F//5r0xJBCCGEKCNN4oUQQtQZue3a4bdylXW0cIfISEpKm0uXcYyMxKlFC7VveAWGk7ZNz+2bNKYkJsba/x3UZuJlih28cCjJQgOU2HugM5egNxdzasjNuHTrRsHGTVXGWHL8OMf6DSIxsDv+fz6IzmzAbO+Og6G89ti+cSMMJ09RtH8/APrAQHSuLjiWFkiU0Xl5Yc7KomjfPjRaLalTP8Kcl4c5JwdDFYOkAWR4N8dg50pAyk6SXn8dAJ9HH1XPk16PQ2Qk9WfNIv6JJ6pM9jWOjtZzklyvC0f876NptjP1E9VWDQoasjybkOrfgXzX+jSPmkNo9+5o7e1t9pPt2QS/0XdgnPWB+vjnX6zP2YWEYCwdrK2is98zj9tuI+vnn23eozKmxCRyV62m5ORJm+VFe/YA4NylC0X79mFMSCDn998qbVuV3L/+UvvBh4eT/dtv1hYLYfPn4dy+PQAOEZEkl44p4H3ffeSvXYfh5KkqW1sIIYQQkrBf53LSCvlr+n5a9gqmbb/Qmg7nqrNYFDSA5gJT3hgNZtbNjcLdx4nudzS+NsGVspgtZCUX4h3kgkZzeVPzZCTkA+AT7GpdplgULIqCTlf9BjKKohB3JJOABh7YO9X+j/2Zwxl4+jvj4ed0wXUTjmfh6e+Mi6fDRR/HUGxi5deH8A12vSbXSX5WCTq9Bic3+wuvfB0zGy3o7K5uA66j25MwFJto2jkARxe7C28AGEvMmNzc8bj7bvZvSUOjKNzw+lhix9xjs56lfiOONr0bjz2ncctXk8JiB08sWjucitIo+1TrPT3RevmQ4NQM3/SD2JnKa+1jwgdxOnwoEYa9BGbs4R//e9GZiunw78c4FWeSv2EDoCav6T6tKHb05kxof/TmYtrt+4zTDW4lKbAbmd7NKHHwosjJlyYnFqK1mEhqcweR/SNxeX8clqxMAAwDxrDkk710G96YzMa9KM43kOcWSnDnRrjMe18dAE6vt2nyDmDR6DDr7IkJG4xf+n5c8xM42PIRLFo7kgO64pN5mIDMfbjfPZpjUSUc35VCg5VnaD+wMZ4fziDtnuEAZHg1w7UgEXt/XwwPTOTM4SzqdwjlyBq1Nvx407utCXtK2I1ENRhhjSH25ldp0aYTWxedoCSgK15ZxyCiNXu9R+AQo6db805oonahAbzuuQfnLl2wCwkh5vbbOdXgFtJ9WhMa/zdnQvrj/+9eGpa933onqFefBosWkvDiS5QcOYLeZFLnYtdoyN+4sVKCX5FjyxbovL3IW7GSjNnfVOsaA7XrxNndJ+xDy3+fXXp0L33vIc0xHE2DpijRxzFWqMU3O1z895kQQoj/Jo1yOfPR/Afk5ubi4eFBTk4O7u7uNR3OORmNRpYvX86QIUOwsyu/Od2y6AT716o3lPe83RUPP+dz7iPxRBYuno7VSoKuNMWisOa7w6DR0Pe+ZiScyCK4iZfNTb3RYCYuKpOwlj7o9OpyQ5GJw5sTiewegE6v5Ze3duLq7cDAh1tiMSu4eavNUQtzDaydc5jI7oE07RTApgXHObhe7YP4wJSeOLuXJ0gFOSXYOeiwd6xe4pqbUYSLu0O1EhCLRWH5rAOcOZRBvweaYzFbMJaYadTeHxeP8hswo8HMzj9P07CtH4GNPavcV3GBkR9e2YbFrHD3pM54+juTkZjPyq8OYTKYGTWpS7WS7+TTOexbG8upvWk0aOPLkMdbA5CZWMCfn/1L855BtB8YRlG+0Xo+AQ6sj+fghngiugTQYVAYGq2G/KwSXDztqyyIMBqNLP1zOQMGDMDFreprLPFkNruXRdN+YBhObvZkJRfi5uNIvfDyz170/jSWf3EQdz8nBj3cEt8Q13MWfKRE57Low924ejlw96QuODjpKS5Qa6mK841sWXSCjoPDCWjoYd0mPT4PT39n9PY6/l0Ty7bFau3a2A974Ohqh1aj4Z8/TuPgrKf9wDAA9q2NJfVMHjfdE4mdg84mBkVROL4zBZ9gV3zrqwUrxhIzy784gMlgpuOQBrj5OOLsbs+Pr21Ho4GhT7fFwUmPh58TGq2GxBNZOLs74Fmv6s+v2WwhO7kQB2c9rl6OVa4D6ucsPT4fn2AXtDotZqMFrV5Dcb4RR1c763k0lpjR22nRaDUUFxjZ8JNaw9zihmCWf3WQG0Y0IaJzAIun7iEvsxgXDwfa9A2hWfdAFIvC3tVncPd1oknHepViiNqayPqfjtLp5gZ0vqVBlXEW5xvR6DQ4OOkxmy3WwqeSQiM6vRZjiZmTe1Jp1iMQjUbDpl+OE9jIg8hugQAknczmt4/U6cJCW3gz9Km2VR7n8OYEzhzKoGFbP7Q6DWvmROHRtJj+w7ux6P1/AXDxsCfYJYuSI0dof0cLkou8OHJSS3aKOq951+jZxDtGEh/YE9DQONSE65+z8Mo+ht/b73M4zZ/DB4rx9TQRFrcW/ZGdlNh7cLDjs5hNCjo7LRFdA4jarDZ39so8QrsDn2PW2pHR8x5OF9en0DnAJm4/XTppZt9zvs9lOvXyorFyBKN/OKvWQ15G1XN9h8evJvzUMg62fAiDZzC+9V0oyjfRVHuEvZbO5OddeO5zjVaDYlFvF3R6LeGtfTm1N5XIoz9iZyzkYKtHL7iPZpE6UnIdyUys3B3BwUVPScG5+9mHaU7TYHBH0gucOLknFf9QV1wWT+NIs/srrdt749OY9E7s7PgKdp5u3Du1D1rFzK5uQ2h9cBuOhkKMej0nG1VdSOc9dix5a9ZQf9YsjEmJxJc2kdfY2RH6/fdY8vOIe+RRHJo0RmNnT3HU+Qf1c2rXjvCf59ssKzp4kIMHjezalE3DwGLCf37e+pzHvfey38+Pfg+Mtfm9v5DEk9mUFJpo0Fq9dip+ti7EbLJQkF2Cu++1vz+oCdU5N4YiE1sWnqBRB3/CWvhYlxsNZvavjaVRe3+yUwrJSi6k3YBQm9+pYzuSKcgpofVN9Tm0MYEGbfwq3XspFuWClQ9nKykyYTZabO5pLkXU1kQOb0qgxQ3BNO8ZBIDJaGbzghMEN/WkaeeA825/rnvSusRQZMJ0Bd6LuiIjMZ+EY9m06hV80df92QzFJjbOP0aDNn407uBf5TrXyzVa3TxUEvbrJGHfteI0p+KiaFS/OemxBTi62mGxKOSmFZF4IhsA/zA3bn++PYpF4eCGeHyCXcnPLMY7yJW4o5nsXhaDRqshqIkHYS19aX1TfU7vSyP1TB6+9V1p3MEfY4kZQ5Gpyh9tRVFIj8tHb68len86LW4MxsFJT1Gega2LT9KgtS96ex31m3lhMSvs+PM0KadzaTcglJJCE3//oI4ErHfQYSoxE9LMi4Zt/SguMNG2XwhLZx4g4VgWXW9vSIdB4QCsnRvFsX+SqR/pRYsbglk1+5BNTDeMbEKDNn4c2hjP3lVq7UTjDv6c3FM+yvJN90ZiMlg4si2RLrc2ZNXXh/Dwd+KuiZ2sBQNny04tRLEoFOYaWPLpvzTtVI/+D7awPm8oNnFoYwKN2vvZFJLsXxfHloUnKu3PwVlPaHNvslIKCWzsSXpcHkkn1f6TPe9qQssbg9HZaSnON3JyTwqFeUZO7kklK0m9sa0f6YVviBv71sZS1hl04MMtrV9Up/amkhaXR0mhCa8AF+wddWz77SRFeZWbWI58rTM+wS4s+eRf67Xj7udEbloRTTrVo/VN9XHzceSn17ZjMqo38j3ubIxGq2HLrycIbe5N9zsa4+7nxPbFJ6nX0AMPPyfijmSwa1k0jq72jJjYGVcvtYDi+K5kspMLCWjkwV/T91eKR6vV0O/B5tbkb8mn/5JwLMv6fOehDWjeM4jUmFz+XROLTq/F2d2ebsMacWRbEjv/igbUxKteAw9O70vD2cMeB2c7spIKsHPQMe7jG0g6lUPUlkRO7EqhxQ1B9Bodwbw3/iEntXwKLb29lmbdgzi4QS3s6fdAcxyc9SybeQCAbsMaEdzUi3U/HMHN25EedzYmM7GAVbMP4eCsZ8QrnTi6PYk9q85gMdl+tQY29rC+52X8w90JaebFnhVncPVy4L73uqPRaMhIzOfQhgS8Al3wC3Vj7dwoctPUOBu08aXX6AibAiBQb/5WfXOYU3tTiegagLObPYe3JGIoUpMg7yAXIrsFkp9ZzKHNCbh6OtD3/mbsWhZD/NEsztbx5nB2L4uxWRYc4YWzuz0ndqUA4OrtQJ97mhEc6YWiKMQfzWLpjPL3uPeYCFrcEIzFbCElOpf9f8fR4kb1c1xSYMLd15Hc9GKCmnjSsJ0fO/88jXvpTW16XD4dh4TjWc+ZtXPUhKhp53okHMvCZLJYkzu9nZY+9zdjx5+ncXazZ+AjLbF30nN8RzIb5h2r9LoAug5ryD+/V+7TfTG0GgsWtNbPY3VpUOg+0J99WzMoyL9wonw+egcdN9zVhPU/HT3velqtQnOHExwqalrtfVd1vTq62lGcf+WabQ97vh3ZqUVs/uW49bvmSmi77zMyQ7sR690ZgJvuieTItiSST+dw/08jcS1Ix+ToSNKD4/AeMxpzQQF2/v6cGfsAjk2bEr6gvOl9cnQOxt3bcSlMwfWmm6w15YW7dqHz9yf+pyWU/PgVSQ37ouvUk7adXEn94APr9GwZDXvi98yzRAxsUSnOmY+VzxDw2AftiX3oYbTOzgTO/poVK1ee80azqkTPZDTz1VNqa4FRb3Th1N5Udq+IYdDDLWnQxu+C52z1N4c4sTuVO17qgKOLHY6u6nGzkgoIaOSBRqPBUGzi8KZEGrazTT6zkgvYsvAknW9pQL0GF38flZtRxPofj9K2XyhhLX3OuV5xgZFVsw8R3sqXNn1DLrhfQ5EJnb22UmIefSCdFV8epMutDaz3GhUpioLJYGHnX6fZV1ohcu873chMLCCoqSf71saxa2m0zTbDnm9HUBN1usactEJ+ev0fAHxDXEmPyyegoQd3vNTBun7s4QyWzTpARNcAfIJcadU7GG1pnGXv7/51cZiMZtoPDEOj0aAoCgve2UleRjEjX+t83sKV9Hj1HqN5zyA0GjixKwVnDwcKcw0c3pxg89n2DXGl3wPNSTqRzcaf1TE1Hp3Ri6zkQhJPZOPsbk+9cHf09jpiDqYT0TkAC+YrlgxZLAqZiQW4+TjicI5KCEVROLErhYCGHle8UMlkNHNiVyoN2vhW2VqrMNdAYW4JvvXdbOJZ+P5uctKKGD25S6Xf4+rKSMynpNBEYOlnrCye/evicHKzp3mPIOu6ZpMFNFywoEmxKJhMFuzsdZWey0krQqvT2FTOXEmKopCTWmStjKio7PvuhpFNaX1T/cs6zvbfT1rv+UdP7kLC8Wxa9AyyOaYk7P8x10PCnp1ayLxJ/1z14+jttKDVYDZa6D06goguAejstOz/O45j/6ijEqfF5lnXb3VTfbSlPypXkruvI/e+ozYZrHhD4x/mRuqZvHNtdkkiuwfS447GODjpSTqVzYZ5x8hJLcJiUdDqNLh4OJCXqdZauXk7Yueow8FZT1ZyofXGNbipJz3ubMLxXSnsW1N50CVXLwfys0ouGIu7nxMF2SWYq3njGtktgB53NOHgxnhr0lodegcdlN6QVEVnp6VRez+O70g57368g1yqrCkD8KnvSrPugRTnG9m9POaCMWn1Gvo/0IL6EV7M+d+WSsmuVqvBYrFd1riDP4knsynMMXAp7nipA4un7LmobZzc7HBys7e+bjtHHcZi8yUdvyrtB4ZxfGdyldeLVq+xnhd7Jz3dhzciNTaP9Ng8Bj7SkgPr462tba4lN29HnNztSY2penRuN29H62foYrl42BPW2tdaO30pPPydKMgqsUkINRq1VXRwhCfFBSYy4vPPuw9HVzsGjGvB2jlRFOZW73rTaDX0vb+ZtbDB2d0eNFzwevUKcCYruZAGbXxp0yeEv388Qm565fNXVsh2tu53NKZeuDsaDbj7OrHuhyPERWVWK+beYyKshRy3PduW4AgvVn51iNP70ghv7cuQx1qx7vsjHNuRfIE9QWgLHxq08WXj/KoLTUBNCvR2OnLSiti7MoZ6DT2oH+HFj69tt+4jtIU3p/akknQqB896zoS18KEwz8DJPanY2YGrJYeQbo3Zv6m8kNbNy4G8c3znliXsSnAwJ3/fg4OznpBm3mi0GkyZmWgdHNC6uJAWl8eeFWc4tTcVB2c997zVzZrE5qYXcXJvKoc3JZCbXkzrjq4c2K1eQ3e83AGX07tJ/ehjXCe+xaKfc0FRk6IbRjQhK7mQ2MOZdBwSzq/v7bLGNe7jG3Bw1qPRaM57o7njr9PsXXkGJzd7GrTx5ca7m6LRaIg/lsUfn/5b6fU27uDPwIdbnrc2Nyu5gPmTdwDQrHsgx3YkYzGXf992vb0h7QeGWa8FDz8nRrzaib2rzmAqsZAcnUNKdC4arYYnZt2EoijEHMzAw9cJ7yCXc77/ZVZ8dZDT/6oDJ47/so91eUF2CTlpRfiHuaG317Ft8Un+Lf2NfeKLm2xqtHMzitj+2yk6DA7Ht74rmYkFLHx/F8ERXnQcEs6pval0GByOnaOOP6ftsxZWN+7gT69REWz97SRe9ZwJbeHNqtmHKcozUFJY3uJDZ6c9729zt2GNaNTenx1/nub0vrQq133s897WSoJf39tlcz/Vc0QT2vQJYc/KGP5dHUvTTvU4uDHBuu9WN9Un5kA6q79RB6AMaOhBSaGR3PRiQpp70/qm+rh6OeAV4IKiKNbC6OY9AvGs58K2305Wiqcidz8nPP2diD2sflcMfrQVWxaesH53O3vY4x/mTsyBdNr0CaHLsPDLSoYURSEuKhM7B7VyIfl0Lg7Oem57rh1+IW7kZxUTczCDpp3rYe+o58TuFFZ/cxgHZz0PfXKjzb7SYvOIOZhOfnYJXW9riLHEjIOTHgfnynHFRWWyb10cXW9viIevE4un7rH+noc092boU23UwhGLwplDGfiHu7P08/2kxeZx18SO+IepeULFQpn2g8LITi6kWY9Awlv5kp1SiNlkQavTsP33U2Qk5KOz01GYU4JviBsDH26Bg7MdJoOZ7yduw1BkIjjCk9a9Qwhr7cOKLw5y5lAGaNT7lNN702jY3o91c49gMpa2rixtIbp9ibr/jkPCSTiWRcO2fuz44zQxBzPocmtD2g1QCxgPbohn9/IYCnMNOLnZMfyFDrj7Oqot8UwWVs0+ZL2v7XJbQ1reGIyhyMTyLw7gH+ZOt+GNztnS0WJRWP/DEfT2Otx8HNn++yl63tWEkkIjmYkFFOUbadjOjy2/qpVZwRFeeNZzJqChO5FdA6t9rfjUd2XHH6eJjcqkILvy9/uNdzelVe/yggBJ2P9jrouEPaWQHX+d4vT+FHwC3UmLtb25dHSx48ZRTa1f5OfTpk8I9k46di2PsdYMNWjjS/yxrEqJh72jjiad6nH4Mm6Wq2LnqKNhWz+i96VhOEey4+bjSIM2vhz4O77K56ujfqRXlTWHZyurCa5OUl0dLh72FJTelDftUo/eoyP5+plz95M8m2+IK34hbsQcTMdQbEajwZpcu/s50WlIOOu+rzxv8dnsHHX4BLlgKDYz4KEW6O10LJu5n6zkQus67r6OGIrM1mbkZ+s/rjk7/oy2SQxCmnkRdzSryppFB18TSoEDhqLK76veXktoCx+MJWbij2TSbXhjwlv5sP33U0TvV2ujgpp4Wm+kLkaDNr7WfZyL3kGHxWSxuREtez1anZaifOM5k06/UDeKC4w2TY4dnPU2N3RlvINcyE4utM7BcXbhw10TO2IyWnBw1vP390dIj8uvVBhxtuCmngx5ojV5GcWsnRtFetz5E8yKgpp44h3oQkmRiVN7U1EUuHFkEw5tTrxgogrqaw9s7EHiiezyxFuhytcOaqI69Mk2HN2RdMFCH4DBj7UiPS6PI9uSqvwM2jvpra0EXDwdMBSZMJssNO8RRG56EbEXSEaHvdAeByc9a76Lso4JUWb05C54+juTHp+Pu58TJ3YmU5Bj4MyhDBp39KekwMjxnSkMebw1fqHqZ3LV7EPWz2OTjv4oiloDk3w6h/YDw/AOdCHpVA71GrgT0SWAvavPEBeVSa9RERzamMD+v9VClS63NaRt3xDMZoWTu1PYMO8YN90bSdPO9TAbLdabTcWiqDV+RgsL3t1l/SyOeKUTv76/y/o59Ap0oettDWnY1rY2NT+rmF/f22VtbTPs+XYoCqz65jBFuQYatfNDUdQEoN2AULKSC8hMLKBRe7X1TmGugRO7U2jWPRB7R721y4pfqBu3T2hHenw+J3en4h3kgounA8tnqa1RympRctOLSDyZzZ4VZ6zdDACb7jlnW/3NIc4czuSOlzrgHagmIGd36zAUm7Cz16HRqjW/f3z6L3lZJRTnGSi7q6lYyAXQrEcgnR+/EdeCdIo8/Pnu7p8B9femZa9gIjoH4OLpQEmRiflv/GNTOBPc1BPf+m4c35VcZculiiK7B9KonZ+1ZU6Zc31nALTuU5+EY9lotNCwrS+H9pyg581tOLY9haI8A8Oeb49Or+WrZzbaJIMjXu2EvaOOI9uS2LOi8hRy/mFu9LmvGX9+tg9nD3uCm3oR0SWAuKOZpETnEtbSh5SY3PMWimn1Glr0CLImkACN2vtxam9apXXbDwojen86WUkFaHUaIroGoFgUslOKGPBQC479k0xoC2/8w9xRFIXdy2NsCpz9Qt3oeltDYg6kc2hTAoqiFmINfbota747bK0ZHvlaZ/IyinDxdMA/zJ3fpu4h6VQODi567nunO0tn7reuq9OrSYmDsx5jibnyb0Bz72oXalUU2T2Q4jwDMQczaNDGl8JcAynR559WsKxlWMXPAqiFgl2GNrDWcF8KrV7DHS92ICupgLVzL3yPcD7n66Ki1WvoP64Z+w7v4raRg6zJ0N7VZ0g7k4e7nxOn/00joJEHN4xoQk5qkbVApUEbX8wmxdqK7WzBEZ7UC/fg0KYEDEUmIrsF0Pf+5qz+9rC1ZddN90SSmVSAvaOO+GNZNq0FnN3tKc43YlEUghp70unmcDKTCkg+nYuTm531ntIn2BVXLwc1Ma6g083htO4TwoH18exaGo1Wp7FeLy1uCCKoqSd6Ox0H1seRcCy7Uvx3vNyBv6bvx2Ky4ObrZG0leTa9nRYHF7tKief5KkLKdBgURnGhiaad/Pn9Y9tCurMriLoNa0RuelGV9/HBEZ607ReKRquxaRkH6ve33k5rbb3VslcwXW9ryLrvj5B6Jo+wlj406VQP3/quxB/NqtT6tTo0Wg0PTOnBsX+SiT+WRaN2/jg46wmO8CLldA71m3mj1Wo4tCnhvAW/FXUf3pjQFt5qSz2NRRL2/5LrIWEH25Kik7vS+PuH8iaQzXsE0ntMJAs/2E1abB4dh4QTHOHFrqXRuHqryVNOqtpX+JbxbdBoNURtSST2cAYtewVTP9KbjIR8Dm9JxD/MjcTj2UTvT6+UxPUeE4Gdg47DmxMrJVVuPo7kZRQT3soHv1A37J30+NZ3Je5IFntXncE/3J2mneoR3toHBye1uV1RvoHCHAOxUZlsW3wSVy8H3LwdSTpl2wyzKjq9lhY3BHF4SyKe/k5kJKhfcKMmdSH6QBqxhzMZ/Ggrkk7nkBGfT/yxTBq29eefP06ht9My8OGWpMfnW0v8zlbWVLeMnaOOJh3rEdrCm+RTOcQdycQrwAVjidn6he/q5UBwhBctegaReiaP6ANpDBjXEmd3e2IPZ7B9ySl6jYrA2cOexR/uwT/cnZufaM2mX45zam8qjTv406CtH8FNPdFoNJiMZgxFZqK2JLDjT/WG5olZN2FRFJZ8/C/Jp9Xz5BPsSsO2vsQczECr09D5lgac2JNK274h+AS7oiiK9Sa3KM/Aiq8OkpNaxC1PtsEv1A1DsYmctCL8QtyIO5LJn5/tA9QbiAc+7IFiUfvNxRxMp36kF+4+TiSezGbb4pM4ONsR1MQDn2BXSooMHEnYxY3d+3BgbQJ5GcXWZGrQIy2tCYDZbKE432htQmYymFn97WGbhLv1TfVJPp1DvYYe1rEIet7VhJa9gjEbLcx+rnx0be8gF+5+rTMFOSU4utixZ9UZa1PuQY+0pCDHQG5aER2GhOHkas/vH++1uX4HPdqSRu3U2AxFJo7tSMbOQUfTLgEU5RmIPZxJw3Z+FOUZ+POzfeRlFBPRJYCeI5qw4suD5GUW03t0BAGNPCjMMeDh50RaXB4Wi4LZYGHFVwdp0rEeWp0Gn2BXa39BUEumzSYLB9fHs/33U9brqP3AMPUzWfoje/uEdgQ39bJus7q06buTm51N8lCxGbuLp9rEXluhVq24wIih2IS7jxM5aUWs/PogmQkF9BodwYZ5R9HqtfQb25y8zGK1f57BTPfhjausmVMsCpsWHOdQhZt4O0cd97zVDWd3exRFIflUDkV5Rpw97DlzKAO/EDdy0osIburJtt9OEdTE06afu6FY7TNaUmiy1riBWrs1/IX2eAe6oNNrrYMu7lkZwz9L1KbtzXsGEbWl/Kaky60N6TA4zKZWIC0+h98/24m5SM8NI5rS8kbbqcmqK+ZgOinRuXQcHG4d28JiUWzOdVXyMovZtvgkzXsEEdLcdrqxkkIj9k768w5UmXAsiyWf/kur3vW58e6mnN6XRn5WCZHdAs47Jkd2SiF7VsZg76in511N0Gg1nDmcwel9aXS7vVG1B+0DtbYjJToXn2DXSuM5gDpQZvzRLGsLrIoKcw2YTRbMJrXf57liViwKFrNySQMXxh/NZOdf0RTlG2k/MIy/fziCTq/ljpc74BfiRolPAA6ZKeS7+PL9PQtsttXZafELcSX59PmTrjJlzZ0v5HJamZTpMCiMqG1JFFWzhUfX2xtaPxt6O+1Fdzlw83aky20NOb4j2aZQrGKB9MUq6w5n56jjzpc6kpGYX62KBlB/k/Myiql0x6pRk5Ltv5XPeFB2P3Ih4a19iTlw/oJee0cdaDTUa+Buk9T7BLty9+udSTiexZJPbJMmZw972vUPZeuik9bH52pZE9DQ/bzXW8WxI6rSslcwsVGZVba2qcjJzQ53XydrgYKLhz2FecbK+9aUFnCUXi8hzb2J7BbAmm+rGqdBoWmXALzqOeMT7MryLw6eN4bzGfRIS1Z/e7hSYQqov4kWi3LOc6jVawhp5k3soYzK10c1Xeg8X46yFl3nEtk9EDs7LVHbkqrdwvJaOLvA81LXOZ/mNwSds7CwVe/69LijMfPf/KdyKzMN5+2O5urlQFATD/LcTjP09sGSsP8XXI8Ju52dHRkJ+fzytjoK7bDn2xPUxJPiAiOFOYZqNUG7kLLmQMtKa0va9Amh54gm1udP7Eph94oYHJz13Dy+DQ5OekqKTJX6H6l9k/LxCT73wGGKReHoP8n4h7th76gn7kgmmUkF7F8bh52DjiGPt8LBxc7aZy72cCa+9V3x8HfCXFpj+vcPR6gX7mFt/nMuZTUmZYOEbPr5GMd3pRDawocWPYPwC3WzJlHfT9yGYlEYMK4FjTv6nzP+wlwD+VnF+IW4VXsgDYvZYu2vBtgk1WczGc3sWXGG8Fa+1v6BJqOZw5sScfdzIryVz0WPRn/28Ss6uj2Jvatj6TAojIgu5x94pqKqmh/lpBWRHpdHw3Z+540xN6OIH18tnz953Ec3WJufHtuRTHpcHl1vb2RtTlg2iN6NdzfFw9/J5sY/P6uYeZN34OHryIhXO1dKGmIOpLPx52O4ejkS2sKbDoPDL5holbFYFIpyDTi526PVqn0KgcueDSAvs5g/pv1LaHMfet7VGK1Oa23iqtVpeGxGb5try2yycOZQBkGNPdm76gzRB9IJa+lD9zsas2zmfmIPZ1r7j5+PoigYitXmg0kns3Fysz/nwHdVMRnNpMbk4lnPhcJcAzq9Bq+Ay//+gfI+as7u9vQaHVGp5hjUApaorYmENvfByd2OuS9vxWJWeHjajVUmg0ajkWXLljOw/0CcXK5OH76rLS+zGGd3+3OOvyHKKYrCka1JeNZzJqiJJwCWwGC0yYnWhL3v/c2IOZhBXkZRpS5Xfcc2I7JrIIknsvhj2j6bZOKmeyNp3iPonOOWlGnQxpf+41oQfySzyoSmYmLt5GZHcIQXJ3enVlqvovqRXgQ09KjU3cjd15F2A8Jw93GkfjNvvnhi/Tn3oXfQEdjQnbgjaiu0ii1Z7Bx0PPhRT/R2OsxGC799tMd6bvrc14xT/6Zy5qBaUF3WasDD38lmPBB7Jz3tB4ZaX9uF6PRaGrbzIyu5gOyUQjzrOdP9jsa4eDiw4N2dl5QUBDb2IDu1iKJcA1q9hk5DwkGjYccfp2nU3p/2A0NZ+P7uStu5eNgz+s2uGEvM1nsFs8liHSOg6+0NadzBHw8/ZwzFJua8vBVTiRmdXkuf+yNp2kn93Tz1byp29jpcPB3YsyKGk3tSbRI3z3rOjHmzK1t+PWFteRPQ0J1hL3QgNSYXo8FMSKR3pXFdygo+QB1U185RR25akbVZc9m+G7b1tfbz7XFnY5r1CCItNg+/UDdQFBZP2WNd385Rx033RBLSzNumxrTHnY1p0zeE9T8d5cjWpEtujXA2Jzc7Og9taB0UtFn3QFZ+fYhTe9VrP6ylD8UFxnO2WHDzdqS4wIi9o47bnmuHV4ALBzfEs+PP02qBe+cAorYk2lY8acDNq3Lh2R0vdaBeA3eObEvi39Wx1tYPnvWcK7WEOJtPsAsZCQXo9Fp8Q1wrxXvTPZE07xmEoqiFkP8sOWUdF8HeUYfZrDBiYie8g1w4uSfVet7LatrtHXUMeqQVxhIz/uFuzJ+8A2NJ5RaMAx5qgdlkYV1py4rAxh44OOmJOVjegqD9oDBcPBzYvKDqVhyhLXxo2z+E4ztTOLEzRe0zD3QYHMaB9fHWVrjhrXyIP5pVqRDQ0cUOdz8nm5aKtzzZBkOxibVzo2w+w+crxKpIb6/FZFBnnek2rBHF+Uba9g9Fb6elKM/I1sUnMBksOHvYV5n4u4YZGP1CX0nY/wuu14RdsSisKe0b2f/B5pedMJxL3JFMorYk0nNEk0seVONSJZ/OwcXT4aoNjnEhMQfTUSxKtQbsEZffX+iv6fuIjcpkwLgWNOlUefTxi1GQXYLeQXfOAWyuF6f3peEV4HxRSXBRvoHUmDxCW3hfte+Fa8VkNKPTaatdEJaRkI9Wd+5Cg+ulT5u4iurXh4QECtz8OPTTP3S5VZ0ETlEU1s6J4vjOFBxd7bj1mbb4hZQPMpUWm8f+dXHWZtW9RkWgs9NiKDIxd+JWjMVmXL0caHFDsDoGyM4UWt4YbDPdZOzhDE7sTqFppwCWfXEAFw977nmrG2vmRHFqTyq3PtOWoKaeJJzIZNfBLQQ7tGT38thKNX8DxrXAwUVvHcTz1mfakpVcUGmKw+9e2kJRrgGvAGdue64dmxecoFF7P9y8HfEOcsHeUS2oi96vFvgtKe0DH97al5ufKO+ucPSfJGsicO+73Yg/msX6H9VWfp1uaUCD1r54+DkRfzSLhONqP1pnD3u8AlxYOnO/Nbm3c9TRb2xz1syJsiacLh72dBgcTv1Ir3N+bo9sS2LrohNo9Vo6DAyzFpD4BLtW6uZSZvCjrWjYzo/oA+lsXVm+J+AAADmgSURBVHSCPvc2sxbaZCTk4+bjiJ29jnmTywce7f9gc3Yti+GmeyKt61aUEpNLcYHRZsT4suUZCfnUj/A674BoFZv2egU40//BFviFupF4IpvfP1Znvag4eF2ZguwSdi+PIaipJ8X5RvzC3Fj84R4CG3sw/IXygewKckrY+Vc0oS28adTOH0OxiZ/f3IG9k567JnZEb2fbIib1TC6rvjlMz7uaENrc26YQMO5IJtH70+l6W0PsnfQoFsU6QPG+dWpSG58US35M+QjpOjst7fqHqrE28ST1TG6lsXLumtiR+GNZtOpdv9KgaKf/TWPFVwet67l4OhBzIJ2c1CKitiYS2NiTPvdFkhKdS0hzb8xGCxqtxmY/ZZ+Vsq4ym34+TkpMLjc/0Rq9vQ5HFz1fPV3eRTGyawB97mtm/Y2xmC3s/ztebcV5YxAb5x/jyNYkmnauh4unAy1uCMLDzxmz0UJRvhEXT3vSYvNw9XJUm9yvjyf5dI46tW/pe2wzEJpBHVCuYVs/XL3ULl5lM79YzBa+GL8BUAsEPXydcPN1xN2n/Jo6vU89R+6+5WOYdLm1AR2HNMBssvDlk+r2rfvU54YRTSnKN6DVaUk4lkVYaeXOrqXR+IW64R3oQmZSASu+VM/53ZM64xOkznSTEpPLii8P4uhqx8hXOpEWl8eJ3ak0au9HQAMPTAb18/vre7vISi6k16imtOyl9h/fslAtgBr8iPoZBNTWcH+cxre+Ky4eDvR7oDlzX95qLRQo02t0BM5u9pzen2YdO8vBRU+/sc0Jb3X+mVMsFoWj25Os300AQX3zuOX22v17Lwl7NV2vCbsQtc3lXqOGYhOFOYaLquEV4mLI96goS9gJDoZ42360xhL1ZjqslY9Nsn4hp/elkXg8m253NKr2NGq5GUXo7XQ4u9tjsSgYikzWZLvidarTqWNd2Dvp6XFnY8xGizWB2vHnabyDXaw1umcrKyDoNqzxBaeeUhSFWY+rNfLdhjei/YAw63NGg5k/Pv0XV08HBj3aivysEr6fuBWAvvc3s063WBVDsYmc1CJcvRxQFLV125lDGRxYH0e7/qEENfE8Z2uvisxmC4pFQW+nI/l0DmmxeUR0DeDA+njSzuSRnVpIUBNPdHotYS18KnU5OZf8rBJ2LVVnp6hqxPgrqeJAZWPe7Gr9rVMsClsWncDBSU/noQ2rta/0+HxcPO1xcj3/+2o2WUDhkrqXnE9Za6VOLW7A2c2BxBPZBDXxxCvAhcST2XgFqGODxJUOsHhkexL+oW7nnMYWwGy08NeMfdg76Rn8WKtKBc7na4l4MY5sS2T/ungGPdLygvcbFrOFghzDNas4Sj2TS9yRTNr1Dz3n5yInrQhnD3v2rjzDsR3JDHu+vTW+mIPpHNqYwE33Rla7km33ihjMRgudhzawOb9mswWNRnPeFoiFuQbS4/IIaV5eQWA2WyjKNVSagtZstNhchwfWx7N5wXHaDwwl7kgWJYVGRr7WGXtHvTrrzLEsCrJKCG9d9ej955J4IptNvxyj863hHIz5p9b/3kvCXk2SsAtxZcg1Kmo7uUbF+RL22qKmrtNdy6JJPpWjTo14njERAP6asZ+U6BxGT+4q81BXk6Io/PPHaSxmhe7nGXX7eiDfpeJyKYpCQbbBOkaBUjo2zZVyvVyj1c1Dr+/2okIIIYQQ4rJ1urnBhVcqNeTxVmqNdxVzPYuqaTQaut3eqKbDEKJW0Gg01lma1Fr867cA61qQhF0IIYQQdcO6dWAygV5ufy6HDHoohBDXzn/iG3fYsGF4eXlx55131nQoQgghhKitIiKgRQv1rxBCCHEd+E8k7M888ww//PBDTYchhBBCCCGEEEJcMf+JhL137964uVV/RFchhBBCCCGEEKK2q/GEfdOmTQwdOpSgoCA0Gg1LliyptM7MmTMJDw/H0dGRLl26sHPnzmsfqBBCCCGub/PnwzffqH+FEEKI60CNJ+wFBQW0adOGmTNnVvn8ggULmDBhAm+88QZ79+6lTZs2DBw4kNTU1GscqRBCCCGuay+9BA8/rP4VQgghrgM1Pkzq4MGDGTx48Dmf/+STT3j44Yd54IEHAPjyyy9ZtmwZ3333Hf/73/8u+nglJSWUlJRYH+fm5gLqfH1Go/Gi93etlMVWm2MUdZtco6K2k2tU6FEnD1IAUy29DuQ6FbWdXKOitrtertHqxlfjCfv5GAwG9uzZw8SJE63LtFot/fr1Y/v27Ze0z/fff58333yz0vLVq1fj7Ox8ybFeK2vWrKnpEIQ4L7lGRW0n12jdNaC4GCeguLiY1cuX13Q45yXXqajt5BoVtV1tv0YLCwurtV6tTtjT09Mxm83Uq1fPZnm9evU4evSo9XG/fv3Yv38/BQUF1K9fn4ULF9KtW7cq9zlx4kQmTJhgfZybm0tISAgDBgzA3d396ryQK8BoNLJmzRr69++PnZ1dTYcjRCVyjYraTq5RoXd0BMDR0ZEhQ4bUcDRVk+tU1HZyjYra7nq5Rstael9IrU7Yq2vt2rXVXtfBwQEHB4dKy+3s7Gr1G1rmeolT1F1yjYraTq5RoYFafw3IdSpqO7lGRW1X26/R6sZW44POnY+vry86nY6UlBSb5SkpKQQEBNRQVEIIIYQQQgghxNVXqxN2e3t7OnTowLp166zLLBYL69atO2eT9+qaOXMmzZs3p1OnTpcbphBCCCGEEEIIccXVeJP4/Px8Tp48aX0cHR3Nvn378Pb2JjQ0lAkTJnD//ffTsWNHOnfuzLRp0ygoKLCOGn+pxo8fz/jx48nNzcXDw+NyX4YQQgghhBBCCHFF1XjCvnv3bm666Sbr47IB4e6//37mzp3LyJEjSUtLY9KkSSQnJ9O2bVtWrlxZaSA6IYQQQgghhBDiv6TGE/bevXujKMp513nyySd58sknr1FEQgghhPhPKhv/RsbBEUIIcZ2o8YRdCCGEEOKa2L27piMQQgghLkqtHnTuapJB54QQQgghhBBC1GZ1NmEfP348UVFR7Nq1q6ZDEUIIIYQQQgghKqmzCbsQQgghhBBCCFGbSR92IYQQQtQNjz4KmZng7Q1ffVXT0QghhBAXJAm7EEIIIeqGZcsgIQGCg2s6EiGEEKJa6myTeBl0TgghhBBCCCFEbVZnE3YZdE4IIYQQQgghRG1WZxN2IYQQQgghhBCiNpOEXQghhBBCCCGEqIUkYRdCCCGEEEIIIWohSdiFEEIIIYQQQohaSBJ2IYQQQgghhBCiFqqzCbtM6yaEEEIIIYQQojbT13QANWX8+PGMHz+e3NxcPDw8ajocIYQQQlxto0ZBVhZ4edV0JEIIIUS11NmEXQghhBB1zNSpNR2BEEIIcVHqbJN4IYQQQgghhBCiNpOEXQghhBBCCCGEqIUkYRdCCCGEEEIIIWohSdiFEEIIUTdERoK7u/pXCCGEuA7U2YRdpnUTQggh6pj8fMjLg4zEmo5ECCGEqJY6m7CPHz+eqKgo/t/efYdHVaZtAL9nMpMy6YVUCAREeq8BlEVQOhawRhfdXVk0Kq67Lq6KsKsulrWsilg+FV1QFAtNUAMiCFJCaIHQWwIhJJSQnkx5vz+eTGaGFAKEzBly/65rrpk558yZ95x5Z+Y8b01NTXV3UoiIiKgxWCvkvrzIvekgIiKqpyYbsBMREVFTo5weqto3IyIi0ggG7ERERNT02CzuTgEREdEFMWAnIiKipsdS5u4UEBERXRADdiIiImp6LBXuTgEREdEFMWAnIiLPt+ZV4Os/MAijujl3W2cNOxEReQAG7ETUtB1aDeTucXcq6HLYbMDPLwA7vwF2L3Z3akjTnCJ2a7n7kkFERFRPDNiJqOnKSgU+Gwd8dKMEfeSZivMcj3N3uy8dpH3OI8OzNQYREXmAJhuwz5o1Cx07dkSfPn3cnRQicpet/5P78gIgj4GexzqX5Xh8fLP70kHa9+wfgQl+wBhfNoknIiKP0GQD9uTkZGRkZCA1NdXdSSEid7DZXJtPH17jvrTQ5cnPdDzOSgXMpe5LC2nbgI5AJyNwrRGwsoadiIi0r8kG7J5IbzMDyoOb7SoFrHsL2J/i7pRQYzm4CvhpGmDWYE1WUQ5QetbxfN+P7ksLXR7nGnZzMbDzW/elhbTNuRk8a9iJiMgDMGD3FPlHMXLHQ/Ba/DBQXgikPAd88ycgP+vCr9WKrI1AyjRg3gSg+JS7U0ON4X+3AL+9Bax5xd0pqc65VlanBw6tAo5vcV966NLZfwd1lX9pG2dzTAKqmfNAc+zDTkREHoABu4fQH1oFg6qAfufXwH+7A+v+C6QvAN7sDPw7Dtj0obuTeGF5TiNx//KS6+A/dPUpzXc83jpPe5+3PchrORDococ8/vEZzwn09iwD3r8eyNzg7pS437ljcj/kacA7EMhJB3Z86d40kTbtOghkWYBsK0eJJyIij8CA3VOUnHZ6XFk7ba9NqiiSQKPgRMO9n80GpH8NFOY03D7z9jkep34IrH3j4vexayFwYGWDJYmuAKtZCpS2zXMsK8rRXgB1rrKGPbgFcMOzgNEEZP4GpP6fe9NVXyv/CZzYDnw8HDj6W+O//8kM4OQubRTEnDkk97E9gUFT5PGSx9jNgap7+hPg4xJgfgmbxBMRkUdgwO4hdPYaJLvmfYCnsoABj8lzaznw367A/CSg5Mzlv2HGd8A3fwQ+GNJwF+Sn9sp9RDu5/2UmkL2t/q8/thlYMBGYexuw8QOgolhef6npKz0L7P0BsFou7fXkQnfoF7TNWQz9hlnSZePHp103WPaka6GNu9lr2EPigZAWwNDp8vzHp4EMjc3lbTXLOZ07Hji1H7BZ5d7uk5HAihkyn/zWucBHw4HsrVcuPXt/AGYnArMHSNccixtrKq1mR8AecS0wYArQYZwMKPbVROBYmvvSRhrEad2IiMizMGD3FAXZAABbuzFATHdg1KuATwBw0/PAn38FwtrIBeqepcCc0RLQ1qd/u7m05ibAuxbKfWE2MO/2hulzbg/Wxr4JXHOjpHfOGGD7fKCipPbXpX4EvNYB+L+hjmXLnwT+HQt8MFiaBb/VE1j48MU1Z/7hH8AXdwKLH5HzcGAl8N4g4NtJjia2zpQCVs0EFj3i2tybAKXgtfhhdDzxNbx+ecF13U0vAPEDZOq0z8YBxy8jgCovBHYvBcoKJFCrazA7c1nthTk2G3DwZ3kc0kLu+/0Z6DwBsJmBr34PbHjv0tPZ0H58RlotHFghhWgr/wUoq6zrdo/cr30DeLcfsCgZyNoAfHbzlQnarRbgp2cdz3d+DbzcCkj71D217WePyGdmNAFBcYDBG5jwMXDNMMBSCnx+B3DqQOOni7TJOYuyhp2IiDyAwd0JoPrRFUgAaev1B+ivHeq6MqYr8GiaDJg1bwKQmyEB7fInAYMfENkB8G8G+IVKbaLNDHS8BfANAj4eAfgEAmPekCb2sT0Bb5Nrf/MDKcA7vYEe98m+jqdJQFtyBmg9GGg/WgoRdLraD6Ao19EEOaIdMP7/gC/vBY78Cnz3ZwlIukwAWvST/Rl8JKjaMR/4/gnnMwHEdncNRHJ2yP2Zg8DJnUDio0BsDyC4OWD0rTk9Niuw/Qt5vP0Lx2NA+r/uWgh0vwcY+YoEAGlzgCVTHNtkbQJu/CcQ319qafxCAb2X3OojPxPwDgBMYfXbvr7sAZP9syg8CXyZJOkb+hxQdk5aJmz+RIKZuN5Au1FA816X976nD0BXnOu6zDcY6HQr0PfPQNe7gE/HSL6aM0aC40FPSB6sj2NpwN7vgc0fS8sIU7ijVveGZ4E+DwJeBiBvL3DmMOBllNYmllIJ5OL7A0OedRznomQg/6g8Dm7hOGe3vi9p2vwx8MNUGX186HTJA7WxmgG9QY7t19eA0ASg74NAQGT9z19NjqyVQrfQlsCWTx3LKwqBdW/K44TrgVtnA61/Byyc7DqLRNk54IPfAeHXVH4Ok1zTtH8FsPRxoO2NQJ8/AcV5UkjR5gZJf23f5y2fAqf3y2cw+Clg1QvyXksek9+KsW/VP1/bbEDRSUmX/btjNQMndgD7fpDfsqHTgWbX1r6PU5UFgRFtAX1lGbSXEbj9U2DOKOk28E4vILYHdN1/D4PVVL+0kWc4fVBam7QbUc8XOEXsnNaNiIg8gE4pLXRAdJ+CggIEBwfj3LlzCAqqZ/DQ2JSCmhkHXUUxzA9thDGqfe3bFmTLAHR7l7kG3fXlHSABx8l0eT76dekrW3au7tcFNZfA4tQ+KRyIaAucOy5N91sNkubvJ3fK8z+tkNdYKoAV06Wvs/P+A6KA1kOkdt95buwhzwAt+kpt7ba5UuvfapBcrO3/SVoXOPMJluDf6CvvZS2vvEDTSb//A5XpCIwBCiv7/7foL8F0obRoQHC8BJ7281EXn2BJj80s5zAwCojsJO+VuV5qfPV6CSqzNkqQF9wCiOokNfZeBqBZBwmE/COAsnxp4usXJvsozJHCCXvQZbXIebMHM/mZ0lrBXAqEJUgBzNmjjjEP6tL1TiBhsKQ9uIUU7AQ3B4x+F34tILWrSx7DWVNrBPW+A15x3YF2I123KS8EPr8TOLpOnke0A7rcDuTuknwXnwg0ayf5J7SlbFNRLE29N31Q9/sbTfLaEzscNc81aT8G8AuRZuOAFBD9fpHrcSoltdUr/+lYZgoHTBGSfyLaAl7ecn5O7ZPA2jcEcC6wMPgC0V2koCqyA9Drfjm/3hcIFpWSQPn0QSkkcw7Aw1oDyZvkXOxeAuTuBkb9B+h6u6wvOSMFYGv+I4UY694Cjq513X/3e4HeD0gB20c3Atm1jIof21O2az9GCnvswfuRtVIQUpYvhVn9/iyFX+vfAVY+L/knMAa4YZoU5uVnyuddlAcExQJxvYCYboC3vxQ6rHtL9uUTBHS6BTD6AxmLHN8/QAod+/5Jzl9QnBQGBEY71q99Q/JIl9ulINBZ4UmZqSA3o2rR4Ygb0PzPX8FoNNb9WZBnmBEs9xOXSAGWs9MHJe+GtXYsiwgAThcDgTpg+ZvAwMcaLan1ZTabsWzZMowaNYr5lDSJeZS0zlPyaH3jUAbsnhCwl56VJqcAzH/PgtFUj3RazcChXyTY3PktcPaw1LRZyuQi+sAKCQZ0XhJQ5O2RYME52InrDTy4UoK+Pd9LrfbhNUB0Z+DaEVKLdWCl7MtcR5N2Z7e8B3S/23VZ2Tlg/SwJmvevcL1YB+RC/MZ/yQV/XXJ3y8j5GYuk4KI+aWo/Brhzrgye5RMogWJpvjSJPry6+vZj3pRCgDWvAnuXu87/rGV6o3ShMIVL94OKIqlJtVnks0UtPwMBURKYmstk27DWQHgbCUgLjktAWnJKLowrirA3ahxa/+nj2n8czWVA+lcS3J1fI+/yvtHSyqL4lMyrDQDtRktQF58oAWJBNtCsvQSwZfk172PAo0BkeyD9G2D7567re04Exr1Vexp2fAV8/zeg/AKFVecLayOtPc6nN0oQb781ay/fx7Q5kv+K86Swpc0Q+a6VnJbCC0uFpMEeINeX1SwtBfZ8L61iKopq3q5Ff2lCD8j7lZx2LSjwC5VzqTcAebslH8T1Bh5Y7tryIHubjHtx+gLNz/VGyYdFdQxo6eUteay8oOb1Ya0l+C86CRxdLwUFQ58Drvtr9W1tNilYWfwIcCwVpcZQGJ7cB6N3Ha0myDOcOwa80UkeD34KGPIP+R+zVsj36Y1OUuD5xG5Ha6twf+BMiQTsS14GBj/pvvTXwlMuNKnpYh4lrfOUPMqAvZ48ImDPSQfeG4QyQxC8ph5qmIxXfFqCzYhrHbV+NqsEvTnpEoy3vVEC/gsxl0otWn4WENlRLo6LTkqt54EUCYB9AqXmdszrEojVxlIutXiZ62UfHcZJOi6WzQbsWw7k7ASgKgMAH7m3moET2yQwGfIMENez+uvLi4D3r5O0931QLv76Pyy1q3ZKSWGD0U+2P71fCjQMvpL2s0ek2bXBTwK04OZyfEZfaSJuKZXCk9w9lbXhh6VG2WqW1/sGSfBTUSz7LD4l3QH8m1U2vzfI7WSGnN+AZkB0VwlkUj+SVggA0Oo64P6l1Y/R7ngasP5dSatviATiZ486AuV6UgY/rG7zDwyc8PCF82hRngTaB1ZIQUxoK0nHmcNy7M4FCKEJwOjXgGuG1rwvq1nybNZGacER3UWC+cAY1y4ROTulFUbRSWnV0HOi5PO62KzyGR9Pk9cFxkjetFklT0R2kFYVmRtl4LPu98j752ZISwovo0y7tvPb6gVR9ZH0tTR3P3dMzlFd3U7qopQU4G36UGrd7S1aOk8AJnwk6bdWAAnXyWez/XNpNVFTwUOX24Fxb9fc+qKiGNjwLrBrkZwDZQM6jKksxDgk3XYqu/dA5yW/B93ukbQdWSPnNbYH0PFm+b4qJSO9//a2fF+LciTtzgUKgOT73y+quym+uQzq5VbQWUphfnANjHHdLuVMkpZsnQcselge97gPuPkd6e6z9HFpJZRf2Q3rwVWO33nngH3hv6Q1isZ4yoUmNV3Mo6R1npJHGbBfwKxZszBr1ixYrVbs27dP2wG71QLz6cP4beVSDJjwiKYz3lWlokQCfC8PHOrBXAq8WNlseMCjMvDbxVBKWnbkH5WLXnuN56n9UhBhLpFmzZGVTfhDWsIcEItlK1Zf/o9jUZ4UGtgsEqDGdK//2ABapZScy+NbpGtITroUxugNEoj3uFdq3G0WmV89b48Upl3/pKNfdkOxF8wVnZQuAT4Btae5oljSWXZO0uYbIoFPfQoOSs9KwZl/uOvy7G3SFN9euHKxygqk6f+p/RKgB8UCLQfVPl6FE9v/xkN/cAWsv3sGXr/7+8W/N2nLd5Md44/E9QIe/NnRRN7ZmDeliwcAhJmAs6USsH/9jAzcqjGecqFJTRfzKGmdp+TR+gbsHhiJNIzk5GQkJydXnShN85KL+nxT6wtvSw3nQv2NtczoBwyeKt0D+idf/Ot1OgmGTGFS42nXZkjtrzGbL/59ahLQTG5XE51OAvPQVkDn2+reNqrTlU2L3ku6taBz3dvpdBLMx3a/tPfxC615eWz3S98nIC1P2o++pJfa2o+F/uAK6HfMl6bQl9pigbTBeZyW3D3S0qkm9oFJAbhO6+bG6Qjp6pa5QVp6dbq19t+Z8iIp+L7Q75ClAshYCFw7vH6tHhtS2hwZfybxEf5eErlRkw3YPUm5tRw78nbgqOUotuVtg8HAj43qodNIuZXlyK0OOlz+H7HFYkGmJRPb87Yzj5ImWSJbQ+frD6/iLGDH/2SGDbokOndfvCsFFBwBfOxjEZiBtS85PXdyIhXI2w4A6AAFbwAVOh12l+cCR36WsRniBwJ6bQQkFosFWZYs7Di1w+N+Sxviv8Tj2WzAgntkRo/DPwF9J0tLKZtNxgnyMsrYGynPSre/0ASg/ShpcQVIV7zcDBmvRaeT2Tt2LgB29AdufL7hW105Kzkj3RmNvjLuyU+VYzx42YDwa2VA4+73wKL3QZYlC+mn0uudR5k3qCZX6r/EYrGgyFZLIa4HarJN4u08oQ97VmEWRn07yt3JICIi8mimUit0kHr2Ej8P72pDRES1Guc3DjNuncEm8dQ4jHoj4gPjUVxcDH9/f/fXbNBVpaHK7JRSKCkpgclkYh69CqjaZg7wYEoplBYXwWQ+XX3guialpu+nrobFTgucvtPKvtzle25//cUuc37fGpbZrE6DbHoBusr74lxp0q43AqZQmbHCTm+QzzcwFig9UznmRoAM2GnLlik+AYTpTK6zJ+i9ZHBU6KQ5s7Vcphw02cdhUDKWg/7KX/wVlxTD3+R/xd+nVpZSGS/CXAL4RzoGmdTVXrur6d+M4jyZpcTLWJlvvGQQUfv4KOWFUrsMSFcgY2WXOC+jo+bbapaWHQZvyW/mUsc2gdGOc2MukbFYAPmOKCWDaNq7YPiFSC36hRi8JY35mbIPZ94mwC+85tp257T5BsnYI1CyzGaR49B7yed7/vtZKuRxSAvZvriWaWFDWqKktAQmP1PNPydE9XAl64wVFLzV1TMbDAN2DxDtH42FYxd6xOAJ1HR5ygAf1HTZ8+jorhEwnNkv0xbm7QHC28pMC/YBFQ2+crFqLpGbUnIxbi6VKeRslsqb9bz78x+f91xZXZ/rjdL01FIhU/xZyivvy2Q7Z/YBMMuLqgJOggzOeN+PwIsxqOqf/vfD0tdX7yV9iT8eIeuGPwCc+BQ4dUS2az0EOLbKdX8PfgaEtAJetY8ZcwIY8JgMTrfgfmDXdzKF4KAnZFaQZu2BwChp7gw4AqjyIgm0ojrK5+rlXXcfYEsF8OENgKUU5ls/wrK0o+77Lc1YJFOb2pkq87x3gEz1Ghjd+Gm6HJYK4IUaxkUxnAJ6/l6mC/1qYt1TjXoHShN3ALjmRiAzTWbW8A2WQTk79waGPCvTCG58H/jl30C3u4G+k4APzxv7pU0H4FjGhdNtCgfG/wi81h41T716UPJffD8goh1wLFUKG46myucFALiE2UkA4Lo3ZLrPvW/UuNp881Is+20n/+9Js+z/91cLBuxERNSkqOZ9gYSB8qTdSKc1g92SnoumVGVAr6QmWVXeQ5332L5O1b6u1n04rbNZHYUNVYUOl7rM4ujLe8HXVs5MUJQDFObI1H6nD7oGVtcMk9rfu+YB85OANje4Tu8X319myfjpGeDHZ+AS+DgPWmeXvRUoOCGPfYKA8gKZVrDrHRKsA8DKf0kwnjZHAra7vgBWTJeazcm/yrKFk4HdS4ARLwNrXpVlN78DtBwg+8jPBObfIwHXyFdkOtOT6QAA4/8NxlCfKOgD0oHr/1avGRAuSlEecGovED+g5hrajR+4Pi+prGUtywf+dytwz1dSA3shNqvMihHdVfKTXi/Ta/7wFND1TqDnfZd9KPXi/Dl7eUueCo6Tz3rT+3ID5DPq/Qdgy2dSs22zSH6zmR3BOiDT1QLy2Y1+HZgzCtj5jdwAmbISkGlu43oC7UZJ32+7gytrT2ur62QWDEDy+2vt6jgwJeMv5O2uviq6q0xFu3ByHa93ojc4BfkAljzmaFli9K82zathVi94d3mnfvsmosvGgJ2IiMiT6HSeOd1kQ7DZpCbR2wTsTwG63yPL248GkjfVXPubmCxTGW6bC6wvB8oV4KMDEk9U33Z/ijSPBoAuEyRgO7waeG+Q63Zpc+S+7JwEbHbbv5QgbfcSef7DVLkvOQV8MgroMFaCwvXvyPSOOenAgRUyDaKTgPKTwK+vAls+le1DW0krj+ytQMdxQFxvCTT9m8nyVoPkGIvzgI43SyHGhtkSfI59C2jeWwoUTGHAZ+Mcg5oNekIKPezB9LFNcqtNbgbwZmd535YDZb77jrdIOvs/BIS0BObeBvhHSMuVta8Dw2YAGYul8MPbHzixXYLS394GYroB1/1VWrgExcqUnmePOAoTivJkXzW1TrCUS0sG56kjbVYJdH2DJXD2MkihASDTP977jZwvLyNwaBWQ+pGkxRQBjJgpI7EPm+G6v9J82ae9WXv6VzJo3IBHgVYDgXHvyHnO3irBvapsHRPfX+5H/UdazeTtkxY7pWdcj+PmWdKqwRQhhTrHUiVfrH9HzgUg3Tta9JWg2lwC3DlXpt3M2iitSI6lSvp8g+W1g6cCHcZI64/yIsnTpnA5F2XngK3/A6CT/Y16Vc7H4sccXUSKTsoNkONc/ZI89o8EinOhg0JkwQ4Ad9aeV+yU0t4I8zar5MfaZjNpDFYz8P1fJR2jXqs+Ber5jm+Rz7zVIO2dz0tlLgPOHpbCL6WAgz/LYLCZ64HDvwI3PCtdSIiDznnCoHMAmxuT9jGPktYxjzZhxaeB/3YDXsoGCpXMw/5EYN2vmbhEgpsv73Us63EvsO0LR1BmNEkAdSFthtZQs1o53ePZw/LUy1ua+FcGmMrgC52lrF6H58IvTIK1LZ/Vb/vAWJnqcf9PjmVBcdIVIGWaNP0GgNs/BRZMrH0/Ya1l1PN1b158mgGpkc7NcDyO6wlsnSvnxN5H3CdIAtTI9lLAUJYPJFwvtebN2gPbPgfOZco+fEOkZUTubgnK+z0EjHzp0tJWH1aLBOY5O6RlSqtB1bdRSmr8s7dKQG8uk24TtSnMkXMS2+PKB5flhVLTfvogsGcpcHKX9IEf86Y8zt4CdLgZ2DAL+PU1HA/pi8iHv4fR+7x+wpkbgZTngIi20mJgzatyPrrfAzTvI5+rlzfw6+uS/8NaS/4rL5RuSoC8X2C0FNaUngVSpkuXjD5/BMLbuL6fzSppDoqVLgE1sVQAa16RIDD8GgkMi/OAtjdKGrvd5doyx+W8FAHnjsnx2Mc8uFxKSSsG+3fU6A90GS+FaPGJlQV0leMfmEukIPHrP8jvTpuh0mooIEryRE2tZJQCDqyUApugONmuvkF+8Wlg8aPAmYPSraPrnUBQzOUdb3mhFCqFt3W0zjmxA/gySVobRXeRPHE8rfprEwYDv3vK0TppfwqQ/rXkjT5/kkJAU0S18+Ap//f1jUMZsDNgJ2oQzKOkdcyjTdyPzwC3z6wesAe3kJrec1lAyVmpNR74mNS2KiW1n+WFUmvdYawEiju/qQwUg6Rm8twxCUqqxhfQSf/ls4eBsDZSe5u7W2rFdy2UC/8bpklAuWKGNIkfMRMIiIL6dCz2BF6Ha+6fBePeJcDmj+SitrCyRYDBVwLDy9H9Xqm52vI/oPyc6zpTuBRWRHUC0j4F0hdIQUW3u2Q8gMz1l/aewfESTLccCBxdd3npvxS3fgB0q0eNMNUtcwPw8XAAgPILhS40QYJdm1W+D/tTLlCIZZ+noQbB8bIPe6uIsNYyGGBZvuO1CddJQUZRrhRk5B8FzhySMUGa95Hg31IGHP2tsuVGnOwj87fak2Twle8pIAUKRpOk8cwhKYCoKAR8giVI9AmQ3wy/0MouQ5XjkeTtkZY/pnApXPDyllvpWRnQz+gvrSD0BuDUPmD34trT4xMMhLaUGvUzB2vfzjtQgt3QVlJTrzdIAJubIYVCdjHdpHCo8AQQ210e6w1yXGUF8tqQeCCouXy/z28FEtJSjrd5bzknVrMsKzkt7+cTKAUgRpMMWGnwk98Xm1Xyg70QDZDPI6Sl7Keo7imHXbS5QdKY9imq5R+fIDkuLx9pLeIfAZu5HL+VtkG/O5/Q9P89A/Z6YsBO1DCYR0nrmEebuFP7gdbtJGAPDwS+mioXup3HN0w/cXMZcHyzBBn+zeTCsSYXaCJcaz4tK5BgxlIhtbh+oXKB7B8hLQF0emkSvXe5NHk+exQY+1+pkcxJl/XHN8tFfq+JEjyYy6TJ/74f5SL9hmmSdt9arofOHpFB1XwCJWAa8jSwfb4EKTu/AU7vBwKipfl29jY5zuytcqF9//dS8BBxLbD/R+DgKuD6J6XZPHRA3l5Jw+jXJU1bPpPPxidIgqHI9sCZw3LcXt7SzaDjzbKf0rPA6QNScOLlLZ9rt7ulYEDvJQFC/4fkmOny2KywfXEPsD8Felhr3iY+EYjpLt09bGbpNpF/FDiW5hq81ZfR5MjHl0wnzftLTst34+g6AEpqqu3N/xvbyFfkuL64R9IS1kbysc1cfdtudwPX/U3GJTiWemXTFd5Waq93fg0c24xaC1guls7L0ToJkO/pbR/KeB4VJVJQuu1zKSAsPiXdVgKjgSPrXF/XZqgUCl2g8HB7i/vR8ff/0fT/PQP2emLATtQwmEdJ65hHCZGhQF4+EBcHHDvm7tTUiPmUtM5sNuOHpYswoncCjMU5UmCi85JClNBWwLUjai+wKsqVJuzhbSQY9G8mBT3XDJPCl6yN0nql1XUyEKPeADTvK7XUpw7IoH9e3kBYAlB4Uvrdt71JArpDq6XQxmaRmmejn9Qql+ZLAULbYU4HUVo5zoFR0nF8szwuyqucDcQqBU2mMKndzVgEQCeFU4XZjnEnvHykBU5EW1lfckrSZDXLmAd+ofLYXCyFbTaLFCJ1GAe0TJR9lJ6VLhw6nWyTt0cKESxlQGiC1F4nXC+vKy8Edn4rXS7MJUDuHilI86lsMWQulkKIhMFyXr28ZdwIb3+ZTSAnXbb1Msq9Ti9B8bljQH6WFGp1vs2xv9MHKwterDLGhCkcCIiUwreASElvWb7sv6JYuuTYzHLOy85JAWZMN6kBt1nl2PIz5fjaj65sHVAo56i2bgmnD0ozeEup7K97kizP2gQ0aydpz97iGEC15DSsOgPWHDdg0ISHNP07ynnYiYiIiJx5+wPId3cqiDyeTW+U0eiNvS7uhQGRcgOA9pUDNrboI/fd7nTtttD6d66vjbhGbrUJa137uvMZ/RyPW/RxpKE2PX9f9/rL4Tw+gcFbBl6rjU9lCxm76C61b2vvL+48noK9kOB8te0nvE31cQMuR4u+cnNmLxyoTXgb4HdTqy+3H4sprNo5s5nNKLiKpnWrYaQCIiIiIiIiInI3BuxEREREREREGsSAnYiIiIiIiEiDmmzAPmvWLHTs2BF9+lygzwoRERERERGRGzTZgD05ORkZGRlITb3CUyMQERGRNvTsCfTvL/dEREQegKPEExERUdOweLG7U0BERHRRmmwNOxEREREREZGWMWAnIiIiIiIi0iAG7EREREREREQaxD7sRERE1DSMGwfk5QHNmrE/OxEReQQG7ERERNQ0bNkCHD8OxMW5OyVERET1wibxRERERERERBrEgJ2IiIiIiIhIgxiwExEREREREWkQA3YiIiIiIiIiDWLATkRERERERKRBDNiJiIiIiIiINIgBOxEREREREZEGNfl52JVSAICCggI3p6RuZrMZJSUlKCgogNFodHdyiKphHiWtYx4l2GyOe43+7zOfktYxj5LWeUoetcef9ni0Nk0+YC8sLAQAtGjRws0pISIiokZx4gQQHOzuVBAREaGwsBDBdfwn6dSFQvqrnM1mQ3Z2NgIDA6HT6dydnFoVFBSgRYsWyMrKQlBQkLuTQ1QN8yhpHfMoeQLmU9I65lHSOk/Jo0opFBYWIjY2Fnp97T3Vm3wNu16vR/Pmzd2djHoLCgrSdMYjYh4lrWMeJU/AfEpaxzxKWucJebSumnU7DjpHREREREREpEEM2ImIiIiIiIg0iAG7h/Dx8cH06dPh4+Pj7qQQ1Yh5lLSOeZQ8AfMpaR3zKGnd1ZZHm/ygc0RERERERERaxBp2IiIiIiIiIg1iwE5ERERERESkQQzYiYiIiIiIiDSIATsRERERERGRBjFg9wCzZs1Cq1at4Ovri379+mHTpk3uThI1ETNnzkSfPn0QGBiIyMhI3HLLLdi7d6/LNmVlZUhOTkZ4eDgCAgIwfvx4nDx50mWbzMxMjB49GiaTCZGRkXjyySdhsVga81CoiXjppZeg0+nw+OOPVy1jHiUtOH78OO69916Eh4fDz88PXbp0webNm6vWK6Xw3HPPISYmBn5+fhg2bBj279/vso8zZ84gKSkJQUFBCAkJwR//+EcUFRU19qHQVchqtWLatGlISEiAn58f2rRpg+effx7OY1Mzj1JjWrNmDcaOHYvY2FjodDosXLjQZX1D5ccdO3bguuuug6+vL1q0aIFXXnnlSh/aRWPArnFffvklnnjiCUyfPh1btmxBt27dMHz4cOTm5ro7adQErF69GsnJydiwYQNSUlJgNptx0003obi4uGqbv/zlL1iyZAkWLFiA1atXIzs7G7fddlvVeqvVitGjR6OiogK//fYbPv30U8yZMwfPPfecOw6JrmKpqal4//330bVrV5flzKPkbmfPnsXAgQNhNBqxfPlyZGRk4LXXXkNoaGjVNq+88greeustvPfee9i4cSP8/f0xfPhwlJWVVW2TlJSEXbt2ISUlBUuXLsWaNWswadIkdxwSXWVefvllzJ49G++88w52796Nl19+Ga+88grefvvtqm2YR6kxFRcXo1u3bpg1a1aN6xsiPxYUFOCmm25Cy5YtkZaWhldffRUzZszABx98cMWP76Io0rS+ffuq5OTkqudWq1XFxsaqmTNnujFV1FTl5uYqAGr16tVKKaXy8/OV0WhUCxYsqNpm9+7dCoBav369UkqpZcuWKb1er3Jycqq2mT17tgoKClLl5eWNewB01SosLFRt27ZVKSkpavDgwWrKlClKKeZR0oapU6eqQYMG1breZrOp6Oho9eqrr1Yty8/PVz4+PuqLL75QSimVkZGhAKjU1NSqbZYvX650Op06fvz4lUs8NQmjR49Wf/jDH1yW3XbbbSopKUkpxTxK7gVAfffdd1XPGyo/vvvuuyo0NNTlv37q1KmqXbt2V/iILg5r2DWsoqICaWlpGDZsWNUyvV6PYcOGYf369W5MGTVV586dAwCEhYUBANLS0mA2m13yaPv27REfH1+VR9evX48uXbogKiqqapvhw4ejoKAAu3btasTU09UsOTkZo0ePdsmLAPMoacPixYvRu3dv3H777YiMjESPHj3w4YcfVq0/fPgwcnJyXPJpcHAw+vXr55JPQ0JC0Lt376pthg0bBr1ej40bNzbewdBVacCAAVi5ciX27dsHANi+fTvWrl2LkSNHAmAeJW1pqPy4fv16XH/99fD29q7aZvjw4di7dy/Onj3bSEdzYQZ3J4Bqd+rUKVitVpeLSACIiorCnj173JQqaqpsNhsef/xxDBw4EJ07dwYA5OTkwNvbGyEhIS7bRkVFIScnp2qbmvKwfR3R5Zo/fz62bNmC1NTUauuYR0kLDh06hNmzZ+OJJ57A008/jdTUVDz22GPw9vbGxIkTq/JZTfnQOZ9GRka6rDcYDAgLC2M+pcv21FNPoaCgAO3bt4eXlxesVitefPFFJCUlAQDzKGlKQ+XHnJwcJCQkVNuHfZ1ztyV3YsBORPWSnJyMnTt3Yu3ate5OClGVrKwsTJkyBSkpKfD19XV3cohqZLPZ0Lt3b/z73/8GAPTo0QM7d+7Ee++9h4kTJ7o5dUTAV199hXnz5uHzzz9Hp06dsG3bNjz++OOIjY1lHiVyMzaJ17CIiAh4eXlVG8345MmTiI6OdlOqqCl65JFHsHTpUqxatQrNmzevWh4dHY2Kigrk5+e7bO+cR6Ojo2vMw/Z1RJcjLS0Nubm56NmzJwwGAwwGA1avXo233noLBoMBUVFRzKPkdjExMejYsaPLsg4dOiAzMxOAI5/V9X8fHR1dbcBZi8WCM2fOMJ/SZXvyySfx1FNP4a677kKXLl1w33334S9/+QtmzpwJgHmUtKWh8qOn/P8zYNcwb29v9OrVCytXrqxaZrPZsHLlSiQmJroxZdRUKKXwyCOP4LvvvsPPP/9crdlQr169YDQaXfLo3r17kZmZWZVHExMTkZ6e7vKjmZKSgqCgoGoXsEQXa+jQoUhPT8e2bduqbr1790ZSUlLVY+ZRcreBAwdWmxJz3759aNmyJQAgISEB0dHRLvm0oKAAGzdudMmn+fn5SEtLq9rm559/hs1mQ79+/RrhKOhqVlJSAr3eNSzw8vKCzWYDwDxK2tJQ+TExMRFr1qyB2Wyu2iYlJQXt2rXTTHN4ABwlXuvmz5+vfHx81Jw5c1RGRoaaNGmSCgkJcRnNmOhKeeihh1RwcLD65Zdf1IkTJ6puJSUlVdtMnjxZxcfHq59//llt3rxZJSYmqsTExKr1FotFde7cWd10001q27Zt6ocfflDNmjVT//jHP9xxSNQEOI8SrxTzKLnfpk2blMFgUC+++KLav3+/mjdvnjKZTGru3LlV27z00ksqJCRELVq0SO3YsUPdfPPNKiEhQZWWllZtM2LECNWjRw+1ceNGtXbtWtW2bVt19913u+OQ6CozceJEFRcXp5YuXaoOHz6svv32WxUREaH+/ve/V23DPEqNqbCwUG3dulVt3bpVAVCvv/662rp1qzp69KhSqmHyY35+voqKilL33Xef2rlzp5o/f74ymUzq/fffb/TjrQsDdg/w9ttvq/j4eOXt7a369u2rNmzY4O4kURMBoMbbJ598UrVNaWmpevjhh1VoaKgymUzq1ltvVSdOnHDZz5EjR9TIkSOVn5+fioiIUH/961+V2Wxu5KOhpuL8gJ15lLRgyZIlqnPnzsrHx0e1b99effDBBy7rbTabmjZtmoqKilI+Pj5q6NChau/evS7bnD59Wt19990qICBABQUFqQceeEAVFhY25mHQVaqgoEBNmTJFxcfHK19fX9W6dWv1zDPPuEx3xTxKjWnVqlU1XoNOnDhRKdVw+XH79u1q0KBBysfHR8XFxamXXnqpsQ6x3nRKKeWeun0iIiIiIiIiqg37sBMRERERERFpEAN2IiIiIiIiIg1iwE5ERERERESkQQzYiYiIiIiIiDSIATsRERERERGRBjFgJyIiIiIiItIgBuxEREREREREGsSAnYiIiIiIiEiDGLATERFRo9LpdFi4cKG7k0FERKR5DNiJiIiakPvvvx86na7abcSIEe5OGhEREZ3H4O4EEBERUeMaMWIEPvnkE5dlPj4+bkoNERER1YY17ERERE2Mj48PoqOjXW6hoaEApLn67NmzMXLkSPj5+aF169b4+uuvXV6fnp6OG264AX5+fggPD8ekSZNQVFTkss3HH3+MTp06wcfHBzExMXjkkUdc1p86dQq33norTCYT2rZti8WLF1/ZgyYiIvJADNiJiIjIxbRp0zB+/Hhs374dSUlJuOuuu7B7924AQHFxMYYPH47Q0FCkpqZiwYIFWLFihUtAPnv2bCQnJ2PSpElIT0/H4sWLcc0117i8xz//+U/ccccd2LFjB0aNGoWkpCScOXOmUY+TiIhI63RKKeXuRBAREVHjuP/++zF37lz4+vq6LH/66afx9NNPQ6fTYfLkyZg9e3bVuv79+6Nnz55499138eGHH2Lq1KnIysqCv78/AGDZsmUYO3YssrOzERUVhbi4ODzwwAN44YUXakyDTqfDs88+i+effx6AFAIEBARg+fLl7EtPRETkhH3YiYiImpghQ4a4BOQAEBYWVvU4MTHRZV1iYiK2bdsGANi9eze6detWFawDwMCBA2Gz2bB3717odDpkZ2dj6NChdaaha9euVY/9/f0RFBSE3NzcSz0kIiKiqxIDdiIioibG39+/WhP1huLn51ev7YxGo8tznU4Hm812JZJERETksdiHnYiIiFxs2LCh2vMOHToAADp06IDt27ejuLi4av26deug1+vRrl07BAYGolWrVli5cmWjppmIiOhqxBp2IiKiJqa8vBw5OTkuywwGAyIiIgAACxYsQO/evTFo0CDMmzcPmzZtwkcffQQASEpKwvTp0zFx4kTMmDEDeXl5ePTRR3HfffchKioKADBjxgxMnjwZkZGRGDlyJAoLC7Fu3To8+uijjXugREREHo4BOxERURPzww8/ICYmxmVZu3btsGfPHgAygvv8+fPx8MMPIyYmBl988QU6duwIADCZTPjxxx8xZcoU9OnTByaTCePHj8frr79eta+JEyeirKwMb7zxBv72t78hIiICEyZMaLwDJCIiukpwlHgiIiKqotPp8N133+GWW25xd1KIiIiaPPZhJyIiIiIiItIgBuxEREREREREGsQ+7ERERFSFPeWIiIi0gzXsRERERERERBrEgJ2IiIiIiIhIgxiwExEREREREWkQA3YiIiIiIiIiDWLATkRERERERKRBDNiJiIiIiIiINIgBOxEREREREZEGMWAnIiIiIiIi0qD/B6KAvE4gMXNTAAAAAElFTkSuQmCC\n"},"metadata":{}},{"name":"stdout","text":"Test RMSE :  1.6645255088806152\nCutoff SoH :  0.7\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-8-hust_gompertz_params.csv\n X_['train'] shape : torch.Size([14, 100, 1]) , y_['train'] shape : torch.Size([14, 3]) Ôºåy_2['train'] shape: torch.Size([14, 1])\nload : \n['train']loader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-5-hust_gompertz_params.csv\n X_['val'] shape : torch.Size([2, 100, 1]) , y_['val'] shape : torch.Size([2, 3]) Ôºåy_2['val'] shape: torch.Size([2, 1])\nload : \n['val']loader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\n X_['test'] shape : torch.Size([5, 100, 1]) , y_['test'] shape : torch.Size([5, 3]) Ôºåy_2['test'] shape: torch.Size([5, 1])\nload : \n['test']loader lengths :  1\n## üß† Model\nLast model window :  last_model_window_100_model_pinn_data_low.pth\nüöÄ Initializing model output to: k=0.960474967956543, a=-3.9311540126800537, b=-19.54881477355957\n‚úÖ Model Output Parameters Initialized!\n##\n        ### üìà Gompertz Function (Physics Law)\n        \n        * `x`: Time (or cycle number)\n        \n        * `k`: Max value (e.g., max capacity)\n        \n        * `a`, `b`: Shape parameters\n## üß† Loss Functions\n\n## ‚öôÔ∏è 1. Data-Informed Loss Function\n        a data loss (what the LSTM learns from data)\n        \n        * Mean Squared Error for Training\n        * RMSE for autoregressive approximation of compound error\n        \n        ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n        You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n        \n        * `alpha`: controls how strongly physics is enforced.\n## üõ†Ô∏è Parameter Strategy\n## üîÅ Training Loop\n‚úÖ Saved best model at epoch 1 (Val Loss = 1.55792487)\nEpoch 1/1000 | Train Loss=40775.60546875 | Val Loss=1.55792487 | Data=407.73483276 | Physics=2.11960579 | Val RMSE: 2.12619686 | ‚àö(Val Loss) = 1.24816859 | Current Learning Rate: 0.002\n\n Epoch :  0 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9726409   -3.941472   -19.5447    ]\n [  0.9725745   -3.941424   -19.544798  ]\n [  0.97257125  -3.9414215  -19.544802  ]\n [  0.9726391   -3.9414709  -19.544703  ]\n [  0.97256345  -3.941416   -19.544813  ]] \n\nFinal Test RMSE:  1.2683526277542114\n‚úÖ Saved best model at epoch 2 (Val Loss = 1.33316183)\nEpoch 2/1000 | Train Loss=40774.52343750 | Val Loss=1.33316183 | Data=407.72363281 | Physics=2.15952898 | Val RMSE: 2.12593985 | ‚àö(Val Loss) = 1.15462625 | Current Learning Rate: 0.002\n‚úÖ Saved best model at epoch 3 (Val Loss = 1.18473661)\nEpoch 3/1000 | Train Loss=40773.25781250 | Val Loss=1.18473661 | Data=407.71136475 | Physics=2.12160818 | Val RMSE: 2.12572455 | ‚àö(Val Loss) = 1.08845603 | Current Learning Rate: 0.002\n‚úÖ Saved best model at epoch 4 (Val Loss = 1.14051044)\nEpoch 4/1000 | Train Loss=40771.62109375 | Val Loss=1.14051044 | Data=407.69500732 | Physics=2.12202658 | Val RMSE: 2.12548614 | ‚àö(Val Loss) = 1.06794679 | Current Learning Rate: 0.002\n‚úÖ Saved best model at epoch 5 (Val Loss = 1.13741255)\nEpoch 5/1000 | Train Loss=40769.28125000 | Val Loss=1.13741255 | Data=407.67141724 | Physics=2.14092227 | Val RMSE: 2.12515330 | ‚àö(Val Loss) = 1.06649542 | Current Learning Rate: 0.002\nEpoch 6/1000 | Train Loss=40765.65234375 | Val Loss=1.15562725 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12460232 | ‚àö(Val Loss) = 1.07500100 | Current Learning Rate: 0.002\nEpoch 7/1000 | Train Loss=40765.65234375 | Val Loss=1.17627907 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12460876 | ‚àö(Val Loss) = 1.08456397 | Current Learning Rate: 0.002\nEpoch 8/1000 | Train Loss=40765.64453125 | Val Loss=1.19618821 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12461352 | ‚àö(Val Loss) = 1.09370387 | Current Learning Rate: 0.002\nEpoch 9/1000 | Train Loss=40765.65234375 | Val Loss=1.21455920 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12461782 | ‚àö(Val Loss) = 1.10207045 | Current Learning Rate: 0.002\nEpoch 10/1000 | Train Loss=40765.65234375 | Val Loss=1.23118222 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12462115 | ‚àö(Val Loss) = 1.10958648 | Current Learning Rate: 0.002\nEpoch 11/1000 | Train Loss=40765.65234375 | Val Loss=1.24609625 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12462401 | ‚àö(Val Loss) = 1.11628687 | Current Learning Rate: 0.002\nEpoch 12/1000 | Train Loss=40765.64453125 | Val Loss=1.25942385 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12462592 | ‚àö(Val Loss) = 1.12224054 | Current Learning Rate: 0.002\nEpoch 13/1000 | Train Loss=40765.64453125 | Val Loss=1.27132130 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12462807 | ‚àö(Val Loss) = 1.12752879 | Current Learning Rate: 0.002\nEpoch 14/1000 | Train Loss=40765.65234375 | Val Loss=1.28194571 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463021 | ‚àö(Val Loss) = 1.13223040 | Current Learning Rate: 0.002\nEpoch 15/1000 | Train Loss=40765.64453125 | Val Loss=1.29143918 | Data=407.63504028 | Physics=2.14256047 | Val RMSE: 2.12463188 | ‚àö(Val Loss) = 1.13641500 | Current Learning Rate: 0.002\nEpoch 16/1000 | Train Loss=40765.64453125 | Val Loss=1.29993832 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463307 | ‚àö(Val Loss) = 1.14014840 | Current Learning Rate: 0.002\nEpoch 17/1000 | Train Loss=40765.64453125 | Val Loss=1.30755603 | Data=407.63504028 | Physics=2.14256047 | Val RMSE: 2.12463355 | ‚àö(Val Loss) = 1.14348412 | Current Learning Rate: 0.002\nEpoch 18/1000 | Train Loss=40765.65234375 | Val Loss=1.31439686 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463522 | ‚àö(Val Loss) = 1.14647150 | Current Learning Rate: 0.002\nEpoch 19/1000 | Train Loss=40765.65234375 | Val Loss=1.32055116 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463522 | ‚àö(Val Loss) = 1.14915240 | Current Learning Rate: 0.002\nEpoch 20/1000 | Train Loss=40765.64453125 | Val Loss=1.32609570 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463617 | ‚àö(Val Loss) = 1.15156233 | Current Learning Rate: 0.002\nEpoch 21/1000 | Train Loss=40765.65234375 | Val Loss=1.33109820 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463665 | ‚àö(Val Loss) = 1.15373230 | Current Learning Rate: 0.002\n\n Epoch :  20 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 22/1000 | Train Loss=40765.64453125 | Val Loss=1.33562124 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463713 | ‚àö(Val Loss) = 1.15569079 | Current Learning Rate: 0.002\nEpoch 23/1000 | Train Loss=40765.65234375 | Val Loss=1.33971620 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463737 | ‚àö(Val Loss) = 1.15746105 | Current Learning Rate: 0.002\nEpoch 24/1000 | Train Loss=40765.64453125 | Val Loss=1.34342861 | Data=407.63504028 | Physics=2.14256047 | Val RMSE: 2.12463760 | ‚àö(Val Loss) = 1.15906370 | Current Learning Rate: 0.002\nEpoch 25/1000 | Train Loss=40765.65234375 | Val Loss=1.34679949 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463760 | ‚àö(Val Loss) = 1.16051686 | Current Learning Rate: 0.002\nEpoch 26/1000 | Train Loss=40765.65234375 | Val Loss=1.34986663 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463784 | ‚àö(Val Loss) = 1.16183758 | Current Learning Rate: 0.002\nEpoch 27/1000 | Train Loss=40765.65234375 | Val Loss=1.35265732 | Data=407.63510132 | Physics=2.14256047 | Val RMSE: 2.12463713 | ‚àö(Val Loss) = 1.16303802 | Current Learning Rate: 0.002\nEpoch 28/1000 | Train Loss=40765.64453125 | Val Loss=1.35520470 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463760 | ‚àö(Val Loss) = 1.16413260 | Current Learning Rate: 0.002\nEpoch 29/1000 | Train Loss=40765.64453125 | Val Loss=1.35753131 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463760 | ‚àö(Val Loss) = 1.16513145 | Current Learning Rate: 0.002\nEpoch 30/1000 | Train Loss=40765.64453125 | Val Loss=1.35965574 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463665 | ‚àö(Val Loss) = 1.16604280 | Current Learning Rate: 0.002\nEpoch 31/1000 | Train Loss=40765.65234375 | Val Loss=1.36160290 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463689 | ‚àö(Val Loss) = 1.16687739 | Current Learning Rate: 0.002\nEpoch 32/1000 | Train Loss=40765.64453125 | Val Loss=1.36338842 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463665 | ‚àö(Val Loss) = 1.16764224 | Current Learning Rate: 0.002\nEpoch 33/1000 | Train Loss=40765.64453125 | Val Loss=1.36502790 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463570 | ‚àö(Val Loss) = 1.16834414 | Current Learning Rate: 0.002\nEpoch 34/1000 | Train Loss=40765.64453125 | Val Loss=1.36653554 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463593 | ‚àö(Val Loss) = 1.16898906 | Current Learning Rate: 0.002\nEpoch 35/1000 | Train Loss=40765.64453125 | Val Loss=1.36792147 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463570 | ‚àö(Val Loss) = 1.16958177 | Current Learning Rate: 0.002\nEpoch 36/1000 | Train Loss=40765.65234375 | Val Loss=1.36920142 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463522 | ‚àö(Val Loss) = 1.17012882 | Current Learning Rate: 0.002\nEpoch 37/1000 | Train Loss=40765.65234375 | Val Loss=1.37038183 | Data=407.63510132 | Physics=2.14256047 | Val RMSE: 2.12463498 | ‚àö(Val Loss) = 1.17063308 | Current Learning Rate: 0.002\nEpoch 38/1000 | Train Loss=40765.65234375 | Val Loss=1.37147439 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463450 | ‚àö(Val Loss) = 1.17109966 | Current Learning Rate: 0.002\nEpoch 39/1000 | Train Loss=40765.64453125 | Val Loss=1.37248743 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463403 | ‚àö(Val Loss) = 1.17153203 | Current Learning Rate: 0.002\nEpoch 40/1000 | Train Loss=40765.65234375 | Val Loss=1.37342429 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463355 | ‚àö(Val Loss) = 1.17193186 | Current Learning Rate: 0.002\nEpoch 41/1000 | Train Loss=40765.65234375 | Val Loss=1.37429500 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463307 | ‚àö(Val Loss) = 1.17230332 | Current Learning Rate: 0.002\n\n Epoch :  40 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 42/1000 | Train Loss=40765.64453125 | Val Loss=1.37510467 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463284 | ‚àö(Val Loss) = 1.17264855 | Current Learning Rate: 0.002\nEpoch 43/1000 | Train Loss=40765.65234375 | Val Loss=1.37585866 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463212 | ‚àö(Val Loss) = 1.17297006 | Current Learning Rate: 0.002\nEpoch 44/1000 | Train Loss=40765.65234375 | Val Loss=1.37656057 | Data=407.63510132 | Physics=2.14256047 | Val RMSE: 2.12463164 | ‚àö(Val Loss) = 1.17326915 | Current Learning Rate: 0.002\nEpoch 45/1000 | Train Loss=40765.65234375 | Val Loss=1.37721741 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12463140 | ‚àö(Val Loss) = 1.17354906 | Current Learning Rate: 0.002\nEpoch 46/1000 | Train Loss=40765.64453125 | Val Loss=1.37783217 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463093 | ‚àö(Val Loss) = 1.17381096 | Current Learning Rate: 0.002\nEpoch 47/1000 | Train Loss=40765.64453125 | Val Loss=1.37840807 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463045 | ‚àö(Val Loss) = 1.17405629 | Current Learning Rate: 0.002\nEpoch 48/1000 | Train Loss=40765.64453125 | Val Loss=1.37894845 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12463045 | ‚àö(Val Loss) = 1.17428637 | Current Learning Rate: 0.002\nEpoch 49/1000 | Train Loss=40765.65234375 | Val Loss=1.37945509 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12462950 | ‚àö(Val Loss) = 1.17450202 | Current Learning Rate: 0.002\nEpoch 50/1000 | Train Loss=40765.64453125 | Val Loss=1.37993479 | Data=407.63504028 | Physics=2.14256023 | Val RMSE: 2.12462902 | ‚àö(Val Loss) = 1.17470622 | Current Learning Rate: 0.002\nEpoch 51/1000 | Train Loss=40765.65234375 | Val Loss=1.38038385 | Data=407.63510132 | Physics=2.14256023 | Val RMSE: 2.12462926 | ‚àö(Val Loss) = 1.17489743 | Current Learning Rate: 0.002\nEpoch 52/1000 | Train Loss=40818.05468750 | Val Loss=1.38049901 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12462544 | ‚àö(Val Loss) = 1.17494643 | Current Learning Rate: 0.002\nEpoch 53/1000 | Train Loss=40818.05468750 | Val Loss=1.38070917 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12462306 | ‚àö(Val Loss) = 1.17503583 | Current Learning Rate: 0.002\nEpoch 54/1000 | Train Loss=40818.05468750 | Val Loss=1.38096631 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12462091 | ‚àö(Val Loss) = 1.17514527 | Current Learning Rate: 0.002\nEpoch 55/1000 | Train Loss=40818.05468750 | Val Loss=1.38124573 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461877 | ‚àö(Val Loss) = 1.17526412 | Current Learning Rate: 0.002\nEpoch 56/1000 | Train Loss=40818.05468750 | Val Loss=1.38153398 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461758 | ‚àö(Val Loss) = 1.17538679 | Current Learning Rate: 0.002\nEpoch 57/1000 | Train Loss=40818.05468750 | Val Loss=1.38182545 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12461686 | ‚àö(Val Loss) = 1.17551076 | Current Learning Rate: 0.002\nEpoch 58/1000 | Train Loss=40818.05468750 | Val Loss=1.38211250 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12461543 | ‚àö(Val Loss) = 1.17563283 | Current Learning Rate: 0.002\nEpoch 59/1000 | Train Loss=40818.05468750 | Val Loss=1.38239312 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461448 | ‚àö(Val Loss) = 1.17575216 | Current Learning Rate: 0.002\nEpoch 60/1000 | Train Loss=40818.05468750 | Val Loss=1.38266778 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461376 | ‚àö(Val Loss) = 1.17586899 | Current Learning Rate: 0.002\nEpoch 61/1000 | Train Loss=40818.05468750 | Val Loss=1.38293207 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461376 | ‚àö(Val Loss) = 1.17598128 | Current Learning Rate: 0.002\n\n Epoch :  60 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 62/1000 | Train Loss=40818.05468750 | Val Loss=1.38319004 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461233 | ‚àö(Val Loss) = 1.17609096 | Current Learning Rate: 0.002\nEpoch 63/1000 | Train Loss=40818.05468750 | Val Loss=1.38343894 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461233 | ‚àö(Val Loss) = 1.17619681 | Current Learning Rate: 0.002\nEpoch 64/1000 | Train Loss=40818.05468750 | Val Loss=1.38367903 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461209 | ‚àö(Val Loss) = 1.17629886 | Current Learning Rate: 0.002\nEpoch 65/1000 | Train Loss=40818.05468750 | Val Loss=1.38390946 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461162 | ‚àö(Val Loss) = 1.17639685 | Current Learning Rate: 0.002\nEpoch 66/1000 | Train Loss=40818.05468750 | Val Loss=1.38413465 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461090 | ‚àö(Val Loss) = 1.17649257 | Current Learning Rate: 0.002\nEpoch 67/1000 | Train Loss=40818.05468750 | Val Loss=1.38435161 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12461066 | ‚àö(Val Loss) = 1.17658472 | Current Learning Rate: 0.002\nEpoch 68/1000 | Train Loss=40818.05468750 | Val Loss=1.38455963 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12461019 | ‚àö(Val Loss) = 1.17667317 | Current Learning Rate: 0.002\nEpoch 69/1000 | Train Loss=40818.05468750 | Val Loss=1.38476193 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460923 | ‚àö(Val Loss) = 1.17675912 | Current Learning Rate: 0.002\nEpoch 70/1000 | Train Loss=40818.05468750 | Val Loss=1.38495910 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12460923 | ‚àö(Val Loss) = 1.17684281 | Current Learning Rate: 0.002\nEpoch 71/1000 | Train Loss=40818.05468750 | Val Loss=1.38515043 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12460899 | ‚àö(Val Loss) = 1.17692411 | Current Learning Rate: 0.002\nEpoch 72/1000 | Train Loss=40818.05468750 | Val Loss=1.38533497 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12460876 | ‚àö(Val Loss) = 1.17700255 | Current Learning Rate: 0.002\nEpoch 73/1000 | Train Loss=40818.05468750 | Val Loss=1.38551450 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460852 | ‚àö(Val Loss) = 1.17707884 | Current Learning Rate: 0.002\nEpoch 74/1000 | Train Loss=40818.05468750 | Val Loss=1.38568795 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460756 | ‚àö(Val Loss) = 1.17715251 | Current Learning Rate: 0.002\nEpoch 75/1000 | Train Loss=40818.05468750 | Val Loss=1.38585651 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12460780 | ‚àö(Val Loss) = 1.17722404 | Current Learning Rate: 0.002\nEpoch 76/1000 | Train Loss=40818.05468750 | Val Loss=1.38602340 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12460756 | ‚àö(Val Loss) = 1.17729497 | Current Learning Rate: 0.002\nEpoch 77/1000 | Train Loss=40818.05468750 | Val Loss=1.38618290 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460732 | ‚àö(Val Loss) = 1.17736268 | Current Learning Rate: 0.002\nEpoch 78/1000 | Train Loss=40818.05468750 | Val Loss=1.38633883 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460709 | ‚àö(Val Loss) = 1.17742896 | Current Learning Rate: 0.002\nEpoch 79/1000 | Train Loss=40818.05468750 | Val Loss=1.38649237 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460685 | ‚àö(Val Loss) = 1.17749405 | Current Learning Rate: 0.002\nEpoch 80/1000 | Train Loss=40818.05468750 | Val Loss=1.38664103 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460661 | ‚àö(Val Loss) = 1.17755723 | Current Learning Rate: 0.002\nEpoch 81/1000 | Train Loss=40818.05468750 | Val Loss=1.38678467 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460637 | ‚àö(Val Loss) = 1.17761827 | Current Learning Rate: 0.002\n\n Epoch :  80 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 82/1000 | Train Loss=40818.05468750 | Val Loss=1.38692558 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460613 | ‚àö(Val Loss) = 1.17767799 | Current Learning Rate: 0.002\nEpoch 83/1000 | Train Loss=40818.05468750 | Val Loss=1.38706446 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460589 | ‚àö(Val Loss) = 1.17773700 | Current Learning Rate: 0.002\nEpoch 84/1000 | Train Loss=40818.05468750 | Val Loss=1.38719881 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460589 | ‚àö(Val Loss) = 1.17779410 | Current Learning Rate: 0.002\nEpoch 85/1000 | Train Loss=40818.05468750 | Val Loss=1.38733184 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460613 | ‚àö(Val Loss) = 1.17785048 | Current Learning Rate: 0.002\nEpoch 86/1000 | Train Loss=40818.05468750 | Val Loss=1.38745892 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460542 | ‚àö(Val Loss) = 1.17790449 | Current Learning Rate: 0.002\nEpoch 87/1000 | Train Loss=40818.05468750 | Val Loss=1.38758695 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12460542 | ‚àö(Val Loss) = 1.17795885 | Current Learning Rate: 0.002\nEpoch 88/1000 | Train Loss=40818.05468750 | Val Loss=1.38771093 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460494 | ‚àö(Val Loss) = 1.17801142 | Current Learning Rate: 0.002\nEpoch 89/1000 | Train Loss=40818.05468750 | Val Loss=1.38783026 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460494 | ‚àö(Val Loss) = 1.17806208 | Current Learning Rate: 0.002\nEpoch 90/1000 | Train Loss=40818.05468750 | Val Loss=1.38794792 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460470 | ‚àö(Val Loss) = 1.17811203 | Current Learning Rate: 0.002\nEpoch 91/1000 | Train Loss=40818.05468750 | Val Loss=1.38806391 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460446 | ‚àö(Val Loss) = 1.17816126 | Current Learning Rate: 0.002\nEpoch 92/1000 | Train Loss=40818.05468750 | Val Loss=1.38817883 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460446 | ‚àö(Val Loss) = 1.17821002 | Current Learning Rate: 0.002\nEpoch 93/1000 | Train Loss=40818.05859375 | Val Loss=1.38828981 | Data=408.15917969 | Physics=2.14256023 | Val RMSE: 2.12460494 | ‚àö(Val Loss) = 1.17825711 | Current Learning Rate: 0.002\nEpoch 94/1000 | Train Loss=40818.05468750 | Val Loss=1.38839841 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460423 | ‚àö(Val Loss) = 1.17830324 | Current Learning Rate: 0.002\nEpoch 95/1000 | Train Loss=40818.05468750 | Val Loss=1.38850594 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460399 | ‚àö(Val Loss) = 1.17834878 | Current Learning Rate: 0.002\nEpoch 96/1000 | Train Loss=40818.05468750 | Val Loss=1.38860881 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460375 | ‚àö(Val Loss) = 1.17839241 | Current Learning Rate: 0.002\nEpoch 97/1000 | Train Loss=40818.05468750 | Val Loss=1.38871348 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12460423 | ‚àö(Val Loss) = 1.17843688 | Current Learning Rate: 0.002\nEpoch 98/1000 | Train Loss=40818.05468750 | Val Loss=1.38881290 | Data=408.15914917 | Physics=2.14256023 | Val RMSE: 2.12460351 | ‚àö(Val Loss) = 1.17847908 | Current Learning Rate: 0.002\nEpoch 99/1000 | Train Loss=40818.05468750 | Val Loss=1.38891077 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12460327 | ‚àö(Val Loss) = 1.17852056 | Current Learning Rate: 0.002\nEpoch 100/1000 | Train Loss=40818.05468750 | Val Loss=1.38900793 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12460327 | ‚àö(Val Loss) = 1.17856181 | Current Learning Rate: 0.002\nEpoch 101/1000 | Train Loss=40818.05468750 | Val Loss=1.38910472 | Data=408.15914917 | Physics=2.14256047 | Val RMSE: 2.12460327 | ‚àö(Val Loss) = 1.17860281 | Current Learning Rate: 0.002\n\n Epoch :  100 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 102/1000 | Train Loss=40818.06640625 | Val Loss=1.39134943 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460303 | ‚àö(Val Loss) = 1.17955470 | Current Learning Rate: 0.002\nEpoch 103/1000 | Train Loss=40818.06640625 | Val Loss=1.39336586 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460303 | ‚àö(Val Loss) = 1.18040919 | Current Learning Rate: 0.002\nEpoch 104/1000 | Train Loss=40818.06640625 | Val Loss=1.39517534 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460303 | ‚àö(Val Loss) = 1.18117535 | Current Learning Rate: 0.002\nEpoch 105/1000 | Train Loss=40818.06640625 | Val Loss=1.39679205 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460279 | ‚àö(Val Loss) = 1.18185961 | Current Learning Rate: 0.002\nEpoch 106/1000 | Train Loss=40818.06640625 | Val Loss=1.39824104 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460256 | ‚àö(Val Loss) = 1.18247247 | Current Learning Rate: 0.002\nEpoch 107/1000 | Train Loss=40818.06640625 | Val Loss=1.39953768 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460303 | ‚àö(Val Loss) = 1.18302059 | Current Learning Rate: 0.002\nEpoch 108/1000 | Train Loss=40818.06640625 | Val Loss=1.40069282 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460256 | ‚àö(Val Loss) = 1.18350863 | Current Learning Rate: 0.002\nEpoch 109/1000 | Train Loss=40818.06640625 | Val Loss=1.40172291 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460256 | ‚àö(Val Loss) = 1.18394375 | Current Learning Rate: 0.002\nEpoch 110/1000 | Train Loss=40818.06640625 | Val Loss=1.40263891 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460279 | ‚àö(Val Loss) = 1.18433058 | Current Learning Rate: 0.002\nEpoch 111/1000 | Train Loss=40818.06640625 | Val Loss=1.40345478 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12460232 | ‚àö(Val Loss) = 1.18467498 | Current Learning Rate: 0.002\nEpoch 112/1000 | Train Loss=40818.06640625 | Val Loss=1.40417695 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12460208 | ‚àö(Val Loss) = 1.18497968 | Current Learning Rate: 0.002\nEpoch 113/1000 | Train Loss=40818.06640625 | Val Loss=1.40481925 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460208 | ‚àö(Val Loss) = 1.18525076 | Current Learning Rate: 0.002\nEpoch 114/1000 | Train Loss=40818.06640625 | Val Loss=1.40538585 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460208 | ‚àö(Val Loss) = 1.18548965 | Current Learning Rate: 0.002\nEpoch 115/1000 | Train Loss=40818.06640625 | Val Loss=1.40588748 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460184 | ‚àö(Val Loss) = 1.18570125 | Current Learning Rate: 0.002\nEpoch 116/1000 | Train Loss=40818.06640625 | Val Loss=1.40632641 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460232 | ‚àö(Val Loss) = 1.18588638 | Current Learning Rate: 0.002\nEpoch 117/1000 | Train Loss=40818.06640625 | Val Loss=1.40671134 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460208 | ‚àö(Val Loss) = 1.18604863 | Current Learning Rate: 0.002\nEpoch 118/1000 | Train Loss=40818.06640625 | Val Loss=1.40704954 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12460208 | ‚àö(Val Loss) = 1.18619120 | Current Learning Rate: 0.002\nEpoch 119/1000 | Train Loss=40818.06640625 | Val Loss=1.40734339 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460184 | ‚àö(Val Loss) = 1.18631506 | Current Learning Rate: 0.002\nEpoch 120/1000 | Train Loss=40818.06640625 | Val Loss=1.40760028 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12460184 | ‚àö(Val Loss) = 1.18642330 | Current Learning Rate: 0.002\nEpoch 121/1000 | Train Loss=40818.06640625 | Val Loss=1.40781963 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460136 | ‚àö(Val Loss) = 1.18651581 | Current Learning Rate: 0.002\n\n Epoch :  120 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 122/1000 | Train Loss=40818.06640625 | Val Loss=1.40800929 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460113 | ‚àö(Val Loss) = 1.18659568 | Current Learning Rate: 0.002\nEpoch 123/1000 | Train Loss=40818.06640625 | Val Loss=1.40817118 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460113 | ‚àö(Val Loss) = 1.18666387 | Current Learning Rate: 0.002\nEpoch 124/1000 | Train Loss=40818.06640625 | Val Loss=1.40830481 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12460089 | ‚àö(Val Loss) = 1.18672013 | Current Learning Rate: 0.002\nEpoch 125/1000 | Train Loss=40818.06640625 | Val Loss=1.40841770 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460089 | ‚àö(Val Loss) = 1.18676770 | Current Learning Rate: 0.002\nEpoch 126/1000 | Train Loss=40818.06640625 | Val Loss=1.40851474 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460136 | ‚àö(Val Loss) = 1.18680859 | Current Learning Rate: 0.002\nEpoch 127/1000 | Train Loss=40818.06640625 | Val Loss=1.40858841 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460136 | ‚àö(Val Loss) = 1.18683970 | Current Learning Rate: 0.002\nEpoch 128/1000 | Train Loss=40818.06640625 | Val Loss=1.40864635 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12460065 | ‚àö(Val Loss) = 1.18686414 | Current Learning Rate: 0.002\nEpoch 129/1000 | Train Loss=40818.06640625 | Val Loss=1.40869343 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12460065 | ‚àö(Val Loss) = 1.18688393 | Current Learning Rate: 0.002\nEpoch 130/1000 | Train Loss=40818.06640625 | Val Loss=1.40872478 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460041 | ‚àö(Val Loss) = 1.18689716 | Current Learning Rate: 0.002\nEpoch 131/1000 | Train Loss=40818.06640625 | Val Loss=1.40874410 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460089 | ‚àö(Val Loss) = 1.18690526 | Current Learning Rate: 0.002\nEpoch 132/1000 | Train Loss=40818.06640625 | Val Loss=1.40875638 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12460017 | ‚àö(Val Loss) = 1.18691039 | Current Learning Rate: 0.002\nEpoch 133/1000 | Train Loss=40818.06640625 | Val Loss=1.40875769 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12460017 | ‚àö(Val Loss) = 1.18691099 | Current Learning Rate: 0.002\nEpoch 134/1000 | Train Loss=40818.06640625 | Val Loss=1.40875173 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460065 | ‚àö(Val Loss) = 1.18690848 | Current Learning Rate: 0.002\nEpoch 135/1000 | Train Loss=40818.06640625 | Val Loss=1.40874004 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460041 | ‚àö(Val Loss) = 1.18690360 | Current Learning Rate: 0.002\nEpoch 136/1000 | Train Loss=40818.06640625 | Val Loss=1.40872025 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459993 | ‚àö(Val Loss) = 1.18689525 | Current Learning Rate: 0.002\nEpoch 137/1000 | Train Loss=40818.06640625 | Val Loss=1.40869606 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459970 | ‚àö(Val Loss) = 1.18688500 | Current Learning Rate: 0.002\nEpoch 138/1000 | Train Loss=40818.06640625 | Val Loss=1.40866470 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460041 | ‚àö(Val Loss) = 1.18687177 | Current Learning Rate: 0.002\nEpoch 139/1000 | Train Loss=40818.06640625 | Val Loss=1.40863025 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459970 | ‚àö(Val Loss) = 1.18685734 | Current Learning Rate: 0.002\nEpoch 140/1000 | Train Loss=40818.06640625 | Val Loss=1.40859413 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459946 | ‚àö(Val Loss) = 1.18684208 | Current Learning Rate: 0.002\nEpoch 141/1000 | Train Loss=40818.06640625 | Val Loss=1.40855300 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12460017 | ‚àö(Val Loss) = 1.18682480 | Current Learning Rate: 0.002\n\n Epoch :  140 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 142/1000 | Train Loss=40818.06640625 | Val Loss=1.40850818 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459946 | ‚àö(Val Loss) = 1.18680584 | Current Learning Rate: 0.002\nEpoch 143/1000 | Train Loss=40818.06640625 | Val Loss=1.40846157 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459993 | ‚àö(Val Loss) = 1.18678629 | Current Learning Rate: 0.002\nEpoch 144/1000 | Train Loss=40818.06640625 | Val Loss=1.40841484 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459993 | ‚àö(Val Loss) = 1.18676651 | Current Learning Rate: 0.002\nEpoch 145/1000 | Train Loss=40818.06640625 | Val Loss=1.40836525 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459922 | ‚àö(Val Loss) = 1.18674564 | Current Learning Rate: 0.002\nEpoch 146/1000 | Train Loss=40818.06640625 | Val Loss=1.40831316 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459970 | ‚àö(Val Loss) = 1.18672371 | Current Learning Rate: 0.002\nEpoch 147/1000 | Train Loss=40818.06640625 | Val Loss=1.40825927 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459898 | ‚àö(Val Loss) = 1.18670106 | Current Learning Rate: 0.002\nEpoch 148/1000 | Train Loss=40818.06640625 | Val Loss=1.40820754 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459898 | ‚àö(Val Loss) = 1.18667924 | Current Learning Rate: 0.002\nEpoch 149/1000 | Train Loss=40818.06640625 | Val Loss=1.40815258 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459946 | ‚àö(Val Loss) = 1.18665600 | Current Learning Rate: 0.002\nEpoch 150/1000 | Train Loss=40818.06640625 | Val Loss=1.40809727 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459874 | ‚àö(Val Loss) = 1.18663275 | Current Learning Rate: 0.002\nEpoch 151/1000 | Train Loss=40818.06640625 | Val Loss=1.40804207 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459946 | ‚àö(Val Loss) = 1.18660951 | Current Learning Rate: 0.002\nEpoch 152/1000 | Train Loss=40818.06640625 | Val Loss=1.40798378 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459850 | ‚àö(Val Loss) = 1.18658495 | Current Learning Rate: 0.002\nEpoch 153/1000 | Train Loss=40818.06640625 | Val Loss=1.40792668 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459898 | ‚àö(Val Loss) = 1.18656087 | Current Learning Rate: 0.002\nEpoch 154/1000 | Train Loss=40818.06640625 | Val Loss=1.40786910 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459826 | ‚àö(Val Loss) = 1.18653655 | Current Learning Rate: 0.002\nEpoch 155/1000 | Train Loss=40818.06640625 | Val Loss=1.40781295 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459826 | ‚àö(Val Loss) = 1.18651295 | Current Learning Rate: 0.002\nEpoch 156/1000 | Train Loss=40818.06640625 | Val Loss=1.40775561 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459874 | ‚àö(Val Loss) = 1.18648875 | Current Learning Rate: 0.002\nEpoch 157/1000 | Train Loss=40818.06640625 | Val Loss=1.40770006 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459826 | ‚àö(Val Loss) = 1.18646538 | Current Learning Rate: 0.002\nEpoch 158/1000 | Train Loss=40818.06640625 | Val Loss=1.40764034 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459874 | ‚àö(Val Loss) = 1.18644023 | Current Learning Rate: 0.002\nEpoch 159/1000 | Train Loss=40818.06640625 | Val Loss=1.40758479 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459803 | ‚àö(Val Loss) = 1.18641675 | Current Learning Rate: 0.002\nEpoch 160/1000 | Train Loss=40818.06640625 | Val Loss=1.40752864 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459850 | ‚àö(Val Loss) = 1.18639314 | Current Learning Rate: 0.002\nEpoch 161/1000 | Train Loss=40818.06640625 | Val Loss=1.40747297 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459803 | ‚àö(Val Loss) = 1.18636966 | Current Learning Rate: 0.002\n\n Epoch :  160 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 162/1000 | Train Loss=40818.06640625 | Val Loss=1.40741503 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459850 | ‚àö(Val Loss) = 1.18634522 | Current Learning Rate: 0.002\nEpoch 163/1000 | Train Loss=40818.06640625 | Val Loss=1.40736079 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459779 | ‚àö(Val Loss) = 1.18632233 | Current Learning Rate: 0.002\nEpoch 164/1000 | Train Loss=40818.06640625 | Val Loss=1.40730488 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459826 | ‚àö(Val Loss) = 1.18629885 | Current Learning Rate: 0.002\nEpoch 165/1000 | Train Loss=40818.06640625 | Val Loss=1.40725112 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459755 | ‚àö(Val Loss) = 1.18627620 | Current Learning Rate: 0.002\nEpoch 166/1000 | Train Loss=40818.06640625 | Val Loss=1.40719724 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459826 | ‚àö(Val Loss) = 1.18625343 | Current Learning Rate: 0.002\nEpoch 167/1000 | Train Loss=40818.06640625 | Val Loss=1.40714335 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459755 | ‚àö(Val Loss) = 1.18623078 | Current Learning Rate: 0.002\nEpoch 168/1000 | Train Loss=40818.06640625 | Val Loss=1.40708888 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459803 | ‚àö(Val Loss) = 1.18620777 | Current Learning Rate: 0.002\nEpoch 169/1000 | Train Loss=40818.06640625 | Val Loss=1.40703487 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459755 | ‚àö(Val Loss) = 1.18618500 | Current Learning Rate: 0.002\nEpoch 170/1000 | Train Loss=40818.06640625 | Val Loss=1.40698302 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459803 | ‚àö(Val Loss) = 1.18616319 | Current Learning Rate: 0.002\nEpoch 171/1000 | Train Loss=40818.06640625 | Val Loss=1.40693104 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459731 | ‚àö(Val Loss) = 1.18614125 | Current Learning Rate: 0.002\nEpoch 172/1000 | Train Loss=40818.06640625 | Val Loss=1.40687978 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459779 | ‚àö(Val Loss) = 1.18611968 | Current Learning Rate: 0.002\nEpoch 173/1000 | Train Loss=40818.06640625 | Val Loss=1.40682888 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459779 | ‚àö(Val Loss) = 1.18609822 | Current Learning Rate: 0.002\nEpoch 174/1000 | Train Loss=40818.06640625 | Val Loss=1.40677762 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459731 | ‚àö(Val Loss) = 1.18607652 | Current Learning Rate: 0.002\nEpoch 175/1000 | Train Loss=40818.06640625 | Val Loss=1.40672755 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459779 | ‚àö(Val Loss) = 1.18605542 | Current Learning Rate: 0.002\nEpoch 176/1000 | Train Loss=40818.06640625 | Val Loss=1.40667748 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459707 | ‚àö(Val Loss) = 1.18603432 | Current Learning Rate: 0.002\nEpoch 177/1000 | Train Loss=40818.06640625 | Val Loss=1.40662944 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459755 | ‚àö(Val Loss) = 1.18601406 | Current Learning Rate: 0.002\nEpoch 178/1000 | Train Loss=40818.06640625 | Val Loss=1.40657914 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459683 | ‚àö(Val Loss) = 1.18599284 | Current Learning Rate: 0.002\nEpoch 179/1000 | Train Loss=40818.06640625 | Val Loss=1.40653324 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459731 | ‚àö(Val Loss) = 1.18597353 | Current Learning Rate: 0.002\nEpoch 180/1000 | Train Loss=40818.06640625 | Val Loss=1.40648627 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459683 | ‚àö(Val Loss) = 1.18595374 | Current Learning Rate: 0.002\nEpoch 181/1000 | Train Loss=40818.06640625 | Val Loss=1.40643811 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459707 | ‚àö(Val Loss) = 1.18593347 | Current Learning Rate: 0.002\n\n Epoch :  180 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 182/1000 | Train Loss=40818.06640625 | Val Loss=1.40639198 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459707 | ‚àö(Val Loss) = 1.18591404 | Current Learning Rate: 0.002\nEpoch 183/1000 | Train Loss=40818.06640625 | Val Loss=1.40634573 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459660 | ‚àö(Val Loss) = 1.18589449 | Current Learning Rate: 0.002\nEpoch 184/1000 | Train Loss=40818.06640625 | Val Loss=1.40630162 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459707 | ‚àö(Val Loss) = 1.18587589 | Current Learning Rate: 0.002\nEpoch 185/1000 | Train Loss=40818.06640625 | Val Loss=1.40625513 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459636 | ‚àö(Val Loss) = 1.18585622 | Current Learning Rate: 0.002\nEpoch 186/1000 | Train Loss=40818.06640625 | Val Loss=1.40620959 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459683 | ‚àö(Val Loss) = 1.18583703 | Current Learning Rate: 0.002\nEpoch 187/1000 | Train Loss=40818.06640625 | Val Loss=1.40616655 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459683 | ‚àö(Val Loss) = 1.18581891 | Current Learning Rate: 0.002\nEpoch 188/1000 | Train Loss=40818.06640625 | Val Loss=1.40612149 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459636 | ‚àö(Val Loss) = 1.18579996 | Current Learning Rate: 0.002\nEpoch 189/1000 | Train Loss=40818.06640625 | Val Loss=1.40607870 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459683 | ‚àö(Val Loss) = 1.18578184 | Current Learning Rate: 0.002\nEpoch 190/1000 | Train Loss=40818.06640625 | Val Loss=1.40603626 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459636 | ‚àö(Val Loss) = 1.18576396 | Current Learning Rate: 0.002\nEpoch 191/1000 | Train Loss=40818.06640625 | Val Loss=1.40599477 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459683 | ‚àö(Val Loss) = 1.18574655 | Current Learning Rate: 0.002\nEpoch 192/1000 | Train Loss=40818.06640625 | Val Loss=1.40595305 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459683 | ‚àö(Val Loss) = 1.18572891 | Current Learning Rate: 0.002\nEpoch 193/1000 | Train Loss=40818.06640625 | Val Loss=1.40591037 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459612 | ‚àö(Val Loss) = 1.18571091 | Current Learning Rate: 0.002\nEpoch 194/1000 | Train Loss=40818.06640625 | Val Loss=1.40587020 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459660 | ‚àö(Val Loss) = 1.18569398 | Current Learning Rate: 0.002\nEpoch 195/1000 | Train Loss=40818.06640625 | Val Loss=1.40583026 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459588 | ‚àö(Val Loss) = 1.18567717 | Current Learning Rate: 0.002\nEpoch 196/1000 | Train Loss=40818.06640625 | Val Loss=1.40578926 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459588 | ‚àö(Val Loss) = 1.18565989 | Current Learning Rate: 0.002\nEpoch 197/1000 | Train Loss=40818.06640625 | Val Loss=1.40575087 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459636 | ‚àö(Val Loss) = 1.18564367 | Current Learning Rate: 0.002\nEpoch 198/1000 | Train Loss=40818.06640625 | Val Loss=1.40571022 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459588 | ‚àö(Val Loss) = 1.18562651 | Current Learning Rate: 0.002\nEpoch 199/1000 | Train Loss=40818.06640625 | Val Loss=1.40567183 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459636 | ‚àö(Val Loss) = 1.18561029 | Current Learning Rate: 0.002\nEpoch 200/1000 | Train Loss=40818.06640625 | Val Loss=1.40563345 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459636 | ‚àö(Val Loss) = 1.18559408 | Current Learning Rate: 0.002\nEpoch 201/1000 | Train Loss=40818.06640625 | Val Loss=1.40559483 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459564 | ‚àö(Val Loss) = 1.18557787 | Current Learning Rate: 0.002\n\n Epoch :  200 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 202/1000 | Train Loss=40818.06640625 | Val Loss=1.40555632 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459612 | ‚àö(Val Loss) = 1.18556166 | Current Learning Rate: 0.002\nEpoch 203/1000 | Train Loss=40818.06640625 | Val Loss=1.40551996 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459636 | ‚àö(Val Loss) = 1.18554628 | Current Learning Rate: 0.002\nEpoch 204/1000 | Train Loss=40818.06640625 | Val Loss=1.40548384 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459564 | ‚àö(Val Loss) = 1.18553102 | Current Learning Rate: 0.002\nEpoch 205/1000 | Train Loss=40818.06640625 | Val Loss=1.40544736 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459612 | ‚àö(Val Loss) = 1.18551564 | Current Learning Rate: 0.002\nEpoch 206/1000 | Train Loss=40818.06640625 | Val Loss=1.40540993 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459612 | ‚àö(Val Loss) = 1.18549991 | Current Learning Rate: 0.002\nEpoch 207/1000 | Train Loss=40818.06640625 | Val Loss=1.40537488 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459540 | ‚àö(Val Loss) = 1.18548512 | Current Learning Rate: 0.002\nEpoch 208/1000 | Train Loss=40818.06640625 | Val Loss=1.40533948 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459588 | ‚àö(Val Loss) = 1.18547010 | Current Learning Rate: 0.002\nEpoch 209/1000 | Train Loss=40818.06640625 | Val Loss=1.40530586 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459588 | ‚àö(Val Loss) = 1.18545592 | Current Learning Rate: 0.002\nEpoch 210/1000 | Train Loss=40818.06640625 | Val Loss=1.40527105 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459540 | ‚àö(Val Loss) = 1.18544126 | Current Learning Rate: 0.002\nEpoch 211/1000 | Train Loss=40818.06640625 | Val Loss=1.40523577 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459588 | ‚àö(Val Loss) = 1.18542635 | Current Learning Rate: 0.002\nEpoch 212/1000 | Train Loss=40818.06640625 | Val Loss=1.40520120 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459588 | ‚àö(Val Loss) = 1.18541181 | Current Learning Rate: 0.002\nEpoch 213/1000 | Train Loss=40818.06640625 | Val Loss=1.40516782 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459517 | ‚àö(Val Loss) = 1.18539774 | Current Learning Rate: 0.002\nEpoch 214/1000 | Train Loss=40818.06640625 | Val Loss=1.40513384 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459564 | ‚àö(Val Loss) = 1.18538344 | Current Learning Rate: 0.002\nEpoch 215/1000 | Train Loss=40818.06640625 | Val Loss=1.40510106 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459564 | ‚àö(Val Loss) = 1.18536961 | Current Learning Rate: 0.002\nEpoch 216/1000 | Train Loss=40818.06640625 | Val Loss=1.40506840 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459493 | ‚àö(Val Loss) = 1.18535578 | Current Learning Rate: 0.002\nEpoch 217/1000 | Train Loss=40818.06640625 | Val Loss=1.40503550 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459540 | ‚àö(Val Loss) = 1.18534195 | Current Learning Rate: 0.002\nEpoch 218/1000 | Train Loss=40818.06640625 | Val Loss=1.40500426 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459540 | ‚àö(Val Loss) = 1.18532872 | Current Learning Rate: 0.002\nEpoch 219/1000 | Train Loss=40818.06640625 | Val Loss=1.40497160 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459493 | ‚àö(Val Loss) = 1.18531501 | Current Learning Rate: 0.002\nEpoch 220/1000 | Train Loss=40818.06640625 | Val Loss=1.40494013 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459540 | ‚àö(Val Loss) = 1.18530166 | Current Learning Rate: 0.002\nEpoch 221/1000 | Train Loss=40818.06640625 | Val Loss=1.40490735 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459517 | ‚àö(Val Loss) = 1.18528783 | Current Learning Rate: 0.002\n\n Epoch :  220 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 222/1000 | Train Loss=40818.06640625 | Val Loss=1.40487647 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459469 | ‚àö(Val Loss) = 1.18527484 | Current Learning Rate: 0.002\nEpoch 223/1000 | Train Loss=40818.06640625 | Val Loss=1.40484643 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459469 | ‚àö(Val Loss) = 1.18526220 | Current Learning Rate: 0.002\nEpoch 224/1000 | Train Loss=40818.06640625 | Val Loss=1.40481567 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459517 | ‚àö(Val Loss) = 1.18524921 | Current Learning Rate: 0.002\nEpoch 225/1000 | Train Loss=40818.06640625 | Val Loss=1.40478837 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459517 | ‚àö(Val Loss) = 1.18523765 | Current Learning Rate: 0.002\nEpoch 226/1000 | Train Loss=40818.06640625 | Val Loss=1.40475655 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459469 | ‚àö(Val Loss) = 1.18522429 | Current Learning Rate: 0.002\nEpoch 227/1000 | Train Loss=40818.06640625 | Val Loss=1.40472722 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459493 | ‚àö(Val Loss) = 1.18521190 | Current Learning Rate: 0.002\nEpoch 228/1000 | Train Loss=40818.06640625 | Val Loss=1.40469790 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459493 | ‚àö(Val Loss) = 1.18519950 | Current Learning Rate: 0.002\nEpoch 229/1000 | Train Loss=40818.06640625 | Val Loss=1.40467024 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459445 | ‚àö(Val Loss) = 1.18518782 | Current Learning Rate: 0.002\nEpoch 230/1000 | Train Loss=40818.06640625 | Val Loss=1.40464139 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459445 | ‚àö(Val Loss) = 1.18517566 | Current Learning Rate: 0.002\nEpoch 231/1000 | Train Loss=40818.06640625 | Val Loss=1.40461254 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459493 | ‚àö(Val Loss) = 1.18516350 | Current Learning Rate: 0.002\nEpoch 232/1000 | Train Loss=40818.06640625 | Val Loss=1.40458357 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459493 | ‚àö(Val Loss) = 1.18515134 | Current Learning Rate: 0.002\nEpoch 233/1000 | Train Loss=40818.06640625 | Val Loss=1.40455687 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459421 | ‚àö(Val Loss) = 1.18514001 | Current Learning Rate: 0.002\nEpoch 234/1000 | Train Loss=40818.06640625 | Val Loss=1.40452802 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459469 | ‚àö(Val Loss) = 1.18512785 | Current Learning Rate: 0.002\nEpoch 235/1000 | Train Loss=40818.06640625 | Val Loss=1.40450048 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459469 | ‚àö(Val Loss) = 1.18511629 | Current Learning Rate: 0.002\nEpoch 236/1000 | Train Loss=40818.06640625 | Val Loss=1.40447402 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459421 | ‚àö(Val Loss) = 1.18510509 | Current Learning Rate: 0.002\nEpoch 237/1000 | Train Loss=40818.06640625 | Val Loss=1.40444660 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459421 | ‚àö(Val Loss) = 1.18509352 | Current Learning Rate: 0.002\nEpoch 238/1000 | Train Loss=40818.06640625 | Val Loss=1.40441978 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459469 | ‚àö(Val Loss) = 1.18508220 | Current Learning Rate: 0.002\nEpoch 239/1000 | Train Loss=40818.06640625 | Val Loss=1.40439487 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459469 | ‚àö(Val Loss) = 1.18507171 | Current Learning Rate: 0.002\nEpoch 240/1000 | Train Loss=40818.06640625 | Val Loss=1.40436769 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459397 | ‚àö(Val Loss) = 1.18506014 | Current Learning Rate: 0.002\nEpoch 241/1000 | Train Loss=40818.06640625 | Val Loss=1.40434289 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459397 | ‚àö(Val Loss) = 1.18504977 | Current Learning Rate: 0.002\n\n Epoch :  240 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 242/1000 | Train Loss=40818.06640625 | Val Loss=1.40431511 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459445 | ‚àö(Val Loss) = 1.18503797 | Current Learning Rate: 0.002\nEpoch 243/1000 | Train Loss=40818.06640625 | Val Loss=1.40429103 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459445 | ‚àö(Val Loss) = 1.18502784 | Current Learning Rate: 0.002\nEpoch 244/1000 | Train Loss=40818.06640625 | Val Loss=1.40426588 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459373 | ‚àö(Val Loss) = 1.18501723 | Current Learning Rate: 0.002\nEpoch 245/1000 | Train Loss=40818.06640625 | Val Loss=1.40423822 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459373 | ‚àö(Val Loss) = 1.18500555 | Current Learning Rate: 0.002\nEpoch 246/1000 | Train Loss=40818.06640625 | Val Loss=1.40421605 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459421 | ‚àö(Val Loss) = 1.18499625 | Current Learning Rate: 0.002\nEpoch 247/1000 | Train Loss=40818.06640625 | Val Loss=1.40418828 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459421 | ‚àö(Val Loss) = 1.18498445 | Current Learning Rate: 0.002\nEpoch 248/1000 | Train Loss=40818.06640625 | Val Loss=1.40416515 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459373 | ‚àö(Val Loss) = 1.18497479 | Current Learning Rate: 0.002\nEpoch 249/1000 | Train Loss=40818.06640625 | Val Loss=1.40414023 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459373 | ‚àö(Val Loss) = 1.18496418 | Current Learning Rate: 0.002\nEpoch 250/1000 | Train Loss=40818.06640625 | Val Loss=1.40411735 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459421 | ‚àö(Val Loss) = 1.18495452 | Current Learning Rate: 0.002\nEpoch 251/1000 | Train Loss=40818.06640625 | Val Loss=1.40409505 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459421 | ‚àö(Val Loss) = 1.18494523 | Current Learning Rate: 0.002\nEpoch 252/1000 | Train Loss=40818.06640625 | Val Loss=1.40406930 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459350 | ‚àö(Val Loss) = 1.18493426 | Current Learning Rate: 0.002\nEpoch 253/1000 | Train Loss=40818.06640625 | Val Loss=1.40404618 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459350 | ‚àö(Val Loss) = 1.18492460 | Current Learning Rate: 0.002\nEpoch 254/1000 | Train Loss=40818.06640625 | Val Loss=1.40402246 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459397 | ‚àö(Val Loss) = 1.18491459 | Current Learning Rate: 0.002\nEpoch 255/1000 | Train Loss=40818.06640625 | Val Loss=1.40400076 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459421 | ‚àö(Val Loss) = 1.18490541 | Current Learning Rate: 0.002\nEpoch 256/1000 | Train Loss=40818.06640625 | Val Loss=1.40397644 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459397 | ‚àö(Val Loss) = 1.18489516 | Current Learning Rate: 0.002\nEpoch 257/1000 | Train Loss=40818.06640625 | Val Loss=1.40395474 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459326 | ‚àö(Val Loss) = 1.18488598 | Current Learning Rate: 0.002\nEpoch 258/1000 | Train Loss=40818.06640625 | Val Loss=1.40393233 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459397 | ‚àö(Val Loss) = 1.18487656 | Current Learning Rate: 0.002\nEpoch 259/1000 | Train Loss=40818.06640625 | Val Loss=1.40390921 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459397 | ‚àö(Val Loss) = 1.18486679 | Current Learning Rate: 0.002\nEpoch 260/1000 | Train Loss=40818.06640625 | Val Loss=1.40388870 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459397 | ‚àö(Val Loss) = 1.18485808 | Current Learning Rate: 0.002\nEpoch 261/1000 | Train Loss=40818.06640625 | Val Loss=1.40386689 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459326 | ‚àö(Val Loss) = 1.18484890 | Current Learning Rate: 0.002\n\n Epoch :  260 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 262/1000 | Train Loss=40818.06640625 | Val Loss=1.40384328 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459326 | ‚àö(Val Loss) = 1.18483889 | Current Learning Rate: 0.002\nEpoch 263/1000 | Train Loss=40818.06640625 | Val Loss=1.40382230 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459373 | ‚àö(Val Loss) = 1.18483007 | Current Learning Rate: 0.002\nEpoch 264/1000 | Train Loss=40818.06640625 | Val Loss=1.40380168 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459373 | ‚àö(Val Loss) = 1.18482137 | Current Learning Rate: 0.002\nEpoch 265/1000 | Train Loss=40818.06640625 | Val Loss=1.40377855 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459373 | ‚àö(Val Loss) = 1.18481159 | Current Learning Rate: 0.002\nEpoch 266/1000 | Train Loss=40818.06640625 | Val Loss=1.40375710 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459326 | ‚àö(Val Loss) = 1.18480253 | Current Learning Rate: 0.002\nEpoch 267/1000 | Train Loss=40818.06640625 | Val Loss=1.40373766 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459373 | ‚àö(Val Loss) = 1.18479431 | Current Learning Rate: 0.002\nEpoch 268/1000 | Train Loss=40818.06640625 | Val Loss=1.40371716 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459373 | ‚àö(Val Loss) = 1.18478572 | Current Learning Rate: 0.002\nEpoch 269/1000 | Train Loss=40818.06640625 | Val Loss=1.40369534 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459373 | ‚àö(Val Loss) = 1.18477654 | Current Learning Rate: 0.002\nEpoch 270/1000 | Train Loss=40818.06640625 | Val Loss=1.40367472 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18476784 | Current Learning Rate: 0.002\nEpoch 271/1000 | Train Loss=40818.06640625 | Val Loss=1.40365505 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18475950 | Current Learning Rate: 0.002\nEpoch 272/1000 | Train Loss=40818.06640625 | Val Loss=1.40363324 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459350 | ‚àö(Val Loss) = 1.18475032 | Current Learning Rate: 0.002\nEpoch 273/1000 | Train Loss=40818.06640625 | Val Loss=1.40361464 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459350 | ‚àö(Val Loss) = 1.18474245 | Current Learning Rate: 0.002\nEpoch 274/1000 | Train Loss=40818.06640625 | Val Loss=1.40359557 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459350 | ‚àö(Val Loss) = 1.18473434 | Current Learning Rate: 0.002\nEpoch 275/1000 | Train Loss=40818.06640625 | Val Loss=1.40357637 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18472624 | Current Learning Rate: 0.002\nEpoch 276/1000 | Train Loss=40818.06640625 | Val Loss=1.40355635 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18471789 | Current Learning Rate: 0.002\nEpoch 277/1000 | Train Loss=40818.06640625 | Val Loss=1.40353537 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459326 | ‚àö(Val Loss) = 1.18470895 | Current Learning Rate: 0.002\nEpoch 278/1000 | Train Loss=40818.06640625 | Val Loss=1.40351617 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459326 | ‚àö(Val Loss) = 1.18470085 | Current Learning Rate: 0.002\nEpoch 279/1000 | Train Loss=40818.06640625 | Val Loss=1.40349698 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459326 | ‚àö(Val Loss) = 1.18469274 | Current Learning Rate: 0.002\nEpoch 280/1000 | Train Loss=40818.06640625 | Val Loss=1.40347981 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18468559 | Current Learning Rate: 0.002\nEpoch 281/1000 | Train Loss=40818.06640625 | Val Loss=1.40345991 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18467712 | Current Learning Rate: 0.002\n\n Epoch :  280 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 282/1000 | Train Loss=40818.06640625 | Val Loss=1.40344191 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459326 | ‚àö(Val Loss) = 1.18466949 | Current Learning Rate: 0.002\nEpoch 283/1000 | Train Loss=40818.06640625 | Val Loss=1.40342152 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459326 | ‚àö(Val Loss) = 1.18466091 | Current Learning Rate: 0.002\nEpoch 284/1000 | Train Loss=40818.06640625 | Val Loss=1.40340269 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459326 | ‚àö(Val Loss) = 1.18465292 | Current Learning Rate: 0.002\nEpoch 285/1000 | Train Loss=40818.06640625 | Val Loss=1.40338492 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18464553 | Current Learning Rate: 0.002\nEpoch 286/1000 | Train Loss=40818.06640625 | Val Loss=1.40336764 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18463814 | Current Learning Rate: 0.002\nEpoch 287/1000 | Train Loss=40818.06640625 | Val Loss=1.40334904 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18463039 | Current Learning Rate: 0.002\nEpoch 288/1000 | Train Loss=40818.06640625 | Val Loss=1.40333200 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18462312 | Current Learning Rate: 0.002\nEpoch 289/1000 | Train Loss=40818.06640625 | Val Loss=1.40331411 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18461561 | Current Learning Rate: 0.002\nEpoch 290/1000 | Train Loss=40818.06640625 | Val Loss=1.40329719 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18460846 | Current Learning Rate: 0.002\nEpoch 291/1000 | Train Loss=40818.06640625 | Val Loss=1.40327930 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18460095 | Current Learning Rate: 0.002\nEpoch 292/1000 | Train Loss=40818.06640625 | Val Loss=1.40326214 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18459368 | Current Learning Rate: 0.002\nEpoch 293/1000 | Train Loss=40818.06640625 | Val Loss=1.40324473 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18458629 | Current Learning Rate: 0.002\nEpoch 294/1000 | Train Loss=40818.06640625 | Val Loss=1.40322757 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18457901 | Current Learning Rate: 0.002\nEpoch 295/1000 | Train Loss=40818.06640625 | Val Loss=1.40320969 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459302 | ‚àö(Val Loss) = 1.18457150 | Current Learning Rate: 0.002\nEpoch 296/1000 | Train Loss=40818.06640625 | Val Loss=1.40319300 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459230 | ‚àö(Val Loss) = 1.18456447 | Current Learning Rate: 0.002\nEpoch 297/1000 | Train Loss=40818.06640625 | Val Loss=1.40317559 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459230 | ‚àö(Val Loss) = 1.18455708 | Current Learning Rate: 0.002\nEpoch 298/1000 | Train Loss=40818.06640625 | Val Loss=1.40315902 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18455017 | Current Learning Rate: 0.002\nEpoch 299/1000 | Train Loss=40818.06640625 | Val Loss=1.40314317 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18454349 | Current Learning Rate: 0.002\nEpoch 300/1000 | Train Loss=40818.06640625 | Val Loss=1.40312576 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18453610 | Current Learning Rate: 0.002\nEpoch 301/1000 | Train Loss=40818.06640625 | Val Loss=1.40310991 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18452942 | Current Learning Rate: 0.002\n\n Epoch :  300 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 302/1000 | Train Loss=40818.06640625 | Val Loss=1.40309441 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459230 | ‚àö(Val Loss) = 1.18452287 | Current Learning Rate: 0.002\nEpoch 303/1000 | Train Loss=40818.06640625 | Val Loss=1.40307772 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459230 | ‚àö(Val Loss) = 1.18451583 | Current Learning Rate: 0.002\nEpoch 304/1000 | Train Loss=40818.06640625 | Val Loss=1.40306199 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18450916 | Current Learning Rate: 0.002\nEpoch 305/1000 | Train Loss=40818.06640625 | Val Loss=1.40304661 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18450272 | Current Learning Rate: 0.002\nEpoch 306/1000 | Train Loss=40818.06640625 | Val Loss=1.40302992 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18449569 | Current Learning Rate: 0.002\nEpoch 307/1000 | Train Loss=40818.06640625 | Val Loss=1.40301383 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459278 | ‚àö(Val Loss) = 1.18448889 | Current Learning Rate: 0.002\nEpoch 308/1000 | Train Loss=40818.06640625 | Val Loss=1.40299833 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459207 | ‚àö(Val Loss) = 1.18448234 | Current Learning Rate: 0.002\nEpoch 309/1000 | Train Loss=40818.06640625 | Val Loss=1.40298522 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459207 | ‚àö(Val Loss) = 1.18447673 | Current Learning Rate: 0.002\nEpoch 310/1000 | Train Loss=40818.06640625 | Val Loss=1.40296984 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18447030 | Current Learning Rate: 0.002\nEpoch 311/1000 | Train Loss=40818.06640625 | Val Loss=1.40295458 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18446386 | Current Learning Rate: 0.002\nEpoch 312/1000 | Train Loss=40818.06640625 | Val Loss=1.40293908 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18445730 | Current Learning Rate: 0.002\nEpoch 313/1000 | Train Loss=40818.06640625 | Val Loss=1.40292394 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18445086 | Current Learning Rate: 0.002\nEpoch 314/1000 | Train Loss=40818.06640625 | Val Loss=1.40290844 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18444431 | Current Learning Rate: 0.002\nEpoch 315/1000 | Train Loss=40818.06640625 | Val Loss=1.40289497 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459207 | ‚àö(Val Loss) = 1.18443871 | Current Learning Rate: 0.002\nEpoch 316/1000 | Train Loss=40818.06640625 | Val Loss=1.40287900 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18443191 | Current Learning Rate: 0.002\nEpoch 317/1000 | Train Loss=40818.06640625 | Val Loss=1.40286422 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459230 | ‚àö(Val Loss) = 1.18442571 | Current Learning Rate: 0.002\nEpoch 318/1000 | Train Loss=40818.06640625 | Val Loss=1.40284956 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18441951 | Current Learning Rate: 0.002\nEpoch 319/1000 | Train Loss=40818.06640625 | Val Loss=1.40283561 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18441367 | Current Learning Rate: 0.002\nEpoch 320/1000 | Train Loss=40818.06640625 | Val Loss=1.40282011 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459254 | ‚àö(Val Loss) = 1.18440711 | Current Learning Rate: 0.002\nEpoch 321/1000 | Train Loss=40818.06640625 | Val Loss=1.40280724 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18440163 | Current Learning Rate: 0.002\n\n Epoch :  320 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 322/1000 | Train Loss=40818.06640625 | Val Loss=1.40279257 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18439543 | Current Learning Rate: 0.002\nEpoch 323/1000 | Train Loss=40818.06640625 | Val Loss=1.40277851 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459230 | ‚àö(Val Loss) = 1.18438947 | Current Learning Rate: 0.002\nEpoch 324/1000 | Train Loss=40818.06640625 | Val Loss=1.40276527 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459230 | ‚àö(Val Loss) = 1.18438387 | Current Learning Rate: 0.002\nEpoch 325/1000 | Train Loss=40818.06640625 | Val Loss=1.40275037 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459230 | ‚àö(Val Loss) = 1.18437767 | Current Learning Rate: 0.002\nEpoch 326/1000 | Train Loss=40818.06640625 | Val Loss=1.40273702 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459230 | ‚àö(Val Loss) = 1.18437195 | Current Learning Rate: 0.002\nEpoch 327/1000 | Train Loss=40818.06640625 | Val Loss=1.40272379 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459230 | ‚àö(Val Loss) = 1.18436646 | Current Learning Rate: 0.002\nEpoch 328/1000 | Train Loss=40818.06640625 | Val Loss=1.40270996 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18436062 | Current Learning Rate: 0.002\nEpoch 329/1000 | Train Loss=40818.06640625 | Val Loss=1.40269744 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18435526 | Current Learning Rate: 0.002\nEpoch 330/1000 | Train Loss=40818.06640625 | Val Loss=1.40268278 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459207 | ‚àö(Val Loss) = 1.18434906 | Current Learning Rate: 0.002\nEpoch 331/1000 | Train Loss=40818.06640625 | Val Loss=1.40266991 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459207 | ‚àö(Val Loss) = 1.18434370 | Current Learning Rate: 0.002\nEpoch 332/1000 | Train Loss=40818.06640625 | Val Loss=1.40265560 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459207 | ‚àö(Val Loss) = 1.18433762 | Current Learning Rate: 0.002\nEpoch 333/1000 | Train Loss=40818.06640625 | Val Loss=1.40264225 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459207 | ‚àö(Val Loss) = 1.18433201 | Current Learning Rate: 0.002\nEpoch 334/1000 | Train Loss=40818.06640625 | Val Loss=1.40263164 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459207 | ‚àö(Val Loss) = 1.18432748 | Current Learning Rate: 0.002\nEpoch 335/1000 | Train Loss=40818.06640625 | Val Loss=1.40261805 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18432176 | Current Learning Rate: 0.002\nEpoch 336/1000 | Train Loss=40818.06640625 | Val Loss=1.40260482 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18431616 | Current Learning Rate: 0.002\nEpoch 337/1000 | Train Loss=40818.06640625 | Val Loss=1.40259135 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18431044 | Current Learning Rate: 0.002\nEpoch 338/1000 | Train Loss=40818.06640625 | Val Loss=1.40257728 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18430459 | Current Learning Rate: 0.002\nEpoch 339/1000 | Train Loss=40818.06640625 | Val Loss=1.40256572 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18429971 | Current Learning Rate: 0.002\nEpoch 340/1000 | Train Loss=40818.06640625 | Val Loss=1.40255427 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18429482 | Current Learning Rate: 0.002\nEpoch 341/1000 | Train Loss=40818.06640625 | Val Loss=1.40254164 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18428946 | Current Learning Rate: 0.002\n\n Epoch :  340 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 342/1000 | Train Loss=40818.06640625 | Val Loss=1.40252721 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18428338 | Current Learning Rate: 0.002\nEpoch 343/1000 | Train Loss=40818.06640625 | Val Loss=1.40251732 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18427920 | Current Learning Rate: 0.002\nEpoch 344/1000 | Train Loss=40818.06640625 | Val Loss=1.40250421 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18427372 | Current Learning Rate: 0.002\nEpoch 345/1000 | Train Loss=40818.06640625 | Val Loss=1.40249157 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18426836 | Current Learning Rate: 0.002\nEpoch 346/1000 | Train Loss=40818.06640625 | Val Loss=1.40247893 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18426299 | Current Learning Rate: 0.002\nEpoch 347/1000 | Train Loss=40818.06640625 | Val Loss=1.40246785 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18425834 | Current Learning Rate: 0.002\nEpoch 348/1000 | Train Loss=40818.06640625 | Val Loss=1.40245581 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18425322 | Current Learning Rate: 0.002\nEpoch 349/1000 | Train Loss=40818.06640625 | Val Loss=1.40244281 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459183 | ‚àö(Val Loss) = 1.18424773 | Current Learning Rate: 0.002\nEpoch 350/1000 | Train Loss=40818.06640625 | Val Loss=1.40243149 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18424296 | Current Learning Rate: 0.002\nEpoch 351/1000 | Train Loss=40818.06640625 | Val Loss=1.40241981 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18423808 | Current Learning Rate: 0.002\nEpoch 352/1000 | Train Loss=40818.06640625 | Val Loss=1.40240788 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18423307 | Current Learning Rate: 0.002\nEpoch 353/1000 | Train Loss=40818.06640625 | Val Loss=1.40239632 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18422818 | Current Learning Rate: 0.002\nEpoch 354/1000 | Train Loss=40818.06640625 | Val Loss=1.40238488 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18422329 | Current Learning Rate: 0.002\nEpoch 355/1000 | Train Loss=40818.06640625 | Val Loss=1.40237391 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18421865 | Current Learning Rate: 0.002\nEpoch 356/1000 | Train Loss=40818.06640625 | Val Loss=1.40236175 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18421352 | Current Learning Rate: 0.002\nEpoch 357/1000 | Train Loss=40818.06640625 | Val Loss=1.40235031 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18420875 | Current Learning Rate: 0.002\nEpoch 358/1000 | Train Loss=40818.06640625 | Val Loss=1.40234089 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18420470 | Current Learning Rate: 0.002\nEpoch 359/1000 | Train Loss=40818.06640625 | Val Loss=1.40232790 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18419921 | Current Learning Rate: 0.002\nEpoch 360/1000 | Train Loss=40818.06640625 | Val Loss=1.40231586 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18419421 | Current Learning Rate: 0.002\nEpoch 361/1000 | Train Loss=40818.06640625 | Val Loss=1.40230441 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18418932 | Current Learning Rate: 0.002\n\n Epoch :  360 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 362/1000 | Train Loss=40818.06640625 | Val Loss=1.40229475 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18418527 | Current Learning Rate: 0.002\nEpoch 363/1000 | Train Loss=40818.06640625 | Val Loss=1.40228331 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18418038 | Current Learning Rate: 0.002\nEpoch 364/1000 | Train Loss=40818.06640625 | Val Loss=1.40227175 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18417561 | Current Learning Rate: 0.002\nEpoch 365/1000 | Train Loss=40818.06640625 | Val Loss=1.40226209 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459159 | ‚àö(Val Loss) = 1.18417144 | Current Learning Rate: 0.002\nEpoch 366/1000 | Train Loss=40818.06640625 | Val Loss=1.40225065 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18416667 | Current Learning Rate: 0.002\nEpoch 367/1000 | Train Loss=40818.06640625 | Val Loss=1.40223908 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18416178 | Current Learning Rate: 0.002\nEpoch 368/1000 | Train Loss=40818.06640625 | Val Loss=1.40222955 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18415773 | Current Learning Rate: 0.002\nEpoch 369/1000 | Train Loss=40818.06640625 | Val Loss=1.40221822 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18415296 | Current Learning Rate: 0.002\nEpoch 370/1000 | Train Loss=40818.06640625 | Val Loss=1.40220797 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18414867 | Current Learning Rate: 0.002\nEpoch 371/1000 | Train Loss=40818.06640625 | Val Loss=1.40219903 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18414485 | Current Learning Rate: 0.002\nEpoch 372/1000 | Train Loss=40818.06640625 | Val Loss=1.40218687 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18413973 | Current Learning Rate: 0.002\nEpoch 373/1000 | Train Loss=40818.06640625 | Val Loss=1.40217590 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18413508 | Current Learning Rate: 0.002\nEpoch 374/1000 | Train Loss=40818.06640625 | Val Loss=1.40216780 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18413162 | Current Learning Rate: 0.002\nEpoch 375/1000 | Train Loss=40818.06640625 | Val Loss=1.40215552 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459087 | ‚àö(Val Loss) = 1.18412650 | Current Learning Rate: 0.002\nEpoch 376/1000 | Train Loss=40818.06640625 | Val Loss=1.40214729 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459087 | ‚àö(Val Loss) = 1.18412304 | Current Learning Rate: 0.002\nEpoch 377/1000 | Train Loss=40818.06640625 | Val Loss=1.40213704 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459087 | ‚àö(Val Loss) = 1.18411863 | Current Learning Rate: 0.002\nEpoch 378/1000 | Train Loss=40818.06640625 | Val Loss=1.40212560 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18411386 | Current Learning Rate: 0.002\nEpoch 379/1000 | Train Loss=40818.06640625 | Val Loss=1.40211594 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18410981 | Current Learning Rate: 0.002\nEpoch 380/1000 | Train Loss=40818.06640625 | Val Loss=1.40210640 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18410575 | Current Learning Rate: 0.002\nEpoch 381/1000 | Train Loss=40818.06640625 | Val Loss=1.40209532 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18410110 | Current Learning Rate: 0.002\n\n Epoch :  380 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 382/1000 | Train Loss=40818.06640625 | Val Loss=1.40208662 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18409741 | Current Learning Rate: 0.002\nEpoch 383/1000 | Train Loss=40818.06640625 | Val Loss=1.40207624 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18409300 | Current Learning Rate: 0.002\nEpoch 384/1000 | Train Loss=40818.06640625 | Val Loss=1.40206814 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459135 | ‚àö(Val Loss) = 1.18408954 | Current Learning Rate: 0.002\nEpoch 385/1000 | Train Loss=40818.06640625 | Val Loss=1.40205693 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459064 | ‚àö(Val Loss) = 1.18408489 | Current Learning Rate: 0.002\nEpoch 386/1000 | Train Loss=40818.06640625 | Val Loss=1.40204692 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459064 | ‚àö(Val Loss) = 1.18408060 | Current Learning Rate: 0.002\nEpoch 387/1000 | Train Loss=40818.06640625 | Val Loss=1.40203810 | Data=408.15927124 | Physics=2.14256047 | Val RMSE: 2.12459064 | ‚àö(Val Loss) = 1.18407691 | Current Learning Rate: 0.002\nEpoch 388/1000 | Train Loss=40818.06640625 | Val Loss=1.40202773 | Data=408.15924072 | Physics=2.14256047 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18407249 | Current Learning Rate: 0.002\nEpoch 389/1000 | Train Loss=40818.06640625 | Val Loss=1.40202022 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18406940 | Current Learning Rate: 0.002\nEpoch 390/1000 | Train Loss=40818.06640625 | Val Loss=1.40200853 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18406439 | Current Learning Rate: 0.002\nEpoch 391/1000 | Train Loss=40818.06640625 | Val Loss=1.40199912 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18406045 | Current Learning Rate: 0.002\nEpoch 392/1000 | Train Loss=40818.06640625 | Val Loss=1.40199137 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18405712 | Current Learning Rate: 0.002\nEpoch 393/1000 | Train Loss=40818.06640625 | Val Loss=1.40198123 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18405294 | Current Learning Rate: 0.002\nEpoch 394/1000 | Train Loss=40818.06640625 | Val Loss=1.40197146 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459064 | ‚àö(Val Loss) = 1.18404877 | Current Learning Rate: 0.002\nEpoch 395/1000 | Train Loss=40818.06640625 | Val Loss=1.40196264 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459064 | ‚àö(Val Loss) = 1.18404508 | Current Learning Rate: 0.002\nEpoch 396/1000 | Train Loss=40818.06640625 | Val Loss=1.40195215 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459064 | ‚àö(Val Loss) = 1.18404055 | Current Learning Rate: 0.002\nEpoch 397/1000 | Train Loss=40818.06640625 | Val Loss=1.40194523 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18403769 | Current Learning Rate: 0.002\nEpoch 398/1000 | Train Loss=40818.06640625 | Val Loss=1.40193510 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18403339 | Current Learning Rate: 0.002\nEpoch 399/1000 | Train Loss=40818.06640625 | Val Loss=1.40192533 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18402922 | Current Learning Rate: 0.002\nEpoch 400/1000 | Train Loss=40818.06640625 | Val Loss=1.40191591 | Data=408.15927124 | Physics=2.14256023 | Val RMSE: 2.12459111 | ‚àö(Val Loss) = 1.18402529 | Current Learning Rate: 0.0002\nEpoch 401/1000 | Train Loss=40818.06640625 | Val Loss=1.14761496 | Data=408.15924072 | Physics=2.14256023 | Val RMSE: 2.12511182 | ‚àö(Val Loss) = 1.07126796 | Current Learning Rate: 0.0002\n\n Epoch :  400 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 402/1000 | Train Loss=1039.49523926 | Val Loss=1.16066790 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12511325 | ‚àö(Val Loss) = 1.07734299 | Current Learning Rate: 0.0002\nEpoch 403/1000 | Train Loss=1039.49536133 | Val Loss=1.16934347 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12511539 | ‚àö(Val Loss) = 1.08136189 | Current Learning Rate: 0.0002\nEpoch 404/1000 | Train Loss=1039.49523926 | Val Loss=1.17573357 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12511683 | ‚àö(Val Loss) = 1.08431244 | Current Learning Rate: 0.0002\nEpoch 405/1000 | Train Loss=1039.49523926 | Val Loss=1.18057942 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12511873 | ‚àö(Val Loss) = 1.08654475 | Current Learning Rate: 0.0002\nEpoch 406/1000 | Train Loss=1039.49536133 | Val Loss=1.18428397 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512016 | ‚àö(Val Loss) = 1.08824813 | Current Learning Rate: 0.0002\nEpoch 407/1000 | Train Loss=1039.49523926 | Val Loss=1.18710649 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512159 | ‚àö(Val Loss) = 1.08954418 | Current Learning Rate: 0.0002\nEpoch 408/1000 | Train Loss=1039.49523926 | Val Loss=1.18923485 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512207 | ‚àö(Val Loss) = 1.09052050 | Current Learning Rate: 0.0002\nEpoch 409/1000 | Train Loss=1039.49523926 | Val Loss=1.19080257 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512326 | ‚àö(Val Loss) = 1.09123898 | Current Learning Rate: 0.0002\nEpoch 410/1000 | Train Loss=1039.49523926 | Val Loss=1.19191766 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512398 | ‚àö(Val Loss) = 1.09174979 | Current Learning Rate: 0.0002\nEpoch 411/1000 | Train Loss=1039.49523926 | Val Loss=1.19266772 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512469 | ‚àö(Val Loss) = 1.09209323 | Current Learning Rate: 0.0002\nEpoch 412/1000 | Train Loss=1039.49523926 | Val Loss=1.19311833 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512517 | ‚àö(Val Loss) = 1.09229958 | Current Learning Rate: 0.0002\nEpoch 413/1000 | Train Loss=1039.49523926 | Val Loss=1.19332767 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512565 | ‚àö(Val Loss) = 1.09239542 | Current Learning Rate: 0.0002\nEpoch 414/1000 | Train Loss=1039.49523926 | Val Loss=1.19333899 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512684 | ‚àö(Val Loss) = 1.09240055 | Current Learning Rate: 0.0002\nEpoch 415/1000 | Train Loss=1039.49523926 | Val Loss=1.19318938 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.09233212 | Current Learning Rate: 0.0002\nEpoch 416/1000 | Train Loss=1039.49523926 | Val Loss=1.19291210 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.09220517 | Current Learning Rate: 0.0002\nEpoch 417/1000 | Train Loss=1039.49523926 | Val Loss=1.19253135 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.09203088 | Current Learning Rate: 0.0002\nEpoch 418/1000 | Train Loss=1039.49523926 | Val Loss=1.19206750 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.09181845 | Current Learning Rate: 0.0002\nEpoch 419/1000 | Train Loss=1039.49523926 | Val Loss=1.19153941 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.09157658 | Current Learning Rate: 0.0002\nEpoch 420/1000 | Train Loss=1039.49523926 | Val Loss=1.19096160 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.09131181 | Current Learning Rate: 0.0002\nEpoch 421/1000 | Train Loss=1039.49536133 | Val Loss=1.19034481 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.09102929 | Current Learning Rate: 0.0002\n\n Epoch :  420 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 422/1000 | Train Loss=1039.49536133 | Val Loss=1.18969989 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.09073365 | Current Learning Rate: 0.0002\nEpoch 423/1000 | Train Loss=1039.49523926 | Val Loss=1.18903661 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.09042954 | Current Learning Rate: 0.0002\nEpoch 424/1000 | Train Loss=1039.49536133 | Val Loss=1.18836069 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.09011960 | Current Learning Rate: 0.0002\nEpoch 425/1000 | Train Loss=1039.49523926 | Val Loss=1.18767762 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08980620 | Current Learning Rate: 0.0002\nEpoch 426/1000 | Train Loss=1039.49523926 | Val Loss=1.18699193 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.08949161 | Current Learning Rate: 0.0002\nEpoch 427/1000 | Train Loss=1039.49523926 | Val Loss=1.18631041 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.08917880 | Current Learning Rate: 0.0002\nEpoch 428/1000 | Train Loss=1039.49536133 | Val Loss=1.18563080 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.08886671 | Current Learning Rate: 0.0002\nEpoch 429/1000 | Train Loss=1039.49523926 | Val Loss=1.18495965 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08855855 | Current Learning Rate: 0.0002\nEpoch 430/1000 | Train Loss=1039.49523926 | Val Loss=1.18429732 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.08825421 | Current Learning Rate: 0.0002\nEpoch 431/1000 | Train Loss=1039.49536133 | Val Loss=1.18364632 | Data=10.37352753 | Physics=2.14256047 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08795512 | Current Learning Rate: 0.0002\nEpoch 432/1000 | Train Loss=1039.49523926 | Val Loss=1.18300796 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08766174 | Current Learning Rate: 0.0002\nEpoch 433/1000 | Train Loss=1039.49523926 | Val Loss=1.18238199 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08737385 | Current Learning Rate: 0.0002\nEpoch 434/1000 | Train Loss=1039.49523926 | Val Loss=1.18177044 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08709264 | Current Learning Rate: 0.0002\nEpoch 435/1000 | Train Loss=1039.49523926 | Val Loss=1.18117261 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08681762 | Current Learning Rate: 0.0002\nEpoch 436/1000 | Train Loss=1039.49523926 | Val Loss=1.18058860 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513018 | ‚àö(Val Loss) = 1.08654892 | Current Learning Rate: 0.0002\nEpoch 437/1000 | Train Loss=1039.49536133 | Val Loss=1.18002093 | Data=10.37352753 | Physics=2.14256047 | Val RMSE: 2.12513018 | ‚àö(Val Loss) = 1.08628774 | Current Learning Rate: 0.0002\nEpoch 438/1000 | Train Loss=1039.49523926 | Val Loss=1.17946756 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08603299 | Current Learning Rate: 0.0002\nEpoch 439/1000 | Train Loss=1039.49523926 | Val Loss=1.17892826 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513018 | ‚àö(Val Loss) = 1.08578467 | Current Learning Rate: 0.0002\nEpoch 440/1000 | Train Loss=1039.49523926 | Val Loss=1.17840433 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513089 | ‚àö(Val Loss) = 1.08554327 | Current Learning Rate: 0.0002\nEpoch 441/1000 | Train Loss=1039.49523926 | Val Loss=1.17789316 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513018 | ‚àö(Val Loss) = 1.08530784 | Current Learning Rate: 0.0002\n\n Epoch :  440 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 442/1000 | Train Loss=1039.49536133 | Val Loss=1.17739677 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12513089 | ‚àö(Val Loss) = 1.08507919 | Current Learning Rate: 0.0002\nEpoch 443/1000 | Train Loss=1039.49523926 | Val Loss=1.17691553 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08485734 | Current Learning Rate: 0.0002\nEpoch 444/1000 | Train Loss=1039.49523926 | Val Loss=1.17644691 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513018 | ‚àö(Val Loss) = 1.08464134 | Current Learning Rate: 0.0002\nEpoch 445/1000 | Train Loss=1039.49523926 | Val Loss=1.17599237 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513089 | ‚àö(Val Loss) = 1.08443177 | Current Learning Rate: 0.0002\nEpoch 446/1000 | Train Loss=1039.49523926 | Val Loss=1.17555022 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08422792 | Current Learning Rate: 0.0002\nEpoch 447/1000 | Train Loss=1039.49536133 | Val Loss=1.17512000 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12513018 | ‚àö(Val Loss) = 1.08402956 | Current Learning Rate: 0.0002\nEpoch 448/1000 | Train Loss=1039.49523926 | Val Loss=1.17470300 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08383715 | Current Learning Rate: 0.0002\nEpoch 449/1000 | Train Loss=1039.49523926 | Val Loss=1.17429757 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513089 | ‚àö(Val Loss) = 1.08365011 | Current Learning Rate: 0.0002\nEpoch 450/1000 | Train Loss=1039.49523926 | Val Loss=1.17390323 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513089 | ‚àö(Val Loss) = 1.08346820 | Current Learning Rate: 0.0002\nEpoch 451/1000 | Train Loss=1039.49523926 | Val Loss=1.17352057 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513018 | ‚àö(Val Loss) = 1.08329153 | Current Learning Rate: 0.0002\nEpoch 452/1000 | Train Loss=1039.49523926 | Val Loss=1.17314839 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513018 | ‚àö(Val Loss) = 1.08311975 | Current Learning Rate: 0.0002\nEpoch 453/1000 | Train Loss=1039.49523926 | Val Loss=1.17278707 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513089 | ‚àö(Val Loss) = 1.08295298 | Current Learning Rate: 0.0002\nEpoch 454/1000 | Train Loss=1039.49523926 | Val Loss=1.17243540 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08279061 | Current Learning Rate: 0.0002\nEpoch 455/1000 | Train Loss=1039.49536133 | Val Loss=1.17209256 | Data=10.37352753 | Physics=2.14256047 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08263218 | Current Learning Rate: 0.0002\nEpoch 456/1000 | Train Loss=1039.49536133 | Val Loss=1.17175984 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08247852 | Current Learning Rate: 0.0002\nEpoch 457/1000 | Train Loss=1039.49523926 | Val Loss=1.17143655 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08232927 | Current Learning Rate: 0.0002\nEpoch 458/1000 | Train Loss=1039.49523926 | Val Loss=1.17112100 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08218348 | Current Learning Rate: 0.0002\nEpoch 459/1000 | Train Loss=1039.49523926 | Val Loss=1.17081547 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08204222 | Current Learning Rate: 0.0002\nEpoch 460/1000 | Train Loss=1039.49523926 | Val Loss=1.17051518 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08190346 | Current Learning Rate: 0.0002\nEpoch 461/1000 | Train Loss=1039.49523926 | Val Loss=1.17022491 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08176935 | Current Learning Rate: 0.0002\n\n Epoch :  460 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 462/1000 | Train Loss=1039.49523926 | Val Loss=1.16994178 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08163846 | Current Learning Rate: 0.0002\nEpoch 463/1000 | Train Loss=1039.49523926 | Val Loss=1.16966701 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08151150 | Current Learning Rate: 0.0002\nEpoch 464/1000 | Train Loss=1039.49536133 | Val Loss=1.16939855 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08138728 | Current Learning Rate: 0.0002\nEpoch 465/1000 | Train Loss=1039.49536133 | Val Loss=1.16913640 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12513065 | ‚àö(Val Loss) = 1.08126616 | Current Learning Rate: 0.0002\nEpoch 466/1000 | Train Loss=1039.49523926 | Val Loss=1.16888022 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08114767 | Current Learning Rate: 0.0002\nEpoch 467/1000 | Train Loss=1039.49523926 | Val Loss=1.16863179 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08103275 | Current Learning Rate: 0.0002\nEpoch 468/1000 | Train Loss=1039.49523926 | Val Loss=1.16838813 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08091998 | Current Learning Rate: 0.0002\nEpoch 469/1000 | Train Loss=1039.49523926 | Val Loss=1.16815150 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08081055 | Current Learning Rate: 0.0002\nEpoch 470/1000 | Train Loss=1039.49523926 | Val Loss=1.16791928 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08070314 | Current Learning Rate: 0.0002\nEpoch 471/1000 | Train Loss=1039.49523926 | Val Loss=1.16769350 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12513041 | ‚àö(Val Loss) = 1.08059871 | Current Learning Rate: 0.0002\nEpoch 472/1000 | Train Loss=1039.49523926 | Val Loss=1.16747367 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08049691 | Current Learning Rate: 0.0002\nEpoch 473/1000 | Train Loss=1039.49523926 | Val Loss=1.16725767 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08039701 | Current Learning Rate: 0.0002\nEpoch 474/1000 | Train Loss=1039.49523926 | Val Loss=1.16704690 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08029950 | Current Learning Rate: 0.0002\nEpoch 475/1000 | Train Loss=1039.49523926 | Val Loss=1.16684127 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08020425 | Current Learning Rate: 0.0002\nEpoch 476/1000 | Train Loss=1039.49523926 | Val Loss=1.16664016 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08011115 | Current Learning Rate: 0.0002\nEpoch 477/1000 | Train Loss=1039.49536133 | Val Loss=1.16644335 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.08002007 | Current Learning Rate: 0.0002\nEpoch 478/1000 | Train Loss=1039.49536133 | Val Loss=1.16625106 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07993102 | Current Learning Rate: 0.0002\nEpoch 479/1000 | Train Loss=1039.49523926 | Val Loss=1.16606379 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07984436 | Current Learning Rate: 0.0002\nEpoch 480/1000 | Train Loss=1039.49523926 | Val Loss=1.16587842 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07975852 | Current Learning Rate: 0.0002\nEpoch 481/1000 | Train Loss=1039.49523926 | Val Loss=1.16569829 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07967508 | Current Learning Rate: 0.0002\n\n Epoch :  480 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 482/1000 | Train Loss=1039.49523926 | Val Loss=1.16552305 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07959390 | Current Learning Rate: 0.0002\nEpoch 483/1000 | Train Loss=1039.49536133 | Val Loss=1.16534984 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07951367 | Current Learning Rate: 0.0002\nEpoch 484/1000 | Train Loss=1039.49536133 | Val Loss=1.16518009 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07943511 | Current Learning Rate: 0.0002\nEpoch 485/1000 | Train Loss=1039.49523926 | Val Loss=1.16501594 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07935905 | Current Learning Rate: 0.0002\nEpoch 486/1000 | Train Loss=1039.49523926 | Val Loss=1.16485322 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07928371 | Current Learning Rate: 0.0002\nEpoch 487/1000 | Train Loss=1039.49523926 | Val Loss=1.16469419 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07921004 | Current Learning Rate: 0.0002\nEpoch 488/1000 | Train Loss=1039.49523926 | Val Loss=1.16453803 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07913768 | Current Learning Rate: 0.0002\nEpoch 489/1000 | Train Loss=1039.49536133 | Val Loss=1.16438437 | Data=10.37352753 | Physics=2.14256047 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07906640 | Current Learning Rate: 0.0002\nEpoch 490/1000 | Train Loss=1039.49523926 | Val Loss=1.16423452 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07899702 | Current Learning Rate: 0.0002\nEpoch 491/1000 | Train Loss=1039.49523926 | Val Loss=1.16408789 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07892907 | Current Learning Rate: 0.0002\nEpoch 492/1000 | Train Loss=1039.49523926 | Val Loss=1.16394496 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07886279 | Current Learning Rate: 0.0002\nEpoch 493/1000 | Train Loss=1039.49523926 | Val Loss=1.16380239 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07879674 | Current Learning Rate: 0.0002\nEpoch 494/1000 | Train Loss=1039.49523926 | Val Loss=1.16366374 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07873249 | Current Learning Rate: 0.0002\nEpoch 495/1000 | Train Loss=1039.49523926 | Val Loss=1.16352701 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07866907 | Current Learning Rate: 0.0002\nEpoch 496/1000 | Train Loss=1039.49523926 | Val Loss=1.16339314 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07860708 | Current Learning Rate: 0.0002\nEpoch 497/1000 | Train Loss=1039.49523926 | Val Loss=1.16326141 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07854593 | Current Learning Rate: 0.0002\nEpoch 498/1000 | Train Loss=1039.49523926 | Val Loss=1.16313314 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07848656 | Current Learning Rate: 0.0002\nEpoch 499/1000 | Train Loss=1039.49523926 | Val Loss=1.16300595 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07842755 | Current Learning Rate: 0.0002\nEpoch 500/1000 | Train Loss=1039.49536133 | Val Loss=1.16288149 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07836986 | Current Learning Rate: 0.0002\nEpoch 501/1000 | Train Loss=1039.49523926 | Val Loss=1.16275954 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07831323 | Current Learning Rate: 0.0002\n\n Epoch :  500 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 502/1000 | Train Loss=1039.49523926 | Val Loss=1.16263914 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07825744 | Current Learning Rate: 0.0002\nEpoch 503/1000 | Train Loss=1039.49536133 | Val Loss=1.16252208 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07820320 | Current Learning Rate: 0.0002\nEpoch 504/1000 | Train Loss=1039.49536133 | Val Loss=1.16240513 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07814896 | Current Learning Rate: 0.0002\nEpoch 505/1000 | Train Loss=1039.49523926 | Val Loss=1.16229129 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07809615 | Current Learning Rate: 0.0002\nEpoch 506/1000 | Train Loss=1039.49548340 | Val Loss=1.16217756 | Data=10.37352848 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07804334 | Current Learning Rate: 0.0002\nEpoch 507/1000 | Train Loss=1039.49523926 | Val Loss=1.16206861 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07799292 | Current Learning Rate: 0.0002\nEpoch 508/1000 | Train Loss=1039.49523926 | Val Loss=1.16195905 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07794201 | Current Learning Rate: 0.0002\nEpoch 509/1000 | Train Loss=1039.49536133 | Val Loss=1.16185260 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07789266 | Current Learning Rate: 0.0002\nEpoch 510/1000 | Train Loss=1039.49523926 | Val Loss=1.16174722 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07784379 | Current Learning Rate: 0.0002\nEpoch 511/1000 | Train Loss=1039.49523926 | Val Loss=1.16164386 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.07779586 | Current Learning Rate: 0.0002\nEpoch 512/1000 | Train Loss=1039.49536133 | Val Loss=1.16154158 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.07774842 | Current Learning Rate: 0.0002\nEpoch 513/1000 | Train Loss=1039.49523926 | Val Loss=1.16144168 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07770205 | Current Learning Rate: 0.0002\nEpoch 514/1000 | Train Loss=1039.49536133 | Val Loss=1.16134238 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.07765591 | Current Learning Rate: 0.0002\nEpoch 515/1000 | Train Loss=1039.49523926 | Val Loss=1.16124570 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512994 | ‚àö(Val Loss) = 1.07761109 | Current Learning Rate: 0.0002\nEpoch 516/1000 | Train Loss=1039.49523926 | Val Loss=1.16114974 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07756662 | Current Learning Rate: 0.0002\nEpoch 517/1000 | Train Loss=1039.49523926 | Val Loss=1.16105545 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07752287 | Current Learning Rate: 0.0002\nEpoch 518/1000 | Train Loss=1039.49523926 | Val Loss=1.16096270 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07747984 | Current Learning Rate: 0.0002\nEpoch 519/1000 | Train Loss=1039.49523926 | Val Loss=1.16087079 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07743716 | Current Learning Rate: 0.0002\nEpoch 520/1000 | Train Loss=1039.49536133 | Val Loss=1.16078115 | Data=10.37352753 | Physics=2.14256047 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07739556 | Current Learning Rate: 0.0002\nEpoch 521/1000 | Train Loss=1039.49523926 | Val Loss=1.16069245 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07735443 | Current Learning Rate: 0.0002\n\n Epoch :  520 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 522/1000 | Train Loss=1039.49536133 | Val Loss=1.16060531 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07731390 | Current Learning Rate: 0.0002\nEpoch 523/1000 | Train Loss=1039.49523926 | Val Loss=1.16051829 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07727349 | Current Learning Rate: 0.0002\nEpoch 524/1000 | Train Loss=1039.49523926 | Val Loss=1.16043413 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07723451 | Current Learning Rate: 0.0002\nEpoch 525/1000 | Train Loss=1039.49523926 | Val Loss=1.16034961 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07719529 | Current Learning Rate: 0.0002\nEpoch 526/1000 | Train Loss=1039.49523926 | Val Loss=1.16026771 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07715726 | Current Learning Rate: 0.0002\nEpoch 527/1000 | Train Loss=1039.49523926 | Val Loss=1.16018534 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07711899 | Current Learning Rate: 0.0002\nEpoch 528/1000 | Train Loss=1039.49523926 | Val Loss=1.16010535 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07708192 | Current Learning Rate: 0.0002\nEpoch 529/1000 | Train Loss=1039.49536133 | Val Loss=1.16002524 | Data=10.37352753 | Physics=2.14256047 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07704473 | Current Learning Rate: 0.0002\nEpoch 530/1000 | Train Loss=1039.49523926 | Val Loss=1.15994823 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07700896 | Current Learning Rate: 0.0002\nEpoch 531/1000 | Train Loss=1039.49523926 | Val Loss=1.15987062 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07697284 | Current Learning Rate: 0.0002\nEpoch 532/1000 | Train Loss=1039.49523926 | Val Loss=1.15979528 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07693791 | Current Learning Rate: 0.0002\nEpoch 533/1000 | Train Loss=1039.49523926 | Val Loss=1.15971923 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512970 | ‚àö(Val Loss) = 1.07690263 | Current Learning Rate: 0.0002\nEpoch 534/1000 | Train Loss=1039.49523926 | Val Loss=1.15964592 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07686853 | Current Learning Rate: 0.0002\nEpoch 535/1000 | Train Loss=1039.49523926 | Val Loss=1.15957236 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07683444 | Current Learning Rate: 0.0002\nEpoch 536/1000 | Train Loss=1039.49523926 | Val Loss=1.15950000 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07680082 | Current Learning Rate: 0.0002\nEpoch 537/1000 | Train Loss=1039.49523926 | Val Loss=1.15942872 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07676768 | Current Learning Rate: 0.0002\nEpoch 538/1000 | Train Loss=1039.49536133 | Val Loss=1.15935898 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07673538 | Current Learning Rate: 0.0002\nEpoch 539/1000 | Train Loss=1039.49523926 | Val Loss=1.15928900 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07670283 | Current Learning Rate: 0.0002\nEpoch 540/1000 | Train Loss=1039.49523926 | Val Loss=1.15922058 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07667100 | Current Learning Rate: 0.0002\nEpoch 541/1000 | Train Loss=1039.49536133 | Val Loss=1.15915310 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07663977 | Current Learning Rate: 0.0002\n\n Epoch :  540 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 542/1000 | Train Loss=1039.49523926 | Val Loss=1.15908635 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07660878 | Current Learning Rate: 0.0002\nEpoch 543/1000 | Train Loss=1039.49523926 | Val Loss=1.15902019 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07657802 | Current Learning Rate: 0.0002\nEpoch 544/1000 | Train Loss=1039.49523926 | Val Loss=1.15895510 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07654774 | Current Learning Rate: 0.0002\nEpoch 545/1000 | Train Loss=1039.49523926 | Val Loss=1.15888977 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07651746 | Current Learning Rate: 0.0002\nEpoch 546/1000 | Train Loss=1039.49523926 | Val Loss=1.15882766 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07648861 | Current Learning Rate: 0.0002\nEpoch 547/1000 | Train Loss=1039.49523926 | Val Loss=1.15876389 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07645893 | Current Learning Rate: 0.0002\nEpoch 548/1000 | Train Loss=1039.49536133 | Val Loss=1.15870261 | Data=10.37352753 | Physics=2.14256047 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07643044 | Current Learning Rate: 0.0002\nEpoch 549/1000 | Train Loss=1039.49523926 | Val Loss=1.15864027 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07640159 | Current Learning Rate: 0.0002\nEpoch 550/1000 | Train Loss=1039.49523926 | Val Loss=1.15858054 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07637382 | Current Learning Rate: 0.0002\nEpoch 551/1000 | Train Loss=1039.49536133 | Val Loss=1.15851998 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07634568 | Current Learning Rate: 0.0002\nEpoch 552/1000 | Train Loss=1039.49523926 | Val Loss=1.15846145 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512946 | ‚àö(Val Loss) = 1.07631850 | Current Learning Rate: 0.0002\nEpoch 553/1000 | Train Loss=1039.49511719 | Val Loss=1.15840244 | Data=10.37352562 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07629108 | Current Learning Rate: 0.0002\nEpoch 554/1000 | Train Loss=1039.49523926 | Val Loss=1.15834570 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07626474 | Current Learning Rate: 0.0002\nEpoch 555/1000 | Train Loss=1039.49523926 | Val Loss=1.15828741 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07623756 | Current Learning Rate: 0.0002\nEpoch 556/1000 | Train Loss=1039.49523926 | Val Loss=1.15823197 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07621181 | Current Learning Rate: 0.0002\nEpoch 557/1000 | Train Loss=1039.49523926 | Val Loss=1.15817535 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07618558 | Current Learning Rate: 0.0002\nEpoch 558/1000 | Train Loss=1039.49536133 | Val Loss=1.15812051 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07616007 | Current Learning Rate: 0.0002\nEpoch 559/1000 | Train Loss=1039.49536133 | Val Loss=1.15806508 | Data=10.37352753 | Physics=2.14256047 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07613432 | Current Learning Rate: 0.0002\nEpoch 560/1000 | Train Loss=1039.49523926 | Val Loss=1.15801227 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07610977 | Current Learning Rate: 0.0002\nEpoch 561/1000 | Train Loss=1039.49523926 | Val Loss=1.15795767 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07608438 | Current Learning Rate: 0.0002\n\n Epoch :  560 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 562/1000 | Train Loss=1039.49523926 | Val Loss=1.15790617 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07606053 | Current Learning Rate: 0.0002\nEpoch 563/1000 | Train Loss=1039.49523926 | Val Loss=1.15785241 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07603550 | Current Learning Rate: 0.0002\nEpoch 564/1000 | Train Loss=1039.49523926 | Val Loss=1.15780163 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07601190 | Current Learning Rate: 0.0002\nEpoch 565/1000 | Train Loss=1039.49523926 | Val Loss=1.15775037 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07598805 | Current Learning Rate: 0.0002\nEpoch 566/1000 | Train Loss=1039.49523926 | Val Loss=1.15770030 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07596481 | Current Learning Rate: 0.0002\nEpoch 567/1000 | Train Loss=1039.49523926 | Val Loss=1.15764976 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07594132 | Current Learning Rate: 0.0002\nEpoch 568/1000 | Train Loss=1039.49523926 | Val Loss=1.15760028 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07591832 | Current Learning Rate: 0.0002\nEpoch 569/1000 | Train Loss=1039.49523926 | Val Loss=1.15755093 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512922 | ‚àö(Val Loss) = 1.07589543 | Current Learning Rate: 0.0002\nEpoch 570/1000 | Train Loss=1039.49536133 | Val Loss=1.15750313 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07587314 | Current Learning Rate: 0.0002\nEpoch 571/1000 | Train Loss=1039.49523926 | Val Loss=1.15745473 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07585073 | Current Learning Rate: 0.0002\nEpoch 572/1000 | Train Loss=1039.49523926 | Val Loss=1.15740764 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07582879 | Current Learning Rate: 0.0002\nEpoch 573/1000 | Train Loss=1039.49523926 | Val Loss=1.15735996 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07580662 | Current Learning Rate: 0.0002\nEpoch 574/1000 | Train Loss=1039.49523926 | Val Loss=1.15731430 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07578540 | Current Learning Rate: 0.0002\nEpoch 575/1000 | Train Loss=1039.49523926 | Val Loss=1.15726733 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07576358 | Current Learning Rate: 0.0002\nEpoch 576/1000 | Train Loss=1039.49523926 | Val Loss=1.15722275 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07574284 | Current Learning Rate: 0.0002\nEpoch 577/1000 | Train Loss=1039.49523926 | Val Loss=1.15717721 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07572174 | Current Learning Rate: 0.0002\nEpoch 578/1000 | Train Loss=1039.49523926 | Val Loss=1.15713382 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07570159 | Current Learning Rate: 0.0002\nEpoch 579/1000 | Train Loss=1039.49523926 | Val Loss=1.15708828 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07568038 | Current Learning Rate: 0.0002\nEpoch 580/1000 | Train Loss=1039.49523926 | Val Loss=1.15704572 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07566059 | Current Learning Rate: 0.0002\nEpoch 581/1000 | Train Loss=1039.49523926 | Val Loss=1.15700114 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07563984 | Current Learning Rate: 0.0002\n\n Epoch :  580 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 582/1000 | Train Loss=1039.49523926 | Val Loss=1.15695930 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07562041 | Current Learning Rate: 0.0002\nEpoch 583/1000 | Train Loss=1039.49536133 | Val Loss=1.15691686 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07560074 | Current Learning Rate: 0.0002\nEpoch 584/1000 | Train Loss=1039.49523926 | Val Loss=1.15687478 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07558107 | Current Learning Rate: 0.0002\nEpoch 585/1000 | Train Loss=1039.49523926 | Val Loss=1.15683317 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07556176 | Current Learning Rate: 0.0002\nEpoch 586/1000 | Train Loss=1039.49523926 | Val Loss=1.15679204 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07554269 | Current Learning Rate: 0.0002\nEpoch 587/1000 | Train Loss=1039.49523926 | Val Loss=1.15675104 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07552361 | Current Learning Rate: 0.0002\nEpoch 588/1000 | Train Loss=1039.49523926 | Val Loss=1.15671051 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512898 | ‚àö(Val Loss) = 1.07550478 | Current Learning Rate: 0.0002\nEpoch 589/1000 | Train Loss=1039.49523926 | Val Loss=1.15666962 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07548571 | Current Learning Rate: 0.0002\nEpoch 590/1000 | Train Loss=1039.49523926 | Val Loss=1.15663123 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07546794 | Current Learning Rate: 0.0002\nEpoch 591/1000 | Train Loss=1039.49536133 | Val Loss=1.15659118 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07544935 | Current Learning Rate: 0.0002\nEpoch 592/1000 | Train Loss=1039.49536133 | Val Loss=1.15655315 | Data=10.37352753 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07543159 | Current Learning Rate: 0.0002\nEpoch 593/1000 | Train Loss=1039.49523926 | Val Loss=1.15651429 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07541358 | Current Learning Rate: 0.0002\nEpoch 594/1000 | Train Loss=1039.49523926 | Val Loss=1.15647614 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07539582 | Current Learning Rate: 0.0002\nEpoch 595/1000 | Train Loss=1039.49523926 | Val Loss=1.15643728 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07537770 | Current Learning Rate: 0.0002\nEpoch 596/1000 | Train Loss=1039.49523926 | Val Loss=1.15640056 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07536066 | Current Learning Rate: 0.0002\nEpoch 597/1000 | Train Loss=1039.49523926 | Val Loss=1.15636230 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07534289 | Current Learning Rate: 0.0002\nEpoch 598/1000 | Train Loss=1039.49523926 | Val Loss=1.15632629 | Data=10.37352657 | Physics=2.14256047 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07532609 | Current Learning Rate: 0.0002\nEpoch 599/1000 | Train Loss=1039.49523926 | Val Loss=1.15628922 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07530892 | Current Learning Rate: 0.0002\nEpoch 600/1000 | Train Loss=1039.49523926 | Val Loss=1.15625310 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07529211 | Current Learning Rate: 0.0002\nEpoch 601/1000 | Train Loss=1039.49523926 | Val Loss=1.15621734 | Data=10.37352657 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07527542 | Current Learning Rate: 0.0002\n\n Epoch :  600 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 602/1000 | Train Loss=4708.21093750 | Val Loss=1.15618229 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07525921 | Current Learning Rate: 0.0002\nEpoch 603/1000 | Train Loss=4708.21044922 | Val Loss=1.15614760 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07524300 | Current Learning Rate: 0.0002\nEpoch 604/1000 | Train Loss=4708.21093750 | Val Loss=1.15611184 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512875 | ‚àö(Val Loss) = 1.07522643 | Current Learning Rate: 0.0002\nEpoch 605/1000 | Train Loss=4708.21044922 | Val Loss=1.15607870 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07521105 | Current Learning Rate: 0.0002\nEpoch 606/1000 | Train Loss=4708.21093750 | Val Loss=1.15604281 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07519436 | Current Learning Rate: 0.0002\nEpoch 607/1000 | Train Loss=4708.21093750 | Val Loss=1.15600979 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07517898 | Current Learning Rate: 0.0002\nEpoch 608/1000 | Train Loss=4708.21044922 | Val Loss=1.15597630 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07516336 | Current Learning Rate: 0.0002\nEpoch 609/1000 | Train Loss=4708.21044922 | Val Loss=1.15594316 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07514799 | Current Learning Rate: 0.0002\nEpoch 610/1000 | Train Loss=4708.21044922 | Val Loss=1.15590858 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07513189 | Current Learning Rate: 0.0002\nEpoch 611/1000 | Train Loss=4708.21093750 | Val Loss=1.15587711 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512851 | ‚àö(Val Loss) = 1.07511723 | Current Learning Rate: 0.0002\nEpoch 612/1000 | Train Loss=4708.21093750 | Val Loss=1.15584338 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07510161 | Current Learning Rate: 0.0002\nEpoch 613/1000 | Train Loss=4708.21044922 | Val Loss=1.15581238 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07508719 | Current Learning Rate: 0.0002\nEpoch 614/1000 | Train Loss=4708.21044922 | Val Loss=1.15578020 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07507217 | Current Learning Rate: 0.0002\nEpoch 615/1000 | Train Loss=4708.21093750 | Val Loss=1.15574813 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07505727 | Current Learning Rate: 0.0002\nEpoch 616/1000 | Train Loss=4708.21093750 | Val Loss=1.15571713 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07504284 | Current Learning Rate: 0.0002\nEpoch 617/1000 | Train Loss=4708.21044922 | Val Loss=1.15568566 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07502818 | Current Learning Rate: 0.0002\nEpoch 618/1000 | Train Loss=4708.21093750 | Val Loss=1.15565550 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07501423 | Current Learning Rate: 0.0002\nEpoch 619/1000 | Train Loss=4708.21044922 | Val Loss=1.15562332 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07499921 | Current Learning Rate: 0.0002\nEpoch 620/1000 | Train Loss=4708.21142578 | Val Loss=1.15559375 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07498550 | Current Learning Rate: 0.0002\nEpoch 621/1000 | Train Loss=4708.21093750 | Val Loss=1.15556312 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07497120 | Current Learning Rate: 0.0002\n\n Epoch :  620 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 622/1000 | Train Loss=4708.21044922 | Val Loss=1.15553367 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07495749 | Current Learning Rate: 0.0002\nEpoch 623/1000 | Train Loss=4708.21044922 | Val Loss=1.15550351 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07494354 | Current Learning Rate: 0.0002\nEpoch 624/1000 | Train Loss=4708.21093750 | Val Loss=1.15547478 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07493007 | Current Learning Rate: 0.0002\nEpoch 625/1000 | Train Loss=4708.21044922 | Val Loss=1.15544486 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07491624 | Current Learning Rate: 0.0002\nEpoch 626/1000 | Train Loss=4708.21044922 | Val Loss=1.15541589 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07490277 | Current Learning Rate: 0.0002\nEpoch 627/1000 | Train Loss=4708.21093750 | Val Loss=1.15538657 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07488906 | Current Learning Rate: 0.0002\nEpoch 628/1000 | Train Loss=4708.21044922 | Val Loss=1.15535951 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07487655 | Current Learning Rate: 0.0002\nEpoch 629/1000 | Train Loss=4708.21142578 | Val Loss=1.15532994 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07486272 | Current Learning Rate: 0.0002\nEpoch 630/1000 | Train Loss=4708.21093750 | Val Loss=1.15530276 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07485008 | Current Learning Rate: 0.0002\nEpoch 631/1000 | Train Loss=4708.21093750 | Val Loss=1.15527415 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07483685 | Current Learning Rate: 0.0002\nEpoch 632/1000 | Train Loss=4708.21044922 | Val Loss=1.15524685 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07482409 | Current Learning Rate: 0.0002\nEpoch 633/1000 | Train Loss=4708.21142578 | Val Loss=1.15522039 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07481182 | Current Learning Rate: 0.0002\nEpoch 634/1000 | Train Loss=4708.21044922 | Val Loss=1.15519226 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07479870 | Current Learning Rate: 0.0002\nEpoch 635/1000 | Train Loss=4708.21142578 | Val Loss=1.15516603 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512803 | ‚àö(Val Loss) = 1.07478654 | Current Learning Rate: 0.0002\nEpoch 636/1000 | Train Loss=4708.21093750 | Val Loss=1.15513802 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07477343 | Current Learning Rate: 0.0002\nEpoch 637/1000 | Train Loss=4708.21044922 | Val Loss=1.15511203 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07476139 | Current Learning Rate: 0.0002\nEpoch 638/1000 | Train Loss=4708.21044922 | Val Loss=1.15508485 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12512827 | ‚àö(Val Loss) = 1.07474875 | Current Learning Rate: 0.0002\nEpoch 639/1000 | Train Loss=4708.21093750 | Val Loss=1.15505958 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07473695 | Current Learning Rate: 0.0002\nEpoch 640/1000 | Train Loss=4708.21044922 | Val Loss=1.15503252 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07472444 | Current Learning Rate: 0.0002\nEpoch 641/1000 | Train Loss=4708.21093750 | Val Loss=1.15500808 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07471299 | Current Learning Rate: 0.0002\n\n Epoch :  640 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 642/1000 | Train Loss=4708.21093750 | Val Loss=1.15498161 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07470071 | Current Learning Rate: 0.0002\nEpoch 643/1000 | Train Loss=4708.21093750 | Val Loss=1.15495622 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07468891 | Current Learning Rate: 0.0002\nEpoch 644/1000 | Train Loss=4708.21142578 | Val Loss=1.15493071 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07467699 | Current Learning Rate: 0.0002\nEpoch 645/1000 | Train Loss=4708.21044922 | Val Loss=1.15490711 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07466602 | Current Learning Rate: 0.0002\nEpoch 646/1000 | Train Loss=4708.21093750 | Val Loss=1.15488124 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07465398 | Current Learning Rate: 0.0002\nEpoch 647/1000 | Train Loss=4708.21093750 | Val Loss=1.15485680 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07464266 | Current Learning Rate: 0.0002\nEpoch 648/1000 | Train Loss=4708.21044922 | Val Loss=1.15483189 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07463109 | Current Learning Rate: 0.0002\nEpoch 649/1000 | Train Loss=4708.21142578 | Val Loss=1.15480793 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07461989 | Current Learning Rate: 0.0002\nEpoch 650/1000 | Train Loss=4708.21044922 | Val Loss=1.15478361 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07460856 | Current Learning Rate: 0.0002\nEpoch 651/1000 | Train Loss=4708.21142578 | Val Loss=1.15475941 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07459736 | Current Learning Rate: 0.0002\nEpoch 652/1000 | Train Loss=4708.21093750 | Val Loss=1.15473628 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07458651 | Current Learning Rate: 0.0002\nEpoch 653/1000 | Train Loss=4708.21142578 | Val Loss=1.15471184 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07457519 | Current Learning Rate: 0.0002\nEpoch 654/1000 | Train Loss=4708.21142578 | Val Loss=1.15468967 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07456481 | Current Learning Rate: 0.0002\nEpoch 655/1000 | Train Loss=4708.21044922 | Val Loss=1.15466511 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07455349 | Current Learning Rate: 0.0002\nEpoch 656/1000 | Train Loss=4708.21044922 | Val Loss=1.15464282 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07454312 | Current Learning Rate: 0.0002\nEpoch 657/1000 | Train Loss=4708.21044922 | Val Loss=1.15461910 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07453203 | Current Learning Rate: 0.0002\nEpoch 658/1000 | Train Loss=4708.21093750 | Val Loss=1.15459704 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07452178 | Current Learning Rate: 0.0002\nEpoch 659/1000 | Train Loss=4708.21093750 | Val Loss=1.15457392 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07451105 | Current Learning Rate: 0.0002\nEpoch 660/1000 | Train Loss=4708.21142578 | Val Loss=1.15455163 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07450068 | Current Learning Rate: 0.0002\nEpoch 661/1000 | Train Loss=4708.21093750 | Val Loss=1.15452957 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07449043 | Current Learning Rate: 0.0002\n\n Epoch :  660 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 662/1000 | Train Loss=4708.21093750 | Val Loss=1.15450788 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07448030 | Current Learning Rate: 0.0002\nEpoch 663/1000 | Train Loss=4708.21044922 | Val Loss=1.15448630 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07447028 | Current Learning Rate: 0.0002\nEpoch 664/1000 | Train Loss=4708.21142578 | Val Loss=1.15446329 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07445955 | Current Learning Rate: 0.0002\nEpoch 665/1000 | Train Loss=4708.21044922 | Val Loss=1.15444255 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07444990 | Current Learning Rate: 0.0002\nEpoch 666/1000 | Train Loss=4708.21093750 | Val Loss=1.15441942 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07443917 | Current Learning Rate: 0.0002\nEpoch 667/1000 | Train Loss=4708.21142578 | Val Loss=1.15439939 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07442975 | Current Learning Rate: 0.0002\nEpoch 668/1000 | Train Loss=4708.21093750 | Val Loss=1.15437710 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07441938 | Current Learning Rate: 0.0002\nEpoch 669/1000 | Train Loss=4708.21044922 | Val Loss=1.15435696 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07441008 | Current Learning Rate: 0.0002\nEpoch 670/1000 | Train Loss=4708.21044922 | Val Loss=1.15433538 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07440007 | Current Learning Rate: 0.0002\nEpoch 671/1000 | Train Loss=4708.21093750 | Val Loss=1.15431511 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07439053 | Current Learning Rate: 0.0002\nEpoch 672/1000 | Train Loss=4708.21093750 | Val Loss=1.15429366 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07438064 | Current Learning Rate: 0.0002\nEpoch 673/1000 | Train Loss=4708.21093750 | Val Loss=1.15427339 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07437122 | Current Learning Rate: 0.0002\nEpoch 674/1000 | Train Loss=4708.21093750 | Val Loss=1.15425336 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07436180 | Current Learning Rate: 0.0002\nEpoch 675/1000 | Train Loss=4708.21044922 | Val Loss=1.15423250 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07435215 | Current Learning Rate: 0.0002\nEpoch 676/1000 | Train Loss=4708.21044922 | Val Loss=1.15421307 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07434309 | Current Learning Rate: 0.0002\nEpoch 677/1000 | Train Loss=4708.21093750 | Val Loss=1.15419197 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07433331 | Current Learning Rate: 0.0002\nEpoch 678/1000 | Train Loss=4708.21142578 | Val Loss=1.15417337 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512779 | ‚àö(Val Loss) = 1.07432461 | Current Learning Rate: 0.0002\nEpoch 679/1000 | Train Loss=4708.21093750 | Val Loss=1.15415239 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07431483 | Current Learning Rate: 0.0002\nEpoch 680/1000 | Train Loss=4708.21044922 | Val Loss=1.15413356 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07430613 | Current Learning Rate: 0.0002\nEpoch 681/1000 | Train Loss=4708.21044922 | Val Loss=1.15411341 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07429671 | Current Learning Rate: 0.0002\n\n Epoch :  680 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 682/1000 | Train Loss=4708.21093750 | Val Loss=1.15409470 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07428801 | Current Learning Rate: 0.0002\nEpoch 683/1000 | Train Loss=4708.21044922 | Val Loss=1.15407431 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07427847 | Current Learning Rate: 0.0002\nEpoch 684/1000 | Train Loss=4708.21044922 | Val Loss=1.15405655 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512755 | ‚àö(Val Loss) = 1.07427025 | Current Learning Rate: 0.0002\nEpoch 685/1000 | Train Loss=4708.21044922 | Val Loss=1.15403771 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12512684 | ‚àö(Val Loss) = 1.07426143 | Current Learning Rate: 0.0002\nEpoch 686/1000 | Train Loss=4708.21093750 | Val Loss=1.15401840 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12512684 | ‚àö(Val Loss) = 1.07425249 | Current Learning Rate: 0.0002\nEpoch 687/1000 | Train Loss=4708.21044922 | Val Loss=1.15400016 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512684 | ‚àö(Val Loss) = 1.07424402 | Current Learning Rate: 0.0002\nEpoch 688/1000 | Train Loss=4708.21093750 | Val Loss=1.15398061 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512684 | ‚àö(Val Loss) = 1.07423484 | Current Learning Rate: 0.0002\nEpoch 689/1000 | Train Loss=4708.21044922 | Val Loss=1.15396333 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07422686 | Current Learning Rate: 0.0002\nEpoch 690/1000 | Train Loss=4708.21093750 | Val Loss=1.15394378 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07421780 | Current Learning Rate: 0.0002\nEpoch 691/1000 | Train Loss=4708.21044922 | Val Loss=1.15392578 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07420933 | Current Learning Rate: 0.0002\nEpoch 692/1000 | Train Loss=4708.21142578 | Val Loss=1.15390778 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07420099 | Current Learning Rate: 0.0002\nEpoch 693/1000 | Train Loss=4708.21044922 | Val Loss=1.15388966 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07419252 | Current Learning Rate: 0.0002\nEpoch 694/1000 | Train Loss=4708.21044922 | Val Loss=1.15387237 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07418454 | Current Learning Rate: 0.0002\nEpoch 695/1000 | Train Loss=4708.21093750 | Val Loss=1.15385354 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07417572 | Current Learning Rate: 0.0002\nEpoch 696/1000 | Train Loss=4708.21093750 | Val Loss=1.15383685 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07416797 | Current Learning Rate: 0.0002\nEpoch 697/1000 | Train Loss=4708.21044922 | Val Loss=1.15381801 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07415926 | Current Learning Rate: 0.0002\nEpoch 698/1000 | Train Loss=4708.21044922 | Val Loss=1.15380216 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512732 | ‚àö(Val Loss) = 1.07415187 | Current Learning Rate: 0.0002\nEpoch 699/1000 | Train Loss=4708.21093750 | Val Loss=1.15378320 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12512684 | ‚àö(Val Loss) = 1.07414305 | Current Learning Rate: 0.0002\nEpoch 700/1000 | Train Loss=4708.21044922 | Val Loss=1.15376735 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512684 | ‚àö(Val Loss) = 1.07413566 | Current Learning Rate: 0.0002\nEpoch 701/1000 | Train Loss=4708.21044922 | Val Loss=1.15374923 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12512684 | ‚àö(Val Loss) = 1.07412720 | Current Learning Rate: 0.0002\n‚úÖ Learning Rate updated to 0.001\n\n Epoch :  700 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 702/1000 | Train Loss=4708.21093750 | Val Loss=1.28200126 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502861 | ‚àö(Val Loss) = 1.13225496 | Current Learning Rate: 0.001\nEpoch 703/1000 | Train Loss=4708.21142578 | Val Loss=1.28180873 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502813 | ‚àö(Val Loss) = 1.13216996 | Current Learning Rate: 0.001\nEpoch 704/1000 | Train Loss=4708.21142578 | Val Loss=1.28161657 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12502813 | ‚àö(Val Loss) = 1.13208508 | Current Learning Rate: 0.001\nEpoch 705/1000 | Train Loss=4708.21142578 | Val Loss=1.28142762 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502861 | ‚àö(Val Loss) = 1.13200164 | Current Learning Rate: 0.001\nEpoch 706/1000 | Train Loss=4708.21093750 | Val Loss=1.28123999 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502789 | ‚àö(Val Loss) = 1.13191867 | Current Learning Rate: 0.001\nEpoch 707/1000 | Train Loss=4708.21093750 | Val Loss=1.28105402 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502837 | ‚àö(Val Loss) = 1.13183653 | Current Learning Rate: 0.001\nEpoch 708/1000 | Train Loss=4708.21093750 | Val Loss=1.28086805 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502837 | ‚àö(Val Loss) = 1.13175440 | Current Learning Rate: 0.001\nEpoch 709/1000 | Train Loss=4708.21093750 | Val Loss=1.28068244 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502766 | ‚àö(Val Loss) = 1.13167238 | Current Learning Rate: 0.001\nEpoch 710/1000 | Train Loss=4708.21044922 | Val Loss=1.28049874 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502813 | ‚àö(Val Loss) = 1.13159120 | Current Learning Rate: 0.001\nEpoch 711/1000 | Train Loss=4708.21142578 | Val Loss=1.28031611 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12502813 | ‚àö(Val Loss) = 1.13151050 | Current Learning Rate: 0.001\nEpoch 712/1000 | Train Loss=4708.21044922 | Val Loss=1.28013647 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502742 | ‚àö(Val Loss) = 1.13143110 | Current Learning Rate: 0.001\nEpoch 713/1000 | Train Loss=4708.21142578 | Val Loss=1.27995586 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502789 | ‚àö(Val Loss) = 1.13135135 | Current Learning Rate: 0.001\nEpoch 714/1000 | Train Loss=4708.21142578 | Val Loss=1.27977669 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502789 | ‚àö(Val Loss) = 1.13127220 | Current Learning Rate: 0.001\nEpoch 715/1000 | Train Loss=4708.21142578 | Val Loss=1.27959919 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502718 | ‚àö(Val Loss) = 1.13119376 | Current Learning Rate: 0.001\nEpoch 716/1000 | Train Loss=4708.21093750 | Val Loss=1.27942288 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502766 | ‚àö(Val Loss) = 1.13111579 | Current Learning Rate: 0.001\nEpoch 717/1000 | Train Loss=4708.21093750 | Val Loss=1.27924705 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502694 | ‚àö(Val Loss) = 1.13103807 | Current Learning Rate: 0.001\nEpoch 718/1000 | Train Loss=4708.21044922 | Val Loss=1.27907205 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502694 | ‚àö(Val Loss) = 1.13096070 | Current Learning Rate: 0.001\nEpoch 719/1000 | Train Loss=4708.21093750 | Val Loss=1.27889800 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12502742 | ‚àö(Val Loss) = 1.13088369 | Current Learning Rate: 0.001\nEpoch 720/1000 | Train Loss=4708.21093750 | Val Loss=1.27872765 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12502670 | ‚àö(Val Loss) = 1.13080835 | Current Learning Rate: 0.001\nEpoch 721/1000 | Train Loss=4708.21142578 | Val Loss=1.27855527 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502670 | ‚àö(Val Loss) = 1.13073218 | Current Learning Rate: 0.001\n\n Epoch :  720 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 722/1000 | Train Loss=4708.21044922 | Val Loss=1.27838552 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502718 | ‚àö(Val Loss) = 1.13065708 | Current Learning Rate: 0.001\nEpoch 723/1000 | Train Loss=4708.21142578 | Val Loss=1.27821517 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502670 | ‚àö(Val Loss) = 1.13058174 | Current Learning Rate: 0.001\nEpoch 724/1000 | Train Loss=4708.21044922 | Val Loss=1.27804697 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502670 | ‚àö(Val Loss) = 1.13050735 | Current Learning Rate: 0.001\nEpoch 725/1000 | Train Loss=4708.21142578 | Val Loss=1.27788103 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502694 | ‚àö(Val Loss) = 1.13043404 | Current Learning Rate: 0.001\nEpoch 726/1000 | Train Loss=4708.21093750 | Val Loss=1.27771461 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502646 | ‚àö(Val Loss) = 1.13036036 | Current Learning Rate: 0.001\nEpoch 727/1000 | Train Loss=4708.21093750 | Val Loss=1.27754891 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502646 | ‚àö(Val Loss) = 1.13028705 | Current Learning Rate: 0.001\nEpoch 728/1000 | Train Loss=4708.21044922 | Val Loss=1.27738452 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502670 | ‚àö(Val Loss) = 1.13021433 | Current Learning Rate: 0.001\nEpoch 729/1000 | Train Loss=4708.21044922 | Val Loss=1.27722204 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502623 | ‚àö(Val Loss) = 1.13014245 | Current Learning Rate: 0.001\nEpoch 730/1000 | Train Loss=4708.21093750 | Val Loss=1.27705920 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502623 | ‚àö(Val Loss) = 1.13007045 | Current Learning Rate: 0.001\nEpoch 731/1000 | Train Loss=4708.21044922 | Val Loss=1.27689767 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502670 | ‚àö(Val Loss) = 1.12999892 | Current Learning Rate: 0.001\nEpoch 732/1000 | Train Loss=4708.21093750 | Val Loss=1.27673805 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12502599 | ‚àö(Val Loss) = 1.12992835 | Current Learning Rate: 0.001\nEpoch 733/1000 | Train Loss=4708.21093750 | Val Loss=1.27657855 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502599 | ‚àö(Val Loss) = 1.12985778 | Current Learning Rate: 0.001\nEpoch 734/1000 | Train Loss=4708.21142578 | Val Loss=1.27641928 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12502646 | ‚àö(Val Loss) = 1.12978733 | Current Learning Rate: 0.001\nEpoch 735/1000 | Train Loss=4708.21044922 | Val Loss=1.27626204 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502575 | ‚àö(Val Loss) = 1.12971771 | Current Learning Rate: 0.001\nEpoch 736/1000 | Train Loss=4708.21093750 | Val Loss=1.27610493 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502575 | ‚àö(Val Loss) = 1.12964809 | Current Learning Rate: 0.001\nEpoch 737/1000 | Train Loss=4708.21044922 | Val Loss=1.27595019 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502623 | ‚àö(Val Loss) = 1.12957966 | Current Learning Rate: 0.001\nEpoch 738/1000 | Train Loss=4708.21093750 | Val Loss=1.27579403 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502551 | ‚àö(Val Loss) = 1.12951052 | Current Learning Rate: 0.001\nEpoch 739/1000 | Train Loss=4708.21093750 | Val Loss=1.27564192 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502599 | ‚àö(Val Loss) = 1.12944317 | Current Learning Rate: 0.001\nEpoch 740/1000 | Train Loss=4708.21044922 | Val Loss=1.27548802 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502599 | ‚àö(Val Loss) = 1.12937510 | Current Learning Rate: 0.001\nEpoch 741/1000 | Train Loss=4708.21093750 | Val Loss=1.27533603 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12502527 | ‚àö(Val Loss) = 1.12930775 | Current Learning Rate: 0.001\n\n Epoch :  740 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 742/1000 | Train Loss=4708.21093750 | Val Loss=1.27518451 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502575 | ‚àö(Val Loss) = 1.12924063 | Current Learning Rate: 0.001\nEpoch 743/1000 | Train Loss=4708.21044922 | Val Loss=1.27503490 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502503 | ‚àö(Val Loss) = 1.12917447 | Current Learning Rate: 0.001\nEpoch 744/1000 | Train Loss=4708.21093750 | Val Loss=1.27488506 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12502503 | ‚àö(Val Loss) = 1.12910807 | Current Learning Rate: 0.001\nEpoch 745/1000 | Train Loss=4708.21044922 | Val Loss=1.27473712 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502551 | ‚àö(Val Loss) = 1.12904263 | Current Learning Rate: 0.001\nEpoch 746/1000 | Train Loss=4708.21044922 | Val Loss=1.27458930 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502480 | ‚àö(Val Loss) = 1.12897706 | Current Learning Rate: 0.001\nEpoch 747/1000 | Train Loss=4708.21093750 | Val Loss=1.27444160 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502480 | ‚àö(Val Loss) = 1.12891173 | Current Learning Rate: 0.001\nEpoch 748/1000 | Train Loss=4708.21044922 | Val Loss=1.27429533 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502527 | ‚àö(Val Loss) = 1.12884688 | Current Learning Rate: 0.001\nEpoch 749/1000 | Train Loss=4708.21142578 | Val Loss=1.27415001 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502480 | ‚àö(Val Loss) = 1.12878251 | Current Learning Rate: 0.001\nEpoch 750/1000 | Train Loss=4708.21142578 | Val Loss=1.27400529 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502480 | ‚àö(Val Loss) = 1.12871838 | Current Learning Rate: 0.001\nEpoch 751/1000 | Train Loss=4708.21142578 | Val Loss=1.27386200 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502503 | ‚àö(Val Loss) = 1.12865496 | Current Learning Rate: 0.001\nEpoch 752/1000 | Train Loss=4708.21044922 | Val Loss=1.27371848 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502456 | ‚àö(Val Loss) = 1.12859142 | Current Learning Rate: 0.001\nEpoch 753/1000 | Train Loss=4708.21044922 | Val Loss=1.27357721 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502456 | ‚àö(Val Loss) = 1.12852883 | Current Learning Rate: 0.001\nEpoch 754/1000 | Train Loss=4708.21044922 | Val Loss=1.27343702 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12502503 | ‚àö(Val Loss) = 1.12846673 | Current Learning Rate: 0.001\nEpoch 755/1000 | Train Loss=4708.21093750 | Val Loss=1.27329600 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502432 | ‚àö(Val Loss) = 1.12840414 | Current Learning Rate: 0.001\nEpoch 756/1000 | Train Loss=4708.21093750 | Val Loss=1.27315450 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12502432 | ‚àö(Val Loss) = 1.12834144 | Current Learning Rate: 0.001\nEpoch 757/1000 | Train Loss=4708.21093750 | Val Loss=1.27301586 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502480 | ‚àö(Val Loss) = 1.12828004 | Current Learning Rate: 0.001\nEpoch 758/1000 | Train Loss=4708.21142578 | Val Loss=1.27287853 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12502408 | ‚àö(Val Loss) = 1.12821913 | Current Learning Rate: 0.001\nEpoch 759/1000 | Train Loss=4708.21142578 | Val Loss=1.27274024 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502408 | ‚àö(Val Loss) = 1.12815785 | Current Learning Rate: 0.001\nEpoch 760/1000 | Train Loss=4708.21044922 | Val Loss=1.27260554 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502456 | ‚àö(Val Loss) = 1.12809825 | Current Learning Rate: 0.001\nEpoch 761/1000 | Train Loss=4708.21142578 | Val Loss=1.27246845 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502408 | ‚àö(Val Loss) = 1.12803745 | Current Learning Rate: 0.001\n\n Epoch :  760 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 762/1000 | Train Loss=4708.21142578 | Val Loss=1.27233231 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502456 | ‚àö(Val Loss) = 1.12797713 | Current Learning Rate: 0.001\nEpoch 763/1000 | Train Loss=4708.21044922 | Val Loss=1.27219856 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502456 | ‚àö(Val Loss) = 1.12791777 | Current Learning Rate: 0.001\nEpoch 764/1000 | Train Loss=4708.21142578 | Val Loss=1.27206469 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502384 | ‚àö(Val Loss) = 1.12785840 | Current Learning Rate: 0.001\nEpoch 765/1000 | Train Loss=4708.21044922 | Val Loss=1.27193117 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12502408 | ‚àö(Val Loss) = 1.12779927 | Current Learning Rate: 0.001\nEpoch 766/1000 | Train Loss=4708.21093750 | Val Loss=1.27180004 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502360 | ‚àö(Val Loss) = 1.12774110 | Current Learning Rate: 0.001\nEpoch 767/1000 | Train Loss=4708.21093750 | Val Loss=1.27166843 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502337 | ‚àö(Val Loss) = 1.12768281 | Current Learning Rate: 0.001\nEpoch 768/1000 | Train Loss=4708.21044922 | Val Loss=1.27153802 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502408 | ‚àö(Val Loss) = 1.12762499 | Current Learning Rate: 0.001\nEpoch 769/1000 | Train Loss=4708.21093750 | Val Loss=1.27140689 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502337 | ‚àö(Val Loss) = 1.12756681 | Current Learning Rate: 0.001\nEpoch 770/1000 | Train Loss=4708.21044922 | Val Loss=1.27127612 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12502337 | ‚àö(Val Loss) = 1.12750876 | Current Learning Rate: 0.001\nEpoch 771/1000 | Train Loss=4708.21093750 | Val Loss=1.27114880 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502384 | ‚àö(Val Loss) = 1.12745237 | Current Learning Rate: 0.001\nEpoch 772/1000 | Train Loss=4708.21093750 | Val Loss=1.27102029 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502313 | ‚àö(Val Loss) = 1.12739539 | Current Learning Rate: 0.001\nEpoch 773/1000 | Train Loss=4708.21044922 | Val Loss=1.27089298 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502313 | ‚àö(Val Loss) = 1.12733889 | Current Learning Rate: 0.001\nEpoch 774/1000 | Train Loss=4708.21093750 | Val Loss=1.27076602 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12502360 | ‚àö(Val Loss) = 1.12728262 | Current Learning Rate: 0.001\nEpoch 775/1000 | Train Loss=4708.21044922 | Val Loss=1.27063906 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502289 | ‚àö(Val Loss) = 1.12722623 | Current Learning Rate: 0.001\nEpoch 776/1000 | Train Loss=4708.21093750 | Val Loss=1.27051580 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502289 | ‚àö(Val Loss) = 1.12717164 | Current Learning Rate: 0.001\nEpoch 777/1000 | Train Loss=4708.21044922 | Val Loss=1.27039087 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502337 | ‚àö(Val Loss) = 1.12711620 | Current Learning Rate: 0.001\nEpoch 778/1000 | Train Loss=4708.21142578 | Val Loss=1.27026653 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12502265 | ‚àö(Val Loss) = 1.12706101 | Current Learning Rate: 0.001\nEpoch 779/1000 | Train Loss=4708.21044922 | Val Loss=1.27014363 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502265 | ‚àö(Val Loss) = 1.12700653 | Current Learning Rate: 0.001\nEpoch 780/1000 | Train Loss=4708.21142578 | Val Loss=1.27001894 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502313 | ‚àö(Val Loss) = 1.12695122 | Current Learning Rate: 0.001\nEpoch 781/1000 | Train Loss=4708.21093750 | Val Loss=1.26989770 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12502241 | ‚àö(Val Loss) = 1.12689734 | Current Learning Rate: 0.001\n\n Epoch :  780 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 782/1000 | Train Loss=4708.21093750 | Val Loss=1.26977599 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12502313 | ‚àö(Val Loss) = 1.12684333 | Current Learning Rate: 0.001\nEpoch 783/1000 | Train Loss=4708.21093750 | Val Loss=1.26965594 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502241 | ‚àö(Val Loss) = 1.12679005 | Current Learning Rate: 0.001\nEpoch 784/1000 | Train Loss=4708.21093750 | Val Loss=1.26953638 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502217 | ‚àö(Val Loss) = 1.12673700 | Current Learning Rate: 0.001\nEpoch 785/1000 | Train Loss=4708.21044922 | Val Loss=1.26941609 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12502265 | ‚àö(Val Loss) = 1.12668371 | Current Learning Rate: 0.001\nEpoch 786/1000 | Train Loss=4708.21093750 | Val Loss=1.26929653 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502193 | ‚àö(Val Loss) = 1.12663066 | Current Learning Rate: 0.001\nEpoch 787/1000 | Train Loss=4708.21044922 | Val Loss=1.26917756 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502193 | ‚àö(Val Loss) = 1.12657785 | Current Learning Rate: 0.001\nEpoch 788/1000 | Train Loss=4708.21093750 | Val Loss=1.26905966 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502265 | ‚àö(Val Loss) = 1.12652552 | Current Learning Rate: 0.001\nEpoch 789/1000 | Train Loss=4708.21142578 | Val Loss=1.26894259 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502193 | ‚àö(Val Loss) = 1.12647355 | Current Learning Rate: 0.001\nEpoch 790/1000 | Train Loss=4708.21142578 | Val Loss=1.26882601 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502193 | ‚àö(Val Loss) = 1.12642181 | Current Learning Rate: 0.001\nEpoch 791/1000 | Train Loss=4708.21044922 | Val Loss=1.26871061 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502241 | ‚àö(Val Loss) = 1.12637055 | Current Learning Rate: 0.001\nEpoch 792/1000 | Train Loss=4708.21044922 | Val Loss=1.26859534 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502170 | ‚àö(Val Loss) = 1.12631941 | Current Learning Rate: 0.001\nEpoch 793/1000 | Train Loss=4708.21044922 | Val Loss=1.26848054 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502170 | ‚àö(Val Loss) = 1.12626839 | Current Learning Rate: 0.001\nEpoch 794/1000 | Train Loss=4708.21093750 | Val Loss=1.26836514 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502193 | ‚àö(Val Loss) = 1.12621713 | Current Learning Rate: 0.001\nEpoch 795/1000 | Train Loss=4708.21093750 | Val Loss=1.26825213 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502146 | ‚àö(Val Loss) = 1.12616706 | Current Learning Rate: 0.001\nEpoch 796/1000 | Train Loss=4708.21142578 | Val Loss=1.26813912 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502146 | ‚àö(Val Loss) = 1.12611687 | Current Learning Rate: 0.001\nEpoch 797/1000 | Train Loss=4708.21044922 | Val Loss=1.26802588 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12502193 | ‚àö(Val Loss) = 1.12606657 | Current Learning Rate: 0.001\nEpoch 798/1000 | Train Loss=4708.21142578 | Val Loss=1.26791322 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12502122 | ‚àö(Val Loss) = 1.12601650 | Current Learning Rate: 0.001\nEpoch 799/1000 | Train Loss=4708.21093750 | Val Loss=1.26780355 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502122 | ‚àö(Val Loss) = 1.12596786 | Current Learning Rate: 0.001\nEpoch 800/1000 | Train Loss=4708.21093750 | Val Loss=1.26769328 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12502170 | ‚àö(Val Loss) = 1.12591887 | Current Learning Rate: 0.0001\nEpoch 801/1000 | Train Loss=4708.21093750 | Val Loss=1.14423299 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513947 | ‚àö(Val Loss) = 1.06968832 | Current Learning Rate: 0.0001\n\n Epoch :  800 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 802/1000 | Train Loss=4708.21142578 | Val Loss=1.14422834 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513947 | ‚àö(Val Loss) = 1.06968606 | Current Learning Rate: 0.0001\nEpoch 803/1000 | Train Loss=4708.21142578 | Val Loss=1.14422464 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513947 | ‚àö(Val Loss) = 1.06968439 | Current Learning Rate: 0.0001\nEpoch 804/1000 | Train Loss=4708.21093750 | Val Loss=1.14422071 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513947 | ‚àö(Val Loss) = 1.06968248 | Current Learning Rate: 0.0001\nEpoch 805/1000 | Train Loss=4708.21093750 | Val Loss=1.14421594 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513947 | ‚àö(Val Loss) = 1.06968033 | Current Learning Rate: 0.0001\nEpoch 806/1000 | Train Loss=4708.21093750 | Val Loss=1.14421141 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513947 | ‚àö(Val Loss) = 1.06967819 | Current Learning Rate: 0.0001\nEpoch 807/1000 | Train Loss=4708.21044922 | Val Loss=1.14420736 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06967628 | Current Learning Rate: 0.0001\nEpoch 808/1000 | Train Loss=4708.21093750 | Val Loss=1.14420307 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06967425 | Current Learning Rate: 0.0001\nEpoch 809/1000 | Train Loss=4708.21093750 | Val Loss=1.14419866 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06967223 | Current Learning Rate: 0.0001\nEpoch 810/1000 | Train Loss=4708.21142578 | Val Loss=1.14419544 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06967068 | Current Learning Rate: 0.0001\nEpoch 811/1000 | Train Loss=4708.21044922 | Val Loss=1.14419150 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06966889 | Current Learning Rate: 0.0001\nEpoch 812/1000 | Train Loss=4708.21093750 | Val Loss=1.14418685 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513947 | ‚àö(Val Loss) = 1.06966674 | Current Learning Rate: 0.0001\nEpoch 813/1000 | Train Loss=4708.21142578 | Val Loss=1.14418304 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513947 | ‚àö(Val Loss) = 1.06966496 | Current Learning Rate: 0.0001\nEpoch 814/1000 | Train Loss=4708.21093750 | Val Loss=1.14417922 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06966317 | Current Learning Rate: 0.0001\nEpoch 815/1000 | Train Loss=4708.21044922 | Val Loss=1.14417529 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06966126 | Current Learning Rate: 0.0001\nEpoch 816/1000 | Train Loss=4708.21044922 | Val Loss=1.14417064 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06965911 | Current Learning Rate: 0.0001\nEpoch 817/1000 | Train Loss=4708.21093750 | Val Loss=1.14416742 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06965756 | Current Learning Rate: 0.0001\nEpoch 818/1000 | Train Loss=4708.21142578 | Val Loss=1.14416349 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06965578 | Current Learning Rate: 0.0001\nEpoch 819/1000 | Train Loss=4708.21093750 | Val Loss=1.14415991 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06965411 | Current Learning Rate: 0.0001\nEpoch 820/1000 | Train Loss=4708.21044922 | Val Loss=1.14415514 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12513947 | ‚àö(Val Loss) = 1.06965184 | Current Learning Rate: 0.0001\nEpoch 821/1000 | Train Loss=4708.21142578 | Val Loss=1.14415109 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06964993 | Current Learning Rate: 0.0001\n\n Epoch :  820 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 822/1000 | Train Loss=4708.21044922 | Val Loss=1.14414716 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06964815 | Current Learning Rate: 0.0001\nEpoch 823/1000 | Train Loss=4708.21142578 | Val Loss=1.14414394 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06964660 | Current Learning Rate: 0.0001\nEpoch 824/1000 | Train Loss=4708.21044922 | Val Loss=1.14414001 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06964481 | Current Learning Rate: 0.0001\nEpoch 825/1000 | Train Loss=4708.21044922 | Val Loss=1.14413607 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06964302 | Current Learning Rate: 0.0001\nEpoch 826/1000 | Train Loss=4708.21044922 | Val Loss=1.14413214 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06964111 | Current Learning Rate: 0.0001\nEpoch 827/1000 | Train Loss=4708.21142578 | Val Loss=1.14412820 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06963933 | Current Learning Rate: 0.0001\nEpoch 828/1000 | Train Loss=4708.21044922 | Val Loss=1.14412487 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06963778 | Current Learning Rate: 0.0001\nEpoch 829/1000 | Train Loss=4708.21093750 | Val Loss=1.14412117 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06963599 | Current Learning Rate: 0.0001\nEpoch 830/1000 | Train Loss=4708.21044922 | Val Loss=1.14411771 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06963444 | Current Learning Rate: 0.0001\nEpoch 831/1000 | Train Loss=4708.21044922 | Val Loss=1.14411318 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06963229 | Current Learning Rate: 0.0001\nEpoch 832/1000 | Train Loss=4708.21093750 | Val Loss=1.14410985 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06963074 | Current Learning Rate: 0.0001\nEpoch 833/1000 | Train Loss=4708.21093750 | Val Loss=1.14410591 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06962883 | Current Learning Rate: 0.0001\nEpoch 834/1000 | Train Loss=4708.21093750 | Val Loss=1.14410245 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06962729 | Current Learning Rate: 0.0001\nEpoch 835/1000 | Train Loss=4708.21093750 | Val Loss=1.14409864 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06962550 | Current Learning Rate: 0.0001\nEpoch 836/1000 | Train Loss=4708.21044922 | Val Loss=1.14409506 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06962383 | Current Learning Rate: 0.0001\nEpoch 837/1000 | Train Loss=4708.21093750 | Val Loss=1.14409173 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06962228 | Current Learning Rate: 0.0001\nEpoch 838/1000 | Train Loss=4708.21044922 | Val Loss=1.14408672 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06961989 | Current Learning Rate: 0.0001\nEpoch 839/1000 | Train Loss=4708.21142578 | Val Loss=1.14408410 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06961870 | Current Learning Rate: 0.0001\nEpoch 840/1000 | Train Loss=4708.21044922 | Val Loss=1.14408064 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06961703 | Current Learning Rate: 0.0001\nEpoch 841/1000 | Train Loss=4708.21044922 | Val Loss=1.14407754 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06961560 | Current Learning Rate: 0.0001\n\n Epoch :  840 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 842/1000 | Train Loss=4708.21044922 | Val Loss=1.14407349 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06961370 | Current Learning Rate: 0.0001\nEpoch 843/1000 | Train Loss=4708.21093750 | Val Loss=1.14407003 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06961215 | Current Learning Rate: 0.0001\nEpoch 844/1000 | Train Loss=4708.21093750 | Val Loss=1.14406669 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06961048 | Current Learning Rate: 0.0001\nEpoch 845/1000 | Train Loss=4708.21142578 | Val Loss=1.14406347 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06960905 | Current Learning Rate: 0.0001\nEpoch 846/1000 | Train Loss=4708.21044922 | Val Loss=1.14406013 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06960750 | Current Learning Rate: 0.0001\nEpoch 847/1000 | Train Loss=4708.21093750 | Val Loss=1.14405596 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06960547 | Current Learning Rate: 0.0001\nEpoch 848/1000 | Train Loss=4708.21093750 | Val Loss=1.14405227 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06960380 | Current Learning Rate: 0.0001\nEpoch 849/1000 | Train Loss=4708.21044922 | Val Loss=1.14404881 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06960213 | Current Learning Rate: 0.0001\nEpoch 850/1000 | Train Loss=4708.21093750 | Val Loss=1.14404547 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06960058 | Current Learning Rate: 0.0001\nEpoch 851/1000 | Train Loss=4708.21093750 | Val Loss=1.14404213 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06959903 | Current Learning Rate: 0.0001\nEpoch 852/1000 | Train Loss=4708.21044922 | Val Loss=1.14403880 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06959748 | Current Learning Rate: 0.0001\nEpoch 853/1000 | Train Loss=4708.21093750 | Val Loss=1.14403534 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06959593 | Current Learning Rate: 0.0001\nEpoch 854/1000 | Train Loss=4708.21093750 | Val Loss=1.14403200 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06959426 | Current Learning Rate: 0.0001\nEpoch 855/1000 | Train Loss=4708.21044922 | Val Loss=1.14402866 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06959271 | Current Learning Rate: 0.0001\nEpoch 856/1000 | Train Loss=4708.21044922 | Val Loss=1.14402521 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06959116 | Current Learning Rate: 0.0001\nEpoch 857/1000 | Train Loss=4708.21044922 | Val Loss=1.14402127 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06958926 | Current Learning Rate: 0.0001\nEpoch 858/1000 | Train Loss=4708.21093750 | Val Loss=1.14401853 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06958807 | Current Learning Rate: 0.0001\nEpoch 859/1000 | Train Loss=4708.21093750 | Val Loss=1.14401519 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06958652 | Current Learning Rate: 0.0001\nEpoch 860/1000 | Train Loss=4708.21093750 | Val Loss=1.14401245 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06958520 | Current Learning Rate: 0.0001\nEpoch 861/1000 | Train Loss=4708.21093750 | Val Loss=1.14400935 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06958377 | Current Learning Rate: 0.0001\n\n Epoch :  860 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 862/1000 | Train Loss=4708.21093750 | Val Loss=1.14400566 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06958199 | Current Learning Rate: 0.0001\nEpoch 863/1000 | Train Loss=4708.21142578 | Val Loss=1.14400244 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12513924 | ‚àö(Val Loss) = 1.06958044 | Current Learning Rate: 0.0001\nEpoch 864/1000 | Train Loss=4708.21142578 | Val Loss=1.14399886 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06957877 | Current Learning Rate: 0.0001\nEpoch 865/1000 | Train Loss=4708.21044922 | Val Loss=1.14399624 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06957757 | Current Learning Rate: 0.0001\nEpoch 866/1000 | Train Loss=4708.21093750 | Val Loss=1.14399278 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06957603 | Current Learning Rate: 0.0001\nEpoch 867/1000 | Train Loss=4708.21044922 | Val Loss=1.14398932 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06957436 | Current Learning Rate: 0.0001\nEpoch 868/1000 | Train Loss=4708.21044922 | Val Loss=1.14398611 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06957281 | Current Learning Rate: 0.0001\nEpoch 869/1000 | Train Loss=4708.21093750 | Val Loss=1.14398348 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06957161 | Current Learning Rate: 0.0001\nEpoch 870/1000 | Train Loss=4708.21142578 | Val Loss=1.14398003 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06957006 | Current Learning Rate: 0.0001\nEpoch 871/1000 | Train Loss=4708.21093750 | Val Loss=1.14397645 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06956840 | Current Learning Rate: 0.0001\nEpoch 872/1000 | Train Loss=4708.21044922 | Val Loss=1.14397383 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06956708 | Current Learning Rate: 0.0001\nEpoch 873/1000 | Train Loss=4708.21044922 | Val Loss=1.14397049 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06956553 | Current Learning Rate: 0.0001\nEpoch 874/1000 | Train Loss=4708.21093750 | Val Loss=1.14396787 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06956434 | Current Learning Rate: 0.0001\nEpoch 875/1000 | Train Loss=4708.21093750 | Val Loss=1.14396429 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06956267 | Current Learning Rate: 0.0001\nEpoch 876/1000 | Train Loss=4708.21142578 | Val Loss=1.14396167 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06956148 | Current Learning Rate: 0.0001\nEpoch 877/1000 | Train Loss=4708.21142578 | Val Loss=1.14395821 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06955981 | Current Learning Rate: 0.0001\nEpoch 878/1000 | Train Loss=4708.21044922 | Val Loss=1.14395547 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06955850 | Current Learning Rate: 0.0001\nEpoch 879/1000 | Train Loss=4708.21093750 | Val Loss=1.14395189 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06955683 | Current Learning Rate: 0.0001\nEpoch 880/1000 | Train Loss=4708.21093750 | Val Loss=1.14394927 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06955564 | Current Learning Rate: 0.0001\nEpoch 881/1000 | Train Loss=4708.21142578 | Val Loss=1.14394569 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06955397 | Current Learning Rate: 0.0001\n\n Epoch :  880 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 882/1000 | Train Loss=4708.21093750 | Val Loss=1.14394319 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06955278 | Current Learning Rate: 0.0001\nEpoch 883/1000 | Train Loss=4708.21093750 | Val Loss=1.14393961 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06955111 | Current Learning Rate: 0.0001\nEpoch 884/1000 | Train Loss=4708.21093750 | Val Loss=1.14393687 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06954980 | Current Learning Rate: 0.0001\nEpoch 885/1000 | Train Loss=4708.21093750 | Val Loss=1.14393353 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06954825 | Current Learning Rate: 0.0001\nEpoch 886/1000 | Train Loss=4708.21142578 | Val Loss=1.14393079 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06954706 | Current Learning Rate: 0.0001\nEpoch 887/1000 | Train Loss=4708.21142578 | Val Loss=1.14392805 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06954575 | Current Learning Rate: 0.0001\nEpoch 888/1000 | Train Loss=4708.21093750 | Val Loss=1.14392436 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06954396 | Current Learning Rate: 0.0001\nEpoch 889/1000 | Train Loss=4708.21093750 | Val Loss=1.14392233 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06954300 | Current Learning Rate: 0.0001\nEpoch 890/1000 | Train Loss=4708.21044922 | Val Loss=1.14391947 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06954169 | Current Learning Rate: 0.0001\nEpoch 891/1000 | Train Loss=4708.21142578 | Val Loss=1.14391685 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513900 | ‚àö(Val Loss) = 1.06954050 | Current Learning Rate: 0.0001\nEpoch 892/1000 | Train Loss=4708.21044922 | Val Loss=1.14391327 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06953883 | Current Learning Rate: 0.0001\nEpoch 893/1000 | Train Loss=4708.21044922 | Val Loss=1.14391053 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06953752 | Current Learning Rate: 0.0001\nEpoch 894/1000 | Train Loss=4708.21093750 | Val Loss=1.14390767 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06953621 | Current Learning Rate: 0.0001\nEpoch 895/1000 | Train Loss=4708.21093750 | Val Loss=1.14390492 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06953490 | Current Learning Rate: 0.0001\nEpoch 896/1000 | Train Loss=4708.21044922 | Val Loss=1.14390135 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06953323 | Current Learning Rate: 0.0001\nEpoch 897/1000 | Train Loss=4708.21142578 | Val Loss=1.14389861 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06953192 | Current Learning Rate: 0.0001\nEpoch 898/1000 | Train Loss=4708.21093750 | Val Loss=1.14389598 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06953073 | Current Learning Rate: 0.0001\nEpoch 899/1000 | Train Loss=4708.21142578 | Val Loss=1.14389372 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06952965 | Current Learning Rate: 0.0001\nEpoch 900/1000 | Train Loss=4708.21093750 | Val Loss=1.14389110 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06952846 | Current Learning Rate: 0.0001\nEpoch 901/1000 | Train Loss=4708.21044922 | Val Loss=1.14388835 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06952715 | Current Learning Rate: 0.0001\n\n Epoch :  900 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 902/1000 | Train Loss=4708.21093750 | Val Loss=1.14388549 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06952584 | Current Learning Rate: 0.0001\nEpoch 903/1000 | Train Loss=4708.21044922 | Val Loss=1.14388192 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06952417 | Current Learning Rate: 0.0001\nEpoch 904/1000 | Train Loss=4708.21044922 | Val Loss=1.14387906 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06952286 | Current Learning Rate: 0.0001\nEpoch 905/1000 | Train Loss=4708.21093750 | Val Loss=1.14387631 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06952155 | Current Learning Rate: 0.0001\nEpoch 906/1000 | Train Loss=4708.21044922 | Val Loss=1.14387345 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06952024 | Current Learning Rate: 0.0001\nEpoch 907/1000 | Train Loss=4708.21044922 | Val Loss=1.14387119 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06951916 | Current Learning Rate: 0.0001\nEpoch 908/1000 | Train Loss=4708.21093750 | Val Loss=1.14386845 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06951785 | Current Learning Rate: 0.0001\nEpoch 909/1000 | Train Loss=4708.21142578 | Val Loss=1.14386547 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06951642 | Current Learning Rate: 0.0001\nEpoch 910/1000 | Train Loss=4708.21044922 | Val Loss=1.14386272 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06951523 | Current Learning Rate: 0.0001\nEpoch 911/1000 | Train Loss=4708.21093750 | Val Loss=1.14385998 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06951392 | Current Learning Rate: 0.0001\nEpoch 912/1000 | Train Loss=4708.21093750 | Val Loss=1.14385712 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06951261 | Current Learning Rate: 0.0001\nEpoch 913/1000 | Train Loss=4708.21142578 | Val Loss=1.14385450 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06951129 | Current Learning Rate: 0.0001\nEpoch 914/1000 | Train Loss=4708.21044922 | Val Loss=1.14385283 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06951058 | Current Learning Rate: 0.0001\nEpoch 915/1000 | Train Loss=4708.21093750 | Val Loss=1.14385009 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06950927 | Current Learning Rate: 0.0001\nEpoch 916/1000 | Train Loss=4708.21044922 | Val Loss=1.14384747 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06950808 | Current Learning Rate: 0.0001\nEpoch 917/1000 | Train Loss=4708.21093750 | Val Loss=1.14384449 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06950665 | Current Learning Rate: 0.0001\nEpoch 918/1000 | Train Loss=4708.21142578 | Val Loss=1.14384162 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06950533 | Current Learning Rate: 0.0001\nEpoch 919/1000 | Train Loss=4708.21142578 | Val Loss=1.14383888 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06950402 | Current Learning Rate: 0.0001\nEpoch 920/1000 | Train Loss=4708.21093750 | Val Loss=1.14383638 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06950283 | Current Learning Rate: 0.0001\nEpoch 921/1000 | Train Loss=4708.21093750 | Val Loss=1.14383435 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513804 | ‚àö(Val Loss) = 1.06950188 | Current Learning Rate: 0.0001\n\n Epoch :  920 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 922/1000 | Train Loss=4708.21142578 | Val Loss=1.14383161 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12513804 | ‚àö(Val Loss) = 1.06950068 | Current Learning Rate: 0.0001\nEpoch 923/1000 | Train Loss=4708.21093750 | Val Loss=1.14382875 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513804 | ‚àö(Val Loss) = 1.06949925 | Current Learning Rate: 0.0001\nEpoch 924/1000 | Train Loss=4708.21093750 | Val Loss=1.14382589 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513804 | ‚àö(Val Loss) = 1.06949794 | Current Learning Rate: 0.0001\nEpoch 925/1000 | Train Loss=4708.21044922 | Val Loss=1.14382350 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06949687 | Current Learning Rate: 0.0001\nEpoch 926/1000 | Train Loss=4708.21044922 | Val Loss=1.14382136 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06949580 | Current Learning Rate: 0.0001\nEpoch 927/1000 | Train Loss=4708.21093750 | Val Loss=1.14381862 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06949461 | Current Learning Rate: 0.0001\nEpoch 928/1000 | Train Loss=4708.21142578 | Val Loss=1.14381588 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06949329 | Current Learning Rate: 0.0001\nEpoch 929/1000 | Train Loss=4708.21044922 | Val Loss=1.14381361 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06949222 | Current Learning Rate: 0.0001\nEpoch 930/1000 | Train Loss=4708.21093750 | Val Loss=1.14381123 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06949115 | Current Learning Rate: 0.0001\nEpoch 931/1000 | Train Loss=4708.21093750 | Val Loss=1.14380848 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06948984 | Current Learning Rate: 0.0001\nEpoch 932/1000 | Train Loss=4708.21044922 | Val Loss=1.14380622 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06948876 | Current Learning Rate: 0.0001\nEpoch 933/1000 | Train Loss=4708.21093750 | Val Loss=1.14380348 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06948745 | Current Learning Rate: 0.0001\nEpoch 934/1000 | Train Loss=4708.21093750 | Val Loss=1.14380074 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06948614 | Current Learning Rate: 0.0001\nEpoch 935/1000 | Train Loss=4708.21142578 | Val Loss=1.14379907 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06948543 | Current Learning Rate: 0.0001\nEpoch 936/1000 | Train Loss=4708.21044922 | Val Loss=1.14379609 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06948400 | Current Learning Rate: 0.0001\nEpoch 937/1000 | Train Loss=4708.21142578 | Val Loss=1.14379394 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06948304 | Current Learning Rate: 0.0001\nEpoch 938/1000 | Train Loss=4708.21093750 | Val Loss=1.14379108 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06948173 | Current Learning Rate: 0.0001\nEpoch 939/1000 | Train Loss=4708.21093750 | Val Loss=1.14378881 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06948066 | Current Learning Rate: 0.0001\nEpoch 940/1000 | Train Loss=4708.21142578 | Val Loss=1.14378655 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06947958 | Current Learning Rate: 0.0001\nEpoch 941/1000 | Train Loss=4708.21093750 | Val Loss=1.14378393 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06947839 | Current Learning Rate: 0.0001\n\n Epoch :  940 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 942/1000 | Train Loss=4708.21044922 | Val Loss=1.14378226 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06947756 | Current Learning Rate: 0.0001\nEpoch 943/1000 | Train Loss=4708.21142578 | Val Loss=1.14377880 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06947589 | Current Learning Rate: 0.0001\nEpoch 944/1000 | Train Loss=4708.21044922 | Val Loss=1.14377713 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06947517 | Current Learning Rate: 0.0001\nEpoch 945/1000 | Train Loss=4708.21093750 | Val Loss=1.14377499 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513876 | ‚àö(Val Loss) = 1.06947422 | Current Learning Rate: 0.0001\nEpoch 946/1000 | Train Loss=4708.21093750 | Val Loss=1.14377213 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06947279 | Current Learning Rate: 0.0001\nEpoch 947/1000 | Train Loss=4708.21093750 | Val Loss=1.14376926 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06947148 | Current Learning Rate: 0.0001\nEpoch 948/1000 | Train Loss=4708.21044922 | Val Loss=1.14376748 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06947064 | Current Learning Rate: 0.0001\nEpoch 949/1000 | Train Loss=4708.21093750 | Val Loss=1.14376533 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513804 | ‚àö(Val Loss) = 1.06946969 | Current Learning Rate: 0.0001\nEpoch 950/1000 | Train Loss=4708.21044922 | Val Loss=1.14376223 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513804 | ‚àö(Val Loss) = 1.06946814 | Current Learning Rate: 0.0001\nEpoch 951/1000 | Train Loss=4708.21093750 | Val Loss=1.14376020 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513804 | ‚àö(Val Loss) = 1.06946731 | Current Learning Rate: 0.0001\nEpoch 952/1000 | Train Loss=4708.21044922 | Val Loss=1.14375782 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513804 | ‚àö(Val Loss) = 1.06946611 | Current Learning Rate: 0.0001\nEpoch 953/1000 | Train Loss=4708.21093750 | Val Loss=1.14375567 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513804 | ‚àö(Val Loss) = 1.06946516 | Current Learning Rate: 0.0001\nEpoch 954/1000 | Train Loss=4708.21142578 | Val Loss=1.14375365 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06946421 | Current Learning Rate: 0.0001\nEpoch 955/1000 | Train Loss=4708.21142578 | Val Loss=1.14375138 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06946313 | Current Learning Rate: 0.0001\nEpoch 956/1000 | Train Loss=4708.21044922 | Val Loss=1.14374888 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06946194 | Current Learning Rate: 0.0001\nEpoch 957/1000 | Train Loss=4708.21044922 | Val Loss=1.14374685 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06946099 | Current Learning Rate: 0.0001\nEpoch 958/1000 | Train Loss=4708.21044922 | Val Loss=1.14374399 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06945968 | Current Learning Rate: 0.0001\nEpoch 959/1000 | Train Loss=4708.21142578 | Val Loss=1.14374220 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06945884 | Current Learning Rate: 0.0001\nEpoch 960/1000 | Train Loss=4708.21044922 | Val Loss=1.14374006 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06945789 | Current Learning Rate: 0.0001\nEpoch 961/1000 | Train Loss=4708.21093750 | Val Loss=1.14373720 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06945646 | Current Learning Rate: 0.0001\n\n Epoch :  960 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 962/1000 | Train Loss=4708.21044922 | Val Loss=1.14373541 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06945562 | Current Learning Rate: 0.0001\nEpoch 963/1000 | Train Loss=4708.21093750 | Val Loss=1.14373326 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06945467 | Current Learning Rate: 0.0001\nEpoch 964/1000 | Train Loss=4708.21093750 | Val Loss=1.14373112 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06945360 | Current Learning Rate: 0.0001\nEpoch 965/1000 | Train Loss=4708.21093750 | Val Loss=1.14372802 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06945217 | Current Learning Rate: 0.0001\nEpoch 966/1000 | Train Loss=4708.21044922 | Val Loss=1.14372647 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06945145 | Current Learning Rate: 0.0001\nEpoch 967/1000 | Train Loss=4708.21142578 | Val Loss=1.14372420 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06945038 | Current Learning Rate: 0.0001\nEpoch 968/1000 | Train Loss=4708.21093750 | Val Loss=1.14372194 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06944931 | Current Learning Rate: 0.0001\nEpoch 969/1000 | Train Loss=4708.21142578 | Val Loss=1.14371967 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06944835 | Current Learning Rate: 0.0001\nEpoch 970/1000 | Train Loss=4708.21044922 | Val Loss=1.14371741 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06944728 | Current Learning Rate: 0.0001\nEpoch 971/1000 | Train Loss=4708.21142578 | Val Loss=1.14371502 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06944609 | Current Learning Rate: 0.0001\nEpoch 972/1000 | Train Loss=4708.21093750 | Val Loss=1.14371347 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06944537 | Current Learning Rate: 0.0001\nEpoch 973/1000 | Train Loss=4708.21093750 | Val Loss=1.14371109 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06944430 | Current Learning Rate: 0.0001\nEpoch 974/1000 | Train Loss=4708.21093750 | Val Loss=1.14370966 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06944358 | Current Learning Rate: 0.0001\nEpoch 975/1000 | Train Loss=4708.21093750 | Val Loss=1.14370608 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06944191 | Current Learning Rate: 0.0001\nEpoch 976/1000 | Train Loss=4708.21044922 | Val Loss=1.14370453 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513852 | ‚àö(Val Loss) = 1.06944120 | Current Learning Rate: 0.0001\nEpoch 977/1000 | Train Loss=4708.21093750 | Val Loss=1.14370275 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513781 | ‚àö(Val Loss) = 1.06944036 | Current Learning Rate: 0.0001\nEpoch 978/1000 | Train Loss=4708.21044922 | Val Loss=1.14370000 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513781 | ‚àö(Val Loss) = 1.06943905 | Current Learning Rate: 0.0001\nEpoch 979/1000 | Train Loss=4708.21093750 | Val Loss=1.14369810 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513781 | ‚àö(Val Loss) = 1.06943822 | Current Learning Rate: 0.0001\nEpoch 980/1000 | Train Loss=4708.21093750 | Val Loss=1.14369595 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513781 | ‚àö(Val Loss) = 1.06943727 | Current Learning Rate: 0.0001\nEpoch 981/1000 | Train Loss=4708.21093750 | Val Loss=1.14369369 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513781 | ‚àö(Val Loss) = 1.06943619 | Current Learning Rate: 0.0001\n\n Epoch :  980 \n Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n        [  0.9456,  -4.3333, -23.9607],\n        [  0.9418,  -4.4592, -22.7630],\n        [  0.9919,  -3.1226, -14.7542],\n        [  0.9476,  -4.0221, -18.8191]]) \n Prediction :  [[  0.9554488  -3.9225414 -19.531675 ]\n [  0.9553731  -3.92253   -19.532564 ]\n [  0.9553694  -3.9225295 -19.532608 ]\n [  0.9554471  -3.922541  -19.531694 ]\n [  0.95536    -3.9225283 -19.532717 ]] \n\nFinal Test RMSE:  1.2676979303359985\nEpoch 982/1000 | Train Loss=4708.21093750 | Val Loss=1.14369190 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06943536 | Current Learning Rate: 0.0001\nEpoch 983/1000 | Train Loss=4708.21142578 | Val Loss=1.14368999 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06943440 | Current Learning Rate: 0.0001\nEpoch 984/1000 | Train Loss=4708.21093750 | Val Loss=1.14368749 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06943321 | Current Learning Rate: 0.0001\nEpoch 985/1000 | Train Loss=4708.21044922 | Val Loss=1.14368594 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06943250 | Current Learning Rate: 0.0001\nEpoch 986/1000 | Train Loss=4708.21044922 | Val Loss=1.14368367 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06943142 | Current Learning Rate: 0.0001\nEpoch 987/1000 | Train Loss=4708.21044922 | Val Loss=1.14368117 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06943035 | Current Learning Rate: 0.0001\nEpoch 988/1000 | Train Loss=4708.21093750 | Val Loss=1.14367950 | Data=47.06068420 | Physics=2.14256047 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942952 | Current Learning Rate: 0.0001\nEpoch 989/1000 | Train Loss=4708.21093750 | Val Loss=1.14367735 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942856 | Current Learning Rate: 0.0001\nEpoch 990/1000 | Train Loss=4708.21044922 | Val Loss=1.14367509 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942749 | Current Learning Rate: 0.0001\nEpoch 991/1000 | Train Loss=4708.21093750 | Val Loss=1.14367330 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942666 | Current Learning Rate: 0.0001\nEpoch 992/1000 | Train Loss=4708.21142578 | Val Loss=1.14367115 | Data=47.06068802 | Physics=2.14256047 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942558 | Current Learning Rate: 0.0001\nEpoch 993/1000 | Train Loss=4708.21142578 | Val Loss=1.14366949 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942487 | Current Learning Rate: 0.0001\nEpoch 994/1000 | Train Loss=4708.21093750 | Val Loss=1.14366794 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942415 | Current Learning Rate: 0.0001\nEpoch 995/1000 | Train Loss=4708.21142578 | Val Loss=1.14366484 | Data=47.06068802 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942272 | Current Learning Rate: 0.0001\nEpoch 996/1000 | Train Loss=4708.21044922 | Val Loss=1.14366317 | Data=47.06068039 | Physics=2.14256047 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942189 | Current Learning Rate: 0.0001\nEpoch 997/1000 | Train Loss=4708.21093750 | Val Loss=1.14366174 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942117 | Current Learning Rate: 0.0001\nEpoch 998/1000 | Train Loss=4708.21044922 | Val Loss=1.14365947 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06942010 | Current Learning Rate: 0.0001\nEpoch 999/1000 | Train Loss=4708.21044922 | Val Loss=1.14365768 | Data=47.06068039 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06941926 | Current Learning Rate: 0.0001\n‚úÖ Saved last model at epoch 1000 \nEpoch 1000/1000 | Train Loss=4708.21093750 | Val Loss=1.14365578 | Data=47.06068420 | Physics=2.14256023 | Val RMSE: 2.12513828 | ‚àö(Val Loss) = 1.06941843 | Current Learning Rate: 0.0001\n‚úÖ Metrics saved successfully!\nPlot losses after training 3:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA/8AAAIjCAYAAABViau2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAnmdJREFUeJzs3XlcVPX+x/H3sIM4CqaAG+65oZam4p4iuOTVXHK7prmlYf7MsrLMtbIst9LytoktdlNbr5pCuKaYplKmabnlkmiZiguynt8fNCdHUMFtZuD1fDx4DPM93znnc4Yv6Ge+m8UwDEMAAAAAAKDAcnN0AAAAAAAA4NYi+QcAAAAAoIAj+QcAAAAAoIAj+QcAAAAAoIAj+QcAAAAAoIAj+QcAAAAAoIAj+QcAAAAAoIAj+QcAAAAAoIAj+QcAAAAAoIAj+QeAAmzAgAGqUKHCdb124sSJslgsNzcgF7dmzRpZLBatWbPGLMvre3zw4EFZLBbFxMTc1JgqVKigAQMG3NRzurKYmBhZLBYdPHjQ0aHkya34PXO1311X+5kBgKsi+QcAB7BYLHn6ujTJLGyysrL06quvqmrVqvL19VXlypU1fPhwnTt3Lk+vr1OnjsqXLy/DMK5Yp2nTpgoKClJGRsbNCvuW2LhxoyZOnKjTp087OhSTLWGzWCz69ttvcxw3DEPlypWTxWLRfffdd13XeOONN276hyU30yOPPCI3Nzf99ddfduV//fWX3Nzc5O3trYsXL9od279/vywWi5555pnbGapDpKWlafbs2brrrrtktVpVvHhx1apVS0OHDtXu3bsdHV6eHTt2TE8//bTuvfdeFS1a9Jp/mzdu3KhmzZrJz89PwcHBGjlyZK5/t1JTU/XUU0+pdOnS8vX1VaNGjRQXF3cL7wRAYUfyDwAO8MEHH9h9tW3bNtfyGjVq3NB13n77be3Zs+e6Xjtu3DilpKTc0PVvxOzZszVmzBjVrl1bs2fPVq9evbRy5Ur9+eefeXp93759dfjwYa1fvz7X4wcPHlRCQoJ69uwpDw+P647zRt7jvNq4caMmTZqUa/K/Z88evf3227f0+lfj4+OjhQsX5ihfu3atjhw5Im9v7+s+9/Uk//369VNKSopCQ0Ov+7p51axZMxmGoQ0bNtiVb9y4UW5ubkpPT9f3339vd8xWt1mzZpIc/3t2K3Xr1k2PP/64ateurZdeekmTJk1SixYt9PXXX2vTpk1mvdv5M7see/bs0csvv6yjR48qLCzsqnUTExPVpk0bXbhwQTNmzNDgwYP11ltvqUePHjnqDhgwQDNmzFDfvn01e/Zsubu7q0OHDrl+mAYAN8P1/28HAHDd/v3vf9s937Rpk+Li4nKUX+7ChQvy8/PL83U8PT2vKz5J8vDwuKGk+Eb997//Va1atfTZZ5+ZQ5inTJmirKysPL2+T58+Gjt2rBYuXKgWLVrkOP7xxx/LMAz17dv3huK8kff4ZriR5Ppm6NChgxYvXqzXXnvNrr0sXLhQ9evXz/OHNTfq/PnzKlKkiNzd3eXu7n5brmlL4L/99lt16tTJLN+wYYPq1KmjlJQUffvtt2Y9W103Nzc1adJEkuN/z26VLVu2aOnSpXrhhRdyjHKYM2eO3QdZt/Nndj3q16+vkydPKjAwUEuWLMk1kbd55plnFBAQoDVr1shqtUrKnpozZMgQxcbGKjIyUpK0efNm/fe//9Urr7yiJ554QpL04IMPqnbt2nryySe1cePGW39jAAodev4BwEm1atVKtWvX1tatW9WiRQv5+fmZ/4n+8ssv1bFjR5UuXVre3t6qXLmypkyZoszMTLtzXD4f3Tbv/NVXX9Vbb72lypUry9vbW/fcc4+2bNli99rc5g1bLBaNGDFCX3zxhWrXri1vb2/VqlVLK1asyBH/mjVr1KBBA/n4+Khy5cr6z3/+k6+5yG5ubsrKyrKr7+bmludEqVy5cmrRooWWLFmi9PT0HMcXLlyoypUrq1GjRvrtt9/0yCOP6M4775Svr69KlCihHj165GkOcm5z/k+fPq0BAwaoWLFiKl68uPr3759rr/2PP/6oAQMGqFKlSvLx8VFwcLAGDhyokydPmnUmTpyoMWPGSJIqVqxoDrW3xZbbnP/9+/erR48eCgwMlJ+fnxo3bqxly5bZ1bGtX7Bo0SK98MILKlu2rHx8fNSmTRvt3bv3mvdt07t3b508edJuuHJaWpqWLFmiPn365PqarKwszZo1S7Vq1ZKPj4+CgoL08MMP69SpU2adChUqaOfOnVq7dq15z61atZL0z5SDtWvX6pFHHlGpUqVUtmxZu2OX/+y+/vprtWzZUkWLFpXVatU999xjN2Lh119/Vbdu3RQcHCwfHx+VLVtWvXr10pkzZ6547+XLl1e5cuVy9Pxv2LBBTZs2VZMmTXI9VqtWLRUvXlzSjf+effvtt7rnnnvsfs9yk5GRoSlTppi/8xUqVNAzzzyj1NRUs87o0aNVokQJu6kyjz76qCwWi1577TWz7Pjx47JYLHrzzTev+N7s27dPUvbUmsu5u7urRIkS5vPLf2a29yS3r0vbel7a0ZWkp6dr9+7dOnbs2DXrFi1aVIGBgdesl5ycbH6Ia0v8peyk3t/fX4sWLTLLlixZInd3dw0dOtQs8/Hx0aBBg5SQkKDDhw9f83oAkF8F76NmAChATp48qfbt26tXr17697//raCgIEnZ/1n29/fX6NGj5e/vr1WrVmn8+PFKTk7WK6+8cs3zLly4UGfPntXDDz8si8WiadOmqWvXrtq/f/81e7K//fZbffbZZ3rkkUdUtGhRvfbaa+rWrZsOHTpk/od++/btateunUJCQjRp0iRlZmZq8uTJKlmyZJ7v/aGHHtLDDz+s//znP3r44Yfz/LpL9e3bV0OHDtXKlSvt5p3v2LFDP/30k8aPHy8pu5dy48aN6tWrl8qWLauDBw/qzTffVKtWrbRr1658jbYwDEOdO3fWt99+q2HDhqlGjRr6/PPP1b9//xx14+LitH//fj300EMKDg7Wzp079dZbb2nnzp3atGmTLBaLunbtql9++UUff/yxZs6cqTvuuEOSrvheHj9+XE2aNNGFCxc0cuRIlShRQgsWLNC//vUvLVmyRPfff79d/Zdeeklubm564okndObMGU2bNk19+/bVd999l6f7rVChgsLDw/Xxxx+rffv2krIT7TNnzqhXr152SaPNww8/rJiYGD300EMaOXKkDhw4oDlz5mj79u3asGGDPD09NWvWLD366KPy9/fXs88+K0lm+7d55JFHVLJkSY0fP17nz5+/YowxMTEaOHCgatWqpbFjx6p48eLavn27VqxYoT59+igtLU1RUVFKTU3Vo48+quDgYB09elRLly7V6dOnVaxYsSueu1mzZvrss8+Umpoqb29vpaWlacuWLRo+fLguXLigJ598UoZhyGKx6NSpU9q1a5eGDRt2zfc1L79nO3bsUGRkpEqWLKmJEycqIyNDEyZMyPE+SdLgwYO1YMECde/eXY8//ri+++47TZ06VT///LM+//xzSVLz5s01c+ZM7dy5U7Vr15YkrV+/Xm5ublq/fr1GjhxplknKdUSNjW0I/0cffaSmTZvma3RD165dVaVKFbuyrVu3atasWSpVqpRZlpd2dCVHjx5VjRo11L9//5u2rsSOHTuUkZGhBg0a2JV7eXmpXr162r59u1m2fft2VatWze5DAklq2LChpOzpA+XKlbspcQGAyQAAOFx0dLRx+Z/kli1bGpKMefPm5ah/4cKFHGUPP/yw4efnZ1y8eNEs69+/vxEaGmo+P3DggCHJKFGihPHXX3+Z5V9++aUhyfjf//5nlk2YMCFHTJIMLy8vY+/evWbZDz/8YEgyXn/9dbOsU6dOhp+fn3H06FGz7NdffzU8PDxynPNKnn76acPLy8twd3c3Pvvsszy95nJ//fWX4e3tbfTu3TvHuSUZe/bsMQwj9/czISHBkGS8//77Ztnq1asNScbq1avNssvf4y+++MKQZEybNs0sy8jIMJo3b25IMubPn2+W53bdjz/+2JBkrFu3zix75ZVXDEnGgQMHctQPDQ01+vfvbz4fNWqUIclYv369WXb27FmjYsWKRoUKFYzMzEy7e6lRo4aRmppq1p09e7YhydixY0eOa11q/vz5hiRjy5Ytxpw5c4yiRYua99OjRw/j3nvvNePr2LGj+br169cbkoyPPvrI7nwrVqzIUV6rVi2jZcuWV7x2s2bNjIyMjFyP2d6r06dPG0WLFjUaNWpkpKSk2NXNysoyDMMwtm/fbkgyFi9efNV7zs3cuXPt3m9bu/ntt9+MXbt2GZKMnTt3GoZhGEuXLs1xjzfye9alSxfDx8fH+O2338yyXbt2Ge7u7nbnTExMNCQZgwcPtrvOE088YUgyVq1aZRiGYZw4ccKQZLzxxhuGYWS/d25ubkaPHj2MoKAg83UjR440AgMDzfcvN1lZWebfsKCgIKN3797G3Llz7WK1ufxndrk//vjDKF++vBEWFmacO3fOMIz8taPc2P4WXvq7kxeLFy/O8Tfg8mOX/u7a9OjRwwgODjaf16pVy2jdunWOejt37rzi330AuFEM+wcAJ+bt7a2HHnooR7mvr6/5/dmzZ/Xnn3+qefPmunDhQp5W0e7Zs6cCAgLM582bN5eUPVz8WiIiIlS5cmXzeZ06dWS1Ws3XZmZm6ptvvlGXLl1UunRps16VKlXMnuFree211zRjxgxt2LBBvXv3Vq9evRQbG2tXx9vbW88999xVzxMQEKAOHTroq6++MnuGDcPQf//7XzVo0EDVqlWTZP9+pqen6+TJk6pSpYqKFy+ubdu25Slmm+XLl8vDw0PDhw83y9zd3fXoo4/mqHvpdS9evKg///xTjRs3lqR8X/fS6zds2NBunrm/v7+GDh2qgwcPateuXXb1H3roIXl5eZnP89MWbB544AGlpKRo6dKlOnv2rJYuXXrFIf+LFy9WsWLF1LZtW/3555/mV/369eXv76/Vq1fn+bpDhgy55lzxuLg4nT17Vk8//bR8fHzsjtmG29t69leuXKkLFy7k+fqS/bx/KXtYf5kyZVS+fHlVr15dgYGB5tD/yxf7u5q8/J6tXLlSXbp0Ufny5c16NWrUUFRUlN25li9fLil7WP+lHn/8cUkyp4SULFlS1atX17p168x43d3dNWbMGB0/fly//vqrpOye/2bNml11Co/FYtHKlSv1/PPPKyAgQB9//LGio6MVGhqqnj175nnniszMTPXu3Vtnz57V559/riJFiki68XZUoUIFGYZxU3eTsC3cmNs6HD4+PnYLO6akpFyx3qXnAoCbieQfAJxYmTJl7BIzm507d+r+++9XsWLFZLVaVbJkSXOxwKvNUba5NFmQZH4QkJe5spe/1vZ622tPnDihlJSUHMN2JeVadrmUlBRNmDBBgwcPVoMGDTR//ny1bt1a999/v5lg/frrr0pLS1OjRo2ueb6+ffvq/Pnz+vLLLyVlr8R+8OBBu4X+UlJSNH78eJUrV07e3t664447VLJkSZ0+fTpP7+elfvvtN4WEhMjf39+u/M4778xR96+//tL//d//KSgoSL6+vipZsqQqVqwoKW8/xytdP7dr2XaO+O233+zKb6Qt2JQsWVIRERFauHChPvvsM2VmZqp79+651v3111915swZlSpVSiVLlrT7OnfunE6cOJHn69req6uxzT23DWO/0nlGjx6td955R3fccYeioqI0d+7cPP0MateureLFi9sl+LZ57haLReHh4XbHypUrl+vv0OWu9Xv2xx9/KCUlRVWrVs1R7/Kf/2+//SY3N7ccv3/BwcEqXry4XZto3ry5Oax//fr1atCggRo0aKDAwECtX79eycnJ+uGHH8wPia7G29tbzz77rH7++Wf9/vvv+vjjj9W4cWMtWrRII0aMuObrpezdEFatWmWu0WFzM9vRzWL7MO/SdRRsLl68aPdhn6+v7xXrXXouALiZmPMPAE4st/8Anj59Wi1btpTVatXkyZNVuXJl+fj4aNu2bXrqqafytBr+lXpLjUsW+roVr82Ln3/+WadPnzZ7wD08PLRkyRK1bt1aHTt21OrVq/Xxxx+rVKlS5haJV3PfffepWLFiWrhwofr06aOFCxfK3d1dvXr1Mus8+uijmj9/vkaNGqXw8HAVK1ZMFotFvXr1yvPuAtfjgQce0MaNGzVmzBjVq1dP/v7+ysrKUrt27W7pdS91s36effr00ZAhQ5SUlKT27dubC9pdLisrS6VKldJHH32U6/H8rAtxMxOk6dOna8CAAfryyy8VGxurkSNHaurUqdq0aZO5mGBu3NzcFB4ero0bN5rb/l26un2TJk303nvvmWsBdOnSJU/x3Irfs7wsttmsWTO9/fbb2r9/v9avX6/mzZvLYrGoWbNmWr9+vUqXLq2srKw8Jf+XCgkJUa9evdStWzfVqlVLixYtUkxMzFXXAvjiiy/08ssva8qUKWrXrp3dsZvZjm6WkJAQScp1EcFjx47ZjYQKCQnR0aNHc60nya4uANwsJP8A4GLWrFmjkydP6rPPPrNbcOvAgQMOjOofpUqVko+PT64rxudlFXlbgnLpatdFihTR8uXL1axZM0VFRenixYt6/vnn87TNnbe3t7p37673339fx48f1+LFi9W6dWsFBwebdZYsWaL+/ftr+vTpZtnFixfzPDT5UqGhoYqPj9e5c+fsev/37NljV+/UqVOKj4/XpEmTzIUHJZlDqy+V1x0SbNe//FqSzOkgt2ov9fvvv18PP/ywNm3apE8++eSK9SpXrqxvvvlGTZs2vWbynp/7vtr1JOmnn3665siTsLAwhYWFady4cdq4caOaNm2qefPm6fnnn7/q65o1a6avv/5aX331lU6cOGG3wn2TJk307LPPavny5UpJScnTkP+8KFmypHx9fXNtL5f//ENDQ5WVlaVff/3VHAEiZS8Oefr0abs2YUvq4+LitGXLFj399NOSshf3e/PNN1W6dGkVKVJE9evXv664PT09VadOHf3666/6888/7X4PL/XLL7+of//+6tKlS46tAqX8taPbpXbt2vLw8ND333+vBx54wCxPS0tTYmKiXVm9evW0evVqJScn2y36Z1tos169erctbgCFB8P+AcDF2HoEL+0BTEtL0xtvvOGokOy4u7srIiJCX3zxhX7//XezfO/evfr666+v+fqwsDAFBQVpzpw5dkN3S5Qoofnz5+vPP/9USkqK3b7q19K3b1+lp6fr4Ycf1h9//GE35N8W8+U9qq+//nqOrRPzokOHDsrIyLDbBi0zM1Ovv/56jmtKOXtyZ82aleOctnnOefkwokOHDtq8ebMSEhLMsvPnz+utt95ShQoVVLNmzbzeSr74+/vrzTff1MSJE6/6s3nggQeUmZmpKVOm5DiWkZFhd49FihS5rg9gLhUZGamiRYtq6tSp5pBqG9t7n5ycrIyMDLtjYWFhcnNzy3Vo9uVsCf3LL78sPz8/u8StYcOG8vDw0LRp0+zq3ih3d3dFRUXpiy++0KFDh8zyn3/+WStXrrSr26FDB0k529aMGTMkSR07djTLKlasqDJlymjmzJlKT083P8ho3ry59u3bpyVLlqhx48bXXL3/119/tYvL5vTp00pISFBAQMAVe+fPnTun+++/X2XKlNGCBQty/RAoP+0oN/nZ6i+vihUrpoiICH344Yc6e/asWf7BBx/o3Llz6tGjh1nWvXt3ZWZm6q233jLLUlNTNX/+fDVq1IiV/gHcEvT8A4CLadKkiQICAtS/f3+NHDlSFotFH3zwwU0bdn8zTJw4UbGxsWratKmGDx+uzMxMzZkzR7Vr11ZiYuJVX+vh4aE5c+aoZ8+eCgsL08MPP6zQ0FD9/PPPeu+99xQWFqYjR46oc+fO2rBhQ46tsnLTsmVLlS1bVl9++aV8fX3VtWtXu+P33XefPvjgAxUrVkw1a9ZUQkKCvvnmG7u9yPOqU6dOatq0qZ5++mkdPHhQNWvW1GeffZZj/rjValWLFi00bdo0paenq0yZMoqNjc11BIetl/XZZ59Vr1695OnpqU6dOpkfClzq6aefNrfdGzlypAIDA7VgwQIdOHBAn376qdzcbt3n/rltZ3i5li1b6uGHH9bUqVOVmJioyMhIeXp66tdff9XixYs1e/Zsc72A+vXr680339Tzzz+vKlWqqFSpUmrdunW+YrJarZo5c6YGDx6se+65R3369FFAQIB++OEHXbhwQQsWLNCqVas0YsQI9ejRQ9WqVVNGRoY++OADubu7q1u3bte8RsOGDeXl5aWEhAS1atXKLjH28/NT3bp1lZCQoOLFi1917YH8mjRpklasWKHmzZvrkUceUUZGhl5//XXVqlVLP/74o1mvbt266t+/v9566y1z2tDmzZu1YMECdenSRffee6/deZs3b67//ve/CgsLM9eAuPvuu1WkSBH98ssvV1zM8VI//PCD+vTpo/bt26t58+YKDAzU0aNHtWDBAv3++++aNWvWFac2TJo0Sbt27dK4cePMtTpsKleurPDw8Hy1o9zkd6s/2+iPnTt3SspO6G1rkIwbN86s98ILL6hJkyZq2bKlhg4dqiNHjmj69OmKjIy0m7rQqFEj9ejRQ2PHjtWJEydUpUoVLViwQAcPHtS77757zXgA4Lo4ZpMBAMClrrTVX61atXKtv2HDBqNx48aGr6+vUbp0aePJJ580Vq5cec1t6GzbW73yyis5zinJmDBhgvn8SluQRUdH53jt5dvNGYZhxMfHG3fddZfh5eVlVK5c2XjnnXeMxx9/3PDx8bnCu2Bv3bp1RlRUlGG1Wg1vb2+jdu3axtSpU40LFy4YX3/9teHm5mZERkYa6enpeTrfmDFjDEnGAw88kOPYqVOnjIceesi44447DH9/fyMqKsrYvXt3jvvKy1Z/hmEYJ0+eNPr162dYrVajWLFiRr9+/czt5C7d6u/IkSPG/fffbxQvXtwoVqyY0aNHD+P333/P8bMwDMOYMmWKUaZMGcPNzc1uW7Tc3vt9+/YZ3bt3N4oXL274+PgYDRs2NJYuXWpXx3Yvl29vZ2sjl8aZm0u3+ruay7f6s3nrrbeM+vXrG76+vkbRokWNsLAw48knnzR+//13s05SUpLRsWNHo2jRooYkc9u/q137StvGffXVV0aTJk0MX19fw2q1Gg0bNjQ+/vhjwzAMY//+/cbAgQONypUrGz4+PkZgYKBx7733Gt98881V7+1S4eHhhiTjmWeeyXFs5MiRhiSjffv2OY7d6O/Z2rVrjfr16xteXl5GpUqVjHnz5uV6zvT0dGPSpElGxYoVDU9PT6NcuXLG2LFj7bYGtbFtXzh8+HC78oiICEOSER8ff8X3web48ePGSy+9ZLRs2dIICQkxPDw8jICAAKN169bGkiVL7Ope/jPr37+/ISnXr8vvPy/tKDf53ervSvHk9l/p9evXG02aNDF8fHyMkiVLGtHR0UZycnKOeikpKcYTTzxhBAcHG97e3sY999xjrFixIk/xAMD1sBiGE3UVAQAKtC5dumjnzp25zlMGAADArcOcfwDALXH5PtW//vqrli9frlatWjkmIAAAgEKMnn8AwC0REhKiAQMGqFKlSvrtt9/05ptvKjU1Vdu3b891b3IAAADcOiz4BwC4Jdq1a6ePP/5YSUlJ8vb2Vnh4uF588UUSfwAAAAeg5x8AAAAAgAKOOf8AAAAAABRwJP8AAAAAABRwzPm/SbKysvT777+raNGislgsjg4HAAAAAFDAGYahs2fPqnTp0nJzu3rfPsn/TfL777+rXLlyjg4DAAAAAFDIHD58WGXLlr1qHZL/m6Ro0aKSst90q9Xq4GiuLD09XbGxsYqMjJSnp6dUvbp07JgUEiLt3u3o8ABJubRTwAnRTuEKaKdwBbRTODtnbqPJyckqV66cmY9eDcn/TWIb6m+1Wp0++ffz85PVas1uuLahIW5ukhPHjcIlRzsFnBDtFK6AdgpXQDuFs3OFNpqXqecs+AcAAAAAQAFH8g8AAAAAQAFH8g8AAAAAQAHHnP/CbssWKTNTcnd3dCQAAABAoWYYhjIyMpSZmenoUHCJ9PR0eXh46OLFi7f9Z+Pu7i4PD4+bsp08yX9hFxLi6AgAAACAQi8tLU3Hjh3ThQsXHB0KLmMYhoKDg3X48OGbkoTnl5+fn0JCQuTl5XVD5yH5BwAAAAAHysrK0oEDB+Tu7q7SpUvLy8vLIUkmcpeVlaVz587J399fbm63b+a8YRhKS0vTH3/8oQMHDqhq1ao3dH2SfwAAAABwoLS0NGVlZalcuXLy8/NzdDi4TFZWltLS0uTj43Nbk39J8vX1laenp3777TczhutF8l/YvfWWdO6c5O8vDR3q6GgAAACAQut2J5ZwDTerXZD8F3aTJ0tHj0plypD8AwAAAEABxUdLAAAAAAAUcCT/AAAAAACnUaFCBc2aNcvRYRQ4JP8AAAAAgHyzWCxX/Zo4ceJ1nXfLli0aeoNTklu1aqVRo0bd0DkKGub8AwAAAADy7dixY+b3n3zyicaPH689e/aYZf7+/ub3hmEoMzNTHh7XTkFLlix5cwOFJHr+AQAAAMDpGIahC2kZDvkyDCNPMQYHB5tfxYoVk8ViMZ/v3r1bRYsW1ddff6369evL29tb3377rfbt26fOnTsrKChI/v7+uueee/TNN9/YnffyYf8Wi0XvvPOO7r//fvn5+alq1ar66quvbuj9/fTTT1WrVi15e3urQoUKmj59ut3xN954Q1WrVpWPj49CQkLUv39/89iSJUsUFhYmX19flShRQhERETp//vwNxXM70PMPAAAAAE4mJT1TNcevdMi1d02Okp/XzUkVn376ab366quqVKmSAgICdPjwYXXo0EEvvPCCvL299f7776tTp07as2ePypcvf8XzTJo0SdOmTdMrr7yi119/XX379tVvv/2mwMDAfMe0detWPfDAA5o4caJ69uypjRs36pFHHlGJEiU0YMAAff/99xo5cqQ++OADNWnSRH/++af5AcWxY8fUu3dvTZs2Tffff7/Onj2r9evX5/kDE0ci+QcAAAAA3BKTJ09W27ZtzeeBgYGqW7eu+XzKlCn6/PPP9dVXX2nEiBFXPM+AAQPUu3dvSdKLL76o1157TZs3b1a7du3yHdOMGTPUpk0bPffcc5KkatWqadeuXXrllVc0YMAAHTp0SEWKFNF9992nokWLqly5cqpcubKk7OQ/IyNDXbt2VWhoqCQpLCws3zE4Ask/nE7SmYtKPHxakvN/eoZbIyMjUz+ctMh953F5eLg7OhzAjtXXU40rlnB0GACAAs7X0127Jkc57No3S4MGDeyenzt3ThMnTtSyZcvMRDolJUWHDh266nnq1Kljfl+kSBFZrVadOHHiumL6+eef1blzZ7uypk2batasWcrMzFTbtm0VGhqqSpUqqV27doqMjFSbNm1ktVpVt25dtWnTRmFhYYqKilJkZKS6d++ugICA64rldiL5L2RmxP2qT7e76/W9G+Tu5qaX/UopINhTZ/wC9dzcDbI4OL4sw9DPx5KVnkniD3e998sPjg4CyNWsnvXUsXYpR4cBACjALBbLTRt670hFihSxe/7EE08oLi5Or776qqpUqSJfX191795daWlpVz2Pp6en3XOLxaKsrKybHq8kFS1aVNu2bdOaNWsUGxuriRMnauLEidqyZYsCAwMVFxenjRs3KjY2Vq+//rqeffZZfffdd6pYseItiedmcf3WhHw5fjZVJy5adOJi9oIUXbpO+efg4dOOCSoXoSX8VNLf29FhwEEMw9Bfp04pMCBAFoujP5IC/nHgz/M6eT5NSckXHR0KAAAuacOGDRowYIDuv/9+SdkjAQ4ePHhbY6hRo4Y2bNiQI65q1arJ3T171IOHh4ciIiIUERGh5557ToGBgVq1apW6d+8ui8Wipk2bqmnTpho/frxCQ0P1+eefa/To0bf1PvKL5L+QGdaiosqkHVLDho3l5u6uzCxDmYahrCxDWU7S2V7cz1MNQkn6CrP09HQtX75cHTo0zPEpL+BITyz+QUu2HpELrOkDAIBTqlq1qj777DN16tRJFotFzz333C3rwf/jjz+UmJhoVxYSEqLHH39c99xzj6ZMmaKePXsqISFBc+bM0RtvvCFJWrp0qfbv368WLVooICBAS5cuVVZWlu6880599913io+PV2RkpEqVKqXvvvtOf/zxh2rUqHFL7uFmIvkvZCreUURVrFLjSoEkVQAAAABuqxkzZmjgwIFq0qSJ7rjjDj311FNKTk6+JddauHChFi5caFc2ZcoUjRs3TosWLdL48eM1ZcoUhYSEaPLkyRowYIAkqXjx4vrss880ceJEXbx4UVWrVtU777yjWrVqac+ePVq3bp1mzZql5ORkhYaGavr06Wrfvv0tuYebieQfAIA8so1HMliQFAAAOwMGDDCTZ0lq1apVrtvfVahQQatWrbIri46Otnt++TSA3M5z+vTpq8azZs2aqx7v1q2bunXrluuxZs2a2b0+KyvL/ICiRo0aWrFixVXP7azcHB2AzUsvvSSLxaJRo0aZZRcvXlR0dLRKlCghf39/devWTcePH7d73aFDh9SxY0f5+fmpVKlSGjNmjDIyMuzqrFmzRnfffbe8vb1VpUoVxcTE5Lj+3LlzVaFCBfn4+KhRo0bavHnzrbhN59O3rxQVlf0IAAAAACiQnCL537Jli/7zn//Ybd8gSY899pj+97//afHixVq7dq1+//13de3a1TyemZmpjh07Ki0tTRs3btSCBQsUExOj8ePHm3UOHDigjh076t5771ViYqJGjRqlwYMHa+XKlWadTz75RKNHj9aECRO0bds21a1bV1FRUde9dYRLWbtWio3NfgQAXJVtKRLm/AMAAFfj8OT/3Llz6tu3r95++227vRHPnDmjd999VzNmzFDr1q1Vv359zZ8/Xxs3btSmTZskSbGxsdq1a5c+/PBD1atXT+3bt9eUKVM0d+5cc6uIefPmqWLFipo+fbpq1KihESNGqHv37po5c6Z5rRkzZmjIkCF66KGHVLNmTc2bN09+fn567733bu+bAQAAAADALeDwOf/R0dHq2LGjIiIi9Pzzz5vlW7duVXp6uiIiIsyy6tWrq3z58kpISFDjxo2VkJCgsLAwBQUFmXWioqI0fPhw7dy5U3fddZcSEhLszmGrY5tekJaWpq1bt2rs2LHmcTc3N0VERCghIeGKcaempio1NdV8bpsDkp6ervT09Ot7M24DW2y2Rw9lz2E1JGU4cdwoXC5vp4CzsM05zMzMpJ3CJdBO4Qpop9n3bhiGsrKybtnK97h+tn//bT+j2y0rK0uGYSg9Pd3citAmP783Dk3+//vf/2rbtm3asmVLjmNJSUny8vJS8eLF7cqDgoKUlJRk1rk08bcdtx27Wp3k5GSlpKTo1KlTyszMzLXO7t27rxj71KlTNWnSpBzlsbGx8vPzu+LrnEVcXJwkKfLiRfkqe32F2OXLHRsUcBlbOwWcxeHDbpLctGfPHsWdz/43gnYKV0A7hSsozO3Uw8NDwcHBOnfunDmCGc7n7NmzDrluWlqaUlJStG7duhzr2124cCHP53FY8n/48GH93//9n+Li4uTj4+OoMK7b2LFjNXr0aPN5cnKyypUrp8jISFmtVgdGdnXp6emKi4tT27Zt5enpKY+/33sfHx916NDBwdEB2S5vp4Cz2PDFTm06cVTVqt2ptk3L0U7h9Ph7CldAO83uiDt8+LD8/f1dMjcq6AzD0NmzZ1W0aFFZbAsA3UYXL16Ur6+vWrRokaN95GebRIcl/1u3btWJEyd09913m2WZmZlat26d5syZo5UrVyotLU2nT5+26/0/fvy4goODJUnBwcE5VuW37QZwaZ3Ldwg4fvy4rFarfH195e7uLnd391zr2M6RG29vb3l7e+co9/T0dIk/WpfHafm7DHAmrvL7hMLDzS17qRx3dzezbdJO4Qpop3AFhbmdZmZmymKxyM3Nzfy3Bs7DNtTf9jO63dzc3GSxWHL9HcnP74zDWlabNm20Y8cOJSYmml8NGjRQ3759ze89PT0VHx9vvmbPnj06dOiQwsPDJUnh4eHasWOH3ar8cXFxslqtqlmzplnn0nPY6tjO4eXlpfr169vVycrKUnx8vFkHAACJ1f4BAIDrcljPf9GiRVW7dm27siJFiqhEiRJm+aBBgzR69GgFBgbKarXq0UcfVXh4uBo3bixJioyMVM2aNdWvXz9NmzZNSUlJGjdunKKjo81e+WHDhmnOnDl68sknNXDgQK1atUqLFi3SsmXLzOuOHj1a/fv3V4MGDdSwYUPNmjVL58+f10MPPXSb3g0AgCsh9wcAAK7G4av9X83MmTPl5uambt26KTU1VVFRUXrjjTfM4+7u7lq6dKmGDx+u8PBwFSlSRP3799fkyZPNOhUrVtSyZcv02GOPafbs2SpbtqzeeecdRUVFmXV69uypP/74Q+PHj1dSUpLq1aunFStW5FgEEABQ2N3+eX4AABR0rVq1Ur169TRr1ixHh1KgOVXyv2bNGrvnPj4+mjt3rubOnXvF14SGhmr5NVapb9WqlbZv337VOiNGjNCIESPyHGuBMWSIdOaMVKyYoyMBAKfHsH8AAP7RqVMnpaena8WKFTmOrV+/Xi1atNAPP/ygOnXq3NB1YmJiNGrUKJ0+ffqGzlPYOVXyDweYMMHREQAAAABwQYMGDVK3bt105MgRlS1b1u7Y/Pnz1aBBgxtO/HHzsJQkAAB5ZBv0bzDrHwBwqxmGlHbeMV95HOJ23333qWTJkoqJibErP3funBYvXqxBgwbp5MmT6t27t8qUKSM/Pz+FhYXp448/vqlv1aFDh9S5c2f5+/vLarXqgQcesNvN7YcfftC9996rokWLymq1qn79+vr+++8lSb/99ps6deqkgIAAFSlSRLVq1brmyHJXRc8/AAAAADib9AvSi6Udc+1nfpe8ilyzmoeHhx588EHFxMTo2WefleXv+XGLFy9WZmamevfurXPnzql+/fp66qmnZLVatWzZMvXr10+VK1dWw4YNbzjUrKwsM/Ffu3atMjIyFB0drZ49e5rTyvv27au77rpLb775ptzd3ZWYmGhukRcdHa20tDStW7dORYoU0a5du+Tv73/DcTkjkn8AAPKIOf8AANgbOHCgXnnlFa1du1atWrWSlD3kv1u3bipWrJiKFSumJ554wqz/6KOPauXKlVq0aNFNSf7j4+O1Y8cOHThwQOXKlZMkvf/++6pVq5a2bNmie+65R4cOHdKYMWNUvXp1SVLVqlXN1x86dEjdunVTWFiYJKlSpUo3HJOzIvkv7MqWlY4elcqUkY4ccXQ0AAAAACTJ0y+7B95R186j6tWrq0mTJnrvvffUqlUr7d27V+vXrzd3YMvMzNSLL76oRYsW6ejRo0pLS1Nqaqr8/PJ+jav5+eefVa5cOTPxl6SaNWuqePHi+vnnn3XPPfdo9OjRGjx4sD744ANFRESoR48eqly5siRp5MiRGj58uGJjYxUREaFu3boV2HUKmPMPAEAeWf6e9U/HPwDglrNYsofeO+LLkr+tbQcNGqRPP/1UZ8+e1fz581W5cmW1bNlSkvTKK69o9uzZeuqpp7R69WolJiYqKipKaWlpt+Jdy9XEiRO1c+dOdezYUatWrVLNmjX1+eefS5IGDx6s/fv3q1+/ftqxY4caNGig119//bbFdjuR/AMAAAAArtsDDzwgNzc3LVy4UO+//74GDhxozv/fsGGDOnfurH//+9+qW7euKlWqpF9++eWmXbtGjRo6fPiwDh8+bJbt2rVLp0+fVs2aNc2yatWq6bHHHlNsbKy6du2q+fPnm8fKlSunYcOG6bPPPtPjjz+ut99++6bF50wY9g8AQB6ZHSFM+gcAwOTv76+ePXtq7NixSk5O1oABA8xjVatW1ZIlS7Rx40YFBARoxowZOn78uF1inheZmZlKTEy0K/P29lZERITCwsLUt29fzZo1SxkZGXrkkUfUsmVLNWjQQCkpKRozZoy6d++uihUr6siRI9qyZYu6desmSRo1apTat2+vatWq6dSpU1q9erVq1Khxo2+JUyL5BwAAAADckEGDBundd99Vhw4dVLr0P7sUjBs3Tvv371dUVJT8/Pw0dOhQdenSRWfOnMnX+c+dO6e77rrLrqxy5crau3evvvzySz366KNq0aKF3Nzc1K5dO3Povru7u06ePKkHH3xQx48f1x133KGuXbtq0qRJkrI/VIiOjtaRI0dktVrVrl07zZw58wbfDedE8g8AQB6ZHf8OjQIAAOcTHh4uI5eRcYGBgfriiy+u+lrblnxXMmDAALvRBJcrX768vvzyy1yPeXl56eOPP77iawvq/P7cMOcfAAAAAIACjuQfAIA8si1exJR/AADgakj+AQDIJ4OB/wAAwMWQ/AMAAAAAUMCx4F9h9+GHUmqq5O3t6EgAwOnZtvpj2D8AAHA1JP+FXatWjo4AAAAAAHCLMewfAIA8svy92R8d/wAAwNWQ/AMAAAAAUMAx7L+wW7Pmnzn/TAEAgKtizj8AAHBVJP+F3b//LR09KpUpIx054uhoAAAAABQyrVq1Ur169TRr1ixHh1KgMewfAIA8+rvjXwaz/gEAUKdOndSuXbtcj61fv14Wi0U//vjjDV8nJiZGFotFFotFbm5uCgkJUc+ePXXo0CG7eq1atZLFYtFLL72U4xwdO3aUxWLRxIkTzbIDBw6oT58+Kl26tHx8fFS2bFl17txZu3fvNutYLBa5u7srICBA7u7uZhz//e9/b/i+bjeSfwAAAABAvg0aNEhxcXE6kssI4vnz56tBgwaqU6fOTbmW1WrVsWPHdPToUX366afas2ePevTokaNeuXLlFBMTY1d29OhRxcfHKyQkxCxLT09X27ZtdebMGX322Wfas2ePPvnkE4WFhen06dN2r3/33Xe1e/duHT16VMeOHdOxY8fUpUuXm3JftxPD/gEAyCPLP13/AADcUoZhKCUjxSHX9vXwlcX8R+/K7rvvPpUsWVIxMTEaN26cWX7u3DktXrxYr7zyik6ePKkRI0Zo3bp1OnXqlCpXrqxnnnlGvXv3zldMFotFwcHBkqSQkBANGjRII0eOVHJysqxWq11MixYt0oYNG9S0aVNJ0oIFCxQZGWk3UmDnzp3at2+f4uPjFRoaKkkKDQ01X3Op4sWLKygoSFarVW5urtt/TvIPAAAAAE4mJSNFjRY2csi1v+vznfw8/a5Zz8PDQw8++KBiYmL07LPPmh8YLF68WJmZmerdu7fOnTun+vXr66mnnpLVatWyZcvUr18/Va5cWQ0bNryu+E6cOKHPP/9c7u7ucnd3tzvm5eWlvn37av78+WYiHxMTo2nTptkN+S9ZsqTc3Ny0ZMkSjRo1Ksd5CiLX/dgCAIDbzPafGjr+AQDINnDgQO3bt09r1641y+bPn69u3bqpWLFiKlOmjJ544gnVq1dPlSpV0qOPPqp27dpp0aJF+brOmTNn5O/vryJFiigoKEirV69WdHS0ihQpkmtMixYt0vnz57Vu3TqdOXNG9913n12dMmXK6LXXXtP48eMVEBCg1q1ba8qUKdq/f3+O8/Xt21dly5aV1WqVv7+//P39c6w34Aro+QcAAAAAJ+Pr4avv+nznsGvnVfXq1dWkSRO99957atWqlfbu3av169dr8uTJkqTMzEy9+OKLWrRokY4ePaq0tDSlpqbKz+/aIwsuVbRoUW3btk3p6en6+uuv9dFHH+mFF17ItW7dunVVtWpVLVmyRKtXr1a/fv3k4ZEz9Y2OjtaDDz6oNWvWaNOmTVq8eLFefPFFffXVV2rbtq1Zb/r06WrcuLH8/f3NYf+lS5fOV/zOgOQfAIA8Mqf8G/T9AwBuLYvFkqeh985g0KBBevTRRzV37lzNnz9flStXVsuWLSVJr7zyimbPnq1Zs2YpLCxMRYoU0ahRo5SWlpava7i5ualKlSqSpBo1amjfvn0aPny4Pvjgg1zrDxw4UHPnztWuXbu0efPmK563aNGi6tSpkzp16qTnn39eUVFRev755+2S/+DgYFWqVMnl5/y7buQAADgIuT8AAP944IEH5ObmpoULF+r999/XwIEDzalyGzZsUOfOnfXvf/9bdevWVaVKlfTLL7/c8DWffvppffLJJ9q2bVuux/v06aMdO3aodu3aqlmzZp7OabFYVL16dZ0/f/6G43NGJP8AAOTVtRc+BgCg0PH391fPnj01duxYHTt2TAMGDDCPVa1aVXFxcdq4caN+/vlnPfzwwzp+/PgNX7NcuXK6//77NX78+FyPBwQE6NixY4qPj8/1eGJiojp37qwlS5Zo165d2rt3r959912999576ty5s13d06dP6/jx40pKSjK/XPEDAob9F3a57MkJALg6Ov4BALA3aNAgvfvuu+rQoYPdfPhx48Zp//79ioqKkp+fn4YOHaouXbrozJkzN3zNxx57TOHh4dq8eXOuOwcUL178iq8tW7asKlSooEmTJungwYOyWCzm88ceeyzHvV1u6tSpevrpp2/4Hm4nkn8AAPLIQtc/AAC5Cg8Pz3VNnMDAQH3xxRdXfe2aNWuuenzAgAF2owlsGjdubHfNa50nMTHR/P6OO+7Q7Nmzr1pfyl7nJysrS8nJycz5BwCgsPh7+iJz/gEAgMsh+QcAAAAAoIBj2H9hN2mSdOaMVKyYNGGCo6MBAKdmbvXHrH8AAOBiSP4Lu7fflo4elcqUIfkHAAAAgAKKYf8AAOQRc/4BAICrIvkHAAAAAKCAI/kHACCP2OoPAAC4KpJ/AAAAAAAKOJJ/AADy6J85/0z6BwAAroXkHwAAAACAAs6hyf+bb76pOnXqyGq1ymq1Kjw8XF9//bV5vFWrVrJYLHZfw4YNszvHoUOH1LFjR/n5+alUqVIaM2aMMjIy7OqsWbNGd999t7y9vVWlShXFxMTkiGXu3LmqUKGCfHx81KhRI23evPmW3DMAwHXZZvzT7w8AQLYBAwaYuZqnp6eCgoLUtm1bvffee8rKysrXuWJiYlS8ePGbElerVq00atSom3KugsKhyX/ZsmX10ksvaevWrfr+++/VunVrde7cWTt37jTrDBkyRMeOHTO/pk2bZh7LzMxUx44dlZaWpo0bN2rBggWKiYnR+PHjzToHDhxQx44dde+99yoxMVGjRo3S4MGDtXLlSrPOJ598otGjR2vChAnatm2b6tatq6ioKJ04ceL2vBEAAAAA4KLatWunY8eO6eDBg/r6669177336v/+7/9033335eiYheM4NPnv1KmTOnTooKpVq6patWp64YUX5O/vr02bNpl1/Pz8FBwcbH5ZrVbzWGxsrHbt2qUPP/xQ9erVU/v27TVlyhTNnTtXaWlpkqR58+apYsWKmj59umrUqKERI0aoe/fumjlzpnmeGTNmaMiQIXrooYdUs2ZNzZs3T35+fnrvvfdu35vhKC1bSpGR2Y8AgKv7e9I/U/4BALeaYRjKunDBIV/5XdvG29tbwcHBKlOmjO6++24988wz+vLLL/X111/bjbqeMWOGwsLCVKRIEZUrV06PPPKIzp07Jyl7tPZDDz2kM2fOmCMJJk6cKEn64IMP1KBBAxUtWlTBwcHq06fPDXfUfvrpp6pVq5a8vb1VoUIFTZ8+3e74G2+8oapVq8rHx0chISHq37+/eWzJkiUKCwuTr6+vSpQooYiICJ0/f/6G4rkdPBwdgE1mZqYWL16s8+fPKzw83Cz/6KOP9OGHHyo4OFidOnXSc889Jz8/P0lSQkKCwsLCFBQUZNaPiorS8OHDtXPnTt11111KSEhQRESE3bWioqLMISBpaWnaunWrxo4dax53c3NTRESEEhISrhhvamqqUlNTzefJycmSpPT0dKWnp1//G3GL2WIzY7x0CoQTx43CJUc7BZxEVmamJCkzK5N2CpdAO4UroJ1m37thGMrKyjKHymdduKBfG9zjkHiqfr9Fbn/nXNdiGIYZ+6VatWqlunXr6tNPP9XAgQMlSRaLRbNmzVLFihW1f/9+jRgxQmPGjNHcuXPVuHFjzZw5UxMmTNDPP/8sSfL391dWVpZSU1M1adIk3XnnnTpx4oSeeOIJ9e/fX8uWLbtmbLlNPdi6daseeOABTZgwQQ888IA2btyoESNGKCAgQAMGDND333+vkSNHasGCBWrSpIlOnjyp+Ph4GYaho0ePqnfv3nr55ZfVpUsXnT17Vt9++60yMzPzPc0hr7KysmQYhtLT0+Xu7m53LD+/Nw5P/nfs2KHw8HBdvHhR/v7++vzzz1WzZk1JUp8+fRQaGqrSpUvrxx9/1FNPPaU9e/bos88+kyQlJSXZJf6SzOdJSUlXrZOcnKyUlBSdOnVKmZmZudbZvXv3FeOeOnWqJk2alKM8NjbW/HDCmcXFxTk6BOCaaKdwNnsPu0ly02+/HVJc3EFJtFO4BtopXEFhbqceHh4KDg7WuXPnzBHMWSkpDosn+exZueVxuH56eroyMjLMztBLVapUSbt27TKPPfTQQ+axwMBAjR07VqNHj9bUqVMlSV5eXpJk5lNZWVlKTk5W9+7dzdfdcccdeuGFF9S6dWv9/vvv8vf3zzWujIwMpaWl5RrXtGnT1LJlS40cOVKS1LVrVyUmJuqVV15R165dtWfPHvn5+alFixYqWrSoAgICVKVKFZ09e1Z79+5VRkaGIiIiFBgYqMDAQIWGhpqx3gppaWlKSUnRunXrckyjuHDhQp7P4/Dk/84771RiYqLOnDmjJUuWqH///lq7dq1q1qypoUOHmvXCwsIUEhKiNm3aaN++fapcubIDo5bZUG2Sk5NVrlw5RUZG2k1NcDbp6emKi4tT27Zt5enp6ehwgFzRTuGs9q3apxVH9ql8+fJq27Yq7RROj7+ncAW0U+nixYs6fPiw/P395ePjI0kyihaV9fstDonH4usri21/22vw9PSUh4dHrjmQh4eH3N3dzWPffPONXn75Ze3evVvJycnKyMjQxYsX5eHhIT8/P/n4+MhiseQ419atWzVp0iT9+OOPOnXqlNnDfvr0aZUuXTrXuDw8POTl5ZVrXPv27dO//vUvu2P33nuv5s2bpyJFiuhf//qXXnnlFd19992KiopSVFSU2rRpo6CgIDVp0kRt2rRRs2bNFBkZqbZt26p79+4KCAjI0/t1PS5evChfX1+1aNHCbB82+fnAweHJv5eXl6pUqSJJql+/vrZs2aLZs2frP//5T466jRo1kiTt3btXlStXVnBwcI5V+Y8fPy5JCg4ONh9tZZfWsVqt8vX1lbu7u9zd3XOtYztHbry9veXt7Z2j3NPT0yX+aLlKnCjcaKdwNm7u2UvluLm5mW2TdgpXQDuFKyjM7TQzM1MWi0Vubm5yc7tkWbYr9Go7E9v8fLu4/7Z7925VrFhRbm5uOnjwoP71r39p+PDheuGFFxQYGKhvv/1WgwYNUkZGht29X3qu8+fPq3379oqKitJHH32kkiVL6tChQ4qKijJfd7XYrnT88mOXXrtYsWLatm2b1qxZo9jYWE2cOFETJ07Uli1bFBgYqLi4OG3cuFGxsbGaO3eunnvuOX333XeqWLHidb2H1+Lm5mbupnD570h+fmccuuBfbmxzOnKTmJgoSQoJCZEkhYeHa8eOHXaLPcTFxclqtZpTB8LDwxUfH293nri4OHNdAS8vL9WvX9+uTlZWluLj4+3WHiiwWreWatXKfgQAXJXl783+WO8PAICrW7VqlXbs2KFu3bpJyu69z8rK0vTp09W4cWNVq1ZNv//+u91rvLy8lPn3+jo2u3fv1smTJ/XSSy+pefPmql69+g0v9lejRg1t2LDBrmzDhg2qVq2aOafew8NDERERmjZtmhITE3Xo0CGtWrVKUvYHB02bNtWkSZO0fft2eXl56fPPP7+hmG4Hh/b8jx07Vu3bt1f58uV19uxZLVy4UGvWrNHKlSu1b98+LVy4UB06dFCJEiX0448/6rHHHlOLFi1Up04dSVJkZKRq1qypfv36adq0aUpKStK4ceMUHR1t9soPGzZMc+bM0ZNPPqmBAwdq1apVWrRokd3iEKNHj1b//v3VoEEDNWzYULNmzdL58+ft5qQUWL/8Ih09Kp054+hIAAAAALig1NRUJSUlKTMzU8ePH9eKFSs0depU3XfffXrwwQclSVWqVFF6erpef/11derUSRs2bNC8efPszlOhQgWdO3dO8fHxqlu3rvz8/FS+fHl5eXnp9ddf17Bhw/TTTz9pypQpeYrrjz/+MDuQbUJCQvT444/rnnvu0ZQpU9SzZ08lJCRozpw5euONNyRJS5cu1f79+9WiRQsFBARo6dKlysrK0p133qnvvvtO8fHxioyMVKlSpfTdd9/pjz/+UI0aNW78jbzVDAcaOHCgERoaanh5eRklS5Y02rRpY8TGxhqGYRiHDh0yWrRoYQQGBhre3t5GlSpVjDFjxhhnzpyxO8fBgweN9u3bG76+vsYdd9xhPP7440Z6erpdndWrVxv16tUzvLy8jEqVKhnz58/PEcvrr79ulC9f3vDy8jIaNmxobNq0KV/3cubMGUNSjvicTVpamvHFF18YaWlp2QVlyhiGlP0IOIkc7RRwErO/+cUIfWqp8fSnP9JO4RJop3AFtFPDSElJMXbt2mWkpKQ4OpR869+/v6HsQXGGh4eHUbJkSSMiIsJ47733jMzMTLu6M2bMMEJCQgxfX18jKirKeP/99w1JxqlTp8w6w4YNM0qUKGFIMiZMmGAYhmEsXLjQqFChguHt7W2Eh4cbX331lSHJ2L59+xXjatmypRnXpV9TpkwxDMMwlixZYtSsWdPw9PQ0ypcvb7zyyivma9evX2+0bNnSCAgIMHx9fY06deqY97Nr1y4jKirKKFmypOHt7W1Uq1bNeP3112/a+5mbq7WP/OShFsNgt+KbITk5WcWKFdOZM2ecfsG/5cuXq0OHDtnzQ8qWze75L1NGOnLE0eEBknJpp4CTeC3+V82I+0W9G5bX5E7Vaadwevw9hSugnWYv6HbgwAFVrFgxx4JucDzbSv5Wq/WqawzcKldrH/nJQ51uzj8AAM7qn3WP+dwcAAC4FpJ/AAAAAAAKOJJ/AADyyLblMRPmAACAqyH5BwAAAACggCP5BwAgjyx/d/3T8w8AAFwNyT8AAAAAAAWch6MDgIONHy+dOyf5+zs6EgBwGQar/QMAABdD8l/YDR3q6AgAwOUw7B8AALgahv0DAJBHttX+AQAAXA3JPwAA+UTHPwAAeRMTE6PixYvfsvOvWbNGFotFp0+fvmXXKChI/gu7Y8ekI0eyHwEAV2URXf8AAFxqwIABslgsslgs8vLyUpUqVTR58mRlZGTclus3adJEx44dU7FixW76uQ8ePCiLxaLExMSbfm5HYM5/YXfPPdLRo1KZMtkfAgAArsg27J85/wAA/KNdu3aaP3++UlNTtXz5ckVHR8vT01Njx4695df28vJScHDwLb9OQUDPPwAAAAA4GcMwlJ6a6ZAvI5+fcnt7eys4OFihoaEaPny4IiIi9NVXX9nVWblypWrUqCF/f3+1a9dOx/4eebxu3Tp5enoqKSnJrv6oUaPUvHlzSdJvv/2mTp06KSAgQEWKFFGtWrW0fPlySbkP+9+wYYNatWolPz8/BQQEKCoqSqdOnZIkLVmyRGFhYfL19VWJEiUUERGh8+fP5+t+bVJTUzVy5EiVKlVKPj4+atasmbZs2WIeP3XqlPr27auSJUvK19dXVatW1fz58yVJaWlpGjFihEJCQuTj46PQ0FBNnTr1uuLIK3r+AQDII9ugf7b6AwDcahlpWXrr/9Y65NpDZ7eUp7f7db/e19dXJ0+eNJ9fuHBBr776qj744AO5ubnp3//+t5544gl99NFHatGihSpVqqQPPvhAY8aMkSSlp6fro48+0rRp0yRJ0dHRSktL07p161SkSBHt2rVL/lfYqjwxMVFt2rTRwIEDNXv2bHl4eGj16tXKzMzUsWPH1Lt3b02bNk3333+/zp49q/Xr1+f7ww6bJ598Up9++qkWLFig0NBQTZs2TVFRUdq7d68CAwP13HPPadeuXfr66691xx13aO/evUpJSZEkvfbaa/rqq6+0aNEilS9fXocPH9bhw4evK468IvkHAAAAANwwwzAUHx+vlStX6tFHHzXL09PTNW/ePFWuXFmSNGLECE2ePNk8PmjQIM2fP99M/v/3v//p4sWLeuCBByRJhw4dUrdu3RQWFiZJqlSp0hVjmDZtmho0aKA33njDLKtVq5Ykadu2bcrIyFDXrl0VGhoqSeY58+v8+fN68803FRMTo/bt20uS3n77bcXFxendd9/VmDFjdOjQId11111q0KCBJKlChQrm6w8dOqSqVauqWbNmslgsZjy3Esk/AAB5ZPmn6x8AgFvKw8tNQ2e3dNi182Pp0qXy9/dXenq6srKy1KdPH02cONE87ufnZyb+khQSEqITJ06YzwcMGKBx48Zp06ZNaty4sWJiYvTAAw+oSJEikqSRI0dq+PDhio2NVUREhLp166Y6derkGktiYqJ69OiR67G6deuqTZs2CgsLU1RUlCIjI9W9e3cFBATk634lad++fUpPT1fTpk3NMk9PTzVs2FA///yzJGn48OHq1q2btm3bpsjISHXp0kVNmjQx77lt27a688471a5dO913332KjIzMdxz5wZx/AAAAAHAyFotFnt7uDvmyWPK3u829996rxMRE/frrr0pJSdGCBQvMxF3KToovv7dLh9qXKlVKnTp10vz583X8+HF9/fXXGjhwoHl88ODB2r9/v/r166cdO3aoQYMGev3113ONxdfX94pxuru7Ky4uTl9//bVq1qyp119/XXfeeacOHDiQr/vNq/bt2+u3337TY489pt9//11t2rTRE088IUm6++67deDAAU2ZMkUpKSl64IEH1L1791sShw3JPwAAeWTb6o+OfwAA/lGkSBFVqVJF5cuXl4fH9Q0uHzx4sD755BO99dZbqly5sl2PuiSVK1dOw4YN02effabHH39cb7/9dq7nqVOnjuLj4694HYvFoqZNm2rSpEnavn27vLy89Pnnn+c73sqVK8vLy0sbNmwwy9LT07VlyxbVrFnTLCtZsqT69++vDz/8ULNmzdJbb71lHrNarerZs6fefvttffLJJ/r000/1119/5TuWvGLYPwAAAADAoaKiomS1WvX888/brQcgZa/83759e1WrVk2nTp3S6tWrVaNGjVzPM3bsWIWFhemRRx7RsGHD5OXlpdWrV6tHjx7at2+f4uPjFRkZqVKlSum7777TH3/8ccVz2ezZs0fnz59XkSJF5OaW3X9eq1YtDR8+XGPGjFFgYKDKly+vadOm6cKFCxo0aJAkafz48apfv75q1aql1NRULV261LzWjBkzFBISorvuuktubm5avHixgoODVbx48Rt8J6+M5B8AgDyyjYK83lWBAQBA7tzc3DRgwAC9+OKLevDBB+2OZWZmKjo6WkeOHJHValW7du00c+bMXM9TrVo1xcbG6plnnlHDhg3l6+urRo0aqXfv3rJarVq3bp1mzZql5ORkhYaGavr06eaCfVfSp0+fHGWHDx/WSy+9pKysLPXr109nz55VgwYNtHLlSnMNAS8vL40dO1YHDx6Ur6+vmjdvrv/+97+SpKJFi2ratGn69ddf5e7urnvuuUfLly83P1y4FUj+AQAAAADXJSYm5qrHBwwYoAEDBtiVdenSJdcP0o8ePaoOHTooJCTErvxK8/slqVWrVjnO1bJlS7vh+DbFixfXihUrrhrvpSpUqCDDMJSVlaXk5GRZrdYcyflrr72m1157LdfXjxs3TuPGjcv12JAhQzRkyJA8x3IzkPwXdvHxUkaGdJ1zcwCgMKLfHwCAm+fMmTPasWOHFi5cqK+++srR4RRYZHyF3Z13OjoCAAAAAIVY586dtXnzZg0bNkxt27Z1dDgFFsk/AAB5ZNv6iCn/AADcPGvWrHF0CIUCW/0BAJBP5P4AAMDV0PNf2C1cKF24IPn5SbmsYgkA+IfF0QEAAAo0dpNBbm5WuyD5L+yefFI6elQqU4bkHwCuga3+AAC3gqenpyTpwoUL8vX1dXA0cDYXLlyQ9E87uV4k/wAAAADgQO7u7ipevLhOnDghSfLz8zPXmYHjZWVlKS0tTRcvXsyx1d+tZBiGLly4oBMnTqh48eJyd3e/ofOR/AMAkEe2/4bR7w8AuNmCg4MlyfwAAM7DMAylpKTI19fXIR/KFC9e3GwfN4LkHwAAAAAczGKxKCQkRKVKlVJ6erqjw8El0tPTtW7dOrVo0eKGh97nl6en5w33+NuQ/AMAkEfmp/10/QMAbhF3d/ebluzh5nB3d1dGRoZ8fHxue/J/M7HVHwAAAAAABRzJPwAAefRPxz9d/wAAwLWQ/AMAAAAAUMCR/AMAkEfmav90/AMAABfDgn+FnW3LiJuwdQQAAAAAwDmR/Bd233/v6AgAwHX8Pemfnn8AAOBqGPYPAAAAAEABR/IPAEAemXP+We0fAAC4GJJ/AADyiWH/AADA1Tg0+X/zzTdVp04dWa1WWa1WhYeH6+uvvzaPX7x4UdHR0SpRooT8/f3VrVs3HT9+3O4chw4dUseOHeXn56dSpUppzJgxysjIsKuzZs0a3X333fL29laVKlUUExOTI5a5c+eqQoUK8vHxUaNGjbR58+Zbcs9O5+GHpR49sh8BAFdlsVy7DgAAgDNyaPJftmxZvfTSS9q6dau+//57tW7dWp07d9bOnTslSY899pj+97//afHixVq7dq1+//13de3a1Xx9ZmamOnbsqLS0NG3cuFELFixQTEyMxo8fb9Y5cOCAOnbsqHvvvVeJiYkaNWqUBg8erJUrV5p1PvnkE40ePVoTJkzQtm3bVLduXUVFRenEiRO3781wlGXLpCVLsh8BAHlCxz8AAHA1Dk3+O3XqpA4dOqhq1aqqVq2aXnjhBfn7+2vTpk06c+aM3n33Xc2YMUOtW7dW/fr1NX/+fG3cuFGbNm2SJMXGxmrXrl368MMPVa9ePbVv315TpkzR3LlzlZaWJkmaN2+eKlasqOnTp6tGjRoaMWKEunfvrpkzZ5pxzJgxQ0OGDNFDDz2kmjVrat68efLz89N7773nkPcFAOCcLKLrHwAAuCan2eovMzNTixcv1vnz5xUeHq6tW7cqPT1dERERZp3q1aurfPnySkhIUOPGjZWQkKCwsDAFBQWZdaKiojR8+HDt3LlTd911lxISEuzOYaszatQoSVJaWpq2bt2qsWPHmsfd3NwUERGhhISEK8abmpqq1NRU83lycrIkKT09Xenp6Tf0XtxKtthsjx7KXsDKkJThxHGjcLm8nQLOIisr8+/HLNopXALtFK6Adgpn58xtND8xOTz537Fjh8LDw3Xx4kX5+/vr888/V82aNZWYmCgvLy8VL17crn5QUJCSkpIkSUlJSXaJv+247djV6iQnJyslJUWnTp1SZmZmrnV27959xbinTp2qSZMm5SiPjY2Vn59f3m7egeLi4iRJkRcvylfZ6yvELl/u2KCAy9jaKeAsdhy3SHLX8ePHFRd3TBLtFK6BdgpXQDuFs3PGNnrhwoU813V48n/nnXcqMTFRZ86c0ZIlS9S/f3+tXbvW0WFd09ixYzV69GjzeXJyssqVK6fIyEhZrVYHRnZ16enpiouLU9u2beXp6SkPHx9Jko+Pjzp06ODg6IBsl7dTwFmc+/6IPtm/S6VKBalt29q0Uzg9/p7CFdBO4eycuY3aRqDnhcOTfy8vL1WpUkWSVL9+fW3ZskWzZ89Wz549lZaWptOnT9v1/h8/flzBwcGSpODg4Byr8tt2A7i0zuU7BBw/flxWq1W+vr5yd3eXu7t7rnVs58iNt7e3vL29c5R7eno6XYPIzeVxWv4uA5yJq/w+ofBwd3eXJLm5Wcy2STuFK6CdwhXQTuHsnLGN5icehy74l5usrCylpqaqfv368vT0VHx8vHlsz549OnTokMLDwyVJ4eHh2rFjh92q/HFxcbJarapZs6ZZ59Jz2OrYzuHl5aX69evb1cnKylJ8fLxZBwAA6Z+t/gyW+wcAAC7GoT3/Y8eOVfv27VW+fHmdPXtWCxcu1Jo1a7Ry5UoVK1ZMgwYN0ujRoxUYGCir1apHH31U4eHhaty4sSQpMjJSNWvWVL9+/TRt2jQlJSVp3Lhxio6ONnvlhw0bpjlz5ujJJ5/UwIEDtWrVKi1atEjLLtnabvTo0erfv78aNGighg0batasWTp//rweeughh7wvAAAAAADcTA5N/k+cOKEHH3xQx44dU7FixVSnTh2tXLlSbdu2lSTNnDlTbm5u6tatm1JTUxUVFaU33njDfL27u7uWLl2q4cOHKzw8XEWKFFH//v01efJks07FihW1bNkyPfbYY5o9e7bKli2rd955R1FRUWadnj176o8//tD48eOVlJSkevXqacWKFTkWAQQAFG62rf7o+AcAAK7Gocn/u+++e9XjPj4+mjt3rubOnXvFOqGhoVp+jVXqW7Vqpe3bt1+1zogRIzRixIir1imQeveWTp2SAgIcHQkAAAAA4BZx+IJ/cLBXXnF0BADgOsw5//T9AwAA1+J0C/4BAAAAAICbi+QfAIA8+rvjnzn/AADA5ZD8AwAAAABQwJH8F3bVq0tWa/YjAOCqLJa/V/un6x8AALgYkv/C7tw56ezZ7EcAQJ6Q+wMAAFdD8g8AQB5Zrl0FAADAKZH8AwCQRxa2+gMAAC6K5B8AAAAAgAKO5B8AgDyyMO4fAAC4KJJ/AAAAAAAKOJJ/AADyyCK2+gMAAK6J5B8AAAAAgAKO5B8AgDwyV/sXXf8AAMC1eDg6ADjYvHlSSork6+voSAAAAAAAtwjJf2F3332OjgAAXA5z/gEAgKth2D8AAAAAAAUcyT8AAHlksbDaPwAAcE0M+y/stm6V0tIkLy+pfn1HRwMAAAAAuAVI/gu7zp2lo0elMmWkI0ccHQ0AOLW/F/tntX8AAOByGPYPAEA+MewfAAC4GpJ/AADyyGK5dh0AAABnRPIPAEAeWf4e+E/HPwAAcDUk/wAAAAAAFHAk/wAA5JHlnxX/AAAAXArJPwAAAAAABRzJPwAAecRWfwAAwFWR/AMAAAAAUMCR/AMAkEe2Of8GHf8AAMDFeDg6ADjYzz9n/y+WzasBAAAAoMAi+S/sihZ1dAQA4EKyPyil4x8AALgahv0DAAAAAFDAkfwDAJBH/8z5p+8fAAC4Fob9F3YzZkjJyZLVKo0e7ehoAAAAAAC3AMl/YTdjhnT0qFSmDMk/AFyDbWlU+v0BAICrYdg/AAD5xKh/AADgakj+AQDIIwvbogIAABdF8g8AQD7R8Q8AAFwNyT8AAHlEvz8AAHBVJP8AAOSROeqfSf8AAMDFkPwDAAAAAFDAOTT5nzp1qu655x4VLVpUpUqVUpcuXbRnzx67Oq1atZLFYrH7GjZsmF2dQ4cOqWPHjvLz81OpUqU0ZswYZWRk2NVZs2aN7r77bnl7e6tKlSqKiYnJEc/cuXNVoUIF+fj4qFGjRtq8efNNv2cAgOuy9fzT7w8AAFyNQ5P/tWvXKjo6Wps2bVJcXJzS09MVGRmp8+fP29UbMmSIjh07Zn5NmzbNPJaZmamOHTsqLS1NGzdu1IIFCxQTE6Px48ebdQ4cOKCOHTvq3nvvVWJiokaNGqXBgwdr5cqVZp1PPvlEo0eP1oQJE7Rt2zbVrVtXUVFROnHixK1/IwAAAAAAuIU8HHnxFStW2D2PiYlRqVKltHXrVrVo0cIs9/PzU3BwcK7niI2N1a5du/TNN98oKChI9erV05QpU/TUU09p4sSJ8vLy0rx581SxYkVNnz5dklSjRg19++23mjlzpqKioiRJM2bM0JAhQ/TQQw9JkubNm6dly5bpvffe09NPP53juqmpqUpNTTWfJycnS5LS09OVnp5+A+/KrWWLzfboXq+eVLasdMcdynTiuFG4XN5OAWeRmZkpScrKMmincAm0U7gC2imcnTO30fzEZDEM51m1aO/evapatap27Nih2rVrS8oe9r9z504ZhqHg4GB16tRJzz33nPz8/CRJ48eP11dffaXExETzPAcOHFClSpW0bds23XXXXWrRooXuvvtuzZo1y6wzf/58jRo1SmfOnFFaWpr8/Py0ZMkSdenSxazTv39/nT59Wl9++WWOWCdOnKhJkyblKF+4cKEZGwCgYNl5yqK3drurXBFDT9TJdHQ4AACgkLtw4YL69OmjM2fOyGq1XrWuQ3v+L5WVlaVRo0apadOmZuIvSX369FFoaKhKly6tH3/8UU899ZT27Nmjzz77TJKUlJSkoKAgu3PZniclJV21TnJyslJSUnTq1CllZmbmWmf37t25xjt27FiNHj3afJ6cnKxy5copMjLymm+6I6WnpysuLk5t27aVp6eno8MBckU7hbMq8ssfemv3dlmLWdW2bQPaKZwef0/hCmincHbO3EZtI9DzwmmS/+joaP3000/69ttv7cqHDh1qfh8WFqaQkBC1adNG+/btU+XKlW93mCZvb295e3vnKPf09HS6BpEbV4kThRvtFM7G3SP7n02LxWK2TdopXAHtFK6Adgpn54xtND/xOMVWfyNGjNDSpUu1evVqlS1b9qp1GzVqJCl7ioAkBQcH6/jx43Z1bM9t6wRcqY7VapWvr6/uuOMOubu751rnSmsNAAAKn78X+5fzTJgDAADIG4cm/4ZhaMSIEfr888+1atUqVaxY8Zqvsc3tDwkJkSSFh4drx44ddqvyx8XFyWq1qmbNmmad+Ph4u/PExcUpPDxckuTl5aX69evb1cnKylJ8fLxZp8D617+k8PDsRwAAAABAgeTQYf/R0dFauHChvvzySxUtWtSco1+sWDH5+vpq3759WrhwoTp06KASJUroxx9/1GOPPaYWLVqoTp06kqTIyEjVrFlT/fr107Rp05SUlKRx48YpOjraHJY/bNgwzZkzR08++aQGDhyoVatWadGiRVq2bJkZy+jRo9W/f381aNBADRs21KxZs3T+/Hlz9f8Ca9s26ehRqUwZR0cCAE7PYsnu+6fnHwAAuBqHJv9vvvmmpOwV/S81f/58DRgwQF5eXvrmm2/MRLxcuXLq1q2bxo0bZ9Z1d3fX0qVLNXz4cIWHh6tIkSLq37+/Jk+ebNapWLGili1bpscee0yzZ89W2bJl9c4775jb/ElSz5499ccff2j8+PFKSkpSvXr1tGLFihyLAAIAAAAA4Gocmvxfa5fBcuXKae3atdc8T2hoqJYvX37VOq1atdL27duvWmfEiBEaMWLENa8HACiczDn/Do0CAAAg/5xiwT8AAFzJtT68BgAAcDYk/wAA5JHFcu06AAAAzojkHwCAPLKI7B8AALgmkn8AAAAAAAo4kn8AAPLINuyfKf8AAMDVkPwDAAAAAFDAOXSrPziB0aOl5GTJanV0JADg9P7Z6o+ufwAA4FpI/gu70aMdHQEAAAAA4BZj2D8AAHnFnH8AAOCiSP4BAAAAACjgGPZf2J09m92FZbFIRYs6OhoAcGqWv7v+6fgHAACuhp7/wq5GDalYsexHAAAAAECBRPIPAEAeWcw5//T9AwAA10LyDwAAAABAAUfyDwBAHv3d8c+cfwAA4HJI/gEAyC+yfwAA4GJI/gEAyCOLbdI/AACAiyH5BwAgn+j4BwAArobkHwCAPKLjHwAAuCqSfwAA8shc8I+t/gAAgIsh+QcAAAAAoIDzcHQAcLAvv5TS0iQvL0dHAgBOzzbsn35/AADgakj+C7v69R0dAQAAAADgFmPYPwAAeZbd9c+UfwAA4GpI/gEAAAAAKOAY9l/YLV0qpaRIvr7Sffc5OhoAcGr/zPmn6x8AALgWkv/Cbtgw6ehRqUwZ6cgRR0cDAAAAALgFGPYPAEAe/d3xz5x/AADgcq4r+T98+LCOXNJLvHnzZo0aNUpvvfXWTQsMAAAAAADcHNeV/Pfp00erV6+WJCUlJalt27bavHmznn32WU2ePPmmBggAgLOwWFjtHwAAuKbrSv5/+uknNWzYUJK0aNEi1a5dWxs3btRHH32kmJiYmxkfAAAAAAC4QdeV/Kenp8vb21uS9M033+hf//qXJKl69eo6duzYzYsOAAAnYrl2FQAAAKd0Xcl/rVq1NG/ePK1fv15xcXFq166dJOn3339XiRIlbmqAAAA4G4Nx/wAAwMVcV/L/8ssv6z//+Y9atWql3r17q27dupKkr776ypwOAABAQWOh6x8AALgoj+t5UatWrfTnn38qOTlZAQEBZvnQoUPl5+d304IDAMCZWP4e+E+/PwAAcDXX1fOfkpKi1NRUM/H/7bffNGvWLO3Zs0elSpW6qQHiFvP3l4oWzX4EAAAAABRI15X8d+7cWe+//74k6fTp02rUqJGmT5+uLl266M0337ypAeIW271bSk7OfgQAXJVt2D9T/gEAgKu5ruR/27Ztat68uSRpyZIlCgoK0m+//ab3339fr7322k0NEAAAAAAA3JjrSv4vXLigokWLSpJiY2PVtWtXubm5qXHjxvrtt99uaoAAADgbg1n/AADAxVxX8l+lShV98cUXOnz4sFauXKnIyEhJ0okTJ2S1Wm9qgAAAAAAA4MZcV/I/fvx4PfHEE6pQoYIaNmyo8PBwSdmjAO666648n2fq1Km65557VLRoUZUqVUpdunTRnj177OpcvHhR0dHRKlGihPz9/dWtWzcdP37crs6hQ4fUsWNH+fn5qVSpUhozZowyMjLs6qxZs0Z33323vL29VaVKFcXExOSIZ+7cuapQoYJ8fHzUqFEjbd68Oc/34rLGjJEGD85+BABcFXP+AQCAq7qu5L979+46dOiQvv/+e61cudIsb9OmjWbOnJnn86xdu1bR0dHatGmT4uLilJ6ersjISJ0/f96s89hjj+l///ufFi9erLVr1+r3339X165dzeOZmZnq2LGj0tLStHHjRi1YsEAxMTEaP368WefAgQPq2LGj7r33XiUmJmrUqFEaPHiwXeyffPKJRo8erQkTJmjbtm2qW7euoqKidOLEiet5i1zHxx9L776b/QgAAAAAKJA8rveFwcHBCg4O1pEjRyRJZcuWVcOGDfN1jhUrVtg9j4mJUalSpbR161a1aNFCZ86c0bvvvquFCxeqdevWkqT58+erRo0a2rRpkxo3bqzY2Fjt2rVL33zzjYKCglSvXj1NmTJFTz31lCZOnCgvLy/NmzdPFStW1PTp0yVJNWrU0LfffquZM2cqKipKkjRjxgwNGTJEDz30kCRp3rx5WrZsmd577z09/fTT1/s2AQAKEIuyu/7p+AcAAK7mupL/rKwsPf/885o+fbrOnTsnSSpatKgef/xxPfvss3Jzu64BBTpz5owkKTAwUJK0detWpaenKyIiwqxTvXp1lS9fXgkJCWrcuLESEhIUFhamoKAgs05UVJSGDx+unTt36q677lJCQoLdOWx1Ro0aJUlKS0vT1q1bNXbsWPO4m5ubIiIilJCQkGusqampSk1NNZ8nJydLktLT05Wenn5d93872GKzPXpIsij7P7IZThw3CpfL2yngLMwpZYZBO4VLoJ3CFdBO4eycuY3mJ6brSv6fffZZvfvuu3rppZfUtGlTSdK3336riRMn6uLFi3rhhRfyfc6srCyNGjVKTZs2Ve3atSVJSUlJ8vLyUvHixe3qBgUFKSkpyaxzaeJvO247drU6ycnJSklJ0alTp5SZmZlrnd27d+ca79SpUzVp0qQc5bGxsfLz88vjXTtOXFycJCny4kX5Kntthdjlyx0bFHAZWzsFnMXR85LkoYupqWb7pJ3CFdBO4Qpop3B2zthGL1y4kOe615X8L1iwQO+8847+9a9/mWV16tRRmTJl9Mgjj1xX8h8dHa2ffvpJ33777fWEdNuNHTtWo0ePNp8nJyerXLlyioyMdOodD9LT0xUXF6e2bdvK09NTHj4+kiQfHx916NDBwdEB2S5vp4Cz2J10VtN+TJC3t7fatm1KO4XT4+8pXAHtFM7OmduobQR6XlxX8v/XX3+pevXqOcqrV6+uv/76K9/nGzFihJYuXap169apbNmyZnlwcLDS0tJ0+vRpu97/48ePKzg42Kxz+ar8tt0ALq1z+Q4Bx48fl9Vqla+vr9zd3eXu7p5rHds5Luft7S1vb+8c5Z6enk7XIHJzeZyWv8sAZ+Iqv08oPDw9PS753tN8pJ3C2dFO4Qpop3B2zthG8xPPdU3Or1u3rubMmZOjfM6cOapTp06ez2MYhkaMGKHPP/9cq1atUsWKFe2O169fX56enoqPjzfL9uzZo0OHDpnbC4aHh2vHjh12q/LHxcXJarWqZs2aZp1Lz2GrYzuHl5eX6tevb1cnKytL8fHxZh0AAGzY6g8AALia6+r5nzZtmjp27KhvvvnGTI4TEhJ0+PBhLc/HvPHo6GgtXLhQX375pYoWLWrO0S9WrJh8fX1VrFgxDRo0SKNHj1ZgYKCsVqseffRRhYeHq3HjxpKkyMhI1axZU/369dO0adOUlJSkcePGKTo62uyZHzZsmObMmaMnn3xSAwcO1KpVq7Ro0SItW7bMjGX06NHq37+/GjRooIYNG2rWrFk6f/68ufo/AAC21f4BAABczXX1/Lds2VK//PKL7r//fp0+fVqnT59W165dtXPnTn3wwQd5Ps+bb76pM2fOqFWrVgoJCTG/PvnkE7POzJkzdd9996lbt25q0aKFgoOD9dlnn5nH3d3dtXTpUrm7uys8PFz//ve/9eCDD2ry5MlmnYoVK2rZsmWKi4tT3bp1NX36dL3zzjvmNn+S1LNnT7366qsaP3686tWrp8TERK1YsSLHIoAAANDxDwAAXM119fxLUunSpXMs7PfDDz/o3Xff1VtvvZWncxh5GDfp4+OjuXPnau7cuVesExoaes0RB61atdL27duvWmfEiBEaMWLENWMqUDp2lP76S/p7e0UAwJVZ6PgHAAAu6rqTfxQQ//mPoyMAAJdhy/3z8uE1AACAM7muYf8AAAAAAMB1kPwDAJBHtmH/9PsDAABXk69h/127dr3q8dOnT99ILAAAAAAA4BbIV/JfrFixax5/8MEHbygg3GYNGkhJSVJwsPT9946OBgCcXHbXP1P+AQCAq8lX8j9//vxbFQccJSlJOnrU0VEAAAAAAG4h5vwDAJBH5px/uv4BAICLIfkHAAAAAKCAI/kHACCP/u74Z7V/AADgckj+AQAAAAAo4Ej+AQDII4s56d+xcQAAAOQXyT8AAAAAAAUcyT8AAHnEnH8AAOCqSP4BAMgntvoDAACuxsPRAcDBpk2TLlyQ/PwcHQkAOD3blH8AAABXQ/Jf2PXp4+gIAMBlWP4e+E+/PwAAcDUM+wcAAAAAoIAj+QcAII/Mnf7o+gcAAC6GYf+F3Z49UkaG5OEh3Xmno6MBAAAAANwCJP+FXZs20tGjUpky0pEjjo4GAFyCwax/AADgYhj2DwAAAABAAUfyDwBAHjHnHwAAuCqSfwAAAAAACjiSfwAA8sjyd9c/Hf8AAMDVkPwDAAAAAFDAkfwDAJBHFts3dP0DAAAXQ/IPAAAAAEABR/IPAEAemav90/UPAABcDMk/AAD5xFZ/AADA1Xg4OgA42JYtUmam5O7u6EgAwOlZ/pn1DwAA4FJI/gu7kBBHRwAALuOfYf8AAACuhWH/AAAAAAAUcCT/AADkkW3Qv8GkfwAA4GIY9l/YvfWWdO6c5O8vDR3q6GgAAAAAALcAyX9hN3mydPSoVKYMyT8AXAtz/gEAgIti2D8AAAAAAAUcyT8AAHlk2+qPKf8AAMDVkPwDAAAAAFDAkfwDAJBHFsu16wAAADgjkn8AAAAAAAo4hyb/69atU6dOnVS6dGlZLBZ98cUXdscHDBggi8Vi99WuXTu7On/99Zf69u0rq9Wq4sWLa9CgQTp37pxdnR9//FHNmzeXj4+PypUrp2nTpuWIZfHixapevbp8fHwUFham5cuX3/T7BQC4tks7/g0m/gMAABfi0OT//Pnzqlu3rubOnXvFOu3atdOxY8fMr48//tjueN++fbVz507FxcVp6dKlWrdunYZesmVdcnKyIiMjFRoaqq1bt+qVV17RxIkT9dZbb5l1Nm7cqN69e2vQoEHavn27unTpoi5duuinn366+TcNAAAAAMBt5uHIi7dv317t27e/ah1vb28FBwfneuznn3/WihUrtGXLFjVo0ECS9Prrr6tDhw569dVXVbp0aX300UdKS0vTe++9Jy8vL9WqVUuJiYmaMWOG+SHB7Nmz1a5dO40ZM0aSNGXKFMXFxWnOnDmaN2/eTbxjAIArs1wy6Z+OfwAA4EocmvznxZo1a1SqVCkFBASodevWev7551WiRAlJUkJCgooXL24m/pIUEREhNzc3fffdd7r//vuVkJCgFi1ayMvLy6wTFRWll19+WadOnVJAQIASEhI0evRou+tGRUXlmIZwqdTUVKWmpprPk5OTJUnp6elKT0+/Gbd+S9hisz26V60qi9UqIyhImU4cNwqXy9sp4CwubZPpGbRTOD/+nsIV0E7h7Jy5jeYnJqdO/tu1a6euXbuqYsWK2rdvn5555hm1b99eCQkJcnd3V1JSkkqVKmX3Gg8PDwUGBiopKUmSlJSUpIoVK9rVCQoKMo8FBAQoKSnJLLu0ju0cuZk6daomTZqUozw2NlZ+fn7Xdb+3U1xcXPY3o0b9U8g6B3AyZjsFnMT5dMn2T2dc3Ddys9BO4Rpop3AFtFM4O2dsoxcuXMhzXadO/nv16mV+HxYWpjp16qhy5cpas2aN2rRp48DIpLFjx9qNFkhOTla5cuUUGRkpq9XqwMiuLj09XXFxcWrbtq08PT0dHQ6QK9opnNWpC2l65vs1kqQ2EW20Oj6edgqnxt9TuALaKZydM7dR2wj0vHDq5P9ylSpV0h133KG9e/eqTZs2Cg4O1okTJ+zqZGRk6K+//jLXCQgODtbx48ft6tieX6vOldYakLLXIvD29s5R7unp6XQNIjeuEicKN9opnI2X5z8T/T09stsm7RSugHYKV0A7hbNzxjaan3gcutp/fh05ckQnT55USEiIJCk8PFynT5/W1q1bzTqrVq1SVlaWGjVqZNZZt26d3VyIuLg43XnnnQoICDDrxMfH210rLi5O4eHht/qWAAAuxHLJZn+s9wcAAFyJQ5P/c+fOKTExUYmJiZKkAwcOKDExUYcOHdK5c+c0ZswYbdq0SQcPHlR8fLw6d+6sKlWqKCoqSpJUo0YNtWvXTkOGDNHmzZu1YcMGjRgxQr169VLp0qUlSX369JGXl5cGDRqknTt36pNPPtHs2bPthuz/3//9n1asWKHp06dr9+7dmjhxor7//nuNGDHitr8nt13fvlJUVPYjAAAAAKBAcuiw/++//1733nuv+dyWkPfv319vvvmmfvzxRy1YsECnT59W6dKlFRkZqSlTptgNt//oo480YsQItWnTRm5uburWrZtee+0183ixYsUUGxur6Oho1a9fX3fccYfGjx9vbvMnSU2aNNHChQs1btw4PfPMM6pataq++OIL1a5d+za8Cw62dq109KhUpoyjIwEA5/dPxz9b/QEAAJfi0OS/VatWMq7yv6eVK1de8xyBgYFauHDhVevUqVNH69evv2qdHj16qEePHte8HgAAAAAArsal5vwDAOBIlkt7/h0XBgAAQL6R/AMAAAAAUMCR/AMAkEeWS58w6R8AALgQkn8AAAAAAAo4kn8AAPLIcsmkf/r9AQCAKyH5BwAAAACggCP5BwAgjy6d88+UfwAA4Eo8HB0AHGzIEOnMGalYMUdHAgAAAAC4RUj+C7sJExwdAQC4jEum/Mtg1j8AAHAhDPsHAOA6MOwfAAC4EpJ/AADyyGI36x8AAMB1kPwDAJBH9sP+AQAAXAfJf2FXtmz2/2bLlnV0JAAAAACAW4TkHwCA68CcfwAA4EpI/gEAAAAAKOBI/gEAyCOL3Xp/dP0DAADXQfIPAAAAAEABR/IPAEAeXbrVH3P+AQCAKyH5BwAAAACggCP5BwAgjy6d80/HPwAAcCUk/wAAAAAAFHAk/wAA5NGli/0z5x8AALgSD0cHAAf78EMpNVXy9nZ0JAAAAACAW4Tkv7Br1crREQCAy7BcMunfYNY/AABwIQz7BwDgOjDsHwAAuBKSfwAA8shy7SoAAABOiWH/hd2aNf/M+WcKAADkGR3/AADAlZD8F3b//rd09KhUpox05IijowEAp2ah6x8AALgohv0DAJBHly74x6R/AADgSkj+AQAAAAAo4Ej+AQC4DvT7AwAAV0LyDwAAAABAAUfyDwBAPtim/TPlHwAAuBKSfwAAAAAACjiSfwAA8sG23j8d/wAAwJWQ/AMAAAAAUMCR/AMAkA+Wvyf9G0z6BwAALsTD0QHAwY4ccXQEAAAAAIBbjJ5/AADygTn/AADAFZH8AwAAAABQwJH8AwCQD39P+RdT/gEAgCtxaPK/bt06derUSaVLl5bFYtEXX3xhd9wwDI0fP14hISHy9fVVRESEfv31V7s6f/31l/r27Sur1arixYtr0KBBOnfunF2dH3/8Uc2bN5ePj4/KlSunadOm5Yhl8eLFql69unx8fBQWFqbly5ff9Pt1SpMmSaNHZz8CAAAAAAokhyb/58+fV926dTV37txcj0+bNk2vvfaa5s2bp++++05FihRRVFSULl68aNbp27evdu7cqbi4OC1dulTr1q3T0KFDzePJycmKjIxUaGiotm7dqldeeUUTJ07UW2+9ZdbZuHGjevfurUGDBmn79u3q0qWLunTpop9++unW3byzePttaebM7EcAwDVZzFn/AAAArsOhq/23b99e7du3z/WYYRiaNWuWxo0bp86dO0uS3n//fQUFBemLL75Qr1699PPPP2vFihXasmWLGjRoIEl6/fXX1aFDB7366qsqXbq0PvroI6Wlpem9996Tl5eXatWqpcTERM2YMcP8kGD27Nlq166dxowZI0maMmWK4uLiNGfOHM2bN+82vBMAAJdhDvtn3D8AAHAdTrvV34EDB5SUlKSIiAizrFixYmrUqJESEhLUq1cvJSQkqHjx4mbiL0kRERFyc3PTd999p/vvv18JCQlq0aKFvLy8zDpRUVF6+eWXderUKQUEBCghIUGjR4+2u35UVFSOaQiXSk1NVWpqqvk8OTlZkpSenq709PQbvf1bxhab7dFD2f+PNSRlOHHcKFwub6eAM0rPyMh+pJ3CifH3FK6Adgpn58xtND8xOW3yn5SUJEkKCgqyKw8KCjKPJSUlqVSpUnbHPTw8FBgYaFenYsWKOc5hOxYQEKCkpKSrXic3U6dO1aRc5snHxsbKz88vL7foUHFxcZKkyIsX5Svp4sWLii0s6xzAZdjaKeBMjEx3SRatX79egd60U7gG2ilcAe0Uzs4Z2+iFCxfyXNdpk39nN3bsWLvRAsnJySpXrpwiIyNltVodGNnVpaenKy4uTm3btpWnp6c8fHwkST4+PurQoYODowOyXd5OAWfy5JZvpIwsNWvWXLu2rKedwqnx9xSugHYKZ+fMbdQ2Aj0vnDb5Dw4OliQdP35cISEhZvnx48dVr149s86JEyfsXpeRkaG//vrLfH1wcLCOHz9uV8f2/Fp1bMdz4+3tLW9v7xzlnp6eTtcgcnN5nJa/ywBn4iq/TyhcbFv9eXhk/xNKO4UroJ3CFdBO4eycsY3mJx6HrvZ/NRUrVlRwcLDi4+PNsuTkZH333XcKDw+XJIWHh+v06dPaunWrWWfVqlXKyspSo0aNzDrr1q2zmwsRFxenO++8UwEBAWadS69jq2O7DgAAAAAArsyhyf+5c+eUmJioxMRESdmL/CUmJurQoUOyWCwaNWqUnn/+eX311VfasWOHHnzwQZUuXVpdunSRJNWoUUPt2rXTkCFDtHnzZm3YsEEjRoxQr169VLp0aUlSnz595OXlpUGDBmnnzp365JNPNHv2bLsh+//3f/+nFStWaPr06dq9e7cmTpyo77//XiNGjLjdbwkAwMnZtvozxGr/AADAdTh02P/333+ve++913xuS8j79++vmJgYPfnkkzp//ryGDh2q06dPq1mzZlqxYoV8/p6nLkkfffSRRowYoTZt2sjNzU3dunXTa6+9Zh4vVqyYYmNjFR0drfr16+uOO+7Q+PHjzW3+JKlJkyZauHChxo0bp2eeeUZVq1bVF198odq1a9+GdwEAAAAAgFvLocl/q1atrrpPssVi0eTJkzV58uQr1gkMDNTChQuvep06depo/fr1V63To0cP9ejR4+oBF0QtW0p//indcYejIwEAl2Cb83+Vf74AAACcjtMu+Ifb5KOPHB0BAAAAAOAWc9oF/wAAcEZ/d/wz4x8AALgUkn8AAAAAAAo4kn8AAPLBYk76d2wcAAAA+UHyX9i1bi3VqpX9CADIM7b6AwAAroQF/wq7X36Rjh6VzpxxdCQA4BIs164CAADgdOj5BwDgOrDVHwAAcCUk/wAA5Add/wAAwAWR/AMAkA/mVn/0/AMAABdC8g8AAAAAQAFH8g8AQD7Ytvqj4x8AALgSkn8AAAAAAAo4kn8AAPLh745/GUz6BwAALoTkHwAAAACAAs7D0QHAwcaPl86dk/z9HR0JALgEc7V/h0YBAACQPyT/hd3QoY6OAAAAAABwizHsHwCAfLCYk/4dGwcAAEB+kPwDAAAAAFDAMey/sDt2TMrMlNzdpZAQR0cDAE7vnzn/dP0DAADXQc9/YXfPPVK5ctmPAAAAAIACieQfAIB8MKf80/EPAABcCMk/AADXgdwfAAC4EpJ/AADyxXLtKgAAAE6G5B8AgHxg2D8AAHBFJP8AAAAAABRwJP8AAOQDW/0BAABXRPIPAAAAAEABR/KPwmnRIumvvxwdBQAXxJx/AADgikj+Ufjs3ClNnCj17SudPu3oaAAAAADgliP5L+zi46Wffsp+LCyqV5eeeUY6f17697+lU6ccHREAF2Jhqz8AAOCCPBwdABzszjsdHcHtZRiSu7vUu7fk5ibNnSv16yd98IEUEODo6AAAAADglqDnH4WLxSJlZWV/ANCzpzR8ePbc/379GAEAIE+Y8w8AAFwRyT8KD9v/1C0W6eLF7A8A+vSRRo2S/vyTDwAAAAAAFFgM+y/sFi6ULlyQ/PyyE+GCyjCyk/6VK6WPP5Z275YiIqT775ceeCD7+GuvZX8A8OGHUvHijo4YgJOyzfg3RNc/AABwHfT8F3ZPPikNGZL9WJBZLNKXX0rdukklSkiDB0uffCI98oi0f7/UvbsUHS0lJ0v/+pd05oyjIwYAAACAm4bkHwVTVlb2o2Fkf504IU2dKr34ojR9uvTQQ9nb/DVrJlWsmD0FoFcvaeBAydc3+0MAAMiF5e9J/8z5BwAAroTkHwXPe+9lD+1PS8vu8bdYJC8vKTMzO8Hfv18qXz57yP/06dnHV6+WUlKkBx+UFi+WypVz9F0AcHLk/gAAwJWQ/KNgycqS3nlHevll6X//y/4AQJLOnZP++ENasUKKipI6dpTefDP72N690pw50qZN2dv/Wa2Oix8AAAAAbgGSfxQchpGdvK9eLYWGZg/z/+KL7JX9y5bNXtBw4ECpWjXprbeyh/pL0vz52aMB7rzToeEDcC0G4/4BAIALYbV/FBwWS3ZPv7d3dkLfuXN2j76bW/YQ/8GDpQMHpDVrskcHSNIPP0gLFkjr12d/QAAA12CxXLsOAACAs3Hqnv+JEyfKYrHYfVWvXt08fvHiRUVHR6tEiRLy9/dXt27ddPz4cbtzHDp0SB07dpSfn59KlSqlMWPGKCMjw67OmjVrdPfdd8vb21tVqlRRTEzM7bg93GyGkT23/7//lR59NLtnf8sWacyY7CkAlSpJEyZkb+c3frw0b550+LC0YYNUt66jowfgImzJP/3+AADAlTh9z3+tWrX0zTffmM89PP4J+bHHHtOyZcu0ePFiFStWTCNGjFDXrl21YcMGSVJmZqY6duyo4OBgbdy4UceOHdODDz4oT09Pvfjii5KkAwcOqGPHjho2bJg++ugjxcfHa/DgwQoJCVFUVNTtvVncGIsle97+oEHS3LlSo0aSn5/Uu7f09NPZx++7T5o2LfsDgRIlpNTU7NX9AQAAAKAAc/rk38PDQ8HBwTnKz5w5o3fffVcLFy5U69atJUnz589XjRo1tGnTJjVu3FixsbHatWuXvvnmGwUFBalevXqaMmWKnnrqKU2cOFFeXl6aN2+eKlasqOnTp0uSatSooW+//VYzZ84k+XdFu3Zlb93XrZtUtGh22dq1UvPm0qhRUkaG1KGDVLJk9jEfH4eFCsA1WUTXPwAAcD1On/z/+uuvKl26tHx8fBQeHq6pU6eqfPny2rp1q9LT0xUREWHWrV69usqXL6+EhAQ1btxYCQkJCgsLU1BQkFknKipKw4cP186dO3XXXXcpISHB7hy2OqNGjbpqXKmpqUpNTTWfJ/+9L3x6errS09Nvwp3fGrbYbI/uQUGySDKCgpTpxHFfk2FIFovcUlLkdvGiMtzdpfR06cKF7N7///xHHuHh0oQJyjQMGZ07OzpiXMXl7RRwJraF/mxTyGincGb8PYUroJ3C2TlzG81PTE6d/Ddq1EgxMTG68847dezYMU2aNEnNmzfXTz/9pKSkJHl5eal48eJ2rwkKClJSUpIkKSkpyS7xtx23HbtaneTkZKWkpMj3CkPCp06dqkmTJuUoj42NlZ+f33Xd7+0UFxeX/c348f8ULl/umGBuIh9vb7U5elS/9e6tnQMHmuXF9+5V9erVZbi768e//lJKAbjXwsBsp4ATSbngLsmizVs2q2JR2ilcA+0UroB2CmfnjG30woULea7r1Ml/+/btze/r1KmjRo0aKTQ0VIsWLbpiUn67jB07VqNHjzafJycnq1y5coqMjJTVifeJT09PV1xcnNq2bStPT09Hh3P9/u7p186dsuzdKxUrJiMkJHtIv8Wiyo8+qooVKijr2WelzEy5bd4sS+3aypwzR/cyx9/pFZh2igJp+p71UmqK7rmnof7cvZl2CqfG31O4AtopnJ0zt1HbCPS8cOrk/3LFixdXtWrVtHfvXrVt21ZpaWk6ffq0Xe//8ePHzTUCgoODtXnzZrtz2HYDuLTO5TsEHD9+XFar9aofMHh7e8vb2ztHuaenp9M1iNy4SpxX9emn0iOPSIGB0vnz2Vv6zZ6dveCfl5fcR46U+xdfZO8A8NdfUlyc3Jz4gxnkVCDaKQoct7+X+3d3d5dEO4Vzmrd2n1btPiHDMPTXSXd9eCxRFvaphJOincJZWX089E7/e8znzvhvfn7icank/9y5c9q3b5/69eun+vXry9PTU/Hx8erWrZskac+ePTp06JDCw8MlSeHh4XrhhRd04sQJlSpVSlL2UA2r1aqaNWuadZZfNgQ8Li7OPAec1Pbt2Un+yy9LDzwg7d8vffih1LWr9Pnn2dv5tW0rrVkjeXhI9etnLwQIAEABl5llaNqK3coyF6W0aN/ZU44MCcgD2imcT4CfcyX6N8qpk/8nnnhCnTp1UmhoqH7//XdNmDBB7u7u6t27t4oVK6ZBgwZp9OjRCgwMlNVq1aOPPqrw8HA1btxYkhQZGamaNWuqX79+mjZtmpKSkjRu3DhFR0ebvfbDhg3TnDlz9OSTT2rgwIFatWqVFi1apGXLljny1m+fhx/O7hUPDJT+8x9HR5NTZqb0d++apOzV+j08pD17pBo1pAEDJG/v7OS+UiUpK0t68kkpLCw72e/Vy2GhAyiYbL1SLPYPZ5WemWUm/q90q61dO37QXXfdZbddMuBMMjIytH37dtopnI6nu5ujQ7ipnPq368iRI+rdu7dOnjypkiVLqlmzZtq0aZNK/r1N28yZM+Xm5qZu3bopNTVVUVFReuONN8zXu7u7a+nSpRo+fLjCw8NVpEgR9e/fX5MnTzbrVKxYUcuWLdNjjz2m2bNnq2zZsnrnnXcKzzZ/y5ZJR49KZco4OpKcDCM78f/pJ2nFCumJJ7ITf5sdO6SkJCk0NLtuQIDUvbu0aJF08iQ9/QCAQik9M8v8vn3tYHn9nqj2/9/enUdHXd/7H3/NZJkkhCwkkoQdlcsOZW+EVgspa3GjVWnESO3lUgIXxFqWCmKtBeXUelUMLnU5dcHiTxS5bDEoiIYtssoiHkW4QFiEmBBIMsl8fn98mSFjggImM5OZ5+OcOUm+38+E9/ec92Hy/n4+78+3S2rALVUF3JxOp8xBQ54C9Sygi/9FixZ97/moqCgtWLBACxYsuOiY1q1b11jW/1033HCDtm7dekUxoh7ZbFJRkdS3r1RWZn3/179a5zp2lDp0kF5+WRo/XnI/seGaa6T4eKmkxE9BAwh27m5Uw9Q/ApSz6kJyRtjpnwYAWAK6+AfkcEg33ij93/9J//iHNaOfkyN172719L/1ltUKkJkpNW0qPfmkdaOgQwd/Rw4AgF+4Z/7D7TbZKf4BAOdR/COwRUdLnTpJn30mPf+8NHGiNd22cKE0d67VFpCbK82bZ/X5Hz0qLV8upaX5O3IAwep8LWXo+keAqqg8X/yHUfgDAC6g+Efg+O7mfm6zZkmrVkmHDlkz+7//vdUSkJNjtQGMHWttABgebt0oaNHC97EDCDks+0egqjy/21+wbVQFAPhxKP4RGNyb++3ebW3Yd/fdUnKyFBtrLesfPFj6+mtp2jTrJsG4cdYNgGeesfr8r7nG31cAIEQwl4pA5172H0nxDwCohuIfgcFmk06flm64QTp50prJLyuTpk+X+vWT7rrL6vMfOVLKyrLGT5wonTsnvfSSv6MHEELcj/oDApV72T8z/wCA6vhUQOCw26XsbCkyUoqIkHr2lG65RbrzTmnjRmnKFGnZMsnlkm67Tfr7361HAB47xvpbAADO82z4R88/AKAaZv4ROOLjrQLfGOnhh6XVq6VRo6wCf/p06fBhKSlJ+stfrK9jxlg3AeLj/R05gBDCo/4Q6Nw9/yz7BwBUR/Ef6kaPtpbbJyb6OxJLfLx0333Wkv/Bg63+/6lTrU39Xn1VatXKKvwlKSrKegEAAA8ny/4BALWg+A918+f759917+zvclnL/atr3Fh64AGrr/+226ye/rvukiZMqP1pAADgQzYe9YcAV3F+2X9EOMv+AQAXUPzD9155RcrPl/7nfySHo/YbALGx0p//bP2VPXastQfA6NH+iRcAgAbEWWXdmAr/7mcrACCk8akA36qslHbulLZskWbPlsrLrcLf5ao5NjZWmjnTemVmSm+95ft4AeA7bOe7/un5R6Cq5FF/AIBa8KkA3woPlx56yHpkX36+NGPGD98A+NOfrPd07uz7eAEAaGBY9g8AqA3L/kNdhw7SkSNSs2bS3r31/+9VVkqNGkm33y4dP249ui8mxloFEBn5w3sAAICfXej5BwKTe9k/G/4BAKqj+A91Z85IJSXWV18ID5fefFN6+mkpLs76d599VqqosB7vd7E9ACj8AQC4JM7zM//0/AMAquNTAb61c6c0fryUlSX961/Sl19aqwA++MCa/a+ouHgLAAAEEHr+Eag8Pf8s+wcAVEPxD986eNDq4x82TGrSRIqKkh55ROrVS3rhBet79x4AAADgslWw7B8AUAs+FeAb7imyhASrt//gQevnqiopPl6aO9da8v/CC9Jf/uK3MAHgh9jOtyEZuv4RoFj2DwCoDZ8KqD/V18S6e/Y7dZLCwqT586XTp63vJav3v0cPqx1g/HjfxwoAl4vaHwHKWcmyfwBATWz4h/phjFXwf/ihlJdn9fYPHy5lZkrvviulp0v33CNNmCC1aSO9+KJUVibdd5+UlOTv6AHgoiinEOicLpb9AwBqovhH/bDZpLfftgr8YcOk1FRrVj83V3ruOemjj6TRo6Vx4ySn09rgb+lSCn8AAY9H/SHQuZf9U/wDAKqj+Ef9+OoracYM6dFHrQJfsh7pl5ZmPe6va1fpk0+kAwekoiLp2mulZs38GTEAAEHBvew/PIx1KgCACyj+UT8qKqTERKvw379f+sUvrCX/c+da57dvl7p3l7p182+cAHCZPDP/POsPAco98x/JzD8AoBqK/1C3cKF07pwUHf3jfo+7x7+y0prZ/+Yb6fBh6eOPreX+w4dLOTnW2E2bpHnzrNd//MePvwYAAOBBzz8AoDYU/6HuV7+qm99js0kbNkh/+IOUny9dd521qd/110ujRll9/m7vvCMdO2Y94g8AGhib3I/6AwKTe9k/xT8AoDqKf9Qd98x/bq40cqR0xx3SkSPSiRPWbH9JibRihfT889aGfykp/o4YAICgc2HDP3r+AQAXUPyj7nTpYhX0r7xiFf+33mo9vm/RImnAAKl9e2u2f906ev0BNFgXev79GwdwMc4qlv0DAGqi+A91BQXW5nyRkVKvXpf+PnePf1WVFBZmHWvUSJo/Xxo4UPr3v6XbbpN++1vrtXu3dWMgLExKSKiXSwEAADzqDwBQOz4VQt1NN1n9+TfddHnvs9mk1autwv7NNy8cb99eGjbMmt2vrJRc1h8g6tRJSkqi8AfQ4LkXUjPxj0DFsn8AQG0o/nHlEhKsnv7586U+faRVq6yVAL/7ndXXv3evZLezNhYAAB9i2T8AoDZ8KuDK9e0r/e//Si+8ILVpI/3xj9LgwVaff3q69Le/WY8RtDHzACCInP8/zXBjEwGqgmX/AIBa0POPS+Pu8S8okLZutb6/7jqpY0fpJz+RFi+W1qyxZv9/+1vpzBmpe3dr6T8AAPCZSpb9AwBqQfGPH+Yu/N9+W5o0SUpLszb3mz5devdd6yaAZG30N3CgdOed1vHf/EZq3Ni/sQNAHfOUU0z8I0B5lv2HM/MPALiA4h8/zGaTPvpI+q//spby/+d/Slu2WMv+MzKsmwJDh17Y3K9rV6lzZ6vfHwCClMtY90aNMbQAIKB4NvzjcxgAUA3FP7y5XFbR7v4qWX37eXnShAlW4X/4sDRqlHT33dYGfzffbO38//OfX7gBwB8cAIKUexuTCW9skxSuKRty/RkOcFEs+wcAVEeFhgvcBf+BA9Ymflu2WMejo6Ubb7Rm90tKrMJ/6FDpxRelceOkigrphhuk99+n6AcQ9Pq2beLvEIAf1KRRpNql0HoHALiAmX9YjLEK9507pV//2lq236LFhfM9e1pfN22yZvvvvdf6OSHB6u1v3Vpq3tznYQOAr80Y1lETrr9WZRUVev/995WRkaGIiAh/hwV4iXWEKzLcLqfT6e9QAAABguIfFptN2rtXuv56q7d/0iSpWbOa444ds3b8d+/iv2iRtbP/nDlSTIxPQwYAf4mPiVBMhBQbYc2wUvwDAIBAR/Ef6vbssWb9y8ul7GzrMX1z514473RaBX9pqdS+vTRypDR8uNStm9Snj7R7t7R+PYU/AAAAAAQwiv9Q534UX2WlVFhobdrntmqVtHKl1duflCRdfbXV1794sfSvf0lnz0ojRkjt2vkndgAAAADAJaH4DzFn8tYoafVqffPlVwoLu7A5n62sTAmffy7n66/r3NGjityxQ1GbNqkqLU3OjAwZh0Mxq1erfNAgld50k/Umu11ascJ6AXWoqsqlpC/218hTIBDYYxsr8fbbpMhIf4cCAABwySj+Q0zphx8qKW+NTuetqXkuIlKt8vMVtnmzwlwuHb/qKpUWl8j52W7JGLU8V6bKLVt08vARP0SOUJMk1ZqnQEAwLsXddZe/owAAALhkFP/fsWDBAs2fP1+FhYXq3r27nnrqKfXt29ffYdWZmL59dejYMbVu3Vp2u10xn3wse3m5XA6Hzl7XXye+/Vb20lJVxcfL1qiRYt1vdLkU9tZiKSlZiQMHWsdsPD8Y9cBmk8vl0oGvD6hN6zay8/hIBJDyL77Q2Y0bdXbrVop/AADQoFD8V/Pmm29q6tSpWrhwofr166cnnnhCQ4YM0b59+9S0aVN/h1cnGo/8lY6H2dV7+HBrd+oWLaTDh6XmzRW3cmXtb6qokB5+WDp9WlqyRLH0+KOeOZ1ObV6+XH3ceQoEiLObN+vrjRtVtnOXv0MBAAC4LDZjjPF3EIGiX79+6tOnj55++mlJksvlUsuWLTVp0iRNnz79e99bXFys+Ph4ffvtt4qLi/NFuJfNGKMD+R9p14YCtT0/o9ru9/co4tQ3cjZJ0v4X/lnjPfFrP1TUF18o/uP1OvjAbJVdfbUfIkeocblc+urrA548BQKFq6JS3zz9/yRJMT/rplPFRUpqkiQbeYoAYlxGlUdPqur0GRljVF5RLkekQzZW7CFAkacIVGGOMPV79QlVVlZq+fLlGh6AE1OXU4cy839eRUWFCgoKNGPGDM8xu92ujIwM5efn1xhfXl6u8vJyz8/FxcWSrBlLp9NZ/wFfgXOV5/T4ivXqcOKnOvi5daxVuU0RksrLbfpgVWOv8QlFh3T9Rx+oKLKxVg1+XKf3t5b2+z5uhKrunjwFAkrn31tfT1lfvjjuv1CAiwqTlOzvIACgYQt3nlGnshKFG6tsDsQ673Jiovg/7+TJk6qqqlJKSorX8ZSUFO3du7fG+Llz5+qhhx6qcXz16tWKCdBn3leYChVFn9CRxl94jlXZKj1fqx+XpCONpa9vulOV4REqdzgleZ8HgFDUqNwo4YzE3BQCWaVdKo2SDIkKAFesMrJMq1aVK9JmPeEnNzfXzxHVdPbs2UseS/F/hWbMmKGpU6d6fi4uLlbLli01ePDggF72P3DgQK1Zs0YDBw5URESEYl9/SDpTpMZxMZr2lzv8HSIgybqDWT1PgUBEnqIhIE/REJCnCGRRYVGqrKxUbm6ufvnLXwZcjrpXoF8Kiv/zkpOTFRYWpmPHjnkdP3bsmFJTU2uMdzgccjgcNY5HREQEXEJUF2eLU6QtUnHRcVac5/uq7Dab4qID86YFQo8z3Omdp0AAIk/REJCnaAjIUwQ6914UgVjrXU487FB0XmRkpHr16qW8vDzPMZfLpby8PKWnp/sxMgAAAAAAfhxm/quZOnWqsrKy1Lt3b/Xt21dPPPGESktLNXbsWH+HBgAAAADAFaP4r+b222/XiRMnNHv2bBUWFuonP/mJVq5cWWMTQAAAAAAAGhKK/++YOHGiJk6c6O8wfKdnT6llS+mqq/wdCQAAAACgnlD8h7qlS/0dAQAAAACgnrHhHwAAAAAAQY7iHwAAAACAIEfxDwAAAABAkKPnP9TdeKN04oS14R/9/wAAAAAQlCj+Q92nn0qHD0vNm/s7EgAAAABAPWHZPwAAAAAAQY7iHwAAAACAIEfxDwAAAABAkKP4BwAAAAAgyFH8AwAAAAAQ5Cj+AQAAAAAIchT/AAAAAAAEuXB/BxAsjDGSpOLiYj9H8v2cTqfOnj2r4uJiRURESC6XdcLlkgI8doSOGnkKBCDyFA0BeYqGgDxFoAvkHHXXn+569PtQ/NeRkpISSVLLli39HMkVOnpUio/3dxQAAAAAgMtUUlKi+B+o52zmUm4R4Ae5XC4dOXJEjRs3ls1m83c4F1VcXKyWLVvq0KFDiouL83c4QK3IUzQE5CkaAvIUDQF5ikAXyDlqjFFJSYmaNWsmu/37u/qZ+a8jdrtdLVq08HcYlywuLi7gEhf4LvIUDQF5ioaAPEVDQJ4i0AVqjv7QjL8bG/4BAAAAABDkKP4BAAAAAAhyFP8hxuFw6MEHH5TD4fB3KMBFkadoCMhTNATkKRoC8hSBLlhylA3/AAAAAAAIcsz8AwAAAAAQ5Cj+AQAAAAAIchT/AAAAAAAEOYp/AAAAAACCHMV/iFmwYIHatGmjqKgo9evXT5s2bfJ3SAgRc+fOVZ8+fdS4cWM1bdpUN998s/bt2+c1pqysTNnZ2UpKSlJsbKxGjRqlY8eOeY05ePCgRowYoZiYGDVt2lT333+/KisrfXkpCCHz5s2TzWbTlClTPMfIUwSCw4cP684771RSUpKio6PVtWtXbdmyxXPeGKPZs2crLS1N0dHRysjI0P79+71+x6lTp5SZmam4uDglJCTonnvu0ZkzZ3x9KQhCVVVVmjVrltq2bavo6Ghdc801evjhh1V9n3FyFL62bt06jRw5Us2aNZPNZtM777zjdb6ucnLHjh362c9+pqioKLVs2VKPPfZYfV/aJaP4DyFvvvmmpk6dqgcffFCffvqpunfvriFDhuj48eP+Dg0hYO3atcrOztaGDRuUm5srp9OpwYMHq7S01DPm3nvv1XvvvafFixdr7dq1OnLkiG699VbP+aqqKo0YMUIVFRX65JNP9Morr+jll1/W7Nmz/XFJCHKbN2/Ws88+q27dunkdJ0/hb6dPn1b//v0VERGhFStWaPfu3fr73/+uxMREz5jHHntMTz75pBYuXKiNGzeqUaNGGjJkiMrKyjxjMjMz9dlnnyk3N1fLli3TunXrNG7cOH9cEoLMo48+qpycHD399NPas2ePHn30UT322GN66qmnPGPIUfhaaWmpunfvrgULFtR6vi5ysri4WIMHD1br1q1VUFCg+fPna86cOXruuefq/fouiUHI6Nu3r8nOzvb8XFVVZZo1a2bmzp3rx6gQqo4fP24kmbVr1xpjjCkqKjIRERFm8eLFnjF79uwxkkx+fr4xxpjly5cbu91uCgsLPWNycnJMXFycKS8v9+0FIKiVlJSYdu3amdzcXHP99debyZMnG2PIUwSGadOmmQEDBlz0vMvlMqmpqWb+/PmeY0VFRcbhcJg33njDGGPM7t27jSSzefNmz5gVK1YYm81mDh8+XH/BIySMGDHC/O53v/M6duutt5rMzExjDDkK/5NklixZ4vm5rnLymWeeMYmJiV6f99OmTTPt27ev5yu6NMz8h4iKigoVFBQoIyPDc8xutysjI0P5+fl+jAyh6ttvv5UkNWnSRJJUUFAgp9PplaMdOnRQq1atPDman5+vrl27KiUlxTNmyJAhKi4u1meffebD6BHssrOzNWLECK98lMhTBIalS5eqd+/e+s1vfqOmTZuqR48eev755z3nv/rqKxUWFnrlaXx8vPr16+eVpwkJCerdu7dnTEZGhux2uzZu3Oi7i0FQuu6665SXl6fPP/9ckrR9+3atX79ew4YNk0SOIvDUVU7m5+fr5z//uSIjIz1jhgwZon379un06dM+upqLC/d3APCNkydPqqqqyuuPUUlKSUnR3r17/RQVQpXL5dKUKVPUv39/denSRZJUWFioyMhIJSQkeI1NSUlRYWGhZ0xtOew+B9SFRYsW6dNPP9XmzZtrnCNPEQi+/PJL5eTkaOrUqZo5c6Y2b96s//7v/1ZkZKSysrI8eVZbHlbP06ZNm3qdDw8PV5MmTchT/GjTp09XcXGxOnTooLCwMFVVVemRRx5RZmamJJGjCDh1lZOFhYVq27Ztjd/hPle9PcsfKP4B+Fx2drZ27dql9evX+zsUwMuhQ4c0efJk5ebmKioqyt/hALVyuVzq3bu3/va3v0mSevTooV27dmnhwoXKysryc3SA9O9//1uvvfaaXn/9dXXu3Fnbtm3TlClT1KxZM3IU8COW/YeI5ORkhYWF1diR+tixY0pNTfVTVAhFEydO1LJly/TBBx+oRYsWnuOpqamqqKhQUVGR1/jqOZqamlprDrvPAT9WQUGBjh8/rp49eyo8PFzh4eFau3atnnzySYWHhyslJYU8hd+lpaWpU6dOXsc6duyogwcPSrqQZ9/3mZ+amlpjw9/KykqdOnWKPMWPdv/992v69Om644471LVrV40ZM0b33nuv5s6dK4kcReCpq5wM9L8BKP5DRGRkpHr16qW8vDzPMZfLpby8PKWnp/sxMoQKY4wmTpyoJUuWaM2aNTWWRPXq1UsRERFeObpv3z4dPHjQk6Pp6enauXOn13+8ubm5iouLq/GHMHAlBg0apJ07d2rbtm2eV+/evZWZmen5njyFv/Xv37/Go1I///xztW7dWpLUtm1bpaameuVpcXGxNm7c6JWnRUVFKigo8IxZs2aNXC6X+vXr54OrQDA7e/as7HbvMiMsLEwul0sSOYrAU1c5mZ6ernXr1snpdHrG5Obmqn379n5f8i+J3f5DyaJFi4zD4TAvv/yy2b17txk3bpxJSEjw2pEaqC9/+MMfTHx8vPnwww/N0aNHPa+zZ896xowfP960atXKrFmzxmzZssWkp6eb9PR0z/nKykrTpUsXM3jwYLNt2zazcuVKc9VVV5kZM2b445IQIqrv9m8MeQr/27RpkwkPDzePPPKI2b9/v3nttddMTEyMefXVVz1j5s2bZxISEsy7775rduzYYW666SbTtm1bc+7cOc+YoUOHmh49epiNGzea9evXm3bt2pnRo0f745IQZLKyskzz5s3NsmXLzFdffWXefvttk5ycbP70pz95xpCj8LWSkhKzdetWs3XrViPJPP7442br1q3m66+/NsbUTU4WFRWZlJQUM2bMGLNr1y6zaNEiExMTY5599lmfX29tKP5DzFNPPWVatWplIiMjTd++fc2GDRv8HRJChKRaXy+99JJnzLlz58yECRNMYmKiiYmJMbfccos5evSo1+85cOCAGTZsmImOjjbJycnmvvvuM06n08dXg1Dy3eKfPEUgeO+990yXLl2Mw+EwHTp0MM8995zXeZfLZWbNmmVSUlKMw+EwgwYNMvv27fMa880335jRo0eb2NhYExcXZ8aOHWtKSkp8eRkIUsXFxWby5MmmVatWJioqylx99dXmz3/+s9fjz8hR+NoHH3xQ69+iWVlZxpi6y8nt27ebAQMGGIfDYZo3b27mzZvnq0v8QTZjjPHPmgMAAAAAAOAL9PwDAAAAABDkKP4BAAAAAAhyFP8AAAAAAAQ5in8AAAAAAIIcxT8AAAAAAEGO4h8AAAAAgCBH8Q8AAAAAQJCj+AcAAAAAIMhR/AMAgAbJZrPpnXfe8XcYAAA0CBT/AADgst19992y2Ww1XkOHDvV3aAAAoBbh/g4AAAA0TEOHDtVLL73kdczhcPgpGgAA8H2Y+QcAAFfE4XAoNTXV65WYmCjJWpKfk5OjYcOGKTo6WldffbXeeustr/fv3LlTAwcOVHR0tJKSkjRu3DidOXPGa8yLL76ozp07y+FwKC0tTRMnTvQ6f/LkSd1yyy2KiYlRu3bttHTp0vq9aAAAGiiKfwAAUC9mzZqlUaNGafv27crMzNQdd9yhPXv2SJJKS0s1ZMgQJSYmavPmzVq8eLHef/99r+I+JydH2dnZGjdunHbu3KmlS5fq2muv9fo3HnroId12223asWOHhg8frszMTJ06dcqn1wkAQENgM8YYfwcBAAAalrvvvluvvvqqoqKivI7PnDlTM2fOlM1m0/jx45WTk+M599Of/lQ9e/bUM888o+eff17Tpk3ToUOH1KhRI0nS8uXLNXLkSB05ckQpKSlq3ry5xo4dq7/+9a+1xmCz2fTAAw/o4YcflmTdUIiNjdWKFSvYewAAgO+g5x8AAFyRX/ziF17FvSQ1adLE8316errXufT0dG3btk2StGfPHnXv3t1T+EtS//795XK5tG/fPtlsNh05ckSDBg363hi6devm+b5Ro0aKi4vT8ePHr/SSAAAIWhT/AADgijRq1KjGMvy6Eh0dfUnjIiIivH622WxyuVz1ERIAAA0aPf8AAKBebNiwocbPHTt2lCR17NhR27dvV2lpqef8xx9/LLvdrvbt26tx48Zq06aN8vLyfBozAADBipl/AABwRcrLy1VYWOh1LDw8XMnJyZKkxYsXq3fv3howYIBee+01bdq0Sf/85z8lSZmZmXrwwQeVlZWlOXPm6MSJE5o0aZLGjBmjlJQUSdKcOXM0fvx4NW3aVMOGDVNJSYk+/vhjTZo0ybcXCgBAEKD4BwAAV2TlypVKS0vzOta+fXvt3btXkrUT/6JFizRhwgSlpaXpjTfeUKdOnSRJMTExWrVqlSZPnqw+ffooJiZGo0aN0uOPP+75XVlZWSorK9M//vEP/fGPf1RycrJ+/etf++4CAQAIIuz2DwAA6pzNZtOSJUt08803+zsUAAAgev4BAAAAAAh6FP8AAAAAAAQ5ev4BAECdo6sQAIDAwsw/AAAAAABBjuIfAAAAAIAgR/EPAAAAAECQo/gHAAAAACDIUfwDAAAAABDkKP4BAAAAAAhyFP8AAAAAAAQ5in8AAAAAAILc/wd0jdvS1ga0nAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Test RMSE :  1.2676979303359985\nCutoff SoH :  0.7\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-5-hust_gompertz_params.csv\n X_['train'] shape : torch.Size([26, 100, 1]) , y_['train'] shape : torch.Size([26, 3]) Ôºåy_2['train'] shape: torch.Size([26, 1])\nload : \n['train']loader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-1-hust_gompertz_params.csv\n X_['val'] shape : torch.Size([5, 100, 1]) , y_['val'] shape : torch.Size([5, 3]) Ôºåy_2['val'] shape: torch.Size([5, 1])\nload : \n['val']loader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n X_['test'] shape : torch.Size([4, 100, 1]) , y_['test'] shape : torch.Size([4, 3]) Ôºåy_2['test'] shape: torch.Size([4, 1])\nload : \n['test']loader lengths :  1\n## üß† Model\nLast model window :  last_model_window_100_model_pinn_data_mid.pth\nüöÄ Initializing model output to: k=0.9727750420570374, a=-3.759854793548584, b=-13.948850631713867\n‚úÖ Model Output Parameters Initialized!\n##\n        ### üìà Gompertz Function (Physics Law)\n        \n        * `x`: Time (or cycle number)\n        \n        * `k`: Max value (e.g., max capacity)\n        \n        * `a`, `b`: Shape parameters\n## üß† Loss Functions\n\n## ‚öôÔ∏è 1. Data-Informed Loss Function\n        a data loss (what the LSTM learns from data)\n        \n        * Mean Squared Error for Training\n        * RMSE for autoregressive approximation of compound error\n        \n        ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n        You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n        \n        * `alpha`: controls how strongly physics is enforced.\n## üõ†Ô∏è Parameter Strategy\n## üîÅ Training Loop\n‚úÖ Saved best model at epoch 1 (Val Loss = 1.56122279)\nEpoch 1/1000 | Train Loss=13330.16796875 | Val Loss=1.56122279 | Data=133.27960205 | Physics=2.20813976 | Val RMSE: 1.20657361 | ‚àö(Val Loss) = 1.24948907 | Current Learning Rate: 0.002\n\n Epoch :  0 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.96152735  -3.748282   -13.946057  ]\n [  0.9615486   -3.7483125  -13.946121  ]\n [  0.96153563  -3.7482939  -13.946082  ]\n [  0.9615324   -3.7482893  -13.946073  ]] \n\nFinal Test RMSE:  0.41918885707855225\n‚úÖ Saved best model at epoch 2 (Val Loss = 1.44333851)\nEpoch 2/1000 | Train Loss=13329.97070312 | Val Loss=1.44333851 | Data=133.27717590 | Physics=2.25292879 | Val RMSE: 1.20632207 | ‚àö(Val Loss) = 1.20139027 | Current Learning Rate: 0.002\n‚úÖ Saved best model at epoch 3 (Val Loss = 1.36625493)\nEpoch 3/1000 | Train Loss=13329.65820312 | Val Loss=1.36625493 | Data=133.27452087 | Physics=2.20613680 | Val RMSE: 1.20580709 | ‚àö(Val Loss) = 1.16886914 | Current Learning Rate: 0.002\n‚úÖ Saved best model at epoch 4 (Val Loss = 1.35073662)\nEpoch 4/1000 | Train Loss=13329.29492188 | Val Loss=1.35073662 | Data=133.27093506 | Physics=2.20154187 | Val RMSE: 1.20559657 | ‚àö(Val Loss) = 1.16221189 | Current Learning Rate: 0.002\nEpoch 5/1000 | Train Loss=13328.80566406 | Val Loss=1.35227001 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20594871 | ‚àö(Val Loss) = 1.16287148 | Current Learning Rate: 0.002\nEpoch 6/1000 | Train Loss=13328.80566406 | Val Loss=1.36472750 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20638144 | ‚àö(Val Loss) = 1.16821551 | Current Learning Rate: 0.002\nEpoch 7/1000 | Train Loss=13328.80566406 | Val Loss=1.37842262 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20669472 | ‚àö(Val Loss) = 1.17406249 | Current Learning Rate: 0.002\nEpoch 8/1000 | Train Loss=13328.80566406 | Val Loss=1.39125848 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20693231 | ‚àö(Val Loss) = 1.17951620 | Current Learning Rate: 0.002\nEpoch 9/1000 | Train Loss=13328.80566406 | Val Loss=1.40270197 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20711911 | ‚àö(Val Loss) = 1.18435717 | Current Learning Rate: 0.002\nEpoch 10/1000 | Train Loss=13328.80566406 | Val Loss=1.41271698 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20726967 | ‚àö(Val Loss) = 1.18857777 | Current Learning Rate: 0.002\nEpoch 11/1000 | Train Loss=13328.80566406 | Val Loss=1.42142856 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20739317 | ‚àö(Val Loss) = 1.19223678 | Current Learning Rate: 0.002\nEpoch 12/1000 | Train Loss=13328.80566406 | Val Loss=1.42899907 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20749593 | ‚àö(Val Loss) = 1.19540751 | Current Learning Rate: 0.002\nEpoch 13/1000 | Train Loss=13328.80566406 | Val Loss=1.43558443 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20758259 | ‚àö(Val Loss) = 1.19815874 | Current Learning Rate: 0.002\nEpoch 14/1000 | Train Loss=13328.80566406 | Val Loss=1.44132614 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20765626 | ‚àö(Val Loss) = 1.20055246 | Current Learning Rate: 0.002\nEpoch 15/1000 | Train Loss=13328.80566406 | Val Loss=1.44634640 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20771909 | ‚àö(Val Loss) = 1.20264137 | Current Learning Rate: 0.002\nEpoch 16/1000 | Train Loss=13328.80566406 | Val Loss=1.45074499 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20777357 | ‚àö(Val Loss) = 1.20446873 | Current Learning Rate: 0.002\nEpoch 17/1000 | Train Loss=13328.80566406 | Val Loss=1.45461345 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20782053 | ‚àö(Val Loss) = 1.20607352 | Current Learning Rate: 0.002\nEpoch 18/1000 | Train Loss=13328.80566406 | Val Loss=1.45802069 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20786130 | ‚àö(Val Loss) = 1.20748532 | Current Learning Rate: 0.002\nEpoch 19/1000 | Train Loss=13328.80566406 | Val Loss=1.46103156 | Data=133.26588440 | Physics=2.21643429 | Val RMSE: 1.20789719 | ‚àö(Val Loss) = 1.20873141 | Current Learning Rate: 0.002\nEpoch 20/1000 | Train Loss=13328.80566406 | Val Loss=1.46369481 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20792842 | ‚àö(Val Loss) = 1.20983255 | Current Learning Rate: 0.002\nEpoch 21/1000 | Train Loss=13328.80566406 | Val Loss=1.46605742 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20795608 | ‚àö(Val Loss) = 1.21080863 | Current Learning Rate: 0.002\n\n Epoch :  20 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 22/1000 | Train Loss=13328.80566406 | Val Loss=1.46815765 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20798004 | ‚àö(Val Loss) = 1.21167552 | Current Learning Rate: 0.002\nEpoch 23/1000 | Train Loss=13328.80566406 | Val Loss=1.47002649 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20800126 | ‚àö(Val Loss) = 1.21244645 | Current Learning Rate: 0.002\nEpoch 24/1000 | Train Loss=13328.80566406 | Val Loss=1.47169089 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20801997 | ‚àö(Val Loss) = 1.21313262 | Current Learning Rate: 0.002\nEpoch 25/1000 | Train Loss=13328.80566406 | Val Loss=1.47317851 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20803666 | ‚àö(Val Loss) = 1.21374559 | Current Learning Rate: 0.002\nEpoch 26/1000 | Train Loss=13328.80566406 | Val Loss=1.47450769 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20805120 | ‚àö(Val Loss) = 1.21429312 | Current Learning Rate: 0.002\nEpoch 27/1000 | Train Loss=13328.80566406 | Val Loss=1.47569549 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20806396 | ‚àö(Val Loss) = 1.21478212 | Current Learning Rate: 0.002\nEpoch 28/1000 | Train Loss=13328.80566406 | Val Loss=1.47676134 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20807528 | ‚àö(Val Loss) = 1.21522069 | Current Learning Rate: 0.002\nEpoch 29/1000 | Train Loss=13328.80566406 | Val Loss=1.47771621 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20808518 | ‚àö(Val Loss) = 1.21561348 | Current Learning Rate: 0.002\nEpoch 30/1000 | Train Loss=13328.80566406 | Val Loss=1.47857153 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20809424 | ‚àö(Val Loss) = 1.21596527 | Current Learning Rate: 0.002\nEpoch 31/1000 | Train Loss=13328.80566406 | Val Loss=1.47934055 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20810211 | ‚àö(Val Loss) = 1.21628141 | Current Learning Rate: 0.002\nEpoch 32/1000 | Train Loss=13328.80566406 | Val Loss=1.48003268 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20810866 | ‚àö(Val Loss) = 1.21656597 | Current Learning Rate: 0.002\nEpoch 33/1000 | Train Loss=13328.80566406 | Val Loss=1.48065436 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20811450 | ‚àö(Val Loss) = 1.21682143 | Current Learning Rate: 0.002\nEpoch 34/1000 | Train Loss=13328.80566406 | Val Loss=1.48121512 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20811999 | ‚àö(Val Loss) = 1.21705186 | Current Learning Rate: 0.002\nEpoch 35/1000 | Train Loss=13328.80566406 | Val Loss=1.48172009 | Data=133.26588440 | Physics=2.21643429 | Val RMSE: 1.20812452 | ‚àö(Val Loss) = 1.21725929 | Current Learning Rate: 0.002\nEpoch 36/1000 | Train Loss=13328.80566406 | Val Loss=1.48217404 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20812857 | ‚àö(Val Loss) = 1.21744573 | Current Learning Rate: 0.002\nEpoch 37/1000 | Train Loss=13328.80566406 | Val Loss=1.48258603 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20813191 | ‚àö(Val Loss) = 1.21761489 | Current Learning Rate: 0.002\nEpoch 38/1000 | Train Loss=13328.80566406 | Val Loss=1.48295546 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20813489 | ‚àö(Val Loss) = 1.21776664 | Current Learning Rate: 0.002\nEpoch 39/1000 | Train Loss=13328.80566406 | Val Loss=1.48329043 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20813751 | ‚àö(Val Loss) = 1.21790409 | Current Learning Rate: 0.002\nEpoch 40/1000 | Train Loss=13328.80566406 | Val Loss=1.48359227 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20813990 | ‚àö(Val Loss) = 1.21802807 | Current Learning Rate: 0.002\nEpoch 41/1000 | Train Loss=13328.80566406 | Val Loss=1.48386681 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20814192 | ‚àö(Val Loss) = 1.21814072 | Current Learning Rate: 0.002\n\n Epoch :  40 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 42/1000 | Train Loss=13328.80566406 | Val Loss=1.48411512 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20814347 | ‚àö(Val Loss) = 1.21824265 | Current Learning Rate: 0.002\nEpoch 43/1000 | Train Loss=13328.80566406 | Val Loss=1.48433828 | Data=133.26588440 | Physics=2.21643429 | Val RMSE: 1.20814490 | ‚àö(Val Loss) = 1.21833420 | Current Learning Rate: 0.002\nEpoch 44/1000 | Train Loss=13328.80566406 | Val Loss=1.48454106 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20814610 | ‚àö(Val Loss) = 1.21841741 | Current Learning Rate: 0.002\nEpoch 45/1000 | Train Loss=13328.80566406 | Val Loss=1.48472536 | Data=133.26588440 | Physics=2.21643429 | Val RMSE: 1.20814729 | ‚àö(Val Loss) = 1.21849310 | Current Learning Rate: 0.002\nEpoch 46/1000 | Train Loss=13328.80566406 | Val Loss=1.48489404 | Data=133.26588440 | Physics=2.21643381 | Val RMSE: 1.20814776 | ‚àö(Val Loss) = 1.21856225 | Current Learning Rate: 0.002\nEpoch 47/1000 | Train Loss=13328.80566406 | Val Loss=1.48504615 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20814848 | ‚àö(Val Loss) = 1.21862471 | Current Learning Rate: 0.002\nEpoch 48/1000 | Train Loss=13328.80566406 | Val Loss=1.48518372 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20814896 | ‚àö(Val Loss) = 1.21868110 | Current Learning Rate: 0.002\nEpoch 49/1000 | Train Loss=13328.80566406 | Val Loss=1.48530912 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20814931 | ‚àö(Val Loss) = 1.21873260 | Current Learning Rate: 0.002\nEpoch 50/1000 | Train Loss=13328.80566406 | Val Loss=1.48542368 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20814955 | ‚àö(Val Loss) = 1.21877956 | Current Learning Rate: 0.002\nEpoch 51/1000 | Train Loss=13328.80566406 | Val Loss=1.48552847 | Data=133.26588440 | Physics=2.21643405 | Val RMSE: 1.20814979 | ‚àö(Val Loss) = 1.21882260 | Current Learning Rate: 0.002\nEpoch 52/1000 | Train Loss=13358.41503906 | Val Loss=1.48572505 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20815086 | ‚àö(Val Loss) = 1.21890318 | Current Learning Rate: 0.002\nEpoch 53/1000 | Train Loss=13358.41503906 | Val Loss=1.48589861 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815182 | ‚àö(Val Loss) = 1.21897435 | Current Learning Rate: 0.002\nEpoch 54/1000 | Train Loss=13358.41503906 | Val Loss=1.48605359 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815253 | ‚àö(Val Loss) = 1.21903801 | Current Learning Rate: 0.002\nEpoch 55/1000 | Train Loss=13358.41503906 | Val Loss=1.48619080 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20815337 | ‚àö(Val Loss) = 1.21909428 | Current Learning Rate: 0.002\nEpoch 56/1000 | Train Loss=13358.41503906 | Val Loss=1.48631358 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815361 | ‚àö(Val Loss) = 1.21914458 | Current Learning Rate: 0.002\nEpoch 57/1000 | Train Loss=13358.41503906 | Val Loss=1.48642278 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815396 | ‚àö(Val Loss) = 1.21918941 | Current Learning Rate: 0.002\nEpoch 58/1000 | Train Loss=13358.41503906 | Val Loss=1.48651910 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20815396 | ‚àö(Val Loss) = 1.21922886 | Current Learning Rate: 0.002\nEpoch 59/1000 | Train Loss=13358.41503906 | Val Loss=1.48660445 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815432 | ‚àö(Val Loss) = 1.21926391 | Current Learning Rate: 0.002\nEpoch 60/1000 | Train Loss=13358.41503906 | Val Loss=1.48668182 | Data=133.56198120 | Physics=2.21643429 | Val RMSE: 1.20815432 | ‚àö(Val Loss) = 1.21929562 | Current Learning Rate: 0.002\nEpoch 61/1000 | Train Loss=13358.41503906 | Val Loss=1.48674941 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815420 | ‚àö(Val Loss) = 1.21932340 | Current Learning Rate: 0.002\n\n Epoch :  60 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 62/1000 | Train Loss=13358.41503906 | Val Loss=1.48680937 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20815396 | ‚àö(Val Loss) = 1.21934795 | Current Learning Rate: 0.002\nEpoch 63/1000 | Train Loss=13358.41503906 | Val Loss=1.48686373 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20815361 | ‚àö(Val Loss) = 1.21937025 | Current Learning Rate: 0.002\nEpoch 64/1000 | Train Loss=13358.41503906 | Val Loss=1.48691308 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20815361 | ‚àö(Val Loss) = 1.21939051 | Current Learning Rate: 0.002\nEpoch 65/1000 | Train Loss=13358.41503906 | Val Loss=1.48695588 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815325 | ‚àö(Val Loss) = 1.21940804 | Current Learning Rate: 0.002\nEpoch 66/1000 | Train Loss=13358.41503906 | Val Loss=1.48699236 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20815301 | ‚àö(Val Loss) = 1.21942294 | Current Learning Rate: 0.002\nEpoch 67/1000 | Train Loss=13358.41503906 | Val Loss=1.48702765 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20815253 | ‚àö(Val Loss) = 1.21943748 | Current Learning Rate: 0.002\nEpoch 68/1000 | Train Loss=13358.41503906 | Val Loss=1.48705721 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815241 | ‚àö(Val Loss) = 1.21944952 | Current Learning Rate: 0.002\nEpoch 69/1000 | Train Loss=13358.41503906 | Val Loss=1.48708498 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815194 | ‚àö(Val Loss) = 1.21946096 | Current Learning Rate: 0.002\nEpoch 70/1000 | Train Loss=13358.41503906 | Val Loss=1.48710907 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815158 | ‚àö(Val Loss) = 1.21947086 | Current Learning Rate: 0.002\nEpoch 71/1000 | Train Loss=13358.41503906 | Val Loss=1.48713052 | Data=133.56198120 | Physics=2.21643429 | Val RMSE: 1.20815134 | ‚àö(Val Loss) = 1.21947956 | Current Learning Rate: 0.002\nEpoch 72/1000 | Train Loss=13358.41503906 | Val Loss=1.48715055 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815074 | ‚àö(Val Loss) = 1.21948779 | Current Learning Rate: 0.002\nEpoch 73/1000 | Train Loss=13358.41503906 | Val Loss=1.48716760 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20815051 | ‚àö(Val Loss) = 1.21949482 | Current Learning Rate: 0.002\nEpoch 74/1000 | Train Loss=13358.41503906 | Val Loss=1.48718071 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814991 | ‚àö(Val Loss) = 1.21950018 | Current Learning Rate: 0.002\nEpoch 75/1000 | Train Loss=13358.41503906 | Val Loss=1.48719513 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814991 | ‚àö(Val Loss) = 1.21950614 | Current Learning Rate: 0.002\nEpoch 76/1000 | Train Loss=13358.41503906 | Val Loss=1.48720837 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814931 | ‚àö(Val Loss) = 1.21951151 | Current Learning Rate: 0.002\nEpoch 77/1000 | Train Loss=13358.41503906 | Val Loss=1.48721766 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20814884 | ‚àö(Val Loss) = 1.21951532 | Current Learning Rate: 0.002\nEpoch 78/1000 | Train Loss=13358.41503906 | Val Loss=1.48722863 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20814872 | ‚àö(Val Loss) = 1.21951985 | Current Learning Rate: 0.002\nEpoch 79/1000 | Train Loss=13358.41503906 | Val Loss=1.48723733 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20814812 | ‚àö(Val Loss) = 1.21952343 | Current Learning Rate: 0.002\nEpoch 80/1000 | Train Loss=13358.41503906 | Val Loss=1.48724544 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20814800 | ‚àö(Val Loss) = 1.21952677 | Current Learning Rate: 0.002\nEpoch 81/1000 | Train Loss=13358.41503906 | Val Loss=1.48725152 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814764 | ‚àö(Val Loss) = 1.21952927 | Current Learning Rate: 0.002\n\n Epoch :  80 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 82/1000 | Train Loss=13358.41503906 | Val Loss=1.48725808 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20814729 | ‚àö(Val Loss) = 1.21953189 | Current Learning Rate: 0.002\nEpoch 83/1000 | Train Loss=13358.41503906 | Val Loss=1.48726439 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814669 | ‚àö(Val Loss) = 1.21953452 | Current Learning Rate: 0.002\nEpoch 84/1000 | Train Loss=13358.41503906 | Val Loss=1.48726833 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814657 | ‚àö(Val Loss) = 1.21953607 | Current Learning Rate: 0.002\nEpoch 85/1000 | Train Loss=13358.41503906 | Val Loss=1.48727334 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814621 | ‚àö(Val Loss) = 1.21953821 | Current Learning Rate: 0.002\nEpoch 86/1000 | Train Loss=13358.41503906 | Val Loss=1.48727787 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814586 | ‚àö(Val Loss) = 1.21954000 | Current Learning Rate: 0.002\nEpoch 87/1000 | Train Loss=13358.41503906 | Val Loss=1.48728120 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814574 | ‚àö(Val Loss) = 1.21954143 | Current Learning Rate: 0.002\nEpoch 88/1000 | Train Loss=13358.41503906 | Val Loss=1.48728514 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20814538 | ‚àö(Val Loss) = 1.21954298 | Current Learning Rate: 0.002\nEpoch 89/1000 | Train Loss=13358.41503906 | Val Loss=1.48728883 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814514 | ‚àö(Val Loss) = 1.21954453 | Current Learning Rate: 0.002\nEpoch 90/1000 | Train Loss=13358.41503906 | Val Loss=1.48729062 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814478 | ‚àö(Val Loss) = 1.21954525 | Current Learning Rate: 0.002\nEpoch 91/1000 | Train Loss=13358.41503906 | Val Loss=1.48729300 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814455 | ‚àö(Val Loss) = 1.21954620 | Current Learning Rate: 0.002\nEpoch 92/1000 | Train Loss=13358.41503906 | Val Loss=1.48729539 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814443 | ‚àö(Val Loss) = 1.21954715 | Current Learning Rate: 0.002\nEpoch 93/1000 | Train Loss=13358.41503906 | Val Loss=1.48729837 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20814407 | ‚àö(Val Loss) = 1.21954846 | Current Learning Rate: 0.002\nEpoch 94/1000 | Train Loss=13358.41503906 | Val Loss=1.48730004 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814395 | ‚àö(Val Loss) = 1.21954906 | Current Learning Rate: 0.002\nEpoch 95/1000 | Train Loss=13358.41503906 | Val Loss=1.48730183 | Data=133.56198120 | Physics=2.21643429 | Val RMSE: 1.20814359 | ‚àö(Val Loss) = 1.21954989 | Current Learning Rate: 0.002\nEpoch 96/1000 | Train Loss=13358.41503906 | Val Loss=1.48730361 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814335 | ‚àö(Val Loss) = 1.21955061 | Current Learning Rate: 0.002\nEpoch 97/1000 | Train Loss=13358.41503906 | Val Loss=1.48730624 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814347 | ‚àö(Val Loss) = 1.21955168 | Current Learning Rate: 0.002\nEpoch 98/1000 | Train Loss=13358.41503906 | Val Loss=1.48730695 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20814312 | ‚àö(Val Loss) = 1.21955192 | Current Learning Rate: 0.002\nEpoch 99/1000 | Train Loss=13358.41503906 | Val Loss=1.48730826 | Data=133.56198120 | Physics=2.21643381 | Val RMSE: 1.20814312 | ‚àö(Val Loss) = 1.21955252 | Current Learning Rate: 0.002\nEpoch 100/1000 | Train Loss=13358.41503906 | Val Loss=1.48731029 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814276 | ‚àö(Val Loss) = 1.21955335 | Current Learning Rate: 0.002\nEpoch 101/1000 | Train Loss=13358.41503906 | Val Loss=1.48731160 | Data=133.56198120 | Physics=2.21643405 | Val RMSE: 1.20814264 | ‚àö(Val Loss) = 1.21955383 | Current Learning Rate: 0.002\n\n Epoch :  100 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 102/1000 | Train Loss=13358.42285156 | Val Loss=1.48765647 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20814097 | ‚àö(Val Loss) = 1.21969521 | Current Learning Rate: 0.002\nEpoch 103/1000 | Train Loss=13358.42285156 | Val Loss=1.48796475 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813930 | ‚àö(Val Loss) = 1.21982157 | Current Learning Rate: 0.002\nEpoch 104/1000 | Train Loss=13358.42382812 | Val Loss=1.48823905 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20813799 | ‚àö(Val Loss) = 1.21993399 | Current Learning Rate: 0.002\nEpoch 105/1000 | Train Loss=13358.42285156 | Val Loss=1.48848379 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813704 | ‚àö(Val Loss) = 1.22003436 | Current Learning Rate: 0.002\nEpoch 106/1000 | Train Loss=13358.42285156 | Val Loss=1.48870111 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813608 | ‚àö(Val Loss) = 1.22012341 | Current Learning Rate: 0.002\nEpoch 107/1000 | Train Loss=13358.42382812 | Val Loss=1.48889375 | Data=133.56207275 | Physics=2.21643429 | Val RMSE: 1.20813513 | ‚àö(Val Loss) = 1.22020233 | Current Learning Rate: 0.002\nEpoch 108/1000 | Train Loss=13358.42285156 | Val Loss=1.48906469 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813417 | ‚àö(Val Loss) = 1.22027242 | Current Learning Rate: 0.002\nEpoch 109/1000 | Train Loss=13358.42285156 | Val Loss=1.48921525 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813334 | ‚àö(Val Loss) = 1.22033405 | Current Learning Rate: 0.002\nEpoch 110/1000 | Train Loss=13358.42285156 | Val Loss=1.48934972 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20813274 | ‚àö(Val Loss) = 1.22038913 | Current Learning Rate: 0.002\nEpoch 111/1000 | Train Loss=13358.42285156 | Val Loss=1.48946726 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813215 | ‚àö(Val Loss) = 1.22043729 | Current Learning Rate: 0.002\nEpoch 112/1000 | Train Loss=13358.42285156 | Val Loss=1.48957086 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813167 | ‚àö(Val Loss) = 1.22047973 | Current Learning Rate: 0.002\nEpoch 113/1000 | Train Loss=13358.42285156 | Val Loss=1.48966098 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813107 | ‚àö(Val Loss) = 1.22051668 | Current Learning Rate: 0.002\nEpoch 114/1000 | Train Loss=13358.42285156 | Val Loss=1.48974097 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20813060 | ‚àö(Val Loss) = 1.22054946 | Current Learning Rate: 0.002\nEpoch 115/1000 | Train Loss=13358.42285156 | Val Loss=1.48980916 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813012 | ‚àö(Val Loss) = 1.22057736 | Current Learning Rate: 0.002\nEpoch 116/1000 | Train Loss=13358.42285156 | Val Loss=1.48986864 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812988 | ‚àö(Val Loss) = 1.22060180 | Current Learning Rate: 0.002\nEpoch 117/1000 | Train Loss=13358.42285156 | Val Loss=1.48991978 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20812953 | ‚àö(Val Loss) = 1.22062266 | Current Learning Rate: 0.002\nEpoch 118/1000 | Train Loss=13358.42285156 | Val Loss=1.48996329 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20812917 | ‚àö(Val Loss) = 1.22064054 | Current Learning Rate: 0.002\nEpoch 119/1000 | Train Loss=13358.42285156 | Val Loss=1.49000144 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812893 | ‚àö(Val Loss) = 1.22065616 | Current Learning Rate: 0.002\nEpoch 120/1000 | Train Loss=13358.42285156 | Val Loss=1.49003100 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20812893 | ‚àö(Val Loss) = 1.22066832 | Current Learning Rate: 0.002\nEpoch 121/1000 | Train Loss=13358.42285156 | Val Loss=1.49005675 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812845 | ‚àö(Val Loss) = 1.22067881 | Current Learning Rate: 0.002\n\n Epoch :  120 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 122/1000 | Train Loss=13358.42285156 | Val Loss=1.49007797 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20812857 | ‚àö(Val Loss) = 1.22068751 | Current Learning Rate: 0.002\nEpoch 123/1000 | Train Loss=13358.42285156 | Val Loss=1.49009323 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812809 | ‚àö(Val Loss) = 1.22069371 | Current Learning Rate: 0.002\nEpoch 124/1000 | Train Loss=13358.42285156 | Val Loss=1.49010694 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812821 | ‚àö(Val Loss) = 1.22069931 | Current Learning Rate: 0.002\nEpoch 125/1000 | Train Loss=13358.42285156 | Val Loss=1.49011660 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20812833 | ‚àö(Val Loss) = 1.22070336 | Current Learning Rate: 0.002\nEpoch 126/1000 | Train Loss=13358.42285156 | Val Loss=1.49012232 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812833 | ‚àö(Val Loss) = 1.22070563 | Current Learning Rate: 0.002\nEpoch 127/1000 | Train Loss=13358.42285156 | Val Loss=1.49012804 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20812821 | ‚àö(Val Loss) = 1.22070801 | Current Learning Rate: 0.002\nEpoch 128/1000 | Train Loss=13358.42285156 | Val Loss=1.49012899 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812821 | ‚àö(Val Loss) = 1.22070837 | Current Learning Rate: 0.002\nEpoch 129/1000 | Train Loss=13358.42285156 | Val Loss=1.49012768 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812809 | ‚àö(Val Loss) = 1.22070789 | Current Learning Rate: 0.002\nEpoch 130/1000 | Train Loss=13358.42285156 | Val Loss=1.49012554 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812809 | ‚àö(Val Loss) = 1.22070694 | Current Learning Rate: 0.002\nEpoch 131/1000 | Train Loss=13358.42285156 | Val Loss=1.49012280 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812798 | ‚àö(Val Loss) = 1.22070587 | Current Learning Rate: 0.002\nEpoch 132/1000 | Train Loss=13358.42285156 | Val Loss=1.49011624 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812821 | ‚àö(Val Loss) = 1.22070312 | Current Learning Rate: 0.002\nEpoch 133/1000 | Train Loss=13358.42285156 | Val Loss=1.49010932 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812798 | ‚àö(Val Loss) = 1.22070038 | Current Learning Rate: 0.002\nEpoch 134/1000 | Train Loss=13358.42382812 | Val Loss=1.49010015 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20812809 | ‚àö(Val Loss) = 1.22069657 | Current Learning Rate: 0.002\nEpoch 135/1000 | Train Loss=13358.42285156 | Val Loss=1.49009275 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20812821 | ‚àö(Val Loss) = 1.22069359 | Current Learning Rate: 0.002\nEpoch 136/1000 | Train Loss=13358.42382812 | Val Loss=1.49008226 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20812821 | ‚àö(Val Loss) = 1.22068930 | Current Learning Rate: 0.002\nEpoch 137/1000 | Train Loss=13358.42285156 | Val Loss=1.49007225 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812786 | ‚àö(Val Loss) = 1.22068512 | Current Learning Rate: 0.002\nEpoch 138/1000 | Train Loss=13358.42285156 | Val Loss=1.49006152 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20812809 | ‚àö(Val Loss) = 1.22068071 | Current Learning Rate: 0.002\nEpoch 139/1000 | Train Loss=13358.42285156 | Val Loss=1.49004805 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20812798 | ‚àö(Val Loss) = 1.22067523 | Current Learning Rate: 0.002\nEpoch 140/1000 | Train Loss=13358.42285156 | Val Loss=1.49003685 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812821 | ‚àö(Val Loss) = 1.22067070 | Current Learning Rate: 0.002\nEpoch 141/1000 | Train Loss=13358.42285156 | Val Loss=1.49002433 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20812809 | ‚àö(Val Loss) = 1.22066557 | Current Learning Rate: 0.002\n\n Epoch :  140 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 142/1000 | Train Loss=13358.42285156 | Val Loss=1.49001098 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20812833 | ‚àö(Val Loss) = 1.22066009 | Current Learning Rate: 0.002\nEpoch 143/1000 | Train Loss=13358.42285156 | Val Loss=1.48999727 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812833 | ‚àö(Val Loss) = 1.22065449 | Current Learning Rate: 0.002\nEpoch 144/1000 | Train Loss=13358.42382812 | Val Loss=1.48998451 | Data=133.56207275 | Physics=2.21643429 | Val RMSE: 1.20812845 | ‚àö(Val Loss) = 1.22064924 | Current Learning Rate: 0.002\nEpoch 145/1000 | Train Loss=13358.42382812 | Val Loss=1.48997056 | Data=133.56207275 | Physics=2.21643381 | Val RMSE: 1.20812845 | ‚àö(Val Loss) = 1.22064352 | Current Learning Rate: 0.002\nEpoch 146/1000 | Train Loss=13358.42285156 | Val Loss=1.48995531 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812869 | ‚àö(Val Loss) = 1.22063720 | Current Learning Rate: 0.002\nEpoch 147/1000 | Train Loss=13358.42285156 | Val Loss=1.48994255 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20812857 | ‚àö(Val Loss) = 1.22063208 | Current Learning Rate: 0.002\nEpoch 148/1000 | Train Loss=13358.42285156 | Val Loss=1.48992682 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812881 | ‚àö(Val Loss) = 1.22062564 | Current Learning Rate: 0.002\nEpoch 149/1000 | Train Loss=13358.42285156 | Val Loss=1.48991489 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20812893 | ‚àö(Val Loss) = 1.22062075 | Current Learning Rate: 0.002\nEpoch 150/1000 | Train Loss=13358.42285156 | Val Loss=1.48989999 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812893 | ‚àö(Val Loss) = 1.22061455 | Current Learning Rate: 0.002\nEpoch 151/1000 | Train Loss=13358.42285156 | Val Loss=1.48988509 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20812929 | ‚àö(Val Loss) = 1.22060847 | Current Learning Rate: 0.002\nEpoch 152/1000 | Train Loss=13358.42285156 | Val Loss=1.48987234 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20812917 | ‚àö(Val Loss) = 1.22060323 | Current Learning Rate: 0.002\nEpoch 153/1000 | Train Loss=13358.42285156 | Val Loss=1.48985791 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812929 | ‚àö(Val Loss) = 1.22059739 | Current Learning Rate: 0.002\nEpoch 154/1000 | Train Loss=13358.42285156 | Val Loss=1.48984420 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812953 | ‚àö(Val Loss) = 1.22059178 | Current Learning Rate: 0.002\nEpoch 155/1000 | Train Loss=13358.42285156 | Val Loss=1.48982930 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812953 | ‚àö(Val Loss) = 1.22058558 | Current Learning Rate: 0.002\nEpoch 156/1000 | Train Loss=13358.42285156 | Val Loss=1.48981392 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20812988 | ‚àö(Val Loss) = 1.22057939 | Current Learning Rate: 0.002\nEpoch 157/1000 | Train Loss=13358.42285156 | Val Loss=1.48980093 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20812964 | ‚àö(Val Loss) = 1.22057402 | Current Learning Rate: 0.002\nEpoch 158/1000 | Train Loss=13358.42285156 | Val Loss=1.48978722 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20812976 | ‚àö(Val Loss) = 1.22056842 | Current Learning Rate: 0.002\nEpoch 159/1000 | Train Loss=13358.42285156 | Val Loss=1.48977387 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813000 | ‚àö(Val Loss) = 1.22056293 | Current Learning Rate: 0.002\nEpoch 160/1000 | Train Loss=13358.42285156 | Val Loss=1.48975885 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813012 | ‚àö(Val Loss) = 1.22055674 | Current Learning Rate: 0.002\nEpoch 161/1000 | Train Loss=13358.42285156 | Val Loss=1.48974609 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813012 | ‚àö(Val Loss) = 1.22055161 | Current Learning Rate: 0.002\n\n Epoch :  160 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 162/1000 | Train Loss=13358.42285156 | Val Loss=1.48973405 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813036 | ‚àö(Val Loss) = 1.22054660 | Current Learning Rate: 0.002\nEpoch 163/1000 | Train Loss=13358.42285156 | Val Loss=1.48971939 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813036 | ‚àö(Val Loss) = 1.22054064 | Current Learning Rate: 0.002\nEpoch 164/1000 | Train Loss=13358.42285156 | Val Loss=1.48970687 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813072 | ‚àö(Val Loss) = 1.22053552 | Current Learning Rate: 0.002\nEpoch 165/1000 | Train Loss=13358.42285156 | Val Loss=1.48969328 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813072 | ‚àö(Val Loss) = 1.22052991 | Current Learning Rate: 0.002\nEpoch 166/1000 | Train Loss=13358.42285156 | Val Loss=1.48968053 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813084 | ‚àö(Val Loss) = 1.22052467 | Current Learning Rate: 0.002\nEpoch 167/1000 | Train Loss=13358.42285156 | Val Loss=1.48966825 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813096 | ‚àö(Val Loss) = 1.22051966 | Current Learning Rate: 0.002\nEpoch 168/1000 | Train Loss=13358.42285156 | Val Loss=1.48965585 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813131 | ‚àö(Val Loss) = 1.22051454 | Current Learning Rate: 0.002\nEpoch 169/1000 | Train Loss=13358.42285156 | Val Loss=1.48964214 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813131 | ‚àö(Val Loss) = 1.22050893 | Current Learning Rate: 0.002\nEpoch 170/1000 | Train Loss=13358.42285156 | Val Loss=1.48963106 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813119 | ‚àö(Val Loss) = 1.22050440 | Current Learning Rate: 0.002\nEpoch 171/1000 | Train Loss=13358.42285156 | Val Loss=1.48961878 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813143 | ‚àö(Val Loss) = 1.22049940 | Current Learning Rate: 0.002\nEpoch 172/1000 | Train Loss=13358.42285156 | Val Loss=1.48960698 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813167 | ‚àö(Val Loss) = 1.22049451 | Current Learning Rate: 0.002\nEpoch 173/1000 | Train Loss=13358.42285156 | Val Loss=1.48959398 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813167 | ‚àö(Val Loss) = 1.22048926 | Current Learning Rate: 0.002\nEpoch 174/1000 | Train Loss=13358.42285156 | Val Loss=1.48958158 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813191 | ‚àö(Val Loss) = 1.22048414 | Current Learning Rate: 0.002\nEpoch 175/1000 | Train Loss=13358.42285156 | Val Loss=1.48956978 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813215 | ‚àö(Val Loss) = 1.22047937 | Current Learning Rate: 0.002\nEpoch 176/1000 | Train Loss=13358.42285156 | Val Loss=1.48955822 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813239 | ‚àö(Val Loss) = 1.22047460 | Current Learning Rate: 0.002\nEpoch 177/1000 | Train Loss=13358.42285156 | Val Loss=1.48954797 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813239 | ‚àö(Val Loss) = 1.22047043 | Current Learning Rate: 0.002\nEpoch 178/1000 | Train Loss=13358.42285156 | Val Loss=1.48953629 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813239 | ‚àö(Val Loss) = 1.22046566 | Current Learning Rate: 0.002\nEpoch 179/1000 | Train Loss=13358.42382812 | Val Loss=1.48952436 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20813239 | ‚àö(Val Loss) = 1.22046077 | Current Learning Rate: 0.002\nEpoch 180/1000 | Train Loss=13358.42285156 | Val Loss=1.48951423 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813274 | ‚àö(Val Loss) = 1.22045660 | Current Learning Rate: 0.002\nEpoch 181/1000 | Train Loss=13358.42285156 | Val Loss=1.48950279 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813298 | ‚àö(Val Loss) = 1.22045183 | Current Learning Rate: 0.002\n\n Epoch :  180 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 182/1000 | Train Loss=13358.42285156 | Val Loss=1.48949218 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813286 | ‚àö(Val Loss) = 1.22044754 | Current Learning Rate: 0.002\nEpoch 183/1000 | Train Loss=13358.42285156 | Val Loss=1.48948169 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813310 | ‚àö(Val Loss) = 1.22044325 | Current Learning Rate: 0.002\nEpoch 184/1000 | Train Loss=13358.42382812 | Val Loss=1.48946965 | Data=133.56207275 | Physics=2.21643429 | Val RMSE: 1.20813334 | ‚àö(Val Loss) = 1.22043824 | Current Learning Rate: 0.002\nEpoch 185/1000 | Train Loss=13358.42285156 | Val Loss=1.48945999 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20813358 | ‚àö(Val Loss) = 1.22043431 | Current Learning Rate: 0.002\nEpoch 186/1000 | Train Loss=13358.42285156 | Val Loss=1.48945045 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813382 | ‚àö(Val Loss) = 1.22043049 | Current Learning Rate: 0.002\nEpoch 187/1000 | Train Loss=13358.42285156 | Val Loss=1.48943949 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813346 | ‚àö(Val Loss) = 1.22042596 | Current Learning Rate: 0.002\nEpoch 188/1000 | Train Loss=13358.42285156 | Val Loss=1.48942924 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813370 | ‚àö(Val Loss) = 1.22042179 | Current Learning Rate: 0.002\nEpoch 189/1000 | Train Loss=13358.42285156 | Val Loss=1.48941958 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813406 | ‚àö(Val Loss) = 1.22041774 | Current Learning Rate: 0.002\nEpoch 190/1000 | Train Loss=13358.42285156 | Val Loss=1.48941004 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813417 | ‚àö(Val Loss) = 1.22041392 | Current Learning Rate: 0.002\nEpoch 191/1000 | Train Loss=13358.42285156 | Val Loss=1.48940015 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813441 | ‚àö(Val Loss) = 1.22040987 | Current Learning Rate: 0.002\nEpoch 192/1000 | Train Loss=13358.42285156 | Val Loss=1.48938942 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813453 | ‚àö(Val Loss) = 1.22040546 | Current Learning Rate: 0.002\nEpoch 193/1000 | Train Loss=13358.42285156 | Val Loss=1.48938119 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813477 | ‚àö(Val Loss) = 1.22040212 | Current Learning Rate: 0.002\nEpoch 194/1000 | Train Loss=13358.42382812 | Val Loss=1.48937118 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20813465 | ‚àö(Val Loss) = 1.22039795 | Current Learning Rate: 0.002\nEpoch 195/1000 | Train Loss=13358.42285156 | Val Loss=1.48936188 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813489 | ‚àö(Val Loss) = 1.22039413 | Current Learning Rate: 0.002\nEpoch 196/1000 | Train Loss=13358.42285156 | Val Loss=1.48935103 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813501 | ‚àö(Val Loss) = 1.22038972 | Current Learning Rate: 0.002\nEpoch 197/1000 | Train Loss=13358.42285156 | Val Loss=1.48934352 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813513 | ‚àö(Val Loss) = 1.22038662 | Current Learning Rate: 0.002\nEpoch 198/1000 | Train Loss=13358.42382812 | Val Loss=1.48933446 | Data=133.56207275 | Physics=2.21643381 | Val RMSE: 1.20813549 | ‚àö(Val Loss) = 1.22038293 | Current Learning Rate: 0.002\nEpoch 199/1000 | Train Loss=13358.42285156 | Val Loss=1.48932445 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813560 | ‚àö(Val Loss) = 1.22037876 | Current Learning Rate: 0.002\nEpoch 200/1000 | Train Loss=13358.42285156 | Val Loss=1.48931670 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813572 | ‚àö(Val Loss) = 1.22037566 | Current Learning Rate: 0.002\nEpoch 201/1000 | Train Loss=13358.42285156 | Val Loss=1.48930740 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813572 | ‚àö(Val Loss) = 1.22037184 | Current Learning Rate: 0.002\n\n Epoch :  200 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 202/1000 | Train Loss=13358.42285156 | Val Loss=1.48929787 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813596 | ‚àö(Val Loss) = 1.22036791 | Current Learning Rate: 0.002\nEpoch 203/1000 | Train Loss=13358.42285156 | Val Loss=1.48929048 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813608 | ‚àö(Val Loss) = 1.22036493 | Current Learning Rate: 0.002\nEpoch 204/1000 | Train Loss=13358.42285156 | Val Loss=1.48928070 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813632 | ‚àö(Val Loss) = 1.22036088 | Current Learning Rate: 0.002\nEpoch 205/1000 | Train Loss=13358.42285156 | Val Loss=1.48927343 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813656 | ‚àö(Val Loss) = 1.22035789 | Current Learning Rate: 0.002\nEpoch 206/1000 | Train Loss=13358.42285156 | Val Loss=1.48926437 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813644 | ‚àö(Val Loss) = 1.22035420 | Current Learning Rate: 0.002\nEpoch 207/1000 | Train Loss=13358.42285156 | Val Loss=1.48925698 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813656 | ‚àö(Val Loss) = 1.22035122 | Current Learning Rate: 0.002\nEpoch 208/1000 | Train Loss=13358.42285156 | Val Loss=1.48924828 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813680 | ‚àö(Val Loss) = 1.22034764 | Current Learning Rate: 0.002\nEpoch 209/1000 | Train Loss=13358.42285156 | Val Loss=1.48924065 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813704 | ‚àö(Val Loss) = 1.22034442 | Current Learning Rate: 0.002\nEpoch 210/1000 | Train Loss=13358.42285156 | Val Loss=1.48923218 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813715 | ‚àö(Val Loss) = 1.22034097 | Current Learning Rate: 0.002\nEpoch 211/1000 | Train Loss=13358.42285156 | Val Loss=1.48922479 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813739 | ‚àö(Val Loss) = 1.22033799 | Current Learning Rate: 0.002\nEpoch 212/1000 | Train Loss=13358.42285156 | Val Loss=1.48921609 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20813763 | ‚àö(Val Loss) = 1.22033441 | Current Learning Rate: 0.002\nEpoch 213/1000 | Train Loss=13358.42285156 | Val Loss=1.48920929 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813775 | ‚àö(Val Loss) = 1.22033167 | Current Learning Rate: 0.002\nEpoch 214/1000 | Train Loss=13358.42285156 | Val Loss=1.48920023 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813763 | ‚àö(Val Loss) = 1.22032797 | Current Learning Rate: 0.002\nEpoch 215/1000 | Train Loss=13358.42285156 | Val Loss=1.48919356 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813787 | ‚àö(Val Loss) = 1.22032523 | Current Learning Rate: 0.002\nEpoch 216/1000 | Train Loss=13358.42285156 | Val Loss=1.48918641 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813811 | ‚àö(Val Loss) = 1.22032225 | Current Learning Rate: 0.002\nEpoch 217/1000 | Train Loss=13358.42285156 | Val Loss=1.48917878 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813823 | ‚àö(Val Loss) = 1.22031915 | Current Learning Rate: 0.002\nEpoch 218/1000 | Train Loss=13358.42285156 | Val Loss=1.48917198 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813835 | ‚àö(Val Loss) = 1.22031629 | Current Learning Rate: 0.002\nEpoch 219/1000 | Train Loss=13358.42285156 | Val Loss=1.48916423 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813859 | ‚àö(Val Loss) = 1.22031319 | Current Learning Rate: 0.002\nEpoch 220/1000 | Train Loss=13358.42285156 | Val Loss=1.48915672 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813882 | ‚àö(Val Loss) = 1.22031009 | Current Learning Rate: 0.002\nEpoch 221/1000 | Train Loss=13358.42285156 | Val Loss=1.48914945 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20813906 | ‚àö(Val Loss) = 1.22030711 | Current Learning Rate: 0.002\n\n Epoch :  220 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 222/1000 | Train Loss=13358.42285156 | Val Loss=1.48914349 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813930 | ‚àö(Val Loss) = 1.22030473 | Current Learning Rate: 0.002\nEpoch 223/1000 | Train Loss=13358.42285156 | Val Loss=1.48913586 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813930 | ‚àö(Val Loss) = 1.22030151 | Current Learning Rate: 0.002\nEpoch 224/1000 | Train Loss=13358.42285156 | Val Loss=1.48912942 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813930 | ‚àö(Val Loss) = 1.22029889 | Current Learning Rate: 0.002\nEpoch 225/1000 | Train Loss=13358.42285156 | Val Loss=1.48912251 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20813942 | ‚àö(Val Loss) = 1.22029603 | Current Learning Rate: 0.002\nEpoch 226/1000 | Train Loss=13358.42285156 | Val Loss=1.48911488 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20813966 | ‚àö(Val Loss) = 1.22029293 | Current Learning Rate: 0.002\nEpoch 227/1000 | Train Loss=13358.42382812 | Val Loss=1.48910880 | Data=133.56207275 | Physics=2.21643381 | Val RMSE: 1.20813990 | ‚àö(Val Loss) = 1.22029042 | Current Learning Rate: 0.002\nEpoch 228/1000 | Train Loss=13358.42285156 | Val Loss=1.48910260 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814013 | ‚àö(Val Loss) = 1.22028792 | Current Learning Rate: 0.002\nEpoch 229/1000 | Train Loss=13358.42285156 | Val Loss=1.48909616 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814013 | ‚àö(Val Loss) = 1.22028530 | Current Learning Rate: 0.002\nEpoch 230/1000 | Train Loss=13358.42285156 | Val Loss=1.48908782 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814037 | ‚àö(Val Loss) = 1.22028184 | Current Learning Rate: 0.002\nEpoch 231/1000 | Train Loss=13358.42285156 | Val Loss=1.48908150 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814061 | ‚àö(Val Loss) = 1.22027922 | Current Learning Rate: 0.002\nEpoch 232/1000 | Train Loss=13358.42285156 | Val Loss=1.48907506 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814085 | ‚àö(Val Loss) = 1.22027659 | Current Learning Rate: 0.002\nEpoch 233/1000 | Train Loss=13358.42285156 | Val Loss=1.48906970 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814085 | ‚àö(Val Loss) = 1.22027445 | Current Learning Rate: 0.002\nEpoch 234/1000 | Train Loss=13358.42285156 | Val Loss=1.48906314 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814085 | ‚àö(Val Loss) = 1.22027171 | Current Learning Rate: 0.002\nEpoch 235/1000 | Train Loss=13358.42285156 | Val Loss=1.48905611 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814097 | ‚àö(Val Loss) = 1.22026885 | Current Learning Rate: 0.002\nEpoch 236/1000 | Train Loss=13358.42285156 | Val Loss=1.48905087 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20814133 | ‚àö(Val Loss) = 1.22026670 | Current Learning Rate: 0.002\nEpoch 237/1000 | Train Loss=13358.42285156 | Val Loss=1.48904383 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814157 | ‚àö(Val Loss) = 1.22026384 | Current Learning Rate: 0.002\nEpoch 238/1000 | Train Loss=13358.42285156 | Val Loss=1.48903847 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814180 | ‚àö(Val Loss) = 1.22026169 | Current Learning Rate: 0.002\nEpoch 239/1000 | Train Loss=13358.42285156 | Val Loss=1.48903310 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814180 | ‚àö(Val Loss) = 1.22025943 | Current Learning Rate: 0.002\nEpoch 240/1000 | Train Loss=13358.42285156 | Val Loss=1.48902667 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814192 | ‚àö(Val Loss) = 1.22025681 | Current Learning Rate: 0.002\nEpoch 241/1000 | Train Loss=13358.42285156 | Val Loss=1.48901987 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814216 | ‚àö(Val Loss) = 1.22025406 | Current Learning Rate: 0.002\n\n Epoch :  240 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 242/1000 | Train Loss=13358.42382812 | Val Loss=1.48901451 | Data=133.56207275 | Physics=2.21643429 | Val RMSE: 1.20814240 | ‚àö(Val Loss) = 1.22025180 | Current Learning Rate: 0.002\nEpoch 243/1000 | Train Loss=13358.42285156 | Val Loss=1.48900843 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814240 | ‚àö(Val Loss) = 1.22024930 | Current Learning Rate: 0.002\nEpoch 244/1000 | Train Loss=13358.42285156 | Val Loss=1.48900282 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814264 | ‚àö(Val Loss) = 1.22024703 | Current Learning Rate: 0.002\nEpoch 245/1000 | Train Loss=13358.42285156 | Val Loss=1.48899758 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814264 | ‚àö(Val Loss) = 1.22024488 | Current Learning Rate: 0.002\nEpoch 246/1000 | Train Loss=13358.42285156 | Val Loss=1.48899126 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814276 | ‚àö(Val Loss) = 1.22024226 | Current Learning Rate: 0.002\nEpoch 247/1000 | Train Loss=13358.42285156 | Val Loss=1.48898590 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814312 | ‚àö(Val Loss) = 1.22024012 | Current Learning Rate: 0.002\nEpoch 248/1000 | Train Loss=13358.42285156 | Val Loss=1.48898125 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814312 | ‚àö(Val Loss) = 1.22023821 | Current Learning Rate: 0.002\nEpoch 249/1000 | Train Loss=13358.42285156 | Val Loss=1.48897493 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814335 | ‚àö(Val Loss) = 1.22023559 | Current Learning Rate: 0.002\nEpoch 250/1000 | Train Loss=13358.42285156 | Val Loss=1.48896992 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814359 | ‚àö(Val Loss) = 1.22023356 | Current Learning Rate: 0.002\nEpoch 251/1000 | Train Loss=13358.42285156 | Val Loss=1.48896480 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814371 | ‚àö(Val Loss) = 1.22023141 | Current Learning Rate: 0.002\nEpoch 252/1000 | Train Loss=13358.42285156 | Val Loss=1.48895884 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20814371 | ‚àö(Val Loss) = 1.22022903 | Current Learning Rate: 0.002\nEpoch 253/1000 | Train Loss=13358.42382812 | Val Loss=1.48895335 | Data=133.56207275 | Physics=2.21643381 | Val RMSE: 1.20814395 | ‚àö(Val Loss) = 1.22022676 | Current Learning Rate: 0.002\nEpoch 254/1000 | Train Loss=13358.42285156 | Val Loss=1.48894870 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814431 | ‚àö(Val Loss) = 1.22022486 | Current Learning Rate: 0.002\nEpoch 255/1000 | Train Loss=13358.42285156 | Val Loss=1.48894370 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20814443 | ‚àö(Val Loss) = 1.22022283 | Current Learning Rate: 0.002\nEpoch 256/1000 | Train Loss=13358.42285156 | Val Loss=1.48893833 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814478 | ‚àö(Val Loss) = 1.22022057 | Current Learning Rate: 0.002\nEpoch 257/1000 | Train Loss=13358.42285156 | Val Loss=1.48893332 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814478 | ‚àö(Val Loss) = 1.22021854 | Current Learning Rate: 0.002\nEpoch 258/1000 | Train Loss=13358.42285156 | Val Loss=1.48892772 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814490 | ‚àö(Val Loss) = 1.22021627 | Current Learning Rate: 0.002\nEpoch 259/1000 | Train Loss=13358.42285156 | Val Loss=1.48892295 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814490 | ‚àö(Val Loss) = 1.22021425 | Current Learning Rate: 0.002\nEpoch 260/1000 | Train Loss=13358.42285156 | Val Loss=1.48891819 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20814526 | ‚àö(Val Loss) = 1.22021234 | Current Learning Rate: 0.002\nEpoch 261/1000 | Train Loss=13358.42285156 | Val Loss=1.48891330 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814526 | ‚àö(Val Loss) = 1.22021031 | Current Learning Rate: 0.002\n\n Epoch :  260 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 262/1000 | Train Loss=13358.42285156 | Val Loss=1.48890853 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814538 | ‚àö(Val Loss) = 1.22020841 | Current Learning Rate: 0.002\nEpoch 263/1000 | Train Loss=13358.42285156 | Val Loss=1.48890281 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814562 | ‚àö(Val Loss) = 1.22020602 | Current Learning Rate: 0.002\nEpoch 264/1000 | Train Loss=13358.42285156 | Val Loss=1.48889768 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814586 | ‚àö(Val Loss) = 1.22020400 | Current Learning Rate: 0.002\nEpoch 265/1000 | Train Loss=13358.42285156 | Val Loss=1.48889363 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814586 | ‚àö(Val Loss) = 1.22020233 | Current Learning Rate: 0.002\nEpoch 266/1000 | Train Loss=13358.42382812 | Val Loss=1.48888803 | Data=133.56207275 | Physics=2.21643429 | Val RMSE: 1.20814610 | ‚àö(Val Loss) = 1.22019994 | Current Learning Rate: 0.002\nEpoch 267/1000 | Train Loss=13358.42285156 | Val Loss=1.48888350 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814645 | ‚àö(Val Loss) = 1.22019815 | Current Learning Rate: 0.002\nEpoch 268/1000 | Train Loss=13358.42285156 | Val Loss=1.48887873 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20814657 | ‚àö(Val Loss) = 1.22019613 | Current Learning Rate: 0.002\nEpoch 269/1000 | Train Loss=13358.42285156 | Val Loss=1.48887503 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814681 | ‚àö(Val Loss) = 1.22019470 | Current Learning Rate: 0.002\nEpoch 270/1000 | Train Loss=13358.42382812 | Val Loss=1.48887062 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20814693 | ‚àö(Val Loss) = 1.22019291 | Current Learning Rate: 0.002\nEpoch 271/1000 | Train Loss=13358.42285156 | Val Loss=1.48886645 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814717 | ‚àö(Val Loss) = 1.22019112 | Current Learning Rate: 0.002\nEpoch 272/1000 | Train Loss=13358.42285156 | Val Loss=1.48886144 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814729 | ‚àö(Val Loss) = 1.22018909 | Current Learning Rate: 0.002\nEpoch 273/1000 | Train Loss=13358.42285156 | Val Loss=1.48885703 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814753 | ‚àö(Val Loss) = 1.22018731 | Current Learning Rate: 0.002\nEpoch 274/1000 | Train Loss=13358.42285156 | Val Loss=1.48885167 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814753 | ‚àö(Val Loss) = 1.22018504 | Current Learning Rate: 0.002\nEpoch 275/1000 | Train Loss=13358.42285156 | Val Loss=1.48884714 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814776 | ‚àö(Val Loss) = 1.22018325 | Current Learning Rate: 0.002\nEpoch 276/1000 | Train Loss=13358.42285156 | Val Loss=1.48884416 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814800 | ‚àö(Val Loss) = 1.22018206 | Current Learning Rate: 0.002\nEpoch 277/1000 | Train Loss=13358.42285156 | Val Loss=1.48883915 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814776 | ‚àö(Val Loss) = 1.22017992 | Current Learning Rate: 0.002\nEpoch 278/1000 | Train Loss=13358.42285156 | Val Loss=1.48883486 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20814800 | ‚àö(Val Loss) = 1.22017825 | Current Learning Rate: 0.002\nEpoch 279/1000 | Train Loss=13358.42285156 | Val Loss=1.48883080 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814812 | ‚àö(Val Loss) = 1.22017658 | Current Learning Rate: 0.002\nEpoch 280/1000 | Train Loss=13358.42285156 | Val Loss=1.48882627 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814848 | ‚àö(Val Loss) = 1.22017467 | Current Learning Rate: 0.002\nEpoch 281/1000 | Train Loss=13358.42382812 | Val Loss=1.48882127 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20814848 | ‚àö(Val Loss) = 1.22017264 | Current Learning Rate: 0.002\n\n Epoch :  280 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 282/1000 | Train Loss=13358.42285156 | Val Loss=1.48881817 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814872 | ‚àö(Val Loss) = 1.22017133 | Current Learning Rate: 0.002\nEpoch 283/1000 | Train Loss=13358.42285156 | Val Loss=1.48881364 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814896 | ‚àö(Val Loss) = 1.22016954 | Current Learning Rate: 0.002\nEpoch 284/1000 | Train Loss=13358.42285156 | Val Loss=1.48881030 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814908 | ‚àö(Val Loss) = 1.22016811 | Current Learning Rate: 0.002\nEpoch 285/1000 | Train Loss=13358.42285156 | Val Loss=1.48880529 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814919 | ‚àö(Val Loss) = 1.22016609 | Current Learning Rate: 0.002\nEpoch 286/1000 | Train Loss=13358.42285156 | Val Loss=1.48880076 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20814943 | ‚àö(Val Loss) = 1.22016418 | Current Learning Rate: 0.002\nEpoch 287/1000 | Train Loss=13358.42285156 | Val Loss=1.48879826 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814979 | ‚àö(Val Loss) = 1.22016323 | Current Learning Rate: 0.002\nEpoch 288/1000 | Train Loss=13358.42285156 | Val Loss=1.48879385 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20814991 | ‚àö(Val Loss) = 1.22016144 | Current Learning Rate: 0.002\nEpoch 289/1000 | Train Loss=13358.42285156 | Val Loss=1.48878932 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20814991 | ‚àö(Val Loss) = 1.22015953 | Current Learning Rate: 0.002\nEpoch 290/1000 | Train Loss=13358.42285156 | Val Loss=1.48878574 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815015 | ‚àö(Val Loss) = 1.22015810 | Current Learning Rate: 0.002\nEpoch 291/1000 | Train Loss=13358.42285156 | Val Loss=1.48878074 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815027 | ‚àö(Val Loss) = 1.22015607 | Current Learning Rate: 0.002\nEpoch 292/1000 | Train Loss=13358.42382812 | Val Loss=1.48877752 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20815051 | ‚àö(Val Loss) = 1.22015476 | Current Learning Rate: 0.002\nEpoch 293/1000 | Train Loss=13358.42285156 | Val Loss=1.48877454 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815051 | ‚àö(Val Loss) = 1.22015345 | Current Learning Rate: 0.002\nEpoch 294/1000 | Train Loss=13358.42285156 | Val Loss=1.48876965 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815074 | ‚àö(Val Loss) = 1.22015154 | Current Learning Rate: 0.002\nEpoch 295/1000 | Train Loss=13358.42285156 | Val Loss=1.48876595 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815098 | ‚àö(Val Loss) = 1.22014999 | Current Learning Rate: 0.002\nEpoch 296/1000 | Train Loss=13358.42285156 | Val Loss=1.48876357 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815122 | ‚àö(Val Loss) = 1.22014904 | Current Learning Rate: 0.002\nEpoch 297/1000 | Train Loss=13358.42285156 | Val Loss=1.48875856 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815122 | ‚àö(Val Loss) = 1.22014689 | Current Learning Rate: 0.002\nEpoch 298/1000 | Train Loss=13358.42285156 | Val Loss=1.48875535 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815146 | ‚àö(Val Loss) = 1.22014558 | Current Learning Rate: 0.002\nEpoch 299/1000 | Train Loss=13358.42285156 | Val Loss=1.48875248 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815170 | ‚àö(Val Loss) = 1.22014451 | Current Learning Rate: 0.002\nEpoch 300/1000 | Train Loss=13358.42285156 | Val Loss=1.48874819 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815170 | ‚àö(Val Loss) = 1.22014272 | Current Learning Rate: 0.002\nEpoch 301/1000 | Train Loss=13358.42285156 | Val Loss=1.48874414 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815170 | ‚àö(Val Loss) = 1.22014105 | Current Learning Rate: 0.002\n\n Epoch :  300 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 302/1000 | Train Loss=13358.42285156 | Val Loss=1.48874032 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815194 | ‚àö(Val Loss) = 1.22013950 | Current Learning Rate: 0.002\nEpoch 303/1000 | Train Loss=13358.42285156 | Val Loss=1.48873758 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815217 | ‚àö(Val Loss) = 1.22013831 | Current Learning Rate: 0.002\nEpoch 304/1000 | Train Loss=13358.42285156 | Val Loss=1.48873448 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815217 | ‚àö(Val Loss) = 1.22013712 | Current Learning Rate: 0.002\nEpoch 305/1000 | Train Loss=13358.42382812 | Val Loss=1.48872948 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20815241 | ‚àö(Val Loss) = 1.22013497 | Current Learning Rate: 0.002\nEpoch 306/1000 | Train Loss=13358.42382812 | Val Loss=1.48872769 | Data=133.56207275 | Physics=2.21643429 | Val RMSE: 1.20815277 | ‚àö(Val Loss) = 1.22013426 | Current Learning Rate: 0.002\nEpoch 307/1000 | Train Loss=13358.42285156 | Val Loss=1.48872316 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815277 | ‚àö(Val Loss) = 1.22013247 | Current Learning Rate: 0.002\nEpoch 308/1000 | Train Loss=13358.42285156 | Val Loss=1.48871994 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815301 | ‚àö(Val Loss) = 1.22013116 | Current Learning Rate: 0.002\nEpoch 309/1000 | Train Loss=13358.42285156 | Val Loss=1.48871708 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815325 | ‚àö(Val Loss) = 1.22012997 | Current Learning Rate: 0.002\nEpoch 310/1000 | Train Loss=13358.42285156 | Val Loss=1.48871315 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815349 | ‚àö(Val Loss) = 1.22012830 | Current Learning Rate: 0.002\nEpoch 311/1000 | Train Loss=13358.42285156 | Val Loss=1.48871017 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815349 | ‚àö(Val Loss) = 1.22012711 | Current Learning Rate: 0.002\nEpoch 312/1000 | Train Loss=13358.42285156 | Val Loss=1.48870695 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20815372 | ‚àö(Val Loss) = 1.22012579 | Current Learning Rate: 0.002\nEpoch 313/1000 | Train Loss=13358.42285156 | Val Loss=1.48870337 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815396 | ‚àö(Val Loss) = 1.22012436 | Current Learning Rate: 0.002\nEpoch 314/1000 | Train Loss=13358.42285156 | Val Loss=1.48870015 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815420 | ‚àö(Val Loss) = 1.22012305 | Current Learning Rate: 0.002\nEpoch 315/1000 | Train Loss=13358.42285156 | Val Loss=1.48869693 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815420 | ‚àö(Val Loss) = 1.22012174 | Current Learning Rate: 0.002\nEpoch 316/1000 | Train Loss=13358.42285156 | Val Loss=1.48869407 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815432 | ‚àö(Val Loss) = 1.22012055 | Current Learning Rate: 0.002\nEpoch 317/1000 | Train Loss=13358.42285156 | Val Loss=1.48868918 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815456 | ‚àö(Val Loss) = 1.22011852 | Current Learning Rate: 0.002\nEpoch 318/1000 | Train Loss=13358.42285156 | Val Loss=1.48868704 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815456 | ‚àö(Val Loss) = 1.22011769 | Current Learning Rate: 0.002\nEpoch 319/1000 | Train Loss=13358.42285156 | Val Loss=1.48868489 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815480 | ‚àö(Val Loss) = 1.22011673 | Current Learning Rate: 0.002\nEpoch 320/1000 | Train Loss=13358.42285156 | Val Loss=1.48867929 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815504 | ‚àö(Val Loss) = 1.22011447 | Current Learning Rate: 0.002\nEpoch 321/1000 | Train Loss=13358.42285156 | Val Loss=1.48867822 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815504 | ‚àö(Val Loss) = 1.22011399 | Current Learning Rate: 0.002\n\n Epoch :  320 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 322/1000 | Train Loss=13358.42285156 | Val Loss=1.48867500 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20815527 | ‚àö(Val Loss) = 1.22011268 | Current Learning Rate: 0.002\nEpoch 323/1000 | Train Loss=13358.42285156 | Val Loss=1.48867118 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815551 | ‚àö(Val Loss) = 1.22011113 | Current Learning Rate: 0.002\nEpoch 324/1000 | Train Loss=13358.42285156 | Val Loss=1.48866844 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815575 | ‚àö(Val Loss) = 1.22011006 | Current Learning Rate: 0.002\nEpoch 325/1000 | Train Loss=13358.42285156 | Val Loss=1.48866534 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815575 | ‚àö(Val Loss) = 1.22010875 | Current Learning Rate: 0.002\nEpoch 326/1000 | Train Loss=13358.42285156 | Val Loss=1.48866343 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815599 | ‚àö(Val Loss) = 1.22010791 | Current Learning Rate: 0.002\nEpoch 327/1000 | Train Loss=13358.42285156 | Val Loss=1.48865855 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815623 | ‚àö(Val Loss) = 1.22010601 | Current Learning Rate: 0.002\nEpoch 328/1000 | Train Loss=13358.42285156 | Val Loss=1.48865688 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815623 | ‚àö(Val Loss) = 1.22010529 | Current Learning Rate: 0.002\nEpoch 329/1000 | Train Loss=13358.42382812 | Val Loss=1.48865402 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20815647 | ‚àö(Val Loss) = 1.22010410 | Current Learning Rate: 0.002\nEpoch 330/1000 | Train Loss=13358.42285156 | Val Loss=1.48865068 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815670 | ‚àö(Val Loss) = 1.22010279 | Current Learning Rate: 0.002\nEpoch 331/1000 | Train Loss=13358.42382812 | Val Loss=1.48864722 | Data=133.56207275 | Physics=2.21643381 | Val RMSE: 1.20815670 | ‚àö(Val Loss) = 1.22010136 | Current Learning Rate: 0.002\nEpoch 332/1000 | Train Loss=13358.42285156 | Val Loss=1.48864412 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815694 | ‚àö(Val Loss) = 1.22010005 | Current Learning Rate: 0.002\nEpoch 333/1000 | Train Loss=13358.42285156 | Val Loss=1.48864269 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815706 | ‚àö(Val Loss) = 1.22009945 | Current Learning Rate: 0.002\nEpoch 334/1000 | Train Loss=13358.42285156 | Val Loss=1.48863924 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815742 | ‚àö(Val Loss) = 1.22009802 | Current Learning Rate: 0.002\nEpoch 335/1000 | Train Loss=13358.42285156 | Val Loss=1.48863721 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815754 | ‚àö(Val Loss) = 1.22009718 | Current Learning Rate: 0.002\nEpoch 336/1000 | Train Loss=13358.42285156 | Val Loss=1.48863387 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815766 | ‚àö(Val Loss) = 1.22009587 | Current Learning Rate: 0.002\nEpoch 337/1000 | Train Loss=13358.42285156 | Val Loss=1.48862994 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815790 | ‚àö(Val Loss) = 1.22009420 | Current Learning Rate: 0.002\nEpoch 338/1000 | Train Loss=13358.42285156 | Val Loss=1.48862803 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20815790 | ‚àö(Val Loss) = 1.22009349 | Current Learning Rate: 0.002\nEpoch 339/1000 | Train Loss=13358.42285156 | Val Loss=1.48862517 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815825 | ‚àö(Val Loss) = 1.22009230 | Current Learning Rate: 0.002\nEpoch 340/1000 | Train Loss=13358.42285156 | Val Loss=1.48862350 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815849 | ‚àö(Val Loss) = 1.22009158 | Current Learning Rate: 0.002\nEpoch 341/1000 | Train Loss=13358.42285156 | Val Loss=1.48862016 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20815849 | ‚àö(Val Loss) = 1.22009027 | Current Learning Rate: 0.002\n\n Epoch :  340 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 342/1000 | Train Loss=13358.42285156 | Val Loss=1.48861837 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815849 | ‚àö(Val Loss) = 1.22008944 | Current Learning Rate: 0.002\nEpoch 343/1000 | Train Loss=13358.42285156 | Val Loss=1.48861396 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20815873 | ‚àö(Val Loss) = 1.22008765 | Current Learning Rate: 0.002\nEpoch 344/1000 | Train Loss=13358.42285156 | Val Loss=1.48861217 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815897 | ‚àö(Val Loss) = 1.22008693 | Current Learning Rate: 0.002\nEpoch 345/1000 | Train Loss=13358.42285156 | Val Loss=1.48860919 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815909 | ‚àö(Val Loss) = 1.22008574 | Current Learning Rate: 0.002\nEpoch 346/1000 | Train Loss=13358.42285156 | Val Loss=1.48860586 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815921 | ‚àö(Val Loss) = 1.22008431 | Current Learning Rate: 0.002\nEpoch 347/1000 | Train Loss=13358.42285156 | Val Loss=1.48860407 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20815945 | ‚àö(Val Loss) = 1.22008359 | Current Learning Rate: 0.002\nEpoch 348/1000 | Train Loss=13358.42285156 | Val Loss=1.48860085 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815945 | ‚àö(Val Loss) = 1.22008228 | Current Learning Rate: 0.002\nEpoch 349/1000 | Train Loss=13358.42285156 | Val Loss=1.48859942 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815969 | ‚àö(Val Loss) = 1.22008169 | Current Learning Rate: 0.002\nEpoch 350/1000 | Train Loss=13358.42285156 | Val Loss=1.48859584 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815992 | ‚àö(Val Loss) = 1.22008026 | Current Learning Rate: 0.002\nEpoch 351/1000 | Train Loss=13358.42285156 | Val Loss=1.48859406 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20815992 | ‚àö(Val Loss) = 1.22007954 | Current Learning Rate: 0.002\nEpoch 352/1000 | Train Loss=13358.42285156 | Val Loss=1.48859072 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816016 | ‚àö(Val Loss) = 1.22007811 | Current Learning Rate: 0.002\nEpoch 353/1000 | Train Loss=13358.42285156 | Val Loss=1.48858905 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816040 | ‚àö(Val Loss) = 1.22007751 | Current Learning Rate: 0.002\nEpoch 354/1000 | Train Loss=13358.42285156 | Val Loss=1.48858583 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816064 | ‚àö(Val Loss) = 1.22007620 | Current Learning Rate: 0.002\nEpoch 355/1000 | Train Loss=13358.42285156 | Val Loss=1.48858380 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816052 | ‚àö(Val Loss) = 1.22007537 | Current Learning Rate: 0.002\nEpoch 356/1000 | Train Loss=13358.42285156 | Val Loss=1.48857927 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816076 | ‚àö(Val Loss) = 1.22007346 | Current Learning Rate: 0.002\nEpoch 357/1000 | Train Loss=13358.42285156 | Val Loss=1.48857927 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816100 | ‚àö(Val Loss) = 1.22007346 | Current Learning Rate: 0.002\nEpoch 358/1000 | Train Loss=13358.42382812 | Val Loss=1.48857498 | Data=133.56207275 | Physics=2.21643381 | Val RMSE: 1.20816100 | ‚àö(Val Loss) = 1.22007167 | Current Learning Rate: 0.002\nEpoch 359/1000 | Train Loss=13358.42285156 | Val Loss=1.48857450 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20816123 | ‚àö(Val Loss) = 1.22007155 | Current Learning Rate: 0.002\nEpoch 360/1000 | Train Loss=13358.42285156 | Val Loss=1.48857152 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816147 | ‚àö(Val Loss) = 1.22007024 | Current Learning Rate: 0.002\nEpoch 361/1000 | Train Loss=13358.42285156 | Val Loss=1.48857009 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816147 | ‚àö(Val Loss) = 1.22006977 | Current Learning Rate: 0.002\n\n Epoch :  360 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 362/1000 | Train Loss=13358.42285156 | Val Loss=1.48856723 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816171 | ‚àö(Val Loss) = 1.22006857 | Current Learning Rate: 0.002\nEpoch 363/1000 | Train Loss=13358.42285156 | Val Loss=1.48856556 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816195 | ‚àö(Val Loss) = 1.22006786 | Current Learning Rate: 0.002\nEpoch 364/1000 | Train Loss=13358.42382812 | Val Loss=1.48856223 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20816195 | ‚àö(Val Loss) = 1.22006643 | Current Learning Rate: 0.002\nEpoch 365/1000 | Train Loss=13358.42285156 | Val Loss=1.48856032 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816219 | ‚àö(Val Loss) = 1.22006571 | Current Learning Rate: 0.002\nEpoch 366/1000 | Train Loss=13358.42285156 | Val Loss=1.48855746 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20816243 | ‚àö(Val Loss) = 1.22006452 | Current Learning Rate: 0.002\nEpoch 367/1000 | Train Loss=13358.42285156 | Val Loss=1.48855579 | Data=133.56205750 | Physics=2.21643429 | Val RMSE: 1.20816243 | ‚àö(Val Loss) = 1.22006381 | Current Learning Rate: 0.002\nEpoch 368/1000 | Train Loss=13358.42285156 | Val Loss=1.48855293 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816267 | ‚àö(Val Loss) = 1.22006261 | Current Learning Rate: 0.002\nEpoch 369/1000 | Train Loss=13358.42285156 | Val Loss=1.48855102 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816290 | ‚àö(Val Loss) = 1.22006190 | Current Learning Rate: 0.002\nEpoch 370/1000 | Train Loss=13358.42285156 | Val Loss=1.48854768 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816290 | ‚àö(Val Loss) = 1.22006047 | Current Learning Rate: 0.002\nEpoch 371/1000 | Train Loss=13358.42285156 | Val Loss=1.48854589 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816314 | ‚àö(Val Loss) = 1.22005975 | Current Learning Rate: 0.002\nEpoch 372/1000 | Train Loss=13358.42285156 | Val Loss=1.48854375 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816338 | ‚àö(Val Loss) = 1.22005892 | Current Learning Rate: 0.002\nEpoch 373/1000 | Train Loss=13358.42285156 | Val Loss=1.48854148 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816362 | ‚àö(Val Loss) = 1.22005796 | Current Learning Rate: 0.002\nEpoch 374/1000 | Train Loss=13358.42285156 | Val Loss=1.48853993 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816362 | ‚àö(Val Loss) = 1.22005737 | Current Learning Rate: 0.002\nEpoch 375/1000 | Train Loss=13358.42285156 | Val Loss=1.48853683 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816386 | ‚àö(Val Loss) = 1.22005606 | Current Learning Rate: 0.002\nEpoch 376/1000 | Train Loss=13358.42285156 | Val Loss=1.48853493 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816386 | ‚àö(Val Loss) = 1.22005534 | Current Learning Rate: 0.002\nEpoch 377/1000 | Train Loss=13358.42285156 | Val Loss=1.48853350 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816410 | ‚àö(Val Loss) = 1.22005475 | Current Learning Rate: 0.002\nEpoch 378/1000 | Train Loss=13358.42285156 | Val Loss=1.48853171 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816433 | ‚àö(Val Loss) = 1.22005403 | Current Learning Rate: 0.002\nEpoch 379/1000 | Train Loss=13358.42285156 | Val Loss=1.48852897 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816445 | ‚àö(Val Loss) = 1.22005284 | Current Learning Rate: 0.002\nEpoch 380/1000 | Train Loss=13358.42285156 | Val Loss=1.48852599 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816469 | ‚àö(Val Loss) = 1.22005165 | Current Learning Rate: 0.002\nEpoch 381/1000 | Train Loss=13358.42285156 | Val Loss=1.48852324 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816493 | ‚àö(Val Loss) = 1.22005045 | Current Learning Rate: 0.002\n\n Epoch :  380 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 382/1000 | Train Loss=13358.42285156 | Val Loss=1.48852205 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816493 | ‚àö(Val Loss) = 1.22004998 | Current Learning Rate: 0.002\nEpoch 383/1000 | Train Loss=13358.42382812 | Val Loss=1.48851991 | Data=133.56207275 | Physics=2.21643429 | Val RMSE: 1.20816505 | ‚àö(Val Loss) = 1.22004914 | Current Learning Rate: 0.002\nEpoch 384/1000 | Train Loss=13358.42285156 | Val Loss=1.48851812 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816541 | ‚àö(Val Loss) = 1.22004843 | Current Learning Rate: 0.002\nEpoch 385/1000 | Train Loss=13358.42285156 | Val Loss=1.48851705 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816541 | ‚àö(Val Loss) = 1.22004795 | Current Learning Rate: 0.002\nEpoch 386/1000 | Train Loss=13358.42285156 | Val Loss=1.48851430 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816565 | ‚àö(Val Loss) = 1.22004688 | Current Learning Rate: 0.002\nEpoch 387/1000 | Train Loss=13358.42285156 | Val Loss=1.48851204 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816588 | ‚àö(Val Loss) = 1.22004592 | Current Learning Rate: 0.002\nEpoch 388/1000 | Train Loss=13358.42285156 | Val Loss=1.48850894 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816612 | ‚àö(Val Loss) = 1.22004461 | Current Learning Rate: 0.002\nEpoch 389/1000 | Train Loss=13358.42285156 | Val Loss=1.48850787 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816612 | ‚àö(Val Loss) = 1.22004426 | Current Learning Rate: 0.002\nEpoch 390/1000 | Train Loss=13358.42285156 | Val Loss=1.48850572 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816636 | ‚àö(Val Loss) = 1.22004330 | Current Learning Rate: 0.002\nEpoch 391/1000 | Train Loss=13358.42285156 | Val Loss=1.48850405 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816660 | ‚àö(Val Loss) = 1.22004259 | Current Learning Rate: 0.002\nEpoch 392/1000 | Train Loss=13358.42285156 | Val Loss=1.48850250 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816660 | ‚àö(Val Loss) = 1.22004199 | Current Learning Rate: 0.002\nEpoch 393/1000 | Train Loss=13358.42285156 | Val Loss=1.48849940 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816684 | ‚àö(Val Loss) = 1.22004068 | Current Learning Rate: 0.002\nEpoch 394/1000 | Train Loss=13358.42285156 | Val Loss=1.48849761 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816708 | ‚àö(Val Loss) = 1.22003996 | Current Learning Rate: 0.002\nEpoch 395/1000 | Train Loss=13358.42285156 | Val Loss=1.48849607 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816708 | ‚àö(Val Loss) = 1.22003937 | Current Learning Rate: 0.002\nEpoch 396/1000 | Train Loss=13358.42285156 | Val Loss=1.48849452 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816731 | ‚àö(Val Loss) = 1.22003877 | Current Learning Rate: 0.002\nEpoch 397/1000 | Train Loss=13358.42382812 | Val Loss=1.48849332 | Data=133.56207275 | Physics=2.21643405 | Val RMSE: 1.20816767 | ‚àö(Val Loss) = 1.22003829 | Current Learning Rate: 0.002\nEpoch 398/1000 | Train Loss=13358.42285156 | Val Loss=1.48848927 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816767 | ‚àö(Val Loss) = 1.22003663 | Current Learning Rate: 0.002\nEpoch 399/1000 | Train Loss=13358.42285156 | Val Loss=1.48848808 | Data=133.56205750 | Physics=2.21643381 | Val RMSE: 1.20816791 | ‚àö(Val Loss) = 1.22003615 | Current Learning Rate: 0.002\nEpoch 400/1000 | Train Loss=13358.42285156 | Val Loss=1.48848653 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20816803 | ‚àö(Val Loss) = 1.22003543 | Current Learning Rate: 0.0002\nEpoch 401/1000 | Train Loss=13358.42285156 | Val Loss=1.35495293 | Data=133.56205750 | Physics=2.21643405 | Val RMSE: 1.20579064 | ‚àö(Val Loss) = 1.16402447 | Current Learning Rate: 0.0002\n\n Epoch :  400 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 402/1000 | Train Loss=510.27127075 | Val Loss=1.35580099 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20575929 | ‚àö(Val Loss) = 1.16438866 | Current Learning Rate: 0.0002\nEpoch 403/1000 | Train Loss=510.27120972 | Val Loss=1.35659194 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20573282 | ‚àö(Val Loss) = 1.16472828 | Current Learning Rate: 0.0002\nEpoch 404/1000 | Train Loss=510.27127075 | Val Loss=1.35732424 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20570958 | ‚àö(Val Loss) = 1.16504264 | Current Learning Rate: 0.0002\nEpoch 405/1000 | Train Loss=510.27127075 | Val Loss=1.35799551 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20568931 | ‚àö(Val Loss) = 1.16533065 | Current Learning Rate: 0.0002\nEpoch 406/1000 | Train Loss=510.27127075 | Val Loss=1.35860455 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20567131 | ‚àö(Val Loss) = 1.16559196 | Current Learning Rate: 0.0002\nEpoch 407/1000 | Train Loss=510.27127075 | Val Loss=1.35915208 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20565534 | ‚àö(Val Loss) = 1.16582680 | Current Learning Rate: 0.0002\nEpoch 408/1000 | Train Loss=510.27127075 | Val Loss=1.35963929 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20564127 | ‚àö(Val Loss) = 1.16603577 | Current Learning Rate: 0.0002\nEpoch 409/1000 | Train Loss=510.27127075 | Val Loss=1.36007035 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20562840 | ‚àö(Val Loss) = 1.16622055 | Current Learning Rate: 0.0002\nEpoch 410/1000 | Train Loss=510.27127075 | Val Loss=1.36044979 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20561719 | ‚àö(Val Loss) = 1.16638327 | Current Learning Rate: 0.0002\nEpoch 411/1000 | Train Loss=510.27120972 | Val Loss=1.36078012 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20560718 | ‚àö(Val Loss) = 1.16652477 | Current Learning Rate: 0.0002\nEpoch 412/1000 | Train Loss=510.27127075 | Val Loss=1.36106575 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20559871 | ‚àö(Val Loss) = 1.16664720 | Current Learning Rate: 0.0002\nEpoch 413/1000 | Train Loss=510.27127075 | Val Loss=1.36131108 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20559049 | ‚àö(Val Loss) = 1.16675234 | Current Learning Rate: 0.0002\nEpoch 414/1000 | Train Loss=510.27127075 | Val Loss=1.36151969 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20558369 | ‚àö(Val Loss) = 1.16684175 | Current Learning Rate: 0.0002\nEpoch 415/1000 | Train Loss=510.27127075 | Val Loss=1.36169529 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20557761 | ‚àö(Val Loss) = 1.16691697 | Current Learning Rate: 0.0002\nEpoch 416/1000 | Train Loss=510.27127075 | Val Loss=1.36183989 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20557225 | ‚àö(Val Loss) = 1.16697896 | Current Learning Rate: 0.0002\nEpoch 417/1000 | Train Loss=510.27127075 | Val Loss=1.36195791 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20556724 | ‚àö(Val Loss) = 1.16702950 | Current Learning Rate: 0.0002\nEpoch 418/1000 | Train Loss=510.27120972 | Val Loss=1.36205244 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20556259 | ‚àö(Val Loss) = 1.16707003 | Current Learning Rate: 0.0002\nEpoch 419/1000 | Train Loss=510.27127075 | Val Loss=1.36212611 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20555878 | ‚àö(Val Loss) = 1.16710162 | Current Learning Rate: 0.0002\nEpoch 420/1000 | Train Loss=510.27127075 | Val Loss=1.36217999 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20555520 | ‚àö(Val Loss) = 1.16712463 | Current Learning Rate: 0.0002\nEpoch 421/1000 | Train Loss=510.27127075 | Val Loss=1.36221886 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20555222 | ‚àö(Val Loss) = 1.16714132 | Current Learning Rate: 0.0002\n\n Epoch :  420 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 422/1000 | Train Loss=510.27127075 | Val Loss=1.36224127 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554960 | ‚àö(Val Loss) = 1.16715097 | Current Learning Rate: 0.0002\nEpoch 423/1000 | Train Loss=510.27127075 | Val Loss=1.36225164 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554686 | ‚àö(Val Loss) = 1.16715539 | Current Learning Rate: 0.0002\nEpoch 424/1000 | Train Loss=510.27120972 | Val Loss=1.36225164 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20554507 | ‚àö(Val Loss) = 1.16715539 | Current Learning Rate: 0.0002\nEpoch 425/1000 | Train Loss=510.27127075 | Val Loss=1.36224020 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554304 | ‚àö(Val Loss) = 1.16715050 | Current Learning Rate: 0.0002\nEpoch 426/1000 | Train Loss=510.27127075 | Val Loss=1.36222136 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554149 | ‚àö(Val Loss) = 1.16714239 | Current Learning Rate: 0.0002\nEpoch 427/1000 | Train Loss=510.27127075 | Val Loss=1.36219501 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553994 | ‚àö(Val Loss) = 1.16713107 | Current Learning Rate: 0.0002\nEpoch 428/1000 | Train Loss=510.27127075 | Val Loss=1.36216152 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20553863 | ‚àö(Val Loss) = 1.16711676 | Current Learning Rate: 0.0002\nEpoch 429/1000 | Train Loss=510.27127075 | Val Loss=1.36212289 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553744 | ‚àö(Val Loss) = 1.16710019 | Current Learning Rate: 0.0002\nEpoch 430/1000 | Train Loss=510.27127075 | Val Loss=1.36207938 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553648 | ‚àö(Val Loss) = 1.16708159 | Current Learning Rate: 0.0002\nEpoch 431/1000 | Train Loss=510.27127075 | Val Loss=1.36203325 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553565 | ‚àö(Val Loss) = 1.16706181 | Current Learning Rate: 0.0002\nEpoch 432/1000 | Train Loss=510.27120972 | Val Loss=1.36198163 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553470 | ‚àö(Val Loss) = 1.16703963 | Current Learning Rate: 0.0002\nEpoch 433/1000 | Train Loss=510.27120972 | Val Loss=1.36192942 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553422 | ‚àö(Val Loss) = 1.16701734 | Current Learning Rate: 0.0002\nEpoch 434/1000 | Train Loss=510.27127075 | Val Loss=1.36187279 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20553362 | ‚àö(Val Loss) = 1.16699302 | Current Learning Rate: 0.0002\nEpoch 435/1000 | Train Loss=510.27127075 | Val Loss=1.36181521 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553327 | ‚àö(Val Loss) = 1.16696835 | Current Learning Rate: 0.0002\nEpoch 436/1000 | Train Loss=510.27120972 | Val Loss=1.36175644 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553279 | ‚àö(Val Loss) = 1.16694319 | Current Learning Rate: 0.0002\nEpoch 437/1000 | Train Loss=510.27120972 | Val Loss=1.36169612 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553255 | ‚àö(Val Loss) = 1.16691732 | Current Learning Rate: 0.0002\nEpoch 438/1000 | Train Loss=510.27127075 | Val Loss=1.36163437 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553219 | ‚àö(Val Loss) = 1.16689086 | Current Learning Rate: 0.0002\nEpoch 439/1000 | Train Loss=510.27127075 | Val Loss=1.36157250 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20553172 | ‚àö(Val Loss) = 1.16686440 | Current Learning Rate: 0.0002\nEpoch 440/1000 | Train Loss=510.27127075 | Val Loss=1.36150980 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553172 | ‚àö(Val Loss) = 1.16683757 | Current Learning Rate: 0.0002\nEpoch 441/1000 | Train Loss=510.27120972 | Val Loss=1.36144781 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553136 | ‚àö(Val Loss) = 1.16681099 | Current Learning Rate: 0.0002\n\n Epoch :  440 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 442/1000 | Train Loss=510.27127075 | Val Loss=1.36138475 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20553136 | ‚àö(Val Loss) = 1.16678393 | Current Learning Rate: 0.0002\nEpoch 443/1000 | Train Loss=510.27127075 | Val Loss=1.36132252 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553136 | ‚àö(Val Loss) = 1.16675723 | Current Learning Rate: 0.0002\nEpoch 444/1000 | Train Loss=510.27120972 | Val Loss=1.36126006 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553136 | ‚àö(Val Loss) = 1.16673052 | Current Learning Rate: 0.0002\nEpoch 445/1000 | Train Loss=510.27120972 | Val Loss=1.36119771 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553100 | ‚àö(Val Loss) = 1.16670382 | Current Learning Rate: 0.0002\nEpoch 446/1000 | Train Loss=510.27120972 | Val Loss=1.36113572 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553124 | ‚àö(Val Loss) = 1.16667724 | Current Learning Rate: 0.0002\nEpoch 447/1000 | Train Loss=510.27127075 | Val Loss=1.36107516 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553124 | ‚àö(Val Loss) = 1.16665125 | Current Learning Rate: 0.0002\nEpoch 448/1000 | Train Loss=510.27127075 | Val Loss=1.36101353 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553124 | ‚àö(Val Loss) = 1.16662490 | Current Learning Rate: 0.0002\nEpoch 449/1000 | Train Loss=510.27120972 | Val Loss=1.36095417 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553136 | ‚àö(Val Loss) = 1.16659939 | Current Learning Rate: 0.0002\nEpoch 450/1000 | Train Loss=510.27120972 | Val Loss=1.36089504 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553124 | ‚àö(Val Loss) = 1.16657400 | Current Learning Rate: 0.0002\nEpoch 451/1000 | Train Loss=510.27120972 | Val Loss=1.36083591 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553148 | ‚àö(Val Loss) = 1.16654873 | Current Learning Rate: 0.0002\nEpoch 452/1000 | Train Loss=510.27127075 | Val Loss=1.36077797 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553160 | ‚àö(Val Loss) = 1.16652393 | Current Learning Rate: 0.0002\nEpoch 453/1000 | Train Loss=510.27120972 | Val Loss=1.36071980 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553148 | ‚àö(Val Loss) = 1.16649890 | Current Learning Rate: 0.0002\nEpoch 454/1000 | Train Loss=510.27120972 | Val Loss=1.36066353 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553172 | ‚àö(Val Loss) = 1.16647482 | Current Learning Rate: 0.0002\nEpoch 455/1000 | Train Loss=510.27127075 | Val Loss=1.36060739 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553184 | ‚àö(Val Loss) = 1.16645074 | Current Learning Rate: 0.0002\nEpoch 456/1000 | Train Loss=510.27127075 | Val Loss=1.36055207 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553195 | ‚àö(Val Loss) = 1.16642702 | Current Learning Rate: 0.0002\nEpoch 457/1000 | Train Loss=510.27127075 | Val Loss=1.36049759 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553195 | ‚àö(Val Loss) = 1.16640365 | Current Learning Rate: 0.0002\nEpoch 458/1000 | Train Loss=510.27127075 | Val Loss=1.36044514 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553207 | ‚àö(Val Loss) = 1.16638124 | Current Learning Rate: 0.0002\nEpoch 459/1000 | Train Loss=510.27127075 | Val Loss=1.36039162 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553219 | ‚àö(Val Loss) = 1.16635823 | Current Learning Rate: 0.0002\nEpoch 460/1000 | Train Loss=510.27120972 | Val Loss=1.36033988 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553231 | ‚àö(Val Loss) = 1.16633606 | Current Learning Rate: 0.0002\nEpoch 461/1000 | Train Loss=510.27127075 | Val Loss=1.36028802 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553255 | ‚àö(Val Loss) = 1.16631389 | Current Learning Rate: 0.0002\n\n Epoch :  460 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 462/1000 | Train Loss=510.27120972 | Val Loss=1.36023831 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553255 | ‚àö(Val Loss) = 1.16629255 | Current Learning Rate: 0.0002\nEpoch 463/1000 | Train Loss=510.27120972 | Val Loss=1.36018825 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553267 | ‚àö(Val Loss) = 1.16627109 | Current Learning Rate: 0.0002\nEpoch 464/1000 | Train Loss=510.27127075 | Val Loss=1.36013901 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553279 | ‚àö(Val Loss) = 1.16624999 | Current Learning Rate: 0.0002\nEpoch 465/1000 | Train Loss=510.27120972 | Val Loss=1.36009121 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553291 | ‚àö(Val Loss) = 1.16622949 | Current Learning Rate: 0.0002\nEpoch 466/1000 | Train Loss=510.27127075 | Val Loss=1.36004388 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553303 | ‚àö(Val Loss) = 1.16620922 | Current Learning Rate: 0.0002\nEpoch 467/1000 | Train Loss=510.27127075 | Val Loss=1.35999715 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553315 | ‚àö(Val Loss) = 1.16618919 | Current Learning Rate: 0.0002\nEpoch 468/1000 | Train Loss=510.27120972 | Val Loss=1.35995138 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553303 | ‚àö(Val Loss) = 1.16616952 | Current Learning Rate: 0.0002\nEpoch 469/1000 | Train Loss=510.27120972 | Val Loss=1.35990596 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553315 | ‚àö(Val Loss) = 1.16615009 | Current Learning Rate: 0.0002\nEpoch 470/1000 | Train Loss=510.27127075 | Val Loss=1.35986125 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553327 | ‚àö(Val Loss) = 1.16613090 | Current Learning Rate: 0.0002\nEpoch 471/1000 | Train Loss=510.27120972 | Val Loss=1.35981762 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553339 | ‚àö(Val Loss) = 1.16611218 | Current Learning Rate: 0.0002\nEpoch 472/1000 | Train Loss=510.27127075 | Val Loss=1.35977471 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553350 | ‚àö(Val Loss) = 1.16609383 | Current Learning Rate: 0.0002\nEpoch 473/1000 | Train Loss=510.27120972 | Val Loss=1.35973215 | Data=5.08054781 | Physics=2.21643429 | Val RMSE: 1.20553362 | ‚àö(Val Loss) = 1.16607559 | Current Learning Rate: 0.0002\nEpoch 474/1000 | Train Loss=510.27127075 | Val Loss=1.35969055 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553362 | ‚àö(Val Loss) = 1.16605771 | Current Learning Rate: 0.0002\nEpoch 475/1000 | Train Loss=510.27120972 | Val Loss=1.35964894 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553362 | ‚àö(Val Loss) = 1.16603982 | Current Learning Rate: 0.0002\nEpoch 476/1000 | Train Loss=510.27120972 | Val Loss=1.35960782 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553398 | ‚àö(Val Loss) = 1.16602218 | Current Learning Rate: 0.0002\nEpoch 477/1000 | Train Loss=510.27127075 | Val Loss=1.35956800 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553398 | ‚àö(Val Loss) = 1.16600513 | Current Learning Rate: 0.0002\nEpoch 478/1000 | Train Loss=510.27127075 | Val Loss=1.35952902 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553398 | ‚àö(Val Loss) = 1.16598845 | Current Learning Rate: 0.0002\nEpoch 479/1000 | Train Loss=510.27120972 | Val Loss=1.35948968 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553422 | ‚àö(Val Loss) = 1.16597152 | Current Learning Rate: 0.0002\nEpoch 480/1000 | Train Loss=510.27120972 | Val Loss=1.35945213 | Data=5.08054781 | Physics=2.21643429 | Val RMSE: 1.20553434 | ‚àö(Val Loss) = 1.16595542 | Current Learning Rate: 0.0002\nEpoch 481/1000 | Train Loss=510.27120972 | Val Loss=1.35941398 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553446 | ‚àö(Val Loss) = 1.16593909 | Current Learning Rate: 0.0002\n\n Epoch :  480 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 482/1000 | Train Loss=510.27127075 | Val Loss=1.35937667 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553446 | ‚àö(Val Loss) = 1.16592312 | Current Learning Rate: 0.0002\nEpoch 483/1000 | Train Loss=510.27120972 | Val Loss=1.35934031 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553470 | ‚àö(Val Loss) = 1.16590750 | Current Learning Rate: 0.0002\nEpoch 484/1000 | Train Loss=510.27127075 | Val Loss=1.35930490 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553482 | ‚àö(Val Loss) = 1.16589236 | Current Learning Rate: 0.0002\nEpoch 485/1000 | Train Loss=510.27127075 | Val Loss=1.35926878 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553493 | ‚àö(Val Loss) = 1.16587687 | Current Learning Rate: 0.0002\nEpoch 486/1000 | Train Loss=510.27120972 | Val Loss=1.35923409 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553493 | ‚àö(Val Loss) = 1.16586196 | Current Learning Rate: 0.0002\nEpoch 487/1000 | Train Loss=510.27127075 | Val Loss=1.35919964 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553505 | ‚àö(Val Loss) = 1.16584718 | Current Learning Rate: 0.0002\nEpoch 488/1000 | Train Loss=510.27127075 | Val Loss=1.35916591 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553505 | ‚àö(Val Loss) = 1.16583276 | Current Learning Rate: 0.0002\nEpoch 489/1000 | Train Loss=510.27120972 | Val Loss=1.35913253 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553529 | ‚àö(Val Loss) = 1.16581833 | Current Learning Rate: 0.0002\nEpoch 490/1000 | Train Loss=510.27120972 | Val Loss=1.35909939 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553541 | ‚àö(Val Loss) = 1.16580415 | Current Learning Rate: 0.0002\nEpoch 491/1000 | Train Loss=510.27127075 | Val Loss=1.35906661 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553541 | ‚àö(Val Loss) = 1.16579008 | Current Learning Rate: 0.0002\nEpoch 492/1000 | Train Loss=510.27127075 | Val Loss=1.35903466 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553553 | ‚àö(Val Loss) = 1.16577637 | Current Learning Rate: 0.0002\nEpoch 493/1000 | Train Loss=510.27120972 | Val Loss=1.35900342 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553565 | ‚àö(Val Loss) = 1.16576302 | Current Learning Rate: 0.0002\nEpoch 494/1000 | Train Loss=510.27120972 | Val Loss=1.35897219 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553565 | ‚àö(Val Loss) = 1.16574967 | Current Learning Rate: 0.0002\nEpoch 495/1000 | Train Loss=510.27127075 | Val Loss=1.35894120 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553565 | ‚àö(Val Loss) = 1.16573632 | Current Learning Rate: 0.0002\nEpoch 496/1000 | Train Loss=510.27120972 | Val Loss=1.35891056 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553577 | ‚àö(Val Loss) = 1.16572320 | Current Learning Rate: 0.0002\nEpoch 497/1000 | Train Loss=510.27127075 | Val Loss=1.35888040 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553601 | ‚àö(Val Loss) = 1.16571021 | Current Learning Rate: 0.0002\nEpoch 498/1000 | Train Loss=510.27127075 | Val Loss=1.35885131 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553613 | ‚àö(Val Loss) = 1.16569781 | Current Learning Rate: 0.0002\nEpoch 499/1000 | Train Loss=510.27127075 | Val Loss=1.35882246 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553613 | ‚àö(Val Loss) = 1.16568542 | Current Learning Rate: 0.0002\nEpoch 500/1000 | Train Loss=510.27127075 | Val Loss=1.35879326 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20553625 | ‚àö(Val Loss) = 1.16567290 | Current Learning Rate: 0.0002\nEpoch 501/1000 | Train Loss=510.27120972 | Val Loss=1.35876501 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553637 | ‚àö(Val Loss) = 1.16566074 | Current Learning Rate: 0.0002\n\n Epoch :  500 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 502/1000 | Train Loss=510.27127075 | Val Loss=1.35873699 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20553637 | ‚àö(Val Loss) = 1.16564870 | Current Learning Rate: 0.0002\nEpoch 503/1000 | Train Loss=510.27127075 | Val Loss=1.35870945 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553648 | ‚àö(Val Loss) = 1.16563690 | Current Learning Rate: 0.0002\nEpoch 504/1000 | Train Loss=510.27120972 | Val Loss=1.35868216 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553648 | ‚àö(Val Loss) = 1.16562521 | Current Learning Rate: 0.0002\nEpoch 505/1000 | Train Loss=510.27127075 | Val Loss=1.35865498 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553648 | ‚àö(Val Loss) = 1.16561353 | Current Learning Rate: 0.0002\nEpoch 506/1000 | Train Loss=510.27120972 | Val Loss=1.35862827 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553672 | ‚àö(Val Loss) = 1.16560209 | Current Learning Rate: 0.0002\nEpoch 507/1000 | Train Loss=510.27127075 | Val Loss=1.35860300 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553660 | ‚àö(Val Loss) = 1.16559124 | Current Learning Rate: 0.0002\nEpoch 508/1000 | Train Loss=510.27127075 | Val Loss=1.35857642 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553672 | ‚àö(Val Loss) = 1.16557992 | Current Learning Rate: 0.0002\nEpoch 509/1000 | Train Loss=510.27120972 | Val Loss=1.35855079 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553684 | ‚àö(Val Loss) = 1.16556883 | Current Learning Rate: 0.0002\nEpoch 510/1000 | Train Loss=510.27127075 | Val Loss=1.35852492 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553684 | ‚àö(Val Loss) = 1.16555774 | Current Learning Rate: 0.0002\nEpoch 511/1000 | Train Loss=510.27127075 | Val Loss=1.35850084 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553708 | ‚àö(Val Loss) = 1.16554749 | Current Learning Rate: 0.0002\nEpoch 512/1000 | Train Loss=510.27120972 | Val Loss=1.35847604 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553708 | ‚àö(Val Loss) = 1.16553676 | Current Learning Rate: 0.0002\nEpoch 513/1000 | Train Loss=510.27127075 | Val Loss=1.35845160 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553720 | ‚àö(Val Loss) = 1.16552627 | Current Learning Rate: 0.0002\nEpoch 514/1000 | Train Loss=510.27120972 | Val Loss=1.35842717 | Data=5.08054781 | Physics=2.21643429 | Val RMSE: 1.20553732 | ‚àö(Val Loss) = 1.16551578 | Current Learning Rate: 0.0002\nEpoch 515/1000 | Train Loss=510.27127075 | Val Loss=1.35840297 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553732 | ‚àö(Val Loss) = 1.16550541 | Current Learning Rate: 0.0002\nEpoch 516/1000 | Train Loss=510.27127075 | Val Loss=1.35838044 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553732 | ‚àö(Val Loss) = 1.16549575 | Current Learning Rate: 0.0002\nEpoch 517/1000 | Train Loss=510.27127075 | Val Loss=1.35835707 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553744 | ‚àö(Val Loss) = 1.16548574 | Current Learning Rate: 0.0002\nEpoch 518/1000 | Train Loss=510.27120972 | Val Loss=1.35833418 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553744 | ‚àö(Val Loss) = 1.16547596 | Current Learning Rate: 0.0002\nEpoch 519/1000 | Train Loss=510.27127075 | Val Loss=1.35831082 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553744 | ‚àö(Val Loss) = 1.16546595 | Current Learning Rate: 0.0002\nEpoch 520/1000 | Train Loss=510.27127075 | Val Loss=1.35828805 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553756 | ‚àö(Val Loss) = 1.16545618 | Current Learning Rate: 0.0002\nEpoch 521/1000 | Train Loss=510.27127075 | Val Loss=1.35826755 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20553768 | ‚àö(Val Loss) = 1.16544735 | Current Learning Rate: 0.0002\n\n Epoch :  520 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 522/1000 | Train Loss=510.27120972 | Val Loss=1.35824537 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553768 | ‚àö(Val Loss) = 1.16543782 | Current Learning Rate: 0.0002\nEpoch 523/1000 | Train Loss=510.27127075 | Val Loss=1.35822296 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553768 | ‚àö(Val Loss) = 1.16542828 | Current Learning Rate: 0.0002\nEpoch 524/1000 | Train Loss=510.27120972 | Val Loss=1.35820150 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553792 | ‚àö(Val Loss) = 1.16541898 | Current Learning Rate: 0.0002\nEpoch 525/1000 | Train Loss=510.27120972 | Val Loss=1.35818017 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553792 | ‚àö(Val Loss) = 1.16540992 | Current Learning Rate: 0.0002\nEpoch 526/1000 | Train Loss=510.27127075 | Val Loss=1.35815978 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553803 | ‚àö(Val Loss) = 1.16540110 | Current Learning Rate: 0.0002\nEpoch 527/1000 | Train Loss=510.27120972 | Val Loss=1.35813880 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553803 | ‚àö(Val Loss) = 1.16539216 | Current Learning Rate: 0.0002\nEpoch 528/1000 | Train Loss=510.27120972 | Val Loss=1.35811806 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553803 | ‚àö(Val Loss) = 1.16538322 | Current Learning Rate: 0.0002\nEpoch 529/1000 | Train Loss=510.27120972 | Val Loss=1.35809803 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553815 | ‚àö(Val Loss) = 1.16537464 | Current Learning Rate: 0.0002\nEpoch 530/1000 | Train Loss=510.27120972 | Val Loss=1.35807753 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553827 | ‚àö(Val Loss) = 1.16536582 | Current Learning Rate: 0.0002\nEpoch 531/1000 | Train Loss=510.27127075 | Val Loss=1.35805917 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553827 | ‚àö(Val Loss) = 1.16535795 | Current Learning Rate: 0.0002\nEpoch 532/1000 | Train Loss=510.27127075 | Val Loss=1.35803866 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553827 | ‚àö(Val Loss) = 1.16534913 | Current Learning Rate: 0.0002\nEpoch 533/1000 | Train Loss=510.27120972 | Val Loss=1.35801911 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553839 | ‚àö(Val Loss) = 1.16534078 | Current Learning Rate: 0.0002\nEpoch 534/1000 | Train Loss=510.27127075 | Val Loss=1.35800052 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553827 | ‚àö(Val Loss) = 1.16533279 | Current Learning Rate: 0.0002\nEpoch 535/1000 | Train Loss=510.27127075 | Val Loss=1.35798180 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553839 | ‚àö(Val Loss) = 1.16532481 | Current Learning Rate: 0.0002\nEpoch 536/1000 | Train Loss=510.27127075 | Val Loss=1.35796273 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20553839 | ‚àö(Val Loss) = 1.16531658 | Current Learning Rate: 0.0002\nEpoch 537/1000 | Train Loss=510.27127075 | Val Loss=1.35794449 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553839 | ‚àö(Val Loss) = 1.16530871 | Current Learning Rate: 0.0002\nEpoch 538/1000 | Train Loss=510.27127075 | Val Loss=1.35792601 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553851 | ‚àö(Val Loss) = 1.16530085 | Current Learning Rate: 0.0002\nEpoch 539/1000 | Train Loss=510.27127075 | Val Loss=1.35790706 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553851 | ‚àö(Val Loss) = 1.16529274 | Current Learning Rate: 0.0002\nEpoch 540/1000 | Train Loss=510.27127075 | Val Loss=1.35788953 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553875 | ‚àö(Val Loss) = 1.16528523 | Current Learning Rate: 0.0002\nEpoch 541/1000 | Train Loss=510.27120972 | Val Loss=1.35787165 | Data=5.08054781 | Physics=2.21643429 | Val RMSE: 1.20553875 | ‚àö(Val Loss) = 1.16527748 | Current Learning Rate: 0.0002\n\n Epoch :  540 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 542/1000 | Train Loss=510.27120972 | Val Loss=1.35785389 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553863 | ‚àö(Val Loss) = 1.16526985 | Current Learning Rate: 0.0002\nEpoch 543/1000 | Train Loss=510.27127075 | Val Loss=1.35783660 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553887 | ‚àö(Val Loss) = 1.16526246 | Current Learning Rate: 0.0002\nEpoch 544/1000 | Train Loss=510.27127075 | Val Loss=1.35781920 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553887 | ‚àö(Val Loss) = 1.16525495 | Current Learning Rate: 0.0002\nEpoch 545/1000 | Train Loss=510.27127075 | Val Loss=1.35780168 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553887 | ‚àö(Val Loss) = 1.16524744 | Current Learning Rate: 0.0002\nEpoch 546/1000 | Train Loss=510.27120972 | Val Loss=1.35778558 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553887 | ‚àö(Val Loss) = 1.16524053 | Current Learning Rate: 0.0002\nEpoch 547/1000 | Train Loss=510.27120972 | Val Loss=1.35776877 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553887 | ‚àö(Val Loss) = 1.16523337 | Current Learning Rate: 0.0002\nEpoch 548/1000 | Train Loss=510.27127075 | Val Loss=1.35775256 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553911 | ‚àö(Val Loss) = 1.16522634 | Current Learning Rate: 0.0002\nEpoch 549/1000 | Train Loss=510.27120972 | Val Loss=1.35773575 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553899 | ‚àö(Val Loss) = 1.16521919 | Current Learning Rate: 0.0002\nEpoch 550/1000 | Train Loss=510.27127075 | Val Loss=1.35771954 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553899 | ‚àö(Val Loss) = 1.16521227 | Current Learning Rate: 0.0002\nEpoch 551/1000 | Train Loss=510.27120972 | Val Loss=1.35770333 | Data=5.08054781 | Physics=2.21643429 | Val RMSE: 1.20553911 | ‚àö(Val Loss) = 1.16520524 | Current Learning Rate: 0.0002\nEpoch 552/1000 | Train Loss=510.27127075 | Val Loss=1.35768747 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553911 | ‚àö(Val Loss) = 1.16519845 | Current Learning Rate: 0.0002\nEpoch 553/1000 | Train Loss=510.27127075 | Val Loss=1.35767233 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553911 | ‚àö(Val Loss) = 1.16519201 | Current Learning Rate: 0.0002\nEpoch 554/1000 | Train Loss=510.27127075 | Val Loss=1.35765564 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553923 | ‚àö(Val Loss) = 1.16518486 | Current Learning Rate: 0.0002\nEpoch 555/1000 | Train Loss=510.27127075 | Val Loss=1.35764074 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553923 | ‚àö(Val Loss) = 1.16517842 | Current Learning Rate: 0.0002\nEpoch 556/1000 | Train Loss=510.27120972 | Val Loss=1.35762513 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553911 | ‚àö(Val Loss) = 1.16517174 | Current Learning Rate: 0.0002\nEpoch 557/1000 | Train Loss=510.27127075 | Val Loss=1.35760999 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553946 | ‚àö(Val Loss) = 1.16516519 | Current Learning Rate: 0.0002\nEpoch 558/1000 | Train Loss=510.27120972 | Val Loss=1.35759521 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553946 | ‚àö(Val Loss) = 1.16515887 | Current Learning Rate: 0.0002\nEpoch 559/1000 | Train Loss=510.27120972 | Val Loss=1.35758018 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553946 | ‚àö(Val Loss) = 1.16515243 | Current Learning Rate: 0.0002\nEpoch 560/1000 | Train Loss=510.27127075 | Val Loss=1.35756576 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553946 | ‚àö(Val Loss) = 1.16514623 | Current Learning Rate: 0.0002\nEpoch 561/1000 | Train Loss=510.27127075 | Val Loss=1.35755122 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553946 | ‚àö(Val Loss) = 1.16514003 | Current Learning Rate: 0.0002\n\n Epoch :  560 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 562/1000 | Train Loss=510.27127075 | Val Loss=1.35753644 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553946 | ‚àö(Val Loss) = 1.16513360 | Current Learning Rate: 0.0002\nEpoch 563/1000 | Train Loss=510.27127075 | Val Loss=1.35752249 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553958 | ‚àö(Val Loss) = 1.16512764 | Current Learning Rate: 0.0002\nEpoch 564/1000 | Train Loss=510.27120972 | Val Loss=1.35750747 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20553958 | ‚àö(Val Loss) = 1.16512120 | Current Learning Rate: 0.0002\nEpoch 565/1000 | Train Loss=510.27127075 | Val Loss=1.35749340 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553958 | ‚àö(Val Loss) = 1.16511524 | Current Learning Rate: 0.0002\nEpoch 566/1000 | Train Loss=510.27127075 | Val Loss=1.35748041 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553958 | ‚àö(Val Loss) = 1.16510963 | Current Learning Rate: 0.0002\nEpoch 567/1000 | Train Loss=510.27127075 | Val Loss=1.35746646 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553970 | ‚àö(Val Loss) = 1.16510367 | Current Learning Rate: 0.0002\nEpoch 568/1000 | Train Loss=510.27120972 | Val Loss=1.35745275 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553982 | ‚àö(Val Loss) = 1.16509771 | Current Learning Rate: 0.0002\nEpoch 569/1000 | Train Loss=510.27127075 | Val Loss=1.35743856 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20553970 | ‚àö(Val Loss) = 1.16509163 | Current Learning Rate: 0.0002\nEpoch 570/1000 | Train Loss=510.27127075 | Val Loss=1.35742557 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553994 | ‚àö(Val Loss) = 1.16508603 | Current Learning Rate: 0.0002\nEpoch 571/1000 | Train Loss=510.27127075 | Val Loss=1.35741210 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20553994 | ‚àö(Val Loss) = 1.16508031 | Current Learning Rate: 0.0002\nEpoch 572/1000 | Train Loss=510.27127075 | Val Loss=1.35739887 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20553982 | ‚àö(Val Loss) = 1.16507459 | Current Learning Rate: 0.0002\nEpoch 573/1000 | Train Loss=510.27120972 | Val Loss=1.35738540 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20553982 | ‚àö(Val Loss) = 1.16506886 | Current Learning Rate: 0.0002\nEpoch 574/1000 | Train Loss=510.27127075 | Val Loss=1.35737312 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554006 | ‚àö(Val Loss) = 1.16506362 | Current Learning Rate: 0.0002\nEpoch 575/1000 | Train Loss=510.27120972 | Val Loss=1.35735965 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20554006 | ‚àö(Val Loss) = 1.16505778 | Current Learning Rate: 0.0002\nEpoch 576/1000 | Train Loss=510.27127075 | Val Loss=1.35734701 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20554006 | ‚àö(Val Loss) = 1.16505241 | Current Learning Rate: 0.0002\nEpoch 577/1000 | Train Loss=510.27120972 | Val Loss=1.35733378 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20554018 | ‚àö(Val Loss) = 1.16504669 | Current Learning Rate: 0.0002\nEpoch 578/1000 | Train Loss=510.27127075 | Val Loss=1.35732174 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554018 | ‚àö(Val Loss) = 1.16504157 | Current Learning Rate: 0.0002\nEpoch 579/1000 | Train Loss=510.27127075 | Val Loss=1.35730946 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554018 | ‚àö(Val Loss) = 1.16503620 | Current Learning Rate: 0.0002\nEpoch 580/1000 | Train Loss=510.27127075 | Val Loss=1.35729671 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20554018 | ‚àö(Val Loss) = 1.16503072 | Current Learning Rate: 0.0002\nEpoch 581/1000 | Train Loss=510.27127075 | Val Loss=1.35728467 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554018 | ‚àö(Val Loss) = 1.16502559 | Current Learning Rate: 0.0002\n\n Epoch :  580 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 582/1000 | Train Loss=510.27127075 | Val Loss=1.35727239 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554030 | ‚àö(Val Loss) = 1.16502035 | Current Learning Rate: 0.0002\nEpoch 583/1000 | Train Loss=510.27127075 | Val Loss=1.35726058 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554030 | ‚àö(Val Loss) = 1.16501522 | Current Learning Rate: 0.0002\nEpoch 584/1000 | Train Loss=510.27127075 | Val Loss=1.35724926 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554030 | ‚àö(Val Loss) = 1.16501045 | Current Learning Rate: 0.0002\nEpoch 585/1000 | Train Loss=510.27127075 | Val Loss=1.35723770 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554018 | ‚àö(Val Loss) = 1.16500545 | Current Learning Rate: 0.0002\nEpoch 586/1000 | Train Loss=510.27120972 | Val Loss=1.35722470 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20554030 | ‚àö(Val Loss) = 1.16499984 | Current Learning Rate: 0.0002\nEpoch 587/1000 | Train Loss=510.27127075 | Val Loss=1.35721397 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554030 | ‚àö(Val Loss) = 1.16499531 | Current Learning Rate: 0.0002\nEpoch 588/1000 | Train Loss=510.27127075 | Val Loss=1.35720193 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20554030 | ‚àö(Val Loss) = 1.16499007 | Current Learning Rate: 0.0002\nEpoch 589/1000 | Train Loss=510.27127075 | Val Loss=1.35719049 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554030 | ‚àö(Val Loss) = 1.16498518 | Current Learning Rate: 0.0002\nEpoch 590/1000 | Train Loss=510.27127075 | Val Loss=1.35717952 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20554030 | ‚àö(Val Loss) = 1.16498053 | Current Learning Rate: 0.0002\nEpoch 591/1000 | Train Loss=510.27120972 | Val Loss=1.35716796 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20554054 | ‚àö(Val Loss) = 1.16497552 | Current Learning Rate: 0.0002\nEpoch 592/1000 | Train Loss=510.27120972 | Val Loss=1.35715687 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20554042 | ‚àö(Val Loss) = 1.16497076 | Current Learning Rate: 0.0002\nEpoch 593/1000 | Train Loss=510.27120972 | Val Loss=1.35714591 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20554042 | ‚àö(Val Loss) = 1.16496611 | Current Learning Rate: 0.0002\nEpoch 594/1000 | Train Loss=510.27127075 | Val Loss=1.35713446 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20554030 | ‚àö(Val Loss) = 1.16496110 | Current Learning Rate: 0.0002\nEpoch 595/1000 | Train Loss=510.27120972 | Val Loss=1.35712373 | Data=5.08054781 | Physics=2.21643381 | Val RMSE: 1.20554054 | ‚àö(Val Loss) = 1.16495657 | Current Learning Rate: 0.0002\nEpoch 596/1000 | Train Loss=510.27120972 | Val Loss=1.35711336 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20554054 | ‚àö(Val Loss) = 1.16495204 | Current Learning Rate: 0.0002\nEpoch 597/1000 | Train Loss=510.27127075 | Val Loss=1.35710263 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554054 | ‚àö(Val Loss) = 1.16494751 | Current Learning Rate: 0.0002\nEpoch 598/1000 | Train Loss=510.27127075 | Val Loss=1.35709190 | Data=5.08054829 | Physics=2.21643381 | Val RMSE: 1.20554054 | ‚àö(Val Loss) = 1.16494286 | Current Learning Rate: 0.0002\nEpoch 599/1000 | Train Loss=510.27120972 | Val Loss=1.35708153 | Data=5.08054781 | Physics=2.21643405 | Val RMSE: 1.20554054 | ‚àö(Val Loss) = 1.16493845 | Current Learning Rate: 0.0002\nEpoch 600/1000 | Train Loss=510.27127075 | Val Loss=1.35707116 | Data=5.08054829 | Physics=2.21643405 | Val RMSE: 1.20554054 | ‚àö(Val Loss) = 1.16493392 | Current Learning Rate: 0.0002\nEpoch 601/1000 | Train Loss=510.27127075 | Val Loss=1.35706055 | Data=5.08054829 | Physics=2.21643429 | Val RMSE: 1.20554066 | ‚àö(Val Loss) = 1.16492939 | Current Learning Rate: 0.0002\n\n Epoch :  600 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 602/1000 | Train Loss=1709.66418457 | Val Loss=1.35705054 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554066 | ‚àö(Val Loss) = 1.16492510 | Current Learning Rate: 0.0002\nEpoch 603/1000 | Train Loss=1709.66418457 | Val Loss=1.35704005 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554090 | ‚àö(Val Loss) = 1.16492057 | Current Learning Rate: 0.0002\nEpoch 604/1000 | Train Loss=1709.66418457 | Val Loss=1.35702944 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554078 | ‚àö(Val Loss) = 1.16491604 | Current Learning Rate: 0.0002\nEpoch 605/1000 | Train Loss=1709.66418457 | Val Loss=1.35701942 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554078 | ‚àö(Val Loss) = 1.16491175 | Current Learning Rate: 0.0002\nEpoch 606/1000 | Train Loss=1709.66418457 | Val Loss=1.35700977 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554101 | ‚àö(Val Loss) = 1.16490757 | Current Learning Rate: 0.0002\nEpoch 607/1000 | Train Loss=1709.66418457 | Val Loss=1.35699975 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554090 | ‚àö(Val Loss) = 1.16490328 | Current Learning Rate: 0.0002\nEpoch 608/1000 | Train Loss=1709.66418457 | Val Loss=1.35698962 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20554113 | ‚àö(Val Loss) = 1.16489899 | Current Learning Rate: 0.0002\nEpoch 609/1000 | Train Loss=1709.66418457 | Val Loss=1.35698009 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554113 | ‚àö(Val Loss) = 1.16489494 | Current Learning Rate: 0.0002\nEpoch 610/1000 | Train Loss=1709.66418457 | Val Loss=1.35697019 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554113 | ‚àö(Val Loss) = 1.16489065 | Current Learning Rate: 0.0002\nEpoch 611/1000 | Train Loss=1709.66418457 | Val Loss=1.35696065 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554113 | ‚àö(Val Loss) = 1.16488659 | Current Learning Rate: 0.0002\nEpoch 612/1000 | Train Loss=1709.66418457 | Val Loss=1.35695148 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20554125 | ‚àö(Val Loss) = 1.16488254 | Current Learning Rate: 0.0002\nEpoch 613/1000 | Train Loss=1709.66406250 | Val Loss=1.35694206 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20554113 | ‚àö(Val Loss) = 1.16487861 | Current Learning Rate: 0.0002\nEpoch 614/1000 | Train Loss=1709.66418457 | Val Loss=1.35693228 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554113 | ‚àö(Val Loss) = 1.16487432 | Current Learning Rate: 0.0002\nEpoch 615/1000 | Train Loss=1709.66418457 | Val Loss=1.35692298 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554125 | ‚àö(Val Loss) = 1.16487038 | Current Learning Rate: 0.0002\nEpoch 616/1000 | Train Loss=1709.66418457 | Val Loss=1.35691464 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554125 | ‚àö(Val Loss) = 1.16486681 | Current Learning Rate: 0.0002\nEpoch 617/1000 | Train Loss=1709.66418457 | Val Loss=1.35690534 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554125 | ‚àö(Val Loss) = 1.16486275 | Current Learning Rate: 0.0002\nEpoch 618/1000 | Train Loss=1709.66406250 | Val Loss=1.35689604 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20554125 | ‚àö(Val Loss) = 1.16485882 | Current Learning Rate: 0.0002\nEpoch 619/1000 | Train Loss=1709.66418457 | Val Loss=1.35688651 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554137 | ‚àö(Val Loss) = 1.16485476 | Current Learning Rate: 0.0002\nEpoch 620/1000 | Train Loss=1709.66418457 | Val Loss=1.35687709 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554137 | ‚àö(Val Loss) = 1.16485071 | Current Learning Rate: 0.0002\nEpoch 621/1000 | Train Loss=1709.66418457 | Val Loss=1.35686862 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554137 | ‚àö(Val Loss) = 1.16484702 | Current Learning Rate: 0.0002\n\n Epoch :  620 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 622/1000 | Train Loss=1709.66418457 | Val Loss=1.35685956 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554137 | ‚àö(Val Loss) = 1.16484320 | Current Learning Rate: 0.0002\nEpoch 623/1000 | Train Loss=1709.66418457 | Val Loss=1.35685110 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554137 | ‚àö(Val Loss) = 1.16483951 | Current Learning Rate: 0.0002\nEpoch 624/1000 | Train Loss=1709.66418457 | Val Loss=1.35684252 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554149 | ‚àö(Val Loss) = 1.16483581 | Current Learning Rate: 0.0002\nEpoch 625/1000 | Train Loss=1709.66418457 | Val Loss=1.35683358 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554149 | ‚àö(Val Loss) = 1.16483200 | Current Learning Rate: 0.0002\nEpoch 626/1000 | Train Loss=1709.66418457 | Val Loss=1.35682523 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554149 | ‚àö(Val Loss) = 1.16482842 | Current Learning Rate: 0.0002\nEpoch 627/1000 | Train Loss=1709.66418457 | Val Loss=1.35681641 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554149 | ‚àö(Val Loss) = 1.16482460 | Current Learning Rate: 0.0002\nEpoch 628/1000 | Train Loss=1709.66418457 | Val Loss=1.35680830 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554149 | ‚àö(Val Loss) = 1.16482115 | Current Learning Rate: 0.0002\nEpoch 629/1000 | Train Loss=1709.66418457 | Val Loss=1.35679901 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554149 | ‚àö(Val Loss) = 1.16481709 | Current Learning Rate: 0.0002\nEpoch 630/1000 | Train Loss=1709.66418457 | Val Loss=1.35679126 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554149 | ‚àö(Val Loss) = 1.16481388 | Current Learning Rate: 0.0002\nEpoch 631/1000 | Train Loss=1709.66406250 | Val Loss=1.35678279 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20554161 | ‚àö(Val Loss) = 1.16481018 | Current Learning Rate: 0.0002\nEpoch 632/1000 | Train Loss=1709.66406250 | Val Loss=1.35677373 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20554161 | ‚àö(Val Loss) = 1.16480625 | Current Learning Rate: 0.0002\nEpoch 633/1000 | Train Loss=1709.66406250 | Val Loss=1.35676587 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20554161 | ‚àö(Val Loss) = 1.16480291 | Current Learning Rate: 0.0002\nEpoch 634/1000 | Train Loss=1709.66418457 | Val Loss=1.35675848 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554161 | ‚àö(Val Loss) = 1.16479981 | Current Learning Rate: 0.0002\nEpoch 635/1000 | Train Loss=1709.66418457 | Val Loss=1.35675013 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554161 | ‚àö(Val Loss) = 1.16479623 | Current Learning Rate: 0.0002\nEpoch 636/1000 | Train Loss=1709.66418457 | Val Loss=1.35674226 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554161 | ‚àö(Val Loss) = 1.16479278 | Current Learning Rate: 0.0002\nEpoch 637/1000 | Train Loss=1709.66418457 | Val Loss=1.35673344 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554173 | ‚àö(Val Loss) = 1.16478896 | Current Learning Rate: 0.0002\nEpoch 638/1000 | Train Loss=1709.66418457 | Val Loss=1.35672557 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554173 | ‚àö(Val Loss) = 1.16478562 | Current Learning Rate: 0.0002\nEpoch 639/1000 | Train Loss=1709.66418457 | Val Loss=1.35671926 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554173 | ‚àö(Val Loss) = 1.16478288 | Current Learning Rate: 0.0002\nEpoch 640/1000 | Train Loss=1709.66406250 | Val Loss=1.35671067 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20554173 | ‚àö(Val Loss) = 1.16477919 | Current Learning Rate: 0.0002\nEpoch 641/1000 | Train Loss=1709.66418457 | Val Loss=1.35670292 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554173 | ‚àö(Val Loss) = 1.16477597 | Current Learning Rate: 0.0002\n\n Epoch :  640 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 642/1000 | Train Loss=1709.66418457 | Val Loss=1.35669470 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554173 | ‚àö(Val Loss) = 1.16477239 | Current Learning Rate: 0.0002\nEpoch 643/1000 | Train Loss=1709.66418457 | Val Loss=1.35668778 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554173 | ‚àö(Val Loss) = 1.16476941 | Current Learning Rate: 0.0002\nEpoch 644/1000 | Train Loss=1709.66418457 | Val Loss=1.35667992 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554173 | ‚àö(Val Loss) = 1.16476607 | Current Learning Rate: 0.0002\nEpoch 645/1000 | Train Loss=1709.66418457 | Val Loss=1.35667205 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554197 | ‚àö(Val Loss) = 1.16476262 | Current Learning Rate: 0.0002\nEpoch 646/1000 | Train Loss=1709.66418457 | Val Loss=1.35666585 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554197 | ‚àö(Val Loss) = 1.16475999 | Current Learning Rate: 0.0002\nEpoch 647/1000 | Train Loss=1709.66418457 | Val Loss=1.35665715 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554197 | ‚àö(Val Loss) = 1.16475630 | Current Learning Rate: 0.0002\nEpoch 648/1000 | Train Loss=1709.66418457 | Val Loss=1.35665011 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554197 | ‚àö(Val Loss) = 1.16475320 | Current Learning Rate: 0.0002\nEpoch 649/1000 | Train Loss=1709.66418457 | Val Loss=1.35664296 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554197 | ‚àö(Val Loss) = 1.16475022 | Current Learning Rate: 0.0002\nEpoch 650/1000 | Train Loss=1709.66418457 | Val Loss=1.35663486 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20554185 | ‚àö(Val Loss) = 1.16474664 | Current Learning Rate: 0.0002\nEpoch 651/1000 | Train Loss=1709.66418457 | Val Loss=1.35662806 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554185 | ‚àö(Val Loss) = 1.16474378 | Current Learning Rate: 0.0002\nEpoch 652/1000 | Train Loss=1709.66418457 | Val Loss=1.35662079 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554185 | ‚àö(Val Loss) = 1.16474068 | Current Learning Rate: 0.0002\nEpoch 653/1000 | Train Loss=1709.66406250 | Val Loss=1.35661423 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16473782 | Current Learning Rate: 0.0002\nEpoch 654/1000 | Train Loss=1709.66418457 | Val Loss=1.35660660 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16473460 | Current Learning Rate: 0.0002\nEpoch 655/1000 | Train Loss=1709.66418457 | Val Loss=1.35660017 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16473174 | Current Learning Rate: 0.0002\nEpoch 656/1000 | Train Loss=1709.66418457 | Val Loss=1.35659206 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16472828 | Current Learning Rate: 0.0002\nEpoch 657/1000 | Train Loss=1709.66418457 | Val Loss=1.35658514 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16472530 | Current Learning Rate: 0.0002\nEpoch 658/1000 | Train Loss=1709.66418457 | Val Loss=1.35657895 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16472268 | Current Learning Rate: 0.0002\nEpoch 659/1000 | Train Loss=1709.66418457 | Val Loss=1.35657227 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16471982 | Current Learning Rate: 0.0002\nEpoch 660/1000 | Train Loss=1709.66418457 | Val Loss=1.35656571 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16471696 | Current Learning Rate: 0.0002\nEpoch 661/1000 | Train Loss=1709.66418457 | Val Loss=1.35655880 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16471410 | Current Learning Rate: 0.0002\n\n Epoch :  660 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 662/1000 | Train Loss=1709.66418457 | Val Loss=1.35655093 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16471064 | Current Learning Rate: 0.0002\nEpoch 663/1000 | Train Loss=1709.66418457 | Val Loss=1.35654414 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16470778 | Current Learning Rate: 0.0002\nEpoch 664/1000 | Train Loss=1709.66418457 | Val Loss=1.35653758 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16470492 | Current Learning Rate: 0.0002\nEpoch 665/1000 | Train Loss=1709.66418457 | Val Loss=1.35653162 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16470242 | Current Learning Rate: 0.0002\nEpoch 666/1000 | Train Loss=1709.66418457 | Val Loss=1.35652530 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16469967 | Current Learning Rate: 0.0002\nEpoch 667/1000 | Train Loss=1709.66418457 | Val Loss=1.35651827 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16469669 | Current Learning Rate: 0.0002\nEpoch 668/1000 | Train Loss=1709.66418457 | Val Loss=1.35651135 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16469371 | Current Learning Rate: 0.0002\nEpoch 669/1000 | Train Loss=1709.66418457 | Val Loss=1.35650480 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16469085 | Current Learning Rate: 0.0002\nEpoch 670/1000 | Train Loss=1709.66418457 | Val Loss=1.35649848 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16468811 | Current Learning Rate: 0.0002\nEpoch 671/1000 | Train Loss=1709.66418457 | Val Loss=1.35649264 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16468561 | Current Learning Rate: 0.0002\nEpoch 672/1000 | Train Loss=1709.66418457 | Val Loss=1.35648596 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554209 | ‚àö(Val Loss) = 1.16468275 | Current Learning Rate: 0.0002\nEpoch 673/1000 | Train Loss=1709.66418457 | Val Loss=1.35648000 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16468024 | Current Learning Rate: 0.0002\nEpoch 674/1000 | Train Loss=1709.66418457 | Val Loss=1.35647333 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16467738 | Current Learning Rate: 0.0002\nEpoch 675/1000 | Train Loss=1709.66418457 | Val Loss=1.35646689 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16467464 | Current Learning Rate: 0.0002\nEpoch 676/1000 | Train Loss=1709.66418457 | Val Loss=1.35646117 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16467214 | Current Learning Rate: 0.0002\nEpoch 677/1000 | Train Loss=1709.66418457 | Val Loss=1.35645485 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16466939 | Current Learning Rate: 0.0002\nEpoch 678/1000 | Train Loss=1709.66418457 | Val Loss=1.35644841 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16466665 | Current Learning Rate: 0.0002\nEpoch 679/1000 | Train Loss=1709.66418457 | Val Loss=1.35644281 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16466427 | Current Learning Rate: 0.0002\nEpoch 680/1000 | Train Loss=1709.66418457 | Val Loss=1.35643637 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16466153 | Current Learning Rate: 0.0002\nEpoch 681/1000 | Train Loss=1709.66418457 | Val Loss=1.35643041 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16465890 | Current Learning Rate: 0.0002\n\n Epoch :  680 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 682/1000 | Train Loss=1709.66418457 | Val Loss=1.35642481 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16465652 | Current Learning Rate: 0.0002\nEpoch 683/1000 | Train Loss=1709.66418457 | Val Loss=1.35641825 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554221 | ‚àö(Val Loss) = 1.16465366 | Current Learning Rate: 0.0002\nEpoch 684/1000 | Train Loss=1709.66406250 | Val Loss=1.35641348 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16465163 | Current Learning Rate: 0.0002\nEpoch 685/1000 | Train Loss=1709.66418457 | Val Loss=1.35640717 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16464889 | Current Learning Rate: 0.0002\nEpoch 686/1000 | Train Loss=1709.66418457 | Val Loss=1.35640156 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16464651 | Current Learning Rate: 0.0002\nEpoch 687/1000 | Train Loss=1709.66418457 | Val Loss=1.35639501 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16464376 | Current Learning Rate: 0.0002\nEpoch 688/1000 | Train Loss=1709.66418457 | Val Loss=1.35638952 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16464138 | Current Learning Rate: 0.0002\nEpoch 689/1000 | Train Loss=1709.66418457 | Val Loss=1.35638392 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16463900 | Current Learning Rate: 0.0002\nEpoch 690/1000 | Train Loss=1709.66418457 | Val Loss=1.35637784 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16463637 | Current Learning Rate: 0.0002\nEpoch 691/1000 | Train Loss=1709.66418457 | Val Loss=1.35637271 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16463411 | Current Learning Rate: 0.0002\nEpoch 692/1000 | Train Loss=1709.66418457 | Val Loss=1.35636687 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16463161 | Current Learning Rate: 0.0002\nEpoch 693/1000 | Train Loss=1709.66418457 | Val Loss=1.35636139 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16462934 | Current Learning Rate: 0.0002\nEpoch 694/1000 | Train Loss=1709.66418457 | Val Loss=1.35635591 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16462696 | Current Learning Rate: 0.0002\nEpoch 695/1000 | Train Loss=1709.66418457 | Val Loss=1.35635030 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16462457 | Current Learning Rate: 0.0002\nEpoch 696/1000 | Train Loss=1709.66418457 | Val Loss=1.35634470 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16462207 | Current Learning Rate: 0.0002\nEpoch 697/1000 | Train Loss=1709.66418457 | Val Loss=1.35633934 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16461980 | Current Learning Rate: 0.0002\nEpoch 698/1000 | Train Loss=1709.66418457 | Val Loss=1.35633397 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16461754 | Current Learning Rate: 0.0002\nEpoch 699/1000 | Train Loss=1709.66406250 | Val Loss=1.35632873 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20554233 | ‚àö(Val Loss) = 1.16461527 | Current Learning Rate: 0.0002\nEpoch 700/1000 | Train Loss=1709.66418457 | Val Loss=1.35632288 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554256 | ‚àö(Val Loss) = 1.16461277 | Current Learning Rate: 0.0002\nEpoch 701/1000 | Train Loss=1709.66418457 | Val Loss=1.35631740 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20554256 | ‚àö(Val Loss) = 1.16461039 | Current Learning Rate: 0.0002\n‚úÖ Learning Rate updated to 0.001\n\n Epoch :  700 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 702/1000 | Train Loss=1709.66418457 | Val Loss=1.41721618 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533144 | ‚àö(Val Loss) = 1.19046891 | Current Learning Rate: 0.001\nEpoch 703/1000 | Train Loss=1709.66418457 | Val Loss=1.41713619 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533156 | ‚àö(Val Loss) = 1.19043529 | Current Learning Rate: 0.001\nEpoch 704/1000 | Train Loss=1709.66418457 | Val Loss=1.41705656 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533168 | ‚àö(Val Loss) = 1.19040179 | Current Learning Rate: 0.001\nEpoch 705/1000 | Train Loss=1709.66418457 | Val Loss=1.41697562 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533168 | ‚àö(Val Loss) = 1.19036782 | Current Learning Rate: 0.001\nEpoch 706/1000 | Train Loss=1709.66418457 | Val Loss=1.41689587 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533168 | ‚àö(Val Loss) = 1.19033432 | Current Learning Rate: 0.001\nEpoch 707/1000 | Train Loss=1709.66418457 | Val Loss=1.41681719 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533168 | ‚àö(Val Loss) = 1.19030130 | Current Learning Rate: 0.001\nEpoch 708/1000 | Train Loss=1709.66418457 | Val Loss=1.41673803 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533168 | ‚àö(Val Loss) = 1.19026804 | Current Learning Rate: 0.001\nEpoch 709/1000 | Train Loss=1709.66418457 | Val Loss=1.41665971 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533168 | ‚àö(Val Loss) = 1.19023514 | Current Learning Rate: 0.001\nEpoch 710/1000 | Train Loss=1709.66418457 | Val Loss=1.41658151 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533156 | ‚àö(Val Loss) = 1.19020236 | Current Learning Rate: 0.001\nEpoch 711/1000 | Train Loss=1709.66418457 | Val Loss=1.41650581 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533180 | ‚àö(Val Loss) = 1.19017053 | Current Learning Rate: 0.001\nEpoch 712/1000 | Train Loss=1709.66418457 | Val Loss=1.41642773 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533192 | ‚àö(Val Loss) = 1.19013774 | Current Learning Rate: 0.001\nEpoch 713/1000 | Train Loss=1709.66418457 | Val Loss=1.41635191 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533180 | ‚àö(Val Loss) = 1.19010580 | Current Learning Rate: 0.001\nEpoch 714/1000 | Train Loss=1709.66418457 | Val Loss=1.41627574 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533180 | ‚àö(Val Loss) = 1.19007385 | Current Learning Rate: 0.001\nEpoch 715/1000 | Train Loss=1709.66418457 | Val Loss=1.41620016 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533180 | ‚àö(Val Loss) = 1.19004214 | Current Learning Rate: 0.001\nEpoch 716/1000 | Train Loss=1709.66418457 | Val Loss=1.41612506 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533180 | ‚àö(Val Loss) = 1.19001055 | Current Learning Rate: 0.001\nEpoch 717/1000 | Train Loss=1709.66418457 | Val Loss=1.41605031 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20533180 | ‚àö(Val Loss) = 1.18997908 | Current Learning Rate: 0.001\nEpoch 718/1000 | Train Loss=1709.66418457 | Val Loss=1.41597486 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533180 | ‚àö(Val Loss) = 1.18994737 | Current Learning Rate: 0.001\nEpoch 719/1000 | Train Loss=1709.66406250 | Val Loss=1.41590118 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20533204 | ‚àö(Val Loss) = 1.18991649 | Current Learning Rate: 0.001\nEpoch 720/1000 | Train Loss=1709.66418457 | Val Loss=1.41582799 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533216 | ‚àö(Val Loss) = 1.18988574 | Current Learning Rate: 0.001\nEpoch 721/1000 | Train Loss=1709.66418457 | Val Loss=1.41575468 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533204 | ‚àö(Val Loss) = 1.18985486 | Current Learning Rate: 0.001\n\n Epoch :  720 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 722/1000 | Train Loss=1709.66418457 | Val Loss=1.41568267 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533204 | ‚àö(Val Loss) = 1.18982458 | Current Learning Rate: 0.001\nEpoch 723/1000 | Train Loss=1709.66406250 | Val Loss=1.41561079 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20533204 | ‚àö(Val Loss) = 1.18979442 | Current Learning Rate: 0.001\nEpoch 724/1000 | Train Loss=1709.66418457 | Val Loss=1.41553831 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533204 | ‚àö(Val Loss) = 1.18976402 | Current Learning Rate: 0.001\nEpoch 725/1000 | Train Loss=1709.66418457 | Val Loss=1.41546762 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533204 | ‚àö(Val Loss) = 1.18973422 | Current Learning Rate: 0.001\nEpoch 726/1000 | Train Loss=1709.66406250 | Val Loss=1.41539598 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20533204 | ‚àö(Val Loss) = 1.18970418 | Current Learning Rate: 0.001\nEpoch 727/1000 | Train Loss=1709.66418457 | Val Loss=1.41532576 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533204 | ‚àö(Val Loss) = 1.18967462 | Current Learning Rate: 0.001\nEpoch 728/1000 | Train Loss=1709.66418457 | Val Loss=1.41525447 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533216 | ‚àö(Val Loss) = 1.18964469 | Current Learning Rate: 0.001\nEpoch 729/1000 | Train Loss=1709.66418457 | Val Loss=1.41518450 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18961525 | Current Learning Rate: 0.001\nEpoch 730/1000 | Train Loss=1709.66418457 | Val Loss=1.41511583 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18958640 | Current Learning Rate: 0.001\nEpoch 731/1000 | Train Loss=1709.66418457 | Val Loss=1.41504669 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18955731 | Current Learning Rate: 0.001\nEpoch 732/1000 | Train Loss=1709.66418457 | Val Loss=1.41497767 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533216 | ‚àö(Val Loss) = 1.18952835 | Current Learning Rate: 0.001\nEpoch 733/1000 | Train Loss=1709.66418457 | Val Loss=1.41490924 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533216 | ‚àö(Val Loss) = 1.18949962 | Current Learning Rate: 0.001\nEpoch 734/1000 | Train Loss=1709.66418457 | Val Loss=1.41484177 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533216 | ‚àö(Val Loss) = 1.18947124 | Current Learning Rate: 0.001\nEpoch 735/1000 | Train Loss=1709.66418457 | Val Loss=1.41477346 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533216 | ‚àö(Val Loss) = 1.18944252 | Current Learning Rate: 0.001\nEpoch 736/1000 | Train Loss=1709.66406250 | Val Loss=1.41470647 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20533216 | ‚àö(Val Loss) = 1.18941438 | Current Learning Rate: 0.001\nEpoch 737/1000 | Train Loss=1709.66418457 | Val Loss=1.41463912 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533216 | ‚àö(Val Loss) = 1.18938601 | Current Learning Rate: 0.001\nEpoch 738/1000 | Train Loss=1709.66418457 | Val Loss=1.41457212 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18935788 | Current Learning Rate: 0.001\nEpoch 739/1000 | Train Loss=1709.66418457 | Val Loss=1.41450679 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18933034 | Current Learning Rate: 0.001\nEpoch 740/1000 | Train Loss=1709.66418457 | Val Loss=1.41444016 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18930233 | Current Learning Rate: 0.001\nEpoch 741/1000 | Train Loss=1709.66406250 | Val Loss=1.41437542 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18927515 | Current Learning Rate: 0.001\n\n Epoch :  740 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 742/1000 | Train Loss=1709.66406250 | Val Loss=1.41430986 | Data=17.07447624 | Physics=2.21643429 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18924761 | Current Learning Rate: 0.001\nEpoch 743/1000 | Train Loss=1709.66418457 | Val Loss=1.41424561 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18922055 | Current Learning Rate: 0.001\nEpoch 744/1000 | Train Loss=1709.66418457 | Val Loss=1.41417968 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18919289 | Current Learning Rate: 0.001\nEpoch 745/1000 | Train Loss=1709.66418457 | Val Loss=1.41411579 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18916595 | Current Learning Rate: 0.001\nEpoch 746/1000 | Train Loss=1709.66418457 | Val Loss=1.41405237 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18913937 | Current Learning Rate: 0.001\nEpoch 747/1000 | Train Loss=1709.66418457 | Val Loss=1.41398942 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18911290 | Current Learning Rate: 0.001\nEpoch 748/1000 | Train Loss=1709.66418457 | Val Loss=1.41392553 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18908596 | Current Learning Rate: 0.001\nEpoch 749/1000 | Train Loss=1709.66418457 | Val Loss=1.41386271 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18905962 | Current Learning Rate: 0.001\nEpoch 750/1000 | Train Loss=1709.66418457 | Val Loss=1.41380048 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18903339 | Current Learning Rate: 0.001\nEpoch 751/1000 | Train Loss=1709.66418457 | Val Loss=1.41373742 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18900692 | Current Learning Rate: 0.001\nEpoch 752/1000 | Train Loss=1709.66418457 | Val Loss=1.41367483 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18898058 | Current Learning Rate: 0.001\nEpoch 753/1000 | Train Loss=1709.66406250 | Val Loss=1.41361427 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18895507 | Current Learning Rate: 0.001\nEpoch 754/1000 | Train Loss=1709.66406250 | Val Loss=1.41355360 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18892956 | Current Learning Rate: 0.001\nEpoch 755/1000 | Train Loss=1709.66418457 | Val Loss=1.41349256 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18890393 | Current Learning Rate: 0.001\nEpoch 756/1000 | Train Loss=1709.66406250 | Val Loss=1.41343093 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20533264 | ‚àö(Val Loss) = 1.18887806 | Current Learning Rate: 0.001\nEpoch 757/1000 | Train Loss=1709.66418457 | Val Loss=1.41337132 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18885291 | Current Learning Rate: 0.001\nEpoch 758/1000 | Train Loss=1709.66418457 | Val Loss=1.41331041 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18882728 | Current Learning Rate: 0.001\nEpoch 759/1000 | Train Loss=1709.66418457 | Val Loss=1.41325116 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18880236 | Current Learning Rate: 0.001\nEpoch 760/1000 | Train Loss=1709.66418457 | Val Loss=1.41319132 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18877721 | Current Learning Rate: 0.001\nEpoch 761/1000 | Train Loss=1709.66418457 | Val Loss=1.41313338 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18875289 | Current Learning Rate: 0.001\n\n Epoch :  760 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 762/1000 | Train Loss=1709.66418457 | Val Loss=1.41307414 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18872797 | Current Learning Rate: 0.001\nEpoch 763/1000 | Train Loss=1709.66418457 | Val Loss=1.41301560 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18870330 | Current Learning Rate: 0.001\nEpoch 764/1000 | Train Loss=1709.66418457 | Val Loss=1.41295588 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18867826 | Current Learning Rate: 0.001\nEpoch 765/1000 | Train Loss=1709.66418457 | Val Loss=1.41289878 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18865418 | Current Learning Rate: 0.001\nEpoch 766/1000 | Train Loss=1709.66418457 | Val Loss=1.41284096 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18862987 | Current Learning Rate: 0.001\nEpoch 767/1000 | Train Loss=1709.66418457 | Val Loss=1.41278303 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18860555 | Current Learning Rate: 0.001\nEpoch 768/1000 | Train Loss=1709.66418457 | Val Loss=1.41272688 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18858182 | Current Learning Rate: 0.001\nEpoch 769/1000 | Train Loss=1709.66418457 | Val Loss=1.41266906 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18855751 | Current Learning Rate: 0.001\nEpoch 770/1000 | Train Loss=1709.66418457 | Val Loss=1.41261411 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533240 | ‚àö(Val Loss) = 1.18853438 | Current Learning Rate: 0.001\nEpoch 771/1000 | Train Loss=1709.66418457 | Val Loss=1.41255748 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18851066 | Current Learning Rate: 0.001\nEpoch 772/1000 | Train Loss=1709.66406250 | Val Loss=1.41250086 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18848681 | Current Learning Rate: 0.001\nEpoch 773/1000 | Train Loss=1709.66418457 | Val Loss=1.41244435 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18846297 | Current Learning Rate: 0.001\nEpoch 774/1000 | Train Loss=1709.66418457 | Val Loss=1.41238976 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18844008 | Current Learning Rate: 0.001\nEpoch 775/1000 | Train Loss=1709.66418457 | Val Loss=1.41233468 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18841684 | Current Learning Rate: 0.001\nEpoch 776/1000 | Train Loss=1709.66418457 | Val Loss=1.41227996 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18839383 | Current Learning Rate: 0.001\nEpoch 777/1000 | Train Loss=1709.66418457 | Val Loss=1.41222465 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533264 | ‚àö(Val Loss) = 1.18837059 | Current Learning Rate: 0.001\nEpoch 778/1000 | Train Loss=1709.66418457 | Val Loss=1.41217148 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533264 | ‚àö(Val Loss) = 1.18834817 | Current Learning Rate: 0.001\nEpoch 779/1000 | Train Loss=1709.66406250 | Val Loss=1.41211689 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20533264 | ‚àö(Val Loss) = 1.18832529 | Current Learning Rate: 0.001\nEpoch 780/1000 | Train Loss=1709.66418457 | Val Loss=1.41206229 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533264 | ‚àö(Val Loss) = 1.18830228 | Current Learning Rate: 0.001\nEpoch 781/1000 | Train Loss=1709.66406250 | Val Loss=1.41200888 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20533264 | ‚àö(Val Loss) = 1.18827975 | Current Learning Rate: 0.001\n\n Epoch :  780 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 782/1000 | Train Loss=1709.66418457 | Val Loss=1.41195524 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533264 | ‚àö(Val Loss) = 1.18825722 | Current Learning Rate: 0.001\nEpoch 783/1000 | Train Loss=1709.66406250 | Val Loss=1.41190255 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20533264 | ‚àö(Val Loss) = 1.18823504 | Current Learning Rate: 0.001\nEpoch 784/1000 | Train Loss=1709.66418457 | Val Loss=1.41184938 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18821263 | Current Learning Rate: 0.001\nEpoch 785/1000 | Train Loss=1709.66418457 | Val Loss=1.41179752 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18819082 | Current Learning Rate: 0.001\nEpoch 786/1000 | Train Loss=1709.66418457 | Val Loss=1.41174483 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18816864 | Current Learning Rate: 0.001\nEpoch 787/1000 | Train Loss=1709.66418457 | Val Loss=1.41169345 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18814707 | Current Learning Rate: 0.001\nEpoch 788/1000 | Train Loss=1709.66406250 | Val Loss=1.41164088 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18812490 | Current Learning Rate: 0.001\nEpoch 789/1000 | Train Loss=1709.66418457 | Val Loss=1.41158915 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18810320 | Current Learning Rate: 0.001\nEpoch 790/1000 | Train Loss=1709.66418457 | Val Loss=1.41153765 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18808150 | Current Learning Rate: 0.001\nEpoch 791/1000 | Train Loss=1709.66418457 | Val Loss=1.41148663 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18806005 | Current Learning Rate: 0.001\nEpoch 792/1000 | Train Loss=1709.66418457 | Val Loss=1.41143644 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18803895 | Current Learning Rate: 0.001\nEpoch 793/1000 | Train Loss=1709.66418457 | Val Loss=1.41138554 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533252 | ‚àö(Val Loss) = 1.18801749 | Current Learning Rate: 0.001\nEpoch 794/1000 | Train Loss=1709.66418457 | Val Loss=1.41133475 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18799615 | Current Learning Rate: 0.001\nEpoch 795/1000 | Train Loss=1709.66418457 | Val Loss=1.41128516 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18797529 | Current Learning Rate: 0.001\nEpoch 796/1000 | Train Loss=1709.66406250 | Val Loss=1.41123486 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18795407 | Current Learning Rate: 0.001\nEpoch 797/1000 | Train Loss=1709.66418457 | Val Loss=1.41118431 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18793273 | Current Learning Rate: 0.001\nEpoch 798/1000 | Train Loss=1709.66418457 | Val Loss=1.41113544 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18791223 | Current Learning Rate: 0.001\nEpoch 799/1000 | Train Loss=1709.66418457 | Val Loss=1.41108727 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18789196 | Current Learning Rate: 0.001\nEpoch 800/1000 | Train Loss=1709.66418457 | Val Loss=1.41103745 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20533228 | ‚àö(Val Loss) = 1.18787098 | Current Learning Rate: 0.0001\nEpoch 801/1000 | Train Loss=1709.66418457 | Val Loss=1.35288501 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313589 | Current Learning Rate: 0.0001\n\n Epoch :  800 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 802/1000 | Train Loss=1709.66418457 | Val Loss=1.35288346 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313517 | Current Learning Rate: 0.0001\nEpoch 803/1000 | Train Loss=1709.66418457 | Val Loss=1.35288227 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313469 | Current Learning Rate: 0.0001\nEpoch 804/1000 | Train Loss=1709.66418457 | Val Loss=1.35288131 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313422 | Current Learning Rate: 0.0001\nEpoch 805/1000 | Train Loss=1709.66418457 | Val Loss=1.35288012 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313374 | Current Learning Rate: 0.0001\nEpoch 806/1000 | Train Loss=1709.66418457 | Val Loss=1.35287905 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313326 | Current Learning Rate: 0.0001\nEpoch 807/1000 | Train Loss=1709.66418457 | Val Loss=1.35287762 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313267 | Current Learning Rate: 0.0001\nEpoch 808/1000 | Train Loss=1709.66418457 | Val Loss=1.35287642 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313219 | Current Learning Rate: 0.0001\nEpoch 809/1000 | Train Loss=1709.66418457 | Val Loss=1.35287559 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313183 | Current Learning Rate: 0.0001\nEpoch 810/1000 | Train Loss=1709.66418457 | Val Loss=1.35287440 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313124 | Current Learning Rate: 0.0001\nEpoch 811/1000 | Train Loss=1709.66418457 | Val Loss=1.35287273 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313052 | Current Learning Rate: 0.0001\nEpoch 812/1000 | Train Loss=1709.66418457 | Val Loss=1.35287201 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16313028 | Current Learning Rate: 0.0001\nEpoch 813/1000 | Train Loss=1709.66418457 | Val Loss=1.35287082 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312981 | Current Learning Rate: 0.0001\nEpoch 814/1000 | Train Loss=1709.66418457 | Val Loss=1.35286951 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312921 | Current Learning Rate: 0.0001\nEpoch 815/1000 | Train Loss=1709.66418457 | Val Loss=1.35286844 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312873 | Current Learning Rate: 0.0001\nEpoch 816/1000 | Train Loss=1709.66418457 | Val Loss=1.35286725 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312826 | Current Learning Rate: 0.0001\nEpoch 817/1000 | Train Loss=1709.66418457 | Val Loss=1.35286653 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312790 | Current Learning Rate: 0.0001\nEpoch 818/1000 | Train Loss=1709.66418457 | Val Loss=1.35286546 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312742 | Current Learning Rate: 0.0001\nEpoch 819/1000 | Train Loss=1709.66418457 | Val Loss=1.35286438 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312695 | Current Learning Rate: 0.0001\nEpoch 820/1000 | Train Loss=1709.66418457 | Val Loss=1.35286295 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312635 | Current Learning Rate: 0.0001\nEpoch 821/1000 | Train Loss=1709.66418457 | Val Loss=1.35286105 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312551 | Current Learning Rate: 0.0001\n\n Epoch :  820 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 822/1000 | Train Loss=1709.66418457 | Val Loss=1.35286021 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312516 | Current Learning Rate: 0.0001\nEpoch 823/1000 | Train Loss=1709.66406250 | Val Loss=1.35285938 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312480 | Current Learning Rate: 0.0001\nEpoch 824/1000 | Train Loss=1709.66418457 | Val Loss=1.35285807 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312432 | Current Learning Rate: 0.0001\nEpoch 825/1000 | Train Loss=1709.66418457 | Val Loss=1.35285711 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312385 | Current Learning Rate: 0.0001\nEpoch 826/1000 | Train Loss=1709.66418457 | Val Loss=1.35285628 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312349 | Current Learning Rate: 0.0001\nEpoch 827/1000 | Train Loss=1709.66418457 | Val Loss=1.35285521 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312301 | Current Learning Rate: 0.0001\nEpoch 828/1000 | Train Loss=1709.66418457 | Val Loss=1.35285389 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312253 | Current Learning Rate: 0.0001\nEpoch 829/1000 | Train Loss=1709.66418457 | Val Loss=1.35285330 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312218 | Current Learning Rate: 0.0001\nEpoch 830/1000 | Train Loss=1709.66418457 | Val Loss=1.35285187 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312158 | Current Learning Rate: 0.0001\nEpoch 831/1000 | Train Loss=1709.66418457 | Val Loss=1.35285079 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312110 | Current Learning Rate: 0.0001\nEpoch 832/1000 | Train Loss=1709.66418457 | Val Loss=1.35284960 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312063 | Current Learning Rate: 0.0001\nEpoch 833/1000 | Train Loss=1709.66418457 | Val Loss=1.35284817 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16312003 | Current Learning Rate: 0.0001\nEpoch 834/1000 | Train Loss=1709.66418457 | Val Loss=1.35284746 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311967 | Current Learning Rate: 0.0001\nEpoch 835/1000 | Train Loss=1709.66418457 | Val Loss=1.35284638 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311920 | Current Learning Rate: 0.0001\nEpoch 836/1000 | Train Loss=1709.66418457 | Val Loss=1.35284519 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311872 | Current Learning Rate: 0.0001\nEpoch 837/1000 | Train Loss=1709.66418457 | Val Loss=1.35284460 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311848 | Current Learning Rate: 0.0001\nEpoch 838/1000 | Train Loss=1709.66418457 | Val Loss=1.35284328 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311789 | Current Learning Rate: 0.0001\nEpoch 839/1000 | Train Loss=1709.66418457 | Val Loss=1.35284245 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311753 | Current Learning Rate: 0.0001\nEpoch 840/1000 | Train Loss=1709.66418457 | Val Loss=1.35284162 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311717 | Current Learning Rate: 0.0001\nEpoch 841/1000 | Train Loss=1709.66418457 | Val Loss=1.35284030 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311657 | Current Learning Rate: 0.0001\n\n Epoch :  840 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 842/1000 | Train Loss=1709.66418457 | Val Loss=1.35283971 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311634 | Current Learning Rate: 0.0001\nEpoch 843/1000 | Train Loss=1709.66418457 | Val Loss=1.35283840 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311586 | Current Learning Rate: 0.0001\nEpoch 844/1000 | Train Loss=1709.66418457 | Val Loss=1.35283768 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311550 | Current Learning Rate: 0.0001\nEpoch 845/1000 | Train Loss=1709.66418457 | Val Loss=1.35283577 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311467 | Current Learning Rate: 0.0001\nEpoch 846/1000 | Train Loss=1709.66418457 | Val Loss=1.35283530 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311443 | Current Learning Rate: 0.0001\nEpoch 847/1000 | Train Loss=1709.66418457 | Val Loss=1.35283351 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311371 | Current Learning Rate: 0.0001\nEpoch 848/1000 | Train Loss=1709.66418457 | Val Loss=1.35283339 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311371 | Current Learning Rate: 0.0001\nEpoch 849/1000 | Train Loss=1709.66418457 | Val Loss=1.35283244 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311324 | Current Learning Rate: 0.0001\nEpoch 850/1000 | Train Loss=1709.66418457 | Val Loss=1.35283196 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311300 | Current Learning Rate: 0.0001\nEpoch 851/1000 | Train Loss=1709.66418457 | Val Loss=1.35283041 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311240 | Current Learning Rate: 0.0001\nEpoch 852/1000 | Train Loss=1709.66418457 | Val Loss=1.35282981 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311216 | Current Learning Rate: 0.0001\nEpoch 853/1000 | Train Loss=1709.66418457 | Val Loss=1.35282838 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311157 | Current Learning Rate: 0.0001\nEpoch 854/1000 | Train Loss=1709.66418457 | Val Loss=1.35282755 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311109 | Current Learning Rate: 0.0001\nEpoch 855/1000 | Train Loss=1709.66418457 | Val Loss=1.35282707 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311097 | Current Learning Rate: 0.0001\nEpoch 856/1000 | Train Loss=1709.66418457 | Val Loss=1.35282612 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16311049 | Current Learning Rate: 0.0001\nEpoch 857/1000 | Train Loss=1709.66418457 | Val Loss=1.35282469 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310990 | Current Learning Rate: 0.0001\nEpoch 858/1000 | Train Loss=1709.66418457 | Val Loss=1.35282397 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310966 | Current Learning Rate: 0.0001\nEpoch 859/1000 | Train Loss=1709.66418457 | Val Loss=1.35282254 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310894 | Current Learning Rate: 0.0001\nEpoch 860/1000 | Train Loss=1709.66418457 | Val Loss=1.35282147 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310859 | Current Learning Rate: 0.0001\nEpoch 861/1000 | Train Loss=1709.66418457 | Val Loss=1.35282040 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310811 | Current Learning Rate: 0.0001\n\n Epoch :  860 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 862/1000 | Train Loss=1709.66406250 | Val Loss=1.35282004 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310787 | Current Learning Rate: 0.0001\nEpoch 863/1000 | Train Loss=1709.66418457 | Val Loss=1.35281920 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310751 | Current Learning Rate: 0.0001\nEpoch 864/1000 | Train Loss=1709.66418457 | Val Loss=1.35281861 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310728 | Current Learning Rate: 0.0001\nEpoch 865/1000 | Train Loss=1709.66418457 | Val Loss=1.35281718 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310668 | Current Learning Rate: 0.0001\nEpoch 866/1000 | Train Loss=1709.66418457 | Val Loss=1.35281587 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310608 | Current Learning Rate: 0.0001\nEpoch 867/1000 | Train Loss=1709.66418457 | Val Loss=1.35281515 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310585 | Current Learning Rate: 0.0001\nEpoch 868/1000 | Train Loss=1709.66418457 | Val Loss=1.35281479 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310573 | Current Learning Rate: 0.0001\nEpoch 869/1000 | Train Loss=1709.66418457 | Val Loss=1.35281384 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310525 | Current Learning Rate: 0.0001\nEpoch 870/1000 | Train Loss=1709.66418457 | Val Loss=1.35281301 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310489 | Current Learning Rate: 0.0001\nEpoch 871/1000 | Train Loss=1709.66418457 | Val Loss=1.35281169 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310430 | Current Learning Rate: 0.0001\nEpoch 872/1000 | Train Loss=1709.66406250 | Val Loss=1.35281110 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310406 | Current Learning Rate: 0.0001\nEpoch 873/1000 | Train Loss=1709.66418457 | Val Loss=1.35280991 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310358 | Current Learning Rate: 0.0001\nEpoch 874/1000 | Train Loss=1709.66418457 | Val Loss=1.35280848 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310298 | Current Learning Rate: 0.0001\nEpoch 875/1000 | Train Loss=1709.66418457 | Val Loss=1.35280812 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310275 | Current Learning Rate: 0.0001\nEpoch 876/1000 | Train Loss=1709.66418457 | Val Loss=1.35280752 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310251 | Current Learning Rate: 0.0001\nEpoch 877/1000 | Train Loss=1709.66418457 | Val Loss=1.35280633 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310203 | Current Learning Rate: 0.0001\nEpoch 878/1000 | Train Loss=1709.66418457 | Val Loss=1.35280538 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310167 | Current Learning Rate: 0.0001\nEpoch 879/1000 | Train Loss=1709.66406250 | Val Loss=1.35280442 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310120 | Current Learning Rate: 0.0001\nEpoch 880/1000 | Train Loss=1709.66406250 | Val Loss=1.35280359 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310084 | Current Learning Rate: 0.0001\nEpoch 881/1000 | Train Loss=1709.66418457 | Val Loss=1.35280299 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310060 | Current Learning Rate: 0.0001\n\n Epoch :  880 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 882/1000 | Train Loss=1709.66418457 | Val Loss=1.35280263 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310048 | Current Learning Rate: 0.0001\nEpoch 883/1000 | Train Loss=1709.66418457 | Val Loss=1.35280168 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16310000 | Current Learning Rate: 0.0001\nEpoch 884/1000 | Train Loss=1709.66418457 | Val Loss=1.35280013 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16309941 | Current Learning Rate: 0.0001\nEpoch 885/1000 | Train Loss=1709.66418457 | Val Loss=1.35279942 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16309905 | Current Learning Rate: 0.0001\nEpoch 886/1000 | Train Loss=1709.66418457 | Val Loss=1.35279822 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16309857 | Current Learning Rate: 0.0001\nEpoch 887/1000 | Train Loss=1709.66406250 | Val Loss=1.35279727 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16309810 | Current Learning Rate: 0.0001\nEpoch 888/1000 | Train Loss=1709.66418457 | Val Loss=1.35279644 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16309774 | Current Learning Rate: 0.0001\nEpoch 889/1000 | Train Loss=1709.66418457 | Val Loss=1.35279584 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309750 | Current Learning Rate: 0.0001\nEpoch 890/1000 | Train Loss=1709.66418457 | Val Loss=1.35279512 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309726 | Current Learning Rate: 0.0001\nEpoch 891/1000 | Train Loss=1709.66418457 | Val Loss=1.35279393 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309667 | Current Learning Rate: 0.0001\nEpoch 892/1000 | Train Loss=1709.66418457 | Val Loss=1.35279310 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309631 | Current Learning Rate: 0.0001\nEpoch 893/1000 | Train Loss=1709.66418457 | Val Loss=1.35279298 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309631 | Current Learning Rate: 0.0001\nEpoch 894/1000 | Train Loss=1709.66418457 | Val Loss=1.35279202 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309583 | Current Learning Rate: 0.0001\nEpoch 895/1000 | Train Loss=1709.66418457 | Val Loss=1.35279107 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309547 | Current Learning Rate: 0.0001\nEpoch 896/1000 | Train Loss=1709.66406250 | Val Loss=1.35279036 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309512 | Current Learning Rate: 0.0001\nEpoch 897/1000 | Train Loss=1709.66418457 | Val Loss=1.35279012 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309512 | Current Learning Rate: 0.0001\nEpoch 898/1000 | Train Loss=1709.66406250 | Val Loss=1.35278833 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309428 | Current Learning Rate: 0.0001\nEpoch 899/1000 | Train Loss=1709.66418457 | Val Loss=1.35278726 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309381 | Current Learning Rate: 0.0001\nEpoch 900/1000 | Train Loss=1709.66418457 | Val Loss=1.35278678 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309357 | Current Learning Rate: 0.0001\nEpoch 901/1000 | Train Loss=1709.66418457 | Val Loss=1.35278594 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309321 | Current Learning Rate: 0.0001\n\n Epoch :  900 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 902/1000 | Train Loss=1709.66418457 | Val Loss=1.35278487 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309285 | Current Learning Rate: 0.0001\nEpoch 903/1000 | Train Loss=1709.66406250 | Val Loss=1.35278475 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309273 | Current Learning Rate: 0.0001\nEpoch 904/1000 | Train Loss=1709.66418457 | Val Loss=1.35278344 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309214 | Current Learning Rate: 0.0001\nEpoch 905/1000 | Train Loss=1709.66418457 | Val Loss=1.35278273 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309190 | Current Learning Rate: 0.0001\nEpoch 906/1000 | Train Loss=1709.66418457 | Val Loss=1.35278189 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309154 | Current Learning Rate: 0.0001\nEpoch 907/1000 | Train Loss=1709.66418457 | Val Loss=1.35278141 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16309130 | Current Learning Rate: 0.0001\nEpoch 908/1000 | Train Loss=1709.66418457 | Val Loss=1.35278118 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16309118 | Current Learning Rate: 0.0001\nEpoch 909/1000 | Train Loss=1709.66418457 | Val Loss=1.35277975 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309059 | Current Learning Rate: 0.0001\nEpoch 910/1000 | Train Loss=1709.66418457 | Val Loss=1.35277891 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309023 | Current Learning Rate: 0.0001\nEpoch 911/1000 | Train Loss=1709.66418457 | Val Loss=1.35277855 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16309011 | Current Learning Rate: 0.0001\nEpoch 912/1000 | Train Loss=1709.66418457 | Val Loss=1.35277772 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308975 | Current Learning Rate: 0.0001\nEpoch 913/1000 | Train Loss=1709.66418457 | Val Loss=1.35277641 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308916 | Current Learning Rate: 0.0001\nEpoch 914/1000 | Train Loss=1709.66406250 | Val Loss=1.35277593 | Data=17.07447624 | Physics=2.21643429 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308892 | Current Learning Rate: 0.0001\nEpoch 915/1000 | Train Loss=1709.66418457 | Val Loss=1.35277510 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308856 | Current Learning Rate: 0.0001\nEpoch 916/1000 | Train Loss=1709.66418457 | Val Loss=1.35277426 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308820 | Current Learning Rate: 0.0001\nEpoch 917/1000 | Train Loss=1709.66418457 | Val Loss=1.35277390 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308808 | Current Learning Rate: 0.0001\nEpoch 918/1000 | Train Loss=1709.66418457 | Val Loss=1.35277283 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308761 | Current Learning Rate: 0.0001\nEpoch 919/1000 | Train Loss=1709.66418457 | Val Loss=1.35277152 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308701 | Current Learning Rate: 0.0001\nEpoch 920/1000 | Train Loss=1709.66418457 | Val Loss=1.35277140 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308701 | Current Learning Rate: 0.0001\nEpoch 921/1000 | Train Loss=1709.66418457 | Val Loss=1.35277033 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308653 | Current Learning Rate: 0.0001\n\n Epoch :  920 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 922/1000 | Train Loss=1709.66418457 | Val Loss=1.35276926 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308606 | Current Learning Rate: 0.0001\nEpoch 923/1000 | Train Loss=1709.66418457 | Val Loss=1.35276926 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308606 | Current Learning Rate: 0.0001\nEpoch 924/1000 | Train Loss=1709.66418457 | Val Loss=1.35276830 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556939 | ‚àö(Val Loss) = 1.16308570 | Current Learning Rate: 0.0001\nEpoch 925/1000 | Train Loss=1709.66418457 | Val Loss=1.35276794 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308558 | Current Learning Rate: 0.0001\nEpoch 926/1000 | Train Loss=1709.66418457 | Val Loss=1.35276699 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308510 | Current Learning Rate: 0.0001\nEpoch 927/1000 | Train Loss=1709.66418457 | Val Loss=1.35276651 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308486 | Current Learning Rate: 0.0001\nEpoch 928/1000 | Train Loss=1709.66418457 | Val Loss=1.35276556 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308451 | Current Learning Rate: 0.0001\nEpoch 929/1000 | Train Loss=1709.66406250 | Val Loss=1.35276449 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308403 | Current Learning Rate: 0.0001\nEpoch 930/1000 | Train Loss=1709.66418457 | Val Loss=1.35276353 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308367 | Current Learning Rate: 0.0001\nEpoch 931/1000 | Train Loss=1709.66418457 | Val Loss=1.35276353 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308367 | Current Learning Rate: 0.0001\nEpoch 932/1000 | Train Loss=1709.66418457 | Val Loss=1.35276234 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308308 | Current Learning Rate: 0.0001\nEpoch 933/1000 | Train Loss=1709.66406250 | Val Loss=1.35276198 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308296 | Current Learning Rate: 0.0001\nEpoch 934/1000 | Train Loss=1709.66418457 | Val Loss=1.35276079 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308248 | Current Learning Rate: 0.0001\nEpoch 935/1000 | Train Loss=1709.66418457 | Val Loss=1.35276055 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308236 | Current Learning Rate: 0.0001\nEpoch 936/1000 | Train Loss=1709.66418457 | Val Loss=1.35275996 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308212 | Current Learning Rate: 0.0001\nEpoch 937/1000 | Train Loss=1709.66418457 | Val Loss=1.35275912 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308177 | Current Learning Rate: 0.0001\nEpoch 938/1000 | Train Loss=1709.66418457 | Val Loss=1.35275805 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308129 | Current Learning Rate: 0.0001\nEpoch 939/1000 | Train Loss=1709.66418457 | Val Loss=1.35275793 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308117 | Current Learning Rate: 0.0001\nEpoch 940/1000 | Train Loss=1709.66418457 | Val Loss=1.35275638 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16308057 | Current Learning Rate: 0.0001\nEpoch 941/1000 | Train Loss=1709.66418457 | Val Loss=1.35275650 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16308057 | Current Learning Rate: 0.0001\n\n Epoch :  940 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 942/1000 | Train Loss=1709.66418457 | Val Loss=1.35275519 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16308010 | Current Learning Rate: 0.0001\nEpoch 943/1000 | Train Loss=1709.66418457 | Val Loss=1.35275519 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16308010 | Current Learning Rate: 0.0001\nEpoch 944/1000 | Train Loss=1709.66418457 | Val Loss=1.35275388 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16307950 | Current Learning Rate: 0.0001\nEpoch 945/1000 | Train Loss=1709.66418457 | Val Loss=1.35275376 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16307938 | Current Learning Rate: 0.0001\nEpoch 946/1000 | Train Loss=1709.66418457 | Val Loss=1.35275245 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16307890 | Current Learning Rate: 0.0001\nEpoch 947/1000 | Train Loss=1709.66418457 | Val Loss=1.35275221 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16307878 | Current Learning Rate: 0.0001\nEpoch 948/1000 | Train Loss=1709.66418457 | Val Loss=1.35275137 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16307843 | Current Learning Rate: 0.0001\nEpoch 949/1000 | Train Loss=1709.66418457 | Val Loss=1.35275006 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16307783 | Current Learning Rate: 0.0001\nEpoch 950/1000 | Train Loss=1709.66418457 | Val Loss=1.35274994 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16307783 | Current Learning Rate: 0.0001\nEpoch 951/1000 | Train Loss=1709.66418457 | Val Loss=1.35274887 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556962 | ‚àö(Val Loss) = 1.16307735 | Current Learning Rate: 0.0001\nEpoch 952/1000 | Train Loss=1709.66418457 | Val Loss=1.35274863 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307724 | Current Learning Rate: 0.0001\nEpoch 953/1000 | Train Loss=1709.66418457 | Val Loss=1.35274792 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307688 | Current Learning Rate: 0.0001\nEpoch 954/1000 | Train Loss=1709.66418457 | Val Loss=1.35274684 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307640 | Current Learning Rate: 0.0001\nEpoch 955/1000 | Train Loss=1709.66418457 | Val Loss=1.35274661 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307640 | Current Learning Rate: 0.0001\nEpoch 956/1000 | Train Loss=1709.66418457 | Val Loss=1.35274613 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307616 | Current Learning Rate: 0.0001\nEpoch 957/1000 | Train Loss=1709.66418457 | Val Loss=1.35274494 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307569 | Current Learning Rate: 0.0001\nEpoch 958/1000 | Train Loss=1709.66418457 | Val Loss=1.35274470 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307557 | Current Learning Rate: 0.0001\nEpoch 959/1000 | Train Loss=1709.66418457 | Val Loss=1.35274363 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307509 | Current Learning Rate: 0.0001\nEpoch 960/1000 | Train Loss=1709.66418457 | Val Loss=1.35274327 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307497 | Current Learning Rate: 0.0001\nEpoch 961/1000 | Train Loss=1709.66418457 | Val Loss=1.35274279 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307473 | Current Learning Rate: 0.0001\n\n Epoch :  960 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 962/1000 | Train Loss=1709.66418457 | Val Loss=1.35274184 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307425 | Current Learning Rate: 0.0001\nEpoch 963/1000 | Train Loss=1709.66418457 | Val Loss=1.35274136 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556951 | ‚àö(Val Loss) = 1.16307414 | Current Learning Rate: 0.0001\nEpoch 964/1000 | Train Loss=1709.66418457 | Val Loss=1.35274076 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307390 | Current Learning Rate: 0.0001\nEpoch 965/1000 | Train Loss=1709.66418457 | Val Loss=1.35274029 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307366 | Current Learning Rate: 0.0001\nEpoch 966/1000 | Train Loss=1709.66418457 | Val Loss=1.35273886 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307306 | Current Learning Rate: 0.0001\nEpoch 967/1000 | Train Loss=1709.66418457 | Val Loss=1.35273969 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307342 | Current Learning Rate: 0.0001\nEpoch 968/1000 | Train Loss=1709.66418457 | Val Loss=1.35273838 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307282 | Current Learning Rate: 0.0001\nEpoch 969/1000 | Train Loss=1709.66418457 | Val Loss=1.35273767 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307247 | Current Learning Rate: 0.0001\nEpoch 970/1000 | Train Loss=1709.66418457 | Val Loss=1.35273719 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307235 | Current Learning Rate: 0.0001\nEpoch 971/1000 | Train Loss=1709.66418457 | Val Loss=1.35273659 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307199 | Current Learning Rate: 0.0001\nEpoch 972/1000 | Train Loss=1709.66418457 | Val Loss=1.35273612 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307187 | Current Learning Rate: 0.0001\nEpoch 973/1000 | Train Loss=1709.66418457 | Val Loss=1.35273504 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307139 | Current Learning Rate: 0.0001\nEpoch 974/1000 | Train Loss=1709.66418457 | Val Loss=1.35273457 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307116 | Current Learning Rate: 0.0001\nEpoch 975/1000 | Train Loss=1709.66418457 | Val Loss=1.35273337 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307068 | Current Learning Rate: 0.0001\nEpoch 976/1000 | Train Loss=1709.66418457 | Val Loss=1.35273361 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307080 | Current Learning Rate: 0.0001\nEpoch 977/1000 | Train Loss=1709.66418457 | Val Loss=1.35273254 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16307032 | Current Learning Rate: 0.0001\nEpoch 978/1000 | Train Loss=1709.66406250 | Val Loss=1.35273170 | Data=17.07447624 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306996 | Current Learning Rate: 0.0001\nEpoch 979/1000 | Train Loss=1709.66418457 | Val Loss=1.35273099 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306961 | Current Learning Rate: 0.0001\nEpoch 980/1000 | Train Loss=1709.66418457 | Val Loss=1.35272980 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306913 | Current Learning Rate: 0.0001\nEpoch 981/1000 | Train Loss=1709.66418457 | Val Loss=1.35273063 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306949 | Current Learning Rate: 0.0001\n\n Epoch :  980 \n Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n        [  0.9680,  -3.9820, -14.9830],\n        [  0.9822,  -3.6727, -13.0703],\n        [  0.9841,  -3.7445, -15.6563]]) \n Prediction :  [[  0.97449857  -3.7601511  -13.948477  ]\n [  0.97446585  -3.7601473  -13.948813  ]\n [  0.9744858   -3.7601497  -13.948608  ]\n [  0.9744909   -3.7601502  -13.948557  ]] \n\nFinal Test RMSE:  0.4138225018978119\nEpoch 982/1000 | Train Loss=1709.66418457 | Val Loss=1.35272956 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306901 | Current Learning Rate: 0.0001\nEpoch 983/1000 | Train Loss=1709.66418457 | Val Loss=1.35272908 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306877 | Current Learning Rate: 0.0001\nEpoch 984/1000 | Train Loss=1709.66418457 | Val Loss=1.35272813 | Data=17.07447815 | Physics=2.21643429 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306841 | Current Learning Rate: 0.0001\nEpoch 985/1000 | Train Loss=1709.66418457 | Val Loss=1.35272813 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306841 | Current Learning Rate: 0.0001\nEpoch 986/1000 | Train Loss=1709.66418457 | Val Loss=1.35272682 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306782 | Current Learning Rate: 0.0001\nEpoch 987/1000 | Train Loss=1709.66418457 | Val Loss=1.35272658 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306770 | Current Learning Rate: 0.0001\nEpoch 988/1000 | Train Loss=1709.66418457 | Val Loss=1.35272634 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306770 | Current Learning Rate: 0.0001\nEpoch 989/1000 | Train Loss=1709.66418457 | Val Loss=1.35272539 | Data=17.07447815 | Physics=2.21643381 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306722 | Current Learning Rate: 0.0001\nEpoch 990/1000 | Train Loss=1709.66418457 | Val Loss=1.35272491 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306698 | Current Learning Rate: 0.0001\nEpoch 991/1000 | Train Loss=1709.66418457 | Val Loss=1.35272408 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306663 | Current Learning Rate: 0.0001\nEpoch 992/1000 | Train Loss=1709.66418457 | Val Loss=1.35272348 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306639 | Current Learning Rate: 0.0001\nEpoch 993/1000 | Train Loss=1709.66418457 | Val Loss=1.35272276 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306615 | Current Learning Rate: 0.0001\nEpoch 994/1000 | Train Loss=1709.66418457 | Val Loss=1.35272193 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306579 | Current Learning Rate: 0.0001\nEpoch 995/1000 | Train Loss=1709.66418457 | Val Loss=1.35272169 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306567 | Current Learning Rate: 0.0001\nEpoch 996/1000 | Train Loss=1709.66418457 | Val Loss=1.35272133 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306543 | Current Learning Rate: 0.0001\nEpoch 997/1000 | Train Loss=1709.66418457 | Val Loss=1.35272026 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306508 | Current Learning Rate: 0.0001\nEpoch 998/1000 | Train Loss=1709.66406250 | Val Loss=1.35272026 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306508 | Current Learning Rate: 0.0001\nEpoch 999/1000 | Train Loss=1709.66406250 | Val Loss=1.35271966 | Data=17.07447624 | Physics=2.21643381 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306472 | Current Learning Rate: 0.0001\n‚úÖ Saved last model at epoch 1000 \nEpoch 1000/1000 | Train Loss=1709.66418457 | Val Loss=1.35271931 | Data=17.07447815 | Physics=2.21643405 | Val RMSE: 1.20556927 | ‚àö(Val Loss) = 1.16306460 | Current Learning Rate: 0.0001\n‚úÖ Metrics saved successfully!\nPlot losses after training 3:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Test RMSE :  0.4138225018978119\nCutoff SoH :  0.7\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-8-hust_gompertz_params.csv\n X_['train'] shape : torch.Size([15, 100, 1]) , y_['train'] shape : torch.Size([15, 3]) Ôºåy_2['train'] shape: torch.Size([15, 1])\nload : \n['train']loader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n X_['val'] shape : torch.Size([4, 100, 1]) , y_['val'] shape : torch.Size([4, 3]) Ôºåy_2['val'] shape: torch.Size([4, 1])\nload : \n['val']loader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n X_['test'] shape : torch.Size([2, 100, 1]) , y_['test'] shape : torch.Size([2, 3]) Ôºåy_2['test'] shape: torch.Size([2, 1])\nload : \n['test']loader lengths :  1\n## üß† Model\nLast model window :  last_model_window_100_model_pinn_data_high.pth\nüöÄ Initializing model output to: k=1.0000176429748535, a=-3.344646692276001, b=-9.55548095703125\n‚úÖ Model Output Parameters Initialized!\n##\n        ### üìà Gompertz Function (Physics Law)\n        \n        * `x`: Time (or cycle number)\n        \n        * `k`: Max value (e.g., max capacity)\n        \n        * `a`, `b`: Shape parameters\n## üß† Loss Functions\n\n## ‚öôÔ∏è 1. Data-Informed Loss Function\n        a data loss (what the LSTM learns from data)\n        \n        * Mean Squared Error for Training\n        * RMSE for autoregressive approximation of compound error\n        \n        ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n        You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n        \n        * `alpha`: controls how strongly physics is enforced.\n## üõ†Ô∏è Parameter Strategy\n## üîÅ Training Loop\n‚úÖ Saved best model at epoch 1 (Val Loss = 0.86655736)\nEpoch 1/1000 | Train Loss=8767.91406250 | Val Loss=0.86655736 | Data=87.65299988 | Physics=2.61427051 | Val RMSE: 0.87183142 | ‚àö(Val Loss) = 0.93089062 | Current Learning Rate: 0.002\n\n Epoch :  0 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 2/1000 | Train Loss=8767.72363281 | Val Loss=0.98816657 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86846429 | ‚àö(Val Loss) = 0.99406570 | Current Learning Rate: 0.002\nEpoch 3/1000 | Train Loss=8767.72363281 | Val Loss=0.94845384 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86702186 | ‚àö(Val Loss) = 0.97388595 | Current Learning Rate: 0.002\nEpoch 4/1000 | Train Loss=8767.72460938 | Val Loss=0.92865103 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86635864 | ‚àö(Val Loss) = 0.96366543 | Current Learning Rate: 0.002\nEpoch 5/1000 | Train Loss=8767.72363281 | Val Loss=0.91660005 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86597633 | ‚àö(Val Loss) = 0.95739233 | Current Learning Rate: 0.002\nEpoch 6/1000 | Train Loss=8767.72363281 | Val Loss=0.90844685 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86572862 | ‚àö(Val Loss) = 0.95312476 | Current Learning Rate: 0.002\nEpoch 7/1000 | Train Loss=8767.72460938 | Val Loss=0.90255970 | Data=87.65369415 | Physics=2.35501184 | Val RMSE: 0.86555600 | ‚àö(Val Loss) = 0.95003140 | Current Learning Rate: 0.002\nEpoch 8/1000 | Train Loss=8767.72363281 | Val Loss=0.89811701 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86542898 | ‚àö(Val Loss) = 0.94769037 | Current Learning Rate: 0.002\nEpoch 9/1000 | Train Loss=8767.72460938 | Val Loss=0.89465940 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86533272 | ‚àö(Val Loss) = 0.94586438 | Current Learning Rate: 0.002\nEpoch 10/1000 | Train Loss=8767.72363281 | Val Loss=0.89190668 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86525732 | ‚àö(Val Loss) = 0.94440812 | Current Learning Rate: 0.002\nEpoch 11/1000 | Train Loss=8767.72460938 | Val Loss=0.88967931 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86519665 | ‚àö(Val Loss) = 0.94322813 | Current Learning Rate: 0.002\nEpoch 12/1000 | Train Loss=8767.72460938 | Val Loss=0.88785511 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86514795 | ‚àö(Val Loss) = 0.94226062 | Current Learning Rate: 0.002\nEpoch 13/1000 | Train Loss=8767.72363281 | Val Loss=0.88634866 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86510718 | ‚àö(Val Loss) = 0.94146091 | Current Learning Rate: 0.002\nEpoch 14/1000 | Train Loss=8767.72363281 | Val Loss=0.88509607 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86507338 | ‚àö(Val Loss) = 0.94079542 | Current Learning Rate: 0.002\nEpoch 15/1000 | Train Loss=8767.72460938 | Val Loss=0.88405144 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86504483 | ‚àö(Val Loss) = 0.94024009 | Current Learning Rate: 0.002\nEpoch 16/1000 | Train Loss=8767.72460938 | Val Loss=0.88317978 | Data=87.65369415 | Physics=2.35501184 | Val RMSE: 0.86502051 | ‚àö(Val Loss) = 0.93977648 | Current Learning Rate: 0.002\nEpoch 17/1000 | Train Loss=8767.72363281 | Val Loss=0.88245267 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86499918 | ‚àö(Val Loss) = 0.93938953 | Current Learning Rate: 0.002\nEpoch 18/1000 | Train Loss=8767.72460938 | Val Loss=0.88184857 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86498123 | ‚àö(Val Loss) = 0.93906790 | Current Learning Rate: 0.002\nEpoch 19/1000 | Train Loss=8767.72460938 | Val Loss=0.88135165 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86496562 | ‚àö(Val Loss) = 0.93880332 | Current Learning Rate: 0.002\nEpoch 20/1000 | Train Loss=8767.72460938 | Val Loss=0.88094276 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86495185 | ‚àö(Val Loss) = 0.93858552 | Current Learning Rate: 0.002\nEpoch 21/1000 | Train Loss=8767.72363281 | Val Loss=0.88061351 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86493957 | ‚àö(Val Loss) = 0.93841010 | Current Learning Rate: 0.002\n\n Epoch :  20 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 22/1000 | Train Loss=8767.72460938 | Val Loss=0.88035345 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86492884 | ‚àö(Val Loss) = 0.93827152 | Current Learning Rate: 0.002\nEpoch 23/1000 | Train Loss=8767.72460938 | Val Loss=0.88015431 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86491954 | ‚àö(Val Loss) = 0.93816537 | Current Learning Rate: 0.002\nEpoch 24/1000 | Train Loss=8767.72363281 | Val Loss=0.88000840 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86491102 | ‚àö(Val Loss) = 0.93808764 | Current Learning Rate: 0.002\nEpoch 25/1000 | Train Loss=8767.72460938 | Val Loss=0.87990957 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86490315 | ‚àö(Val Loss) = 0.93803495 | Current Learning Rate: 0.002\nEpoch 26/1000 | Train Loss=8767.72363281 | Val Loss=0.87985247 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86489654 | ‚àö(Val Loss) = 0.93800449 | Current Learning Rate: 0.002\nEpoch 27/1000 | Train Loss=8767.72460938 | Val Loss=0.87983412 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86489022 | ‚àö(Val Loss) = 0.93799472 | Current Learning Rate: 0.002\nEpoch 28/1000 | Train Loss=8767.72460938 | Val Loss=0.87984848 | Data=87.65369415 | Physics=2.35501184 | Val RMSE: 0.86488485 | ‚àö(Val Loss) = 0.93800241 | Current Learning Rate: 0.002\nEpoch 29/1000 | Train Loss=8767.72363281 | Val Loss=0.87989336 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86487913 | ‚àö(Val Loss) = 0.93802631 | Current Learning Rate: 0.002\nEpoch 30/1000 | Train Loss=8767.72460938 | Val Loss=0.87996280 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86487466 | ‚àö(Val Loss) = 0.93806332 | Current Learning Rate: 0.002\nEpoch 31/1000 | Train Loss=8767.72460938 | Val Loss=0.88005698 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86487049 | ‚àö(Val Loss) = 0.93811351 | Current Learning Rate: 0.002\nEpoch 32/1000 | Train Loss=8767.72363281 | Val Loss=0.88017297 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86486638 | ‚àö(Val Loss) = 0.93817532 | Current Learning Rate: 0.002\nEpoch 33/1000 | Train Loss=8767.72460938 | Val Loss=0.88030964 | Data=87.65369415 | Physics=2.35501184 | Val RMSE: 0.86486304 | ‚àö(Val Loss) = 0.93824816 | Current Learning Rate: 0.002\nEpoch 34/1000 | Train Loss=8767.72460938 | Val Loss=0.88046241 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86485934 | ‚àö(Val Loss) = 0.93832958 | Current Learning Rate: 0.002\nEpoch 35/1000 | Train Loss=8767.72363281 | Val Loss=0.88063055 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86485636 | ‚àö(Val Loss) = 0.93841916 | Current Learning Rate: 0.002\nEpoch 36/1000 | Train Loss=8767.72363281 | Val Loss=0.88081259 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86485332 | ‚àö(Val Loss) = 0.93851614 | Current Learning Rate: 0.002\nEpoch 37/1000 | Train Loss=8767.72363281 | Val Loss=0.88100785 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86485106 | ‚àö(Val Loss) = 0.93862021 | Current Learning Rate: 0.002\nEpoch 38/1000 | Train Loss=8767.72460938 | Val Loss=0.88121444 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86484820 | ‚àö(Val Loss) = 0.93873024 | Current Learning Rate: 0.002\nEpoch 39/1000 | Train Loss=8767.72460938 | Val Loss=0.88142914 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86484569 | ‚àö(Val Loss) = 0.93884456 | Current Learning Rate: 0.002\nEpoch 40/1000 | Train Loss=8767.72460938 | Val Loss=0.88165486 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86484349 | ‚àö(Val Loss) = 0.93896478 | Current Learning Rate: 0.002\nEpoch 41/1000 | Train Loss=8767.72363281 | Val Loss=0.88188684 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86484146 | ‚àö(Val Loss) = 0.93908828 | Current Learning Rate: 0.002\n\n Epoch :  40 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 42/1000 | Train Loss=8767.72363281 | Val Loss=0.88212681 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86483943 | ‚àö(Val Loss) = 0.93921608 | Current Learning Rate: 0.002\nEpoch 43/1000 | Train Loss=8767.72363281 | Val Loss=0.88237375 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86483729 | ‚àö(Val Loss) = 0.93934751 | Current Learning Rate: 0.002\nEpoch 44/1000 | Train Loss=8767.72460938 | Val Loss=0.88262457 | Data=87.65369415 | Physics=2.35501184 | Val RMSE: 0.86483514 | ‚àö(Val Loss) = 0.93948102 | Current Learning Rate: 0.002\nEpoch 45/1000 | Train Loss=8767.72363281 | Val Loss=0.88288170 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86483365 | ‚àö(Val Loss) = 0.93961787 | Current Learning Rate: 0.002\nEpoch 46/1000 | Train Loss=8767.72363281 | Val Loss=0.88314289 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86483186 | ‚àö(Val Loss) = 0.93975681 | Current Learning Rate: 0.002\nEpoch 47/1000 | Train Loss=8767.72363281 | Val Loss=0.88340896 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86483002 | ‚àö(Val Loss) = 0.93989837 | Current Learning Rate: 0.002\nEpoch 48/1000 | Train Loss=8767.72363281 | Val Loss=0.88367617 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86482805 | ‚àö(Val Loss) = 0.94004053 | Current Learning Rate: 0.002\nEpoch 49/1000 | Train Loss=8767.72363281 | Val Loss=0.88394803 | Data=87.65367889 | Physics=2.35501160 | Val RMSE: 0.86482686 | ‚àö(Val Loss) = 0.94018513 | Current Learning Rate: 0.002\nEpoch 50/1000 | Train Loss=8767.72460938 | Val Loss=0.88422114 | Data=87.65368652 | Physics=2.35501184 | Val RMSE: 0.86482513 | ‚àö(Val Loss) = 0.94033033 | Current Learning Rate: 0.002\nEpoch 51/1000 | Train Loss=8767.72363281 | Val Loss=0.88449574 | Data=87.65367889 | Physics=2.35501184 | Val RMSE: 0.86482364 | ‚àö(Val Loss) = 0.94047636 | Current Learning Rate: 0.002\nEpoch 52/1000 | Train Loss=8838.53125000 | Val Loss=0.89463824 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86504334 | ‚àö(Val Loss) = 0.94585317 | Current Learning Rate: 0.002\nEpoch 53/1000 | Train Loss=8838.53222656 | Val Loss=0.90286225 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86522233 | ‚àö(Val Loss) = 0.95019066 | Current Learning Rate: 0.002\nEpoch 54/1000 | Train Loss=8838.53125000 | Val Loss=0.90966934 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86536992 | ‚àö(Val Loss) = 0.95376587 | Current Learning Rate: 0.002\nEpoch 55/1000 | Train Loss=8838.53222656 | Val Loss=0.91536766 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86549371 | ‚àö(Val Loss) = 0.95674849 | Current Learning Rate: 0.002\nEpoch 56/1000 | Train Loss=8838.53222656 | Val Loss=0.92017794 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86559772 | ‚àö(Val Loss) = 0.95925903 | Current Learning Rate: 0.002\nEpoch 57/1000 | Train Loss=8838.53125000 | Val Loss=0.92426533 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86568505 | ‚àö(Val Loss) = 0.96138722 | Current Learning Rate: 0.002\nEpoch 58/1000 | Train Loss=8838.53222656 | Val Loss=0.92775607 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86575955 | ‚àö(Val Loss) = 0.96320093 | Current Learning Rate: 0.002\nEpoch 59/1000 | Train Loss=8838.53125000 | Val Loss=0.93075109 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86582291 | ‚àö(Val Loss) = 0.96475440 | Current Learning Rate: 0.002\nEpoch 60/1000 | Train Loss=8838.53125000 | Val Loss=0.93332732 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86587608 | ‚àö(Val Loss) = 0.96608865 | Current Learning Rate: 0.002\nEpoch 61/1000 | Train Loss=8838.53125000 | Val Loss=0.93555462 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86592185 | ‚àö(Val Loss) = 0.96724075 | Current Learning Rate: 0.002\n\n Epoch :  60 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 62/1000 | Train Loss=8838.53125000 | Val Loss=0.93748093 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86596006 | ‚àö(Val Loss) = 0.96823597 | Current Learning Rate: 0.002\nEpoch 63/1000 | Train Loss=8838.53125000 | Val Loss=0.93915528 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86599320 | ‚àö(Val Loss) = 0.96910024 | Current Learning Rate: 0.002\nEpoch 64/1000 | Train Loss=8838.53222656 | Val Loss=0.94061261 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86602068 | ‚àö(Val Loss) = 0.96985185 | Current Learning Rate: 0.002\nEpoch 65/1000 | Train Loss=8838.53125000 | Val Loss=0.94188380 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86604422 | ‚àö(Val Loss) = 0.97050697 | Current Learning Rate: 0.002\nEpoch 66/1000 | Train Loss=8838.53125000 | Val Loss=0.94299489 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86606348 | ‚àö(Val Loss) = 0.97107923 | Current Learning Rate: 0.002\nEpoch 67/1000 | Train Loss=8838.53125000 | Val Loss=0.94397122 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86607969 | ‚àö(Val Loss) = 0.97158182 | Current Learning Rate: 0.002\nEpoch 68/1000 | Train Loss=8838.53222656 | Val Loss=0.94482732 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86609352 | ‚àö(Val Loss) = 0.97202229 | Current Learning Rate: 0.002\nEpoch 69/1000 | Train Loss=8838.53125000 | Val Loss=0.94558316 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86610436 | ‚àö(Val Loss) = 0.97241098 | Current Learning Rate: 0.002\nEpoch 70/1000 | Train Loss=8838.53222656 | Val Loss=0.94625038 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86611307 | ‚àö(Val Loss) = 0.97275400 | Current Learning Rate: 0.002\nEpoch 71/1000 | Train Loss=8838.53125000 | Val Loss=0.94684297 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86612004 | ‚àö(Val Loss) = 0.97305858 | Current Learning Rate: 0.002\nEpoch 72/1000 | Train Loss=8838.53125000 | Val Loss=0.94736868 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86612588 | ‚àö(Val Loss) = 0.97332865 | Current Learning Rate: 0.002\nEpoch 73/1000 | Train Loss=8838.53125000 | Val Loss=0.94783974 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86612982 | ‚àö(Val Loss) = 0.97357059 | Current Learning Rate: 0.002\nEpoch 74/1000 | Train Loss=8838.53222656 | Val Loss=0.94826192 | Data=88.36176300 | Physics=2.35501160 | Val RMSE: 0.86613262 | ‚àö(Val Loss) = 0.97378743 | Current Learning Rate: 0.002\nEpoch 75/1000 | Train Loss=8838.53125000 | Val Loss=0.94864130 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86613446 | ‚àö(Val Loss) = 0.97398221 | Current Learning Rate: 0.002\nEpoch 76/1000 | Train Loss=8838.53222656 | Val Loss=0.94898599 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86613530 | ‚àö(Val Loss) = 0.97415912 | Current Learning Rate: 0.002\nEpoch 77/1000 | Train Loss=8838.53125000 | Val Loss=0.94929928 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86613548 | ‚àö(Val Loss) = 0.97431988 | Current Learning Rate: 0.002\nEpoch 78/1000 | Train Loss=8838.53125000 | Val Loss=0.94958466 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86613548 | ‚àö(Val Loss) = 0.97446632 | Current Learning Rate: 0.002\nEpoch 79/1000 | Train Loss=8838.53222656 | Val Loss=0.94984829 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86613435 | ‚àö(Val Loss) = 0.97460163 | Current Learning Rate: 0.002\nEpoch 80/1000 | Train Loss=8838.53125000 | Val Loss=0.95009315 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86613274 | ‚àö(Val Loss) = 0.97472721 | Current Learning Rate: 0.002\nEpoch 81/1000 | Train Loss=8838.53222656 | Val Loss=0.95032084 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86613071 | ‚àö(Val Loss) = 0.97484398 | Current Learning Rate: 0.002\n\n Epoch :  80 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 82/1000 | Train Loss=8838.53222656 | Val Loss=0.95053291 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86612844 | ‚àö(Val Loss) = 0.97495276 | Current Learning Rate: 0.002\nEpoch 83/1000 | Train Loss=8838.53125000 | Val Loss=0.95073467 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86612582 | ‚àö(Val Loss) = 0.97505623 | Current Learning Rate: 0.002\nEpoch 84/1000 | Train Loss=8838.53125000 | Val Loss=0.95092499 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86612284 | ‚àö(Val Loss) = 0.97515380 | Current Learning Rate: 0.002\nEpoch 85/1000 | Train Loss=8838.53222656 | Val Loss=0.95110804 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86611992 | ‚àö(Val Loss) = 0.97524768 | Current Learning Rate: 0.002\nEpoch 86/1000 | Train Loss=8838.53125000 | Val Loss=0.95128417 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86611664 | ‚àö(Val Loss) = 0.97533798 | Current Learning Rate: 0.002\nEpoch 87/1000 | Train Loss=8838.53222656 | Val Loss=0.95145476 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86611331 | ‚àö(Val Loss) = 0.97542542 | Current Learning Rate: 0.002\nEpoch 88/1000 | Train Loss=8838.53222656 | Val Loss=0.95161963 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86611038 | ‚àö(Val Loss) = 0.97550994 | Current Learning Rate: 0.002\nEpoch 89/1000 | Train Loss=8838.53222656 | Val Loss=0.95178080 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86610681 | ‚àö(Val Loss) = 0.97559255 | Current Learning Rate: 0.002\nEpoch 90/1000 | Train Loss=8838.53222656 | Val Loss=0.95193982 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86610317 | ‚àö(Val Loss) = 0.97567403 | Current Learning Rate: 0.002\nEpoch 91/1000 | Train Loss=8838.53125000 | Val Loss=0.95209628 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86609954 | ‚àö(Val Loss) = 0.97575420 | Current Learning Rate: 0.002\nEpoch 92/1000 | Train Loss=8838.53125000 | Val Loss=0.95225054 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86609584 | ‚àö(Val Loss) = 0.97583324 | Current Learning Rate: 0.002\nEpoch 93/1000 | Train Loss=8838.53125000 | Val Loss=0.95240432 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86609215 | ‚àö(Val Loss) = 0.97591203 | Current Learning Rate: 0.002\nEpoch 94/1000 | Train Loss=8838.53222656 | Val Loss=0.95255673 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86608845 | ‚àö(Val Loss) = 0.97599012 | Current Learning Rate: 0.002\nEpoch 95/1000 | Train Loss=8838.53222656 | Val Loss=0.95270896 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86608469 | ‚àö(Val Loss) = 0.97606814 | Current Learning Rate: 0.002\nEpoch 96/1000 | Train Loss=8838.53125000 | Val Loss=0.95286173 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86608106 | ‚àö(Val Loss) = 0.97614634 | Current Learning Rate: 0.002\nEpoch 97/1000 | Train Loss=8838.53125000 | Val Loss=0.95301402 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86607748 | ‚àö(Val Loss) = 0.97622436 | Current Learning Rate: 0.002\nEpoch 98/1000 | Train Loss=8838.53125000 | Val Loss=0.95316666 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86607373 | ‚àö(Val Loss) = 0.97630256 | Current Learning Rate: 0.002\nEpoch 99/1000 | Train Loss=8838.53222656 | Val Loss=0.95332074 | Data=88.36176300 | Physics=2.35501184 | Val RMSE: 0.86607027 | ‚àö(Val Loss) = 0.97638148 | Current Learning Rate: 0.002\nEpoch 100/1000 | Train Loss=8838.53125000 | Val Loss=0.95347470 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86606669 | ‚àö(Val Loss) = 0.97646028 | Current Learning Rate: 0.002\nEpoch 101/1000 | Train Loss=8838.53125000 | Val Loss=0.95363039 | Data=88.36175537 | Physics=2.35501184 | Val RMSE: 0.86606294 | ‚àö(Val Loss) = 0.97654003 | Current Learning Rate: 0.002\n\n Epoch :  100 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 102/1000 | Train Loss=8838.61523438 | Val Loss=0.95080596 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86606038 | ‚àö(Val Loss) = 0.97509277 | Current Learning Rate: 0.002\nEpoch 103/1000 | Train Loss=8838.61523438 | Val Loss=0.94829416 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86605746 | ‚àö(Val Loss) = 0.97380394 | Current Learning Rate: 0.002\nEpoch 104/1000 | Train Loss=8838.61523438 | Val Loss=0.94605762 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86605495 | ‚àö(Val Loss) = 0.97265494 | Current Learning Rate: 0.002\nEpoch 105/1000 | Train Loss=8838.61523438 | Val Loss=0.94406325 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86605233 | ‚àö(Val Loss) = 0.97162914 | Current Learning Rate: 0.002\nEpoch 106/1000 | Train Loss=8838.61523438 | Val Loss=0.94228452 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86604935 | ‚àö(Val Loss) = 0.97071344 | Current Learning Rate: 0.002\nEpoch 107/1000 | Train Loss=8838.61523438 | Val Loss=0.94069791 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86604673 | ‚àö(Val Loss) = 0.96989584 | Current Learning Rate: 0.002\nEpoch 108/1000 | Train Loss=8838.61523438 | Val Loss=0.93928015 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86604393 | ‚àö(Val Loss) = 0.96916467 | Current Learning Rate: 0.002\nEpoch 109/1000 | Train Loss=8838.61523438 | Val Loss=0.93801212 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86604136 | ‚àö(Val Loss) = 0.96851027 | Current Learning Rate: 0.002\nEpoch 110/1000 | Train Loss=8838.61523438 | Val Loss=0.93687969 | Data=88.36260223 | Physics=2.35501160 | Val RMSE: 0.86603856 | ‚àö(Val Loss) = 0.96792543 | Current Learning Rate: 0.002\nEpoch 111/1000 | Train Loss=8838.61523438 | Val Loss=0.93586719 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86603600 | ‚àö(Val Loss) = 0.96740228 | Current Learning Rate: 0.002\nEpoch 112/1000 | Train Loss=8838.61523438 | Val Loss=0.93496162 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86603332 | ‚àö(Val Loss) = 0.96693414 | Current Learning Rate: 0.002\nEpoch 113/1000 | Train Loss=8838.61523438 | Val Loss=0.93415356 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86603063 | ‚àö(Val Loss) = 0.96651620 | Current Learning Rate: 0.002\nEpoch 114/1000 | Train Loss=8838.61523438 | Val Loss=0.93343180 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86602801 | ‚àö(Val Loss) = 0.96614277 | Current Learning Rate: 0.002\nEpoch 115/1000 | Train Loss=8838.61523438 | Val Loss=0.93278646 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86602539 | ‚àö(Val Loss) = 0.96580869 | Current Learning Rate: 0.002\nEpoch 116/1000 | Train Loss=8838.61523438 | Val Loss=0.93221170 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86602294 | ‚àö(Val Loss) = 0.96551108 | Current Learning Rate: 0.002\nEpoch 117/1000 | Train Loss=8838.61523438 | Val Loss=0.93169832 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86602056 | ‚àö(Val Loss) = 0.96524519 | Current Learning Rate: 0.002\nEpoch 118/1000 | Train Loss=8838.61523438 | Val Loss=0.93124169 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86601800 | ‚àö(Val Loss) = 0.96500868 | Current Learning Rate: 0.002\nEpoch 119/1000 | Train Loss=8838.61523438 | Val Loss=0.93083692 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86601555 | ‚àö(Val Loss) = 0.96479893 | Current Learning Rate: 0.002\nEpoch 120/1000 | Train Loss=8838.61523438 | Val Loss=0.93047673 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86601317 | ‚àö(Val Loss) = 0.96461219 | Current Learning Rate: 0.002\nEpoch 121/1000 | Train Loss=8838.61523438 | Val Loss=0.93015838 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86601138 | ‚àö(Val Loss) = 0.96444720 | Current Learning Rate: 0.002\n\n Epoch :  120 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 122/1000 | Train Loss=8838.61523438 | Val Loss=0.92987692 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86600900 | ‚àö(Val Loss) = 0.96430129 | Current Learning Rate: 0.002\nEpoch 123/1000 | Train Loss=8838.61523438 | Val Loss=0.92962885 | Data=88.36260223 | Physics=2.35501160 | Val RMSE: 0.86600649 | ‚àö(Val Loss) = 0.96417260 | Current Learning Rate: 0.002\nEpoch 124/1000 | Train Loss=8838.61523438 | Val Loss=0.92941207 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86600447 | ‚àö(Val Loss) = 0.96406019 | Current Learning Rate: 0.002\nEpoch 125/1000 | Train Loss=8838.61523438 | Val Loss=0.92922193 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86600208 | ‚àö(Val Loss) = 0.96396160 | Current Learning Rate: 0.002\nEpoch 126/1000 | Train Loss=8838.61523438 | Val Loss=0.92905837 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86600006 | ‚àö(Val Loss) = 0.96387672 | Current Learning Rate: 0.002\nEpoch 127/1000 | Train Loss=8838.61523438 | Val Loss=0.92891586 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86599785 | ‚àö(Val Loss) = 0.96380281 | Current Learning Rate: 0.002\nEpoch 128/1000 | Train Loss=8838.61523438 | Val Loss=0.92879426 | Data=88.36260223 | Physics=2.35501160 | Val RMSE: 0.86599565 | ‚àö(Val Loss) = 0.96373975 | Current Learning Rate: 0.002\nEpoch 129/1000 | Train Loss=8838.61523438 | Val Loss=0.92869192 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86599368 | ‚àö(Val Loss) = 0.96368665 | Current Learning Rate: 0.002\nEpoch 130/1000 | Train Loss=8838.61523438 | Val Loss=0.92860550 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86599153 | ‚àö(Val Loss) = 0.96364176 | Current Learning Rate: 0.002\nEpoch 131/1000 | Train Loss=8838.61523438 | Val Loss=0.92853546 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86598945 | ‚àö(Val Loss) = 0.96360546 | Current Learning Rate: 0.002\nEpoch 132/1000 | Train Loss=8838.61523438 | Val Loss=0.92847824 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86598760 | ‚àö(Val Loss) = 0.96357578 | Current Learning Rate: 0.002\nEpoch 133/1000 | Train Loss=8838.61523438 | Val Loss=0.92843324 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86598563 | ‚àö(Val Loss) = 0.96355242 | Current Learning Rate: 0.002\nEpoch 134/1000 | Train Loss=8838.61523438 | Val Loss=0.92840177 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86598366 | ‚àö(Val Loss) = 0.96353608 | Current Learning Rate: 0.002\nEpoch 135/1000 | Train Loss=8838.61523438 | Val Loss=0.92837912 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86598158 | ‚àö(Val Loss) = 0.96352434 | Current Learning Rate: 0.002\nEpoch 136/1000 | Train Loss=8838.61523438 | Val Loss=0.92836565 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86597991 | ‚àö(Val Loss) = 0.96351731 | Current Learning Rate: 0.002\nEpoch 137/1000 | Train Loss=8838.61523438 | Val Loss=0.92836177 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86597812 | ‚àö(Val Loss) = 0.96351534 | Current Learning Rate: 0.002\nEpoch 138/1000 | Train Loss=8838.61523438 | Val Loss=0.92836535 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86597621 | ‚àö(Val Loss) = 0.96351719 | Current Learning Rate: 0.002\nEpoch 139/1000 | Train Loss=8838.61523438 | Val Loss=0.92837572 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86597455 | ‚àö(Val Loss) = 0.96352255 | Current Learning Rate: 0.002\nEpoch 140/1000 | Train Loss=8838.61523438 | Val Loss=0.92839372 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86597276 | ‚àö(Val Loss) = 0.96353191 | Current Learning Rate: 0.002\nEpoch 141/1000 | Train Loss=8838.61523438 | Val Loss=0.92841560 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86597097 | ‚àö(Val Loss) = 0.96354324 | Current Learning Rate: 0.002\n\n Epoch :  140 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 142/1000 | Train Loss=8838.61523438 | Val Loss=0.92844552 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86596942 | ‚àö(Val Loss) = 0.96355879 | Current Learning Rate: 0.002\nEpoch 143/1000 | Train Loss=8838.61523438 | Val Loss=0.92847818 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86596769 | ‚àö(Val Loss) = 0.96357572 | Current Learning Rate: 0.002\nEpoch 144/1000 | Train Loss=8838.61523438 | Val Loss=0.92851692 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86596602 | ‚àö(Val Loss) = 0.96359581 | Current Learning Rate: 0.002\nEpoch 145/1000 | Train Loss=8838.61523438 | Val Loss=0.92855954 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86596447 | ‚àö(Val Loss) = 0.96361792 | Current Learning Rate: 0.002\nEpoch 146/1000 | Train Loss=8838.61523438 | Val Loss=0.92860603 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86596286 | ‚àö(Val Loss) = 0.96364206 | Current Learning Rate: 0.002\nEpoch 147/1000 | Train Loss=8838.61523438 | Val Loss=0.92865509 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86596119 | ‚àö(Val Loss) = 0.96366751 | Current Learning Rate: 0.002\nEpoch 148/1000 | Train Loss=8838.61523438 | Val Loss=0.92870736 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86595982 | ‚àö(Val Loss) = 0.96369463 | Current Learning Rate: 0.002\nEpoch 149/1000 | Train Loss=8838.61523438 | Val Loss=0.92876256 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86595821 | ‚àö(Val Loss) = 0.96372330 | Current Learning Rate: 0.002\nEpoch 150/1000 | Train Loss=8838.61523438 | Val Loss=0.92882031 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86595666 | ‚àö(Val Loss) = 0.96375322 | Current Learning Rate: 0.002\nEpoch 151/1000 | Train Loss=8838.61523438 | Val Loss=0.92888224 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86595535 | ‚àö(Val Loss) = 0.96378535 | Current Learning Rate: 0.002\nEpoch 152/1000 | Train Loss=8838.61523438 | Val Loss=0.92894411 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86595392 | ‚àö(Val Loss) = 0.96381748 | Current Learning Rate: 0.002\nEpoch 153/1000 | Train Loss=8838.61523438 | Val Loss=0.92900997 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86595231 | ‚àö(Val Loss) = 0.96385163 | Current Learning Rate: 0.002\nEpoch 154/1000 | Train Loss=8838.61523438 | Val Loss=0.92907768 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86595106 | ‚àö(Val Loss) = 0.96388674 | Current Learning Rate: 0.002\nEpoch 155/1000 | Train Loss=8838.61523438 | Val Loss=0.92914569 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86594963 | ‚àö(Val Loss) = 0.96392202 | Current Learning Rate: 0.002\nEpoch 156/1000 | Train Loss=8838.61523438 | Val Loss=0.92921579 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86594832 | ‚àö(Val Loss) = 0.96395838 | Current Learning Rate: 0.002\nEpoch 157/1000 | Train Loss=8838.61523438 | Val Loss=0.92928851 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86594701 | ‚àö(Val Loss) = 0.96399611 | Current Learning Rate: 0.002\nEpoch 158/1000 | Train Loss=8838.61523438 | Val Loss=0.92936093 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86594558 | ‚àö(Val Loss) = 0.96403366 | Current Learning Rate: 0.002\nEpoch 159/1000 | Train Loss=8838.61523438 | Val Loss=0.92943561 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86594439 | ‚àö(Val Loss) = 0.96407241 | Current Learning Rate: 0.002\nEpoch 160/1000 | Train Loss=8838.61523438 | Val Loss=0.92951143 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86594307 | ‚àö(Val Loss) = 0.96411175 | Current Learning Rate: 0.002\nEpoch 161/1000 | Train Loss=8838.61523438 | Val Loss=0.92958713 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86594164 | ‚àö(Val Loss) = 0.96415097 | Current Learning Rate: 0.002\n\n Epoch :  160 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 162/1000 | Train Loss=8838.61523438 | Val Loss=0.92966592 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86594057 | ‚àö(Val Loss) = 0.96419185 | Current Learning Rate: 0.002\nEpoch 163/1000 | Train Loss=8838.61523438 | Val Loss=0.92974412 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86593926 | ‚àö(Val Loss) = 0.96423239 | Current Learning Rate: 0.002\nEpoch 164/1000 | Train Loss=8838.61523438 | Val Loss=0.92982441 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86593825 | ‚àö(Val Loss) = 0.96427405 | Current Learning Rate: 0.002\nEpoch 165/1000 | Train Loss=8838.61523438 | Val Loss=0.92990512 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86593693 | ‚àö(Val Loss) = 0.96431589 | Current Learning Rate: 0.002\nEpoch 166/1000 | Train Loss=8838.61523438 | Val Loss=0.92998672 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86593580 | ‚àö(Val Loss) = 0.96435821 | Current Learning Rate: 0.002\nEpoch 167/1000 | Train Loss=8838.61523438 | Val Loss=0.93006700 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86593467 | ‚àö(Val Loss) = 0.96439981 | Current Learning Rate: 0.002\nEpoch 168/1000 | Train Loss=8838.61523438 | Val Loss=0.93014985 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86593330 | ‚àö(Val Loss) = 0.96444279 | Current Learning Rate: 0.002\nEpoch 169/1000 | Train Loss=8838.61523438 | Val Loss=0.93023342 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86593235 | ‚àö(Val Loss) = 0.96448606 | Current Learning Rate: 0.002\nEpoch 170/1000 | Train Loss=8838.61523438 | Val Loss=0.93031663 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86593109 | ‚àö(Val Loss) = 0.96452922 | Current Learning Rate: 0.002\nEpoch 171/1000 | Train Loss=8838.61523438 | Val Loss=0.93040150 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86593068 | ‚àö(Val Loss) = 0.96457320 | Current Learning Rate: 0.002\nEpoch 172/1000 | Train Loss=8838.61523438 | Val Loss=0.93048692 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86592954 | ‚àö(Val Loss) = 0.96461749 | Current Learning Rate: 0.002\nEpoch 173/1000 | Train Loss=8838.61523438 | Val Loss=0.93057179 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86592859 | ‚àö(Val Loss) = 0.96466148 | Current Learning Rate: 0.002\nEpoch 174/1000 | Train Loss=8838.61523438 | Val Loss=0.93065637 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86592740 | ‚àö(Val Loss) = 0.96470535 | Current Learning Rate: 0.002\nEpoch 175/1000 | Train Loss=8838.61523438 | Val Loss=0.93074256 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86592638 | ‚àö(Val Loss) = 0.96474999 | Current Learning Rate: 0.002\nEpoch 176/1000 | Train Loss=8838.61523438 | Val Loss=0.93083054 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86592531 | ‚àö(Val Loss) = 0.96479559 | Current Learning Rate: 0.002\nEpoch 177/1000 | Train Loss=8838.61523438 | Val Loss=0.93091583 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86592436 | ‚àö(Val Loss) = 0.96483982 | Current Learning Rate: 0.002\nEpoch 178/1000 | Train Loss=8838.61523438 | Val Loss=0.93100446 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86592335 | ‚àö(Val Loss) = 0.96488571 | Current Learning Rate: 0.002\nEpoch 179/1000 | Train Loss=8838.61523438 | Val Loss=0.93109065 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86592239 | ‚àö(Val Loss) = 0.96493042 | Current Learning Rate: 0.002\nEpoch 180/1000 | Train Loss=8838.61523438 | Val Loss=0.93117851 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86592144 | ‚àö(Val Loss) = 0.96497589 | Current Learning Rate: 0.002\nEpoch 181/1000 | Train Loss=8838.61523438 | Val Loss=0.93126696 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86592048 | ‚àö(Val Loss) = 0.96502173 | Current Learning Rate: 0.002\n\n Epoch :  180 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 182/1000 | Train Loss=8838.61523438 | Val Loss=0.93135488 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86591947 | ‚àö(Val Loss) = 0.96506727 | Current Learning Rate: 0.002\nEpoch 183/1000 | Train Loss=8838.61523438 | Val Loss=0.93144447 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86591858 | ‚àö(Val Loss) = 0.96511370 | Current Learning Rate: 0.002\nEpoch 184/1000 | Train Loss=8838.61523438 | Val Loss=0.93153310 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86591756 | ‚àö(Val Loss) = 0.96515960 | Current Learning Rate: 0.002\nEpoch 185/1000 | Train Loss=8838.61523438 | Val Loss=0.93162245 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86591679 | ‚àö(Val Loss) = 0.96520591 | Current Learning Rate: 0.002\nEpoch 186/1000 | Train Loss=8838.61523438 | Val Loss=0.93171185 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86591578 | ‚àö(Val Loss) = 0.96525222 | Current Learning Rate: 0.002\nEpoch 187/1000 | Train Loss=8838.61523438 | Val Loss=0.93180120 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86591500 | ‚àö(Val Loss) = 0.96529847 | Current Learning Rate: 0.002\nEpoch 188/1000 | Train Loss=8838.61523438 | Val Loss=0.93189108 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86591399 | ‚àö(Val Loss) = 0.96534508 | Current Learning Rate: 0.002\nEpoch 189/1000 | Train Loss=8838.61523438 | Val Loss=0.93198007 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86591321 | ‚àö(Val Loss) = 0.96539116 | Current Learning Rate: 0.002\nEpoch 190/1000 | Train Loss=8838.61523438 | Val Loss=0.93207026 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86591220 | ‚àö(Val Loss) = 0.96543783 | Current Learning Rate: 0.002\nEpoch 191/1000 | Train Loss=8838.61523438 | Val Loss=0.93216109 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86591148 | ‚àö(Val Loss) = 0.96548492 | Current Learning Rate: 0.002\nEpoch 192/1000 | Train Loss=8838.61523438 | Val Loss=0.93225151 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86591041 | ‚àö(Val Loss) = 0.96553171 | Current Learning Rate: 0.002\nEpoch 193/1000 | Train Loss=8838.61523438 | Val Loss=0.93234181 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86590976 | ‚àö(Val Loss) = 0.96557850 | Current Learning Rate: 0.002\nEpoch 194/1000 | Train Loss=8838.61523438 | Val Loss=0.93243331 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86590904 | ‚àö(Val Loss) = 0.96562588 | Current Learning Rate: 0.002\nEpoch 195/1000 | Train Loss=8838.61523438 | Val Loss=0.93252444 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86590815 | ‚àö(Val Loss) = 0.96567303 | Current Learning Rate: 0.002\nEpoch 196/1000 | Train Loss=8838.61523438 | Val Loss=0.93261552 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86590737 | ‚àö(Val Loss) = 0.96572018 | Current Learning Rate: 0.002\nEpoch 197/1000 | Train Loss=8838.61523438 | Val Loss=0.93270695 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86590648 | ‚àö(Val Loss) = 0.96576756 | Current Learning Rate: 0.002\nEpoch 198/1000 | Train Loss=8838.61425781 | Val Loss=0.93279833 | Data=88.36258698 | Physics=2.35501184 | Val RMSE: 0.86590576 | ‚àö(Val Loss) = 0.96581483 | Current Learning Rate: 0.002\nEpoch 199/1000 | Train Loss=8838.61523438 | Val Loss=0.93288934 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86590481 | ‚àö(Val Loss) = 0.96586198 | Current Learning Rate: 0.002\nEpoch 200/1000 | Train Loss=8838.61523438 | Val Loss=0.93298191 | Data=88.36260223 | Physics=2.35501160 | Val RMSE: 0.86590433 | ‚àö(Val Loss) = 0.96590990 | Current Learning Rate: 0.002\nEpoch 201/1000 | Train Loss=8838.61523438 | Val Loss=0.93307191 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86590356 | ‚àö(Val Loss) = 0.96595645 | Current Learning Rate: 0.002\n\n Epoch :  200 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 202/1000 | Train Loss=8838.61523438 | Val Loss=0.93316448 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86590272 | ‚àö(Val Loss) = 0.96600437 | Current Learning Rate: 0.002\nEpoch 203/1000 | Train Loss=8838.61523438 | Val Loss=0.93325543 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86590207 | ‚àö(Val Loss) = 0.96605146 | Current Learning Rate: 0.002\nEpoch 204/1000 | Train Loss=8838.61523438 | Val Loss=0.93334872 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86590111 | ‚àö(Val Loss) = 0.96609974 | Current Learning Rate: 0.002\nEpoch 205/1000 | Train Loss=8838.61523438 | Val Loss=0.93344003 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86590052 | ‚àö(Val Loss) = 0.96614701 | Current Learning Rate: 0.002\nEpoch 206/1000 | Train Loss=8838.61523438 | Val Loss=0.93353254 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589986 | ‚àö(Val Loss) = 0.96619487 | Current Learning Rate: 0.002\nEpoch 207/1000 | Train Loss=8838.61523438 | Val Loss=0.93362439 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589903 | ‚àö(Val Loss) = 0.96624243 | Current Learning Rate: 0.002\nEpoch 208/1000 | Train Loss=8838.61523438 | Val Loss=0.93371683 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86589849 | ‚àö(Val Loss) = 0.96629024 | Current Learning Rate: 0.002\nEpoch 209/1000 | Train Loss=8838.61523438 | Val Loss=0.93381041 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589760 | ‚àö(Val Loss) = 0.96633863 | Current Learning Rate: 0.002\nEpoch 210/1000 | Train Loss=8838.61523438 | Val Loss=0.93390268 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589694 | ‚àö(Val Loss) = 0.96638638 | Current Learning Rate: 0.002\nEpoch 211/1000 | Train Loss=8838.61523438 | Val Loss=0.93399465 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589640 | ‚àö(Val Loss) = 0.96643400 | Current Learning Rate: 0.002\nEpoch 212/1000 | Train Loss=8838.61523438 | Val Loss=0.93408757 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589557 | ‚àö(Val Loss) = 0.96648204 | Current Learning Rate: 0.002\nEpoch 213/1000 | Train Loss=8838.61523438 | Val Loss=0.93418014 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589503 | ‚àö(Val Loss) = 0.96652997 | Current Learning Rate: 0.002\nEpoch 214/1000 | Train Loss=8838.61523438 | Val Loss=0.93427265 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589420 | ‚àö(Val Loss) = 0.96657783 | Current Learning Rate: 0.002\nEpoch 215/1000 | Train Loss=8838.61523438 | Val Loss=0.93436486 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589366 | ‚àö(Val Loss) = 0.96662551 | Current Learning Rate: 0.002\nEpoch 216/1000 | Train Loss=8838.61523438 | Val Loss=0.93445814 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589307 | ‚àö(Val Loss) = 0.96667373 | Current Learning Rate: 0.002\nEpoch 217/1000 | Train Loss=8838.61523438 | Val Loss=0.93455148 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589229 | ‚àö(Val Loss) = 0.96672201 | Current Learning Rate: 0.002\nEpoch 218/1000 | Train Loss=8838.61523438 | Val Loss=0.93464351 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86589175 | ‚àö(Val Loss) = 0.96676964 | Current Learning Rate: 0.002\nEpoch 219/1000 | Train Loss=8838.61523438 | Val Loss=0.93473643 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589116 | ‚àö(Val Loss) = 0.96681768 | Current Learning Rate: 0.002\nEpoch 220/1000 | Train Loss=8838.61523438 | Val Loss=0.93482876 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86589044 | ‚àö(Val Loss) = 0.96686542 | Current Learning Rate: 0.002\nEpoch 221/1000 | Train Loss=8838.61523438 | Val Loss=0.93492293 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588991 | ‚àö(Val Loss) = 0.96691412 | Current Learning Rate: 0.002\n\n Epoch :  220 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 222/1000 | Train Loss=8838.61425781 | Val Loss=0.93501580 | Data=88.36258698 | Physics=2.35501184 | Val RMSE: 0.86588907 | ‚àö(Val Loss) = 0.96696216 | Current Learning Rate: 0.002\nEpoch 223/1000 | Train Loss=8838.61523438 | Val Loss=0.93510878 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588866 | ‚àö(Val Loss) = 0.96701020 | Current Learning Rate: 0.002\nEpoch 224/1000 | Train Loss=8838.61523438 | Val Loss=0.93520194 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588812 | ‚àö(Val Loss) = 0.96705842 | Current Learning Rate: 0.002\nEpoch 225/1000 | Train Loss=8838.61523438 | Val Loss=0.93529475 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86588728 | ‚àö(Val Loss) = 0.96710640 | Current Learning Rate: 0.002\nEpoch 226/1000 | Train Loss=8838.61523438 | Val Loss=0.93538785 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588693 | ‚àö(Val Loss) = 0.96715450 | Current Learning Rate: 0.002\nEpoch 227/1000 | Train Loss=8838.61523438 | Val Loss=0.93548137 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588627 | ‚àö(Val Loss) = 0.96720284 | Current Learning Rate: 0.002\nEpoch 228/1000 | Train Loss=8838.61523438 | Val Loss=0.93557382 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588562 | ‚àö(Val Loss) = 0.96725065 | Current Learning Rate: 0.002\nEpoch 229/1000 | Train Loss=8838.61523438 | Val Loss=0.93566787 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588526 | ‚àö(Val Loss) = 0.96729928 | Current Learning Rate: 0.002\nEpoch 230/1000 | Train Loss=8838.61523438 | Val Loss=0.93576056 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588466 | ‚àö(Val Loss) = 0.96734720 | Current Learning Rate: 0.002\nEpoch 231/1000 | Train Loss=8838.61523438 | Val Loss=0.93585420 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588395 | ‚àö(Val Loss) = 0.96739560 | Current Learning Rate: 0.002\nEpoch 232/1000 | Train Loss=8838.61523438 | Val Loss=0.93594766 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86588341 | ‚àö(Val Loss) = 0.96744388 | Current Learning Rate: 0.002\nEpoch 233/1000 | Train Loss=8838.61523438 | Val Loss=0.93604046 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588293 | ‚àö(Val Loss) = 0.96749187 | Current Learning Rate: 0.002\nEpoch 234/1000 | Train Loss=8838.61523438 | Val Loss=0.93613368 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588228 | ‚àö(Val Loss) = 0.96754003 | Current Learning Rate: 0.002\nEpoch 235/1000 | Train Loss=8838.61523438 | Val Loss=0.93622679 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86588180 | ‚àö(Val Loss) = 0.96758813 | Current Learning Rate: 0.002\nEpoch 236/1000 | Train Loss=8838.61523438 | Val Loss=0.93632025 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588132 | ‚àö(Val Loss) = 0.96763641 | Current Learning Rate: 0.002\nEpoch 237/1000 | Train Loss=8838.61523438 | Val Loss=0.93641347 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86588067 | ‚àö(Val Loss) = 0.96768457 | Current Learning Rate: 0.002\nEpoch 238/1000 | Train Loss=8838.61523438 | Val Loss=0.93650687 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86588025 | ‚àö(Val Loss) = 0.96773285 | Current Learning Rate: 0.002\nEpoch 239/1000 | Train Loss=8838.61523438 | Val Loss=0.93660045 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587971 | ‚àö(Val Loss) = 0.96778119 | Current Learning Rate: 0.002\nEpoch 240/1000 | Train Loss=8838.61523438 | Val Loss=0.93669361 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587906 | ‚àö(Val Loss) = 0.96782935 | Current Learning Rate: 0.002\nEpoch 241/1000 | Train Loss=8838.61523438 | Val Loss=0.93678677 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587876 | ‚àö(Val Loss) = 0.96787745 | Current Learning Rate: 0.002\n\n Epoch :  240 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 242/1000 | Train Loss=8838.61523438 | Val Loss=0.93688107 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587828 | ‚àö(Val Loss) = 0.96792614 | Current Learning Rate: 0.002\nEpoch 243/1000 | Train Loss=8838.61523438 | Val Loss=0.93697435 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587757 | ‚àö(Val Loss) = 0.96797436 | Current Learning Rate: 0.002\nEpoch 244/1000 | Train Loss=8838.61523438 | Val Loss=0.93706757 | Data=88.36260223 | Physics=2.35501160 | Val RMSE: 0.86587721 | ‚àö(Val Loss) = 0.96802253 | Current Learning Rate: 0.002\nEpoch 245/1000 | Train Loss=8838.61523438 | Val Loss=0.93716002 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587679 | ‚àö(Val Loss) = 0.96807027 | Current Learning Rate: 0.002\nEpoch 246/1000 | Train Loss=8838.61523438 | Val Loss=0.93725353 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587602 | ‚àö(Val Loss) = 0.96811855 | Current Learning Rate: 0.002\nEpoch 247/1000 | Train Loss=8838.61523438 | Val Loss=0.93734795 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587572 | ‚àö(Val Loss) = 0.96816730 | Current Learning Rate: 0.002\nEpoch 248/1000 | Train Loss=8838.61523438 | Val Loss=0.93744147 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587530 | ‚àö(Val Loss) = 0.96821558 | Current Learning Rate: 0.002\nEpoch 249/1000 | Train Loss=8838.61523438 | Val Loss=0.93753397 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587489 | ‚àö(Val Loss) = 0.96826339 | Current Learning Rate: 0.002\nEpoch 250/1000 | Train Loss=8838.61523438 | Val Loss=0.93762779 | Data=88.36260223 | Physics=2.35501160 | Val RMSE: 0.86587423 | ‚àö(Val Loss) = 0.96831185 | Current Learning Rate: 0.002\nEpoch 251/1000 | Train Loss=8838.61523438 | Val Loss=0.93772018 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587393 | ‚àö(Val Loss) = 0.96835953 | Current Learning Rate: 0.002\nEpoch 252/1000 | Train Loss=8838.61523438 | Val Loss=0.93781418 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587340 | ‚àö(Val Loss) = 0.96840805 | Current Learning Rate: 0.002\nEpoch 253/1000 | Train Loss=8838.61523438 | Val Loss=0.93790644 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587280 | ‚àö(Val Loss) = 0.96845567 | Current Learning Rate: 0.002\nEpoch 254/1000 | Train Loss=8838.61523438 | Val Loss=0.93800074 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587250 | ‚àö(Val Loss) = 0.96850437 | Current Learning Rate: 0.002\nEpoch 255/1000 | Train Loss=8838.61523438 | Val Loss=0.93809348 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587214 | ‚àö(Val Loss) = 0.96855229 | Current Learning Rate: 0.002\nEpoch 256/1000 | Train Loss=8838.61523438 | Val Loss=0.93818659 | Data=88.36260223 | Physics=2.35501160 | Val RMSE: 0.86587149 | ‚àö(Val Loss) = 0.96860033 | Current Learning Rate: 0.002\nEpoch 257/1000 | Train Loss=8838.61425781 | Val Loss=0.93827981 | Data=88.36258698 | Physics=2.35501184 | Val RMSE: 0.86587107 | ‚àö(Val Loss) = 0.96864843 | Current Learning Rate: 0.002\nEpoch 258/1000 | Train Loss=8838.61523438 | Val Loss=0.93837351 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587065 | ‚àö(Val Loss) = 0.96869683 | Current Learning Rate: 0.002\nEpoch 259/1000 | Train Loss=8838.61523438 | Val Loss=0.93846607 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86587024 | ‚àö(Val Loss) = 0.96874458 | Current Learning Rate: 0.002\nEpoch 260/1000 | Train Loss=8838.61523438 | Val Loss=0.93856019 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586970 | ‚àö(Val Loss) = 0.96879315 | Current Learning Rate: 0.002\nEpoch 261/1000 | Train Loss=8838.61523438 | Val Loss=0.93865371 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586940 | ‚àö(Val Loss) = 0.96884143 | Current Learning Rate: 0.002\n\n Epoch :  260 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 262/1000 | Train Loss=8838.61523438 | Val Loss=0.93874586 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586905 | ‚àö(Val Loss) = 0.96888900 | Current Learning Rate: 0.002\nEpoch 263/1000 | Train Loss=8838.61523438 | Val Loss=0.93883955 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586869 | ‚àö(Val Loss) = 0.96893734 | Current Learning Rate: 0.002\nEpoch 264/1000 | Train Loss=8838.61523438 | Val Loss=0.93893224 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586803 | ‚àö(Val Loss) = 0.96898514 | Current Learning Rate: 0.002\nEpoch 265/1000 | Train Loss=8838.61523438 | Val Loss=0.93902445 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586773 | ‚àö(Val Loss) = 0.96903276 | Current Learning Rate: 0.002\nEpoch 266/1000 | Train Loss=8838.61523438 | Val Loss=0.93911761 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586738 | ‚àö(Val Loss) = 0.96908081 | Current Learning Rate: 0.002\nEpoch 267/1000 | Train Loss=8838.61523438 | Val Loss=0.93921071 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86586666 | ‚àö(Val Loss) = 0.96912885 | Current Learning Rate: 0.002\nEpoch 268/1000 | Train Loss=8838.61523438 | Val Loss=0.93930429 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86586648 | ‚àö(Val Loss) = 0.96917713 | Current Learning Rate: 0.002\nEpoch 269/1000 | Train Loss=8838.61523438 | Val Loss=0.93939716 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586607 | ‚àö(Val Loss) = 0.96922505 | Current Learning Rate: 0.002\nEpoch 270/1000 | Train Loss=8838.61523438 | Val Loss=0.93948990 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586571 | ‚àö(Val Loss) = 0.96927285 | Current Learning Rate: 0.002\nEpoch 271/1000 | Train Loss=8838.61523438 | Val Loss=0.93958300 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586517 | ‚àö(Val Loss) = 0.96932089 | Current Learning Rate: 0.002\nEpoch 272/1000 | Train Loss=8838.61523438 | Val Loss=0.93967658 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86586487 | ‚àö(Val Loss) = 0.96936917 | Current Learning Rate: 0.002\nEpoch 273/1000 | Train Loss=8838.61523438 | Val Loss=0.93976855 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86586452 | ‚àö(Val Loss) = 0.96941662 | Current Learning Rate: 0.002\nEpoch 274/1000 | Train Loss=8838.61523438 | Val Loss=0.93986231 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586416 | ‚àö(Val Loss) = 0.96946496 | Current Learning Rate: 0.002\nEpoch 275/1000 | Train Loss=8838.61523438 | Val Loss=0.93995345 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86586362 | ‚àö(Val Loss) = 0.96951199 | Current Learning Rate: 0.002\nEpoch 276/1000 | Train Loss=8838.61523438 | Val Loss=0.94004709 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586332 | ‚àö(Val Loss) = 0.96956027 | Current Learning Rate: 0.002\nEpoch 277/1000 | Train Loss=8838.61523438 | Val Loss=0.94014078 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586297 | ‚àö(Val Loss) = 0.96960855 | Current Learning Rate: 0.002\nEpoch 278/1000 | Train Loss=8838.61523438 | Val Loss=0.94023198 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86586267 | ‚àö(Val Loss) = 0.96965557 | Current Learning Rate: 0.002\nEpoch 279/1000 | Train Loss=8838.61523438 | Val Loss=0.94032586 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586207 | ‚àö(Val Loss) = 0.96970403 | Current Learning Rate: 0.002\nEpoch 280/1000 | Train Loss=8838.61523438 | Val Loss=0.94041932 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586183 | ‚àö(Val Loss) = 0.96975219 | Current Learning Rate: 0.002\nEpoch 281/1000 | Train Loss=8838.61523438 | Val Loss=0.94051081 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86586207 | ‚àö(Val Loss) = 0.96979934 | Current Learning Rate: 0.002\n\n Epoch :  280 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 282/1000 | Train Loss=8838.61523438 | Val Loss=0.94060367 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586171 | ‚àö(Val Loss) = 0.96984726 | Current Learning Rate: 0.002\nEpoch 283/1000 | Train Loss=8838.61523438 | Val Loss=0.94069594 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586118 | ‚àö(Val Loss) = 0.96989483 | Current Learning Rate: 0.002\nEpoch 284/1000 | Train Loss=8838.61523438 | Val Loss=0.94078827 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586094 | ‚àö(Val Loss) = 0.96994239 | Current Learning Rate: 0.002\nEpoch 285/1000 | Train Loss=8838.61523438 | Val Loss=0.94088137 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586064 | ‚àö(Val Loss) = 0.96999037 | Current Learning Rate: 0.002\nEpoch 286/1000 | Train Loss=8838.61523438 | Val Loss=0.94097471 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86586028 | ‚àö(Val Loss) = 0.97003853 | Current Learning Rate: 0.002\nEpoch 287/1000 | Train Loss=8838.61523438 | Val Loss=0.94106591 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585975 | ‚àö(Val Loss) = 0.97008550 | Current Learning Rate: 0.002\nEpoch 288/1000 | Train Loss=8838.61523438 | Val Loss=0.94116020 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585951 | ‚àö(Val Loss) = 0.97013414 | Current Learning Rate: 0.002\nEpoch 289/1000 | Train Loss=8838.61523438 | Val Loss=0.94125205 | Data=88.36260223 | Physics=2.35501160 | Val RMSE: 0.86585921 | ‚àö(Val Loss) = 0.97018147 | Current Learning Rate: 0.002\nEpoch 290/1000 | Train Loss=8838.61523438 | Val Loss=0.94134331 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585891 | ‚àö(Val Loss) = 0.97022849 | Current Learning Rate: 0.002\nEpoch 291/1000 | Train Loss=8838.61523438 | Val Loss=0.94143713 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585838 | ‚àö(Val Loss) = 0.97027683 | Current Learning Rate: 0.002\nEpoch 292/1000 | Train Loss=8838.61523438 | Val Loss=0.94152880 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585814 | ‚àö(Val Loss) = 0.97032404 | Current Learning Rate: 0.002\nEpoch 293/1000 | Train Loss=8838.61523438 | Val Loss=0.94162005 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86585796 | ‚àö(Val Loss) = 0.97037107 | Current Learning Rate: 0.002\nEpoch 294/1000 | Train Loss=8838.61523438 | Val Loss=0.94171315 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585766 | ‚àö(Val Loss) = 0.97041905 | Current Learning Rate: 0.002\nEpoch 295/1000 | Train Loss=8838.61523438 | Val Loss=0.94180459 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585695 | ‚àö(Val Loss) = 0.97046620 | Current Learning Rate: 0.002\nEpoch 296/1000 | Train Loss=8838.61523438 | Val Loss=0.94189805 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585677 | ‚àö(Val Loss) = 0.97051430 | Current Learning Rate: 0.002\nEpoch 297/1000 | Train Loss=8838.61523438 | Val Loss=0.94198972 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585659 | ‚àö(Val Loss) = 0.97056156 | Current Learning Rate: 0.002\nEpoch 298/1000 | Train Loss=8838.61523438 | Val Loss=0.94208229 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86585629 | ‚àö(Val Loss) = 0.97060925 | Current Learning Rate: 0.002\nEpoch 299/1000 | Train Loss=8838.61523438 | Val Loss=0.94217366 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585593 | ‚àö(Val Loss) = 0.97065628 | Current Learning Rate: 0.002\nEpoch 300/1000 | Train Loss=8838.61523438 | Val Loss=0.94226640 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585546 | ‚àö(Val Loss) = 0.97070408 | Current Learning Rate: 0.002\nEpoch 301/1000 | Train Loss=8838.61523438 | Val Loss=0.94235760 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86585522 | ‚àö(Val Loss) = 0.97075105 | Current Learning Rate: 0.002\n\n Epoch :  300 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 302/1000 | Train Loss=8838.61523438 | Val Loss=0.94244969 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86585492 | ‚àö(Val Loss) = 0.97079849 | Current Learning Rate: 0.002\nEpoch 303/1000 | Train Loss=8838.61523438 | Val Loss=0.94254261 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585474 | ‚àö(Val Loss) = 0.97084635 | Current Learning Rate: 0.002\nEpoch 304/1000 | Train Loss=8838.61523438 | Val Loss=0.94263405 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585414 | ‚àö(Val Loss) = 0.97089344 | Current Learning Rate: 0.002\nEpoch 305/1000 | Train Loss=8838.61523438 | Val Loss=0.94272649 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585397 | ‚àö(Val Loss) = 0.97094101 | Current Learning Rate: 0.002\nEpoch 306/1000 | Train Loss=8838.61523438 | Val Loss=0.94281733 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585373 | ‚àö(Val Loss) = 0.97098780 | Current Learning Rate: 0.002\nEpoch 307/1000 | Train Loss=8838.61523438 | Val Loss=0.94290966 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585349 | ‚àö(Val Loss) = 0.97103536 | Current Learning Rate: 0.002\nEpoch 308/1000 | Train Loss=8838.61523438 | Val Loss=0.94300061 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86585313 | ‚àö(Val Loss) = 0.97108221 | Current Learning Rate: 0.002\nEpoch 309/1000 | Train Loss=8838.61523438 | Val Loss=0.94309264 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585265 | ‚àö(Val Loss) = 0.97112960 | Current Learning Rate: 0.002\nEpoch 310/1000 | Train Loss=8838.61523438 | Val Loss=0.94318390 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585248 | ‚àö(Val Loss) = 0.97117656 | Current Learning Rate: 0.002\nEpoch 311/1000 | Train Loss=8838.61523438 | Val Loss=0.94327599 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86585230 | ‚àö(Val Loss) = 0.97122395 | Current Learning Rate: 0.002\nEpoch 312/1000 | Train Loss=8838.61523438 | Val Loss=0.94336820 | Data=88.36259460 | Physics=2.35501160 | Val RMSE: 0.86585200 | ‚àö(Val Loss) = 0.97127146 | Current Learning Rate: 0.002\nEpoch 313/1000 | Train Loss=8838.61523438 | Val Loss=0.94345921 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585152 | ‚àö(Val Loss) = 0.97131830 | Current Learning Rate: 0.002\nEpoch 314/1000 | Train Loss=8838.61523438 | Val Loss=0.94355059 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585122 | ‚àö(Val Loss) = 0.97136533 | Current Learning Rate: 0.002\nEpoch 315/1000 | Train Loss=8838.61523438 | Val Loss=0.94364178 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585104 | ‚àö(Val Loss) = 0.97141224 | Current Learning Rate: 0.002\nEpoch 316/1000 | Train Loss=8838.61523438 | Val Loss=0.94373363 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86585075 | ‚àö(Val Loss) = 0.97145951 | Current Learning Rate: 0.002\nEpoch 317/1000 | Train Loss=8838.61523438 | Val Loss=0.94382542 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585063 | ‚àö(Val Loss) = 0.97150677 | Current Learning Rate: 0.002\nEpoch 318/1000 | Train Loss=8838.61523438 | Val Loss=0.94391739 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86585003 | ‚àö(Val Loss) = 0.97155410 | Current Learning Rate: 0.002\nEpoch 319/1000 | Train Loss=8838.61523438 | Val Loss=0.94400781 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584985 | ‚àö(Val Loss) = 0.97160065 | Current Learning Rate: 0.002\nEpoch 320/1000 | Train Loss=8838.61425781 | Val Loss=0.94409943 | Data=88.36258698 | Physics=2.35501184 | Val RMSE: 0.86584967 | ‚àö(Val Loss) = 0.97164780 | Current Learning Rate: 0.002\nEpoch 321/1000 | Train Loss=8838.61523438 | Val Loss=0.94419003 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584938 | ‚àö(Val Loss) = 0.97169441 | Current Learning Rate: 0.002\n\n Epoch :  320 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 322/1000 | Train Loss=8838.61425781 | Val Loss=0.94428170 | Data=88.36258698 | Physics=2.35501184 | Val RMSE: 0.86584920 | ‚àö(Val Loss) = 0.97174156 | Current Learning Rate: 0.002\nEpoch 323/1000 | Train Loss=8838.61523438 | Val Loss=0.94437313 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584866 | ‚àö(Val Loss) = 0.97178864 | Current Learning Rate: 0.002\nEpoch 324/1000 | Train Loss=8838.61523438 | Val Loss=0.94446468 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584848 | ‚àö(Val Loss) = 0.97183573 | Current Learning Rate: 0.002\nEpoch 325/1000 | Train Loss=8838.61523438 | Val Loss=0.94455552 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584830 | ‚àö(Val Loss) = 0.97188246 | Current Learning Rate: 0.002\nEpoch 326/1000 | Train Loss=8838.61523438 | Val Loss=0.94464594 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584800 | ‚àö(Val Loss) = 0.97192895 | Current Learning Rate: 0.002\nEpoch 327/1000 | Train Loss=8838.61523438 | Val Loss=0.94473761 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584789 | ‚àö(Val Loss) = 0.97197616 | Current Learning Rate: 0.002\nEpoch 328/1000 | Train Loss=8838.61523438 | Val Loss=0.94482821 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86584747 | ‚àö(Val Loss) = 0.97202277 | Current Learning Rate: 0.002\nEpoch 329/1000 | Train Loss=8838.61523438 | Val Loss=0.94491863 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584711 | ‚àö(Val Loss) = 0.97206926 | Current Learning Rate: 0.002\nEpoch 330/1000 | Train Loss=8838.61523438 | Val Loss=0.94501042 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584693 | ‚àö(Val Loss) = 0.97211647 | Current Learning Rate: 0.002\nEpoch 331/1000 | Train Loss=8838.61523438 | Val Loss=0.94510055 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584669 | ‚àö(Val Loss) = 0.97216284 | Current Learning Rate: 0.002\nEpoch 332/1000 | Train Loss=8838.61523438 | Val Loss=0.94519162 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584651 | ‚àö(Val Loss) = 0.97220963 | Current Learning Rate: 0.002\nEpoch 333/1000 | Train Loss=8838.61523438 | Val Loss=0.94528216 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584628 | ‚àö(Val Loss) = 0.97225624 | Current Learning Rate: 0.002\nEpoch 334/1000 | Train Loss=8838.61523438 | Val Loss=0.94537318 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584586 | ‚àö(Val Loss) = 0.97230303 | Current Learning Rate: 0.002\nEpoch 335/1000 | Train Loss=8838.61523438 | Val Loss=0.94546378 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584568 | ‚àö(Val Loss) = 0.97234964 | Current Learning Rate: 0.002\nEpoch 336/1000 | Train Loss=8838.61523438 | Val Loss=0.94555455 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584556 | ‚àö(Val Loss) = 0.97239631 | Current Learning Rate: 0.002\nEpoch 337/1000 | Train Loss=8838.61523438 | Val Loss=0.94564474 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86584526 | ‚àö(Val Loss) = 0.97244269 | Current Learning Rate: 0.002\nEpoch 338/1000 | Train Loss=8838.61523438 | Val Loss=0.94573599 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584502 | ‚àö(Val Loss) = 0.97248960 | Current Learning Rate: 0.002\nEpoch 339/1000 | Train Loss=8838.61523438 | Val Loss=0.94582599 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584461 | ‚àö(Val Loss) = 0.97253585 | Current Learning Rate: 0.002\nEpoch 340/1000 | Train Loss=8838.61523438 | Val Loss=0.94591796 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584437 | ‚àö(Val Loss) = 0.97258312 | Current Learning Rate: 0.002\nEpoch 341/1000 | Train Loss=8838.61523438 | Val Loss=0.94600749 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584425 | ‚àö(Val Loss) = 0.97262919 | Current Learning Rate: 0.002\n\n Epoch :  340 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 342/1000 | Train Loss=8838.61523438 | Val Loss=0.94609809 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86584395 | ‚àö(Val Loss) = 0.97267574 | Current Learning Rate: 0.002\nEpoch 343/1000 | Train Loss=8838.61523438 | Val Loss=0.94618851 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584389 | ‚àö(Val Loss) = 0.97272223 | Current Learning Rate: 0.002\nEpoch 344/1000 | Train Loss=8838.61523438 | Val Loss=0.94627959 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584336 | ‚àö(Val Loss) = 0.97276902 | Current Learning Rate: 0.002\nEpoch 345/1000 | Train Loss=8838.61523438 | Val Loss=0.94636887 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584318 | ‚àö(Val Loss) = 0.97281492 | Current Learning Rate: 0.002\nEpoch 346/1000 | Train Loss=8838.61523438 | Val Loss=0.94645917 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584300 | ‚àö(Val Loss) = 0.97286135 | Current Learning Rate: 0.002\nEpoch 347/1000 | Train Loss=8838.61523438 | Val Loss=0.94654840 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584288 | ‚àö(Val Loss) = 0.97290719 | Current Learning Rate: 0.002\nEpoch 348/1000 | Train Loss=8838.61523438 | Val Loss=0.94663882 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584258 | ‚àö(Val Loss) = 0.97295368 | Current Learning Rate: 0.002\nEpoch 349/1000 | Train Loss=8838.61523438 | Val Loss=0.94672900 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584240 | ‚àö(Val Loss) = 0.97299999 | Current Learning Rate: 0.002\nEpoch 350/1000 | Train Loss=8838.61523438 | Val Loss=0.94681948 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584198 | ‚àö(Val Loss) = 0.97304648 | Current Learning Rate: 0.002\nEpoch 351/1000 | Train Loss=8838.61523438 | Val Loss=0.94690907 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584181 | ‚àö(Val Loss) = 0.97309256 | Current Learning Rate: 0.002\nEpoch 352/1000 | Train Loss=8838.61523438 | Val Loss=0.94700062 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584169 | ‚àö(Val Loss) = 0.97313958 | Current Learning Rate: 0.002\nEpoch 353/1000 | Train Loss=8838.61523438 | Val Loss=0.94709092 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584145 | ‚àö(Val Loss) = 0.97318596 | Current Learning Rate: 0.002\nEpoch 354/1000 | Train Loss=8838.61523438 | Val Loss=0.94717956 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584133 | ‚àö(Val Loss) = 0.97323149 | Current Learning Rate: 0.002\nEpoch 355/1000 | Train Loss=8838.61523438 | Val Loss=0.94726926 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584103 | ‚àö(Val Loss) = 0.97327757 | Current Learning Rate: 0.002\nEpoch 356/1000 | Train Loss=8838.61523438 | Val Loss=0.94735986 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584061 | ‚àö(Val Loss) = 0.97332412 | Current Learning Rate: 0.002\nEpoch 357/1000 | Train Loss=8838.61523438 | Val Loss=0.94745004 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584049 | ‚àö(Val Loss) = 0.97337043 | Current Learning Rate: 0.002\nEpoch 358/1000 | Train Loss=8838.61523438 | Val Loss=0.94753897 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584038 | ‚àö(Val Loss) = 0.97341615 | Current Learning Rate: 0.002\nEpoch 359/1000 | Train Loss=8838.61523438 | Val Loss=0.94762975 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86584008 | ‚àö(Val Loss) = 0.97346276 | Current Learning Rate: 0.002\nEpoch 360/1000 | Train Loss=8838.61523438 | Val Loss=0.94771850 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583996 | ‚àö(Val Loss) = 0.97350836 | Current Learning Rate: 0.002\nEpoch 361/1000 | Train Loss=8838.61523438 | Val Loss=0.94780821 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86583954 | ‚àö(Val Loss) = 0.97355443 | Current Learning Rate: 0.002\n\n Epoch :  360 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 362/1000 | Train Loss=8838.61523438 | Val Loss=0.94789803 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583936 | ‚àö(Val Loss) = 0.97360057 | Current Learning Rate: 0.002\nEpoch 363/1000 | Train Loss=8838.61523438 | Val Loss=0.94798666 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86583918 | ‚àö(Val Loss) = 0.97364604 | Current Learning Rate: 0.002\nEpoch 364/1000 | Train Loss=8838.61523438 | Val Loss=0.94807655 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583906 | ‚àö(Val Loss) = 0.97369224 | Current Learning Rate: 0.002\nEpoch 365/1000 | Train Loss=8838.61523438 | Val Loss=0.94816643 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583883 | ‚àö(Val Loss) = 0.97373837 | Current Learning Rate: 0.002\nEpoch 366/1000 | Train Loss=8838.61523438 | Val Loss=0.94825631 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583871 | ‚àö(Val Loss) = 0.97378451 | Current Learning Rate: 0.002\nEpoch 367/1000 | Train Loss=8838.61523438 | Val Loss=0.94834536 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583829 | ‚àö(Val Loss) = 0.97383028 | Current Learning Rate: 0.002\nEpoch 368/1000 | Train Loss=8838.61523438 | Val Loss=0.94843417 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583805 | ‚àö(Val Loss) = 0.97387588 | Current Learning Rate: 0.002\nEpoch 369/1000 | Train Loss=8838.61523438 | Val Loss=0.94852424 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583793 | ‚àö(Val Loss) = 0.97392207 | Current Learning Rate: 0.002\nEpoch 370/1000 | Train Loss=8838.61425781 | Val Loss=0.94861215 | Data=88.36258698 | Physics=2.35501184 | Val RMSE: 0.86583781 | ‚àö(Val Loss) = 0.97396719 | Current Learning Rate: 0.002\nEpoch 371/1000 | Train Loss=8838.61523438 | Val Loss=0.94870168 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583757 | ‚àö(Val Loss) = 0.97401321 | Current Learning Rate: 0.002\nEpoch 372/1000 | Train Loss=8838.61523438 | Val Loss=0.94879133 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583745 | ‚àö(Val Loss) = 0.97405922 | Current Learning Rate: 0.002\nEpoch 373/1000 | Train Loss=8838.61523438 | Val Loss=0.94888020 | Data=88.36259460 | Physics=2.35501160 | Val RMSE: 0.86583728 | ‚àö(Val Loss) = 0.97410482 | Current Learning Rate: 0.002\nEpoch 374/1000 | Train Loss=8838.61523438 | Val Loss=0.94896948 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583686 | ‚àö(Val Loss) = 0.97415066 | Current Learning Rate: 0.002\nEpoch 375/1000 | Train Loss=8838.61523438 | Val Loss=0.94905847 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583674 | ‚àö(Val Loss) = 0.97419631 | Current Learning Rate: 0.002\nEpoch 376/1000 | Train Loss=8838.61523438 | Val Loss=0.94914681 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86583662 | ‚àö(Val Loss) = 0.97424167 | Current Learning Rate: 0.002\nEpoch 377/1000 | Train Loss=8838.61523438 | Val Loss=0.94923723 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86583650 | ‚àö(Val Loss) = 0.97428805 | Current Learning Rate: 0.002\nEpoch 378/1000 | Train Loss=8838.61523438 | Val Loss=0.94932592 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583632 | ‚àö(Val Loss) = 0.97433358 | Current Learning Rate: 0.002\nEpoch 379/1000 | Train Loss=8838.61523438 | Val Loss=0.94941431 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583602 | ‚àö(Val Loss) = 0.97437894 | Current Learning Rate: 0.002\nEpoch 380/1000 | Train Loss=8838.61523438 | Val Loss=0.94950289 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583567 | ‚àö(Val Loss) = 0.97442436 | Current Learning Rate: 0.002\nEpoch 381/1000 | Train Loss=8838.61523438 | Val Loss=0.94959211 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583555 | ‚àö(Val Loss) = 0.97447020 | Current Learning Rate: 0.002\n\n Epoch :  380 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\nEpoch 382/1000 | Train Loss=8838.61425781 | Val Loss=0.94968116 | Data=88.36258698 | Physics=2.35501184 | Val RMSE: 0.86583543 | ‚àö(Val Loss) = 0.97451586 | Current Learning Rate: 0.002\nEpoch 383/1000 | Train Loss=8838.61523438 | Val Loss=0.94976896 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86583531 | ‚àö(Val Loss) = 0.97456092 | Current Learning Rate: 0.002\nEpoch 384/1000 | Train Loss=8838.61523438 | Val Loss=0.94985777 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583513 | ‚àö(Val Loss) = 0.97460645 | Current Learning Rate: 0.002\nEpoch 385/1000 | Train Loss=8838.61523438 | Val Loss=0.94994670 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583495 | ‚àö(Val Loss) = 0.97465211 | Current Learning Rate: 0.002\nEpoch 386/1000 | Train Loss=8838.61523438 | Val Loss=0.95003515 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583459 | ‚àö(Val Loss) = 0.97469747 | Current Learning Rate: 0.002\nEpoch 387/1000 | Train Loss=8838.61523438 | Val Loss=0.95012438 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583441 | ‚àö(Val Loss) = 0.97474325 | Current Learning Rate: 0.002\nEpoch 388/1000 | Train Loss=8838.61523438 | Val Loss=0.95021230 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583424 | ‚àö(Val Loss) = 0.97478831 | Current Learning Rate: 0.002\nEpoch 389/1000 | Train Loss=8838.61523438 | Val Loss=0.95030153 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583418 | ‚àö(Val Loss) = 0.97483408 | Current Learning Rate: 0.002\nEpoch 390/1000 | Train Loss=8838.61523438 | Val Loss=0.95038933 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583406 | ‚àö(Val Loss) = 0.97487915 | Current Learning Rate: 0.002\nEpoch 391/1000 | Train Loss=8838.61523438 | Val Loss=0.95047766 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583382 | ‚àö(Val Loss) = 0.97492445 | Current Learning Rate: 0.002\nEpoch 392/1000 | Train Loss=8838.61523438 | Val Loss=0.95056611 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583376 | ‚àö(Val Loss) = 0.97496980 | Current Learning Rate: 0.002\nEpoch 393/1000 | Train Loss=8838.61523438 | Val Loss=0.95065516 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583334 | ‚àö(Val Loss) = 0.97501546 | Current Learning Rate: 0.002\nEpoch 394/1000 | Train Loss=8838.61523438 | Val Loss=0.95074254 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583316 | ‚àö(Val Loss) = 0.97506028 | Current Learning Rate: 0.002\nEpoch 395/1000 | Train Loss=8838.61523438 | Val Loss=0.95083123 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86583304 | ‚àö(Val Loss) = 0.97510576 | Current Learning Rate: 0.002\nEpoch 396/1000 | Train Loss=8838.61523438 | Val Loss=0.95091933 | Data=88.36259460 | Physics=2.35501184 | Val RMSE: 0.86583292 | ‚àö(Val Loss) = 0.97515094 | Current Learning Rate: 0.002\nEpoch 397/1000 | Train Loss=8838.61523438 | Val Loss=0.95100677 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583269 | ‚àö(Val Loss) = 0.97519577 | Current Learning Rate: 0.002\nEpoch 398/1000 | Train Loss=8838.61523438 | Val Loss=0.95109564 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583263 | ‚àö(Val Loss) = 0.97524130 | Current Learning Rate: 0.002\nEpoch 399/1000 | Train Loss=8838.61523438 | Val Loss=0.95118421 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583239 | ‚àö(Val Loss) = 0.97528672 | Current Learning Rate: 0.002\nEpoch 400/1000 | Train Loss=8838.61523438 | Val Loss=0.95127225 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.86583197 | ‚àö(Val Loss) = 0.97533184 | Current Learning Rate: 0.0002\nEpoch 401/1000 | Train Loss=8838.61523438 | Val Loss=0.87482369 | Data=88.36260223 | Physics=2.35501184 | Val RMSE: 0.87119812 | ‚àö(Val Loss) = 0.93532008 | Current Learning Rate: 0.0002\n\n Epoch :  400 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.98844355 -3.3342812  -9.564512  ]\n [ 0.9884738  -3.3343067  -9.564461  ]] \n\nFinal Test RMSE:  0.5964834094047546\n‚úÖ Saved best model at epoch 402 (Val Loss = 0.85106158)\nEpoch 402/1000 | Train Loss=1638.42846680 | Val Loss=0.85106158 | Data=16.36073494 | Physics=2.35501184 | Val RMSE: 0.87125301 | ‚àö(Val Loss) = 0.92253000 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 403 (Val Loss = 0.83363926)\nEpoch 403/1000 | Train Loss=1627.51660156 | Val Loss=0.83363926 | Data=16.25154686 | Physics=2.36199580 | Val RMSE: 0.87073874 | ‚àö(Val Loss) = 0.91303849 | Current Learning Rate: 0.0002\n‚úÖ Saved best model at epoch 404 (Val Loss = 0.82570404)\nEpoch 404/1000 | Train Loss=1613.85461426 | Val Loss=0.82570404 | Data=16.11470985 | Physics=2.38370948 | Val RMSE: 0.87028527 | ‚àö(Val Loss) = 0.90868258 | Current Learning Rate: 0.0002\nEpoch 405/1000 | Train Loss=1602.19592285 | Val Loss=0.83413482 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.86988866 | ‚àö(Val Loss) = 0.91330981 | Current Learning Rate: 0.0002\nEpoch 406/1000 | Train Loss=1602.19592285 | Val Loss=0.83527136 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.86992723 | ‚àö(Val Loss) = 0.91393179 | Current Learning Rate: 0.0002\nEpoch 407/1000 | Train Loss=1602.19580078 | Val Loss=0.83617997 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.86996275 | ‚àö(Val Loss) = 0.91442877 | Current Learning Rate: 0.0002\nEpoch 408/1000 | Train Loss=1602.19580078 | Val Loss=0.83688360 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.86999422 | ‚àö(Val Loss) = 0.91481340 | Current Learning Rate: 0.0002\nEpoch 409/1000 | Train Loss=1602.19580078 | Val Loss=0.83740842 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87002236 | ‚àö(Val Loss) = 0.91510022 | Current Learning Rate: 0.0002\nEpoch 410/1000 | Train Loss=1602.19592285 | Val Loss=0.83778292 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87004787 | ‚àö(Val Loss) = 0.91530484 | Current Learning Rate: 0.0002\nEpoch 411/1000 | Train Loss=1602.19592285 | Val Loss=0.83802885 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87007082 | ‚àö(Val Loss) = 0.91543913 | Current Learning Rate: 0.0002\nEpoch 412/1000 | Train Loss=1602.19592285 | Val Loss=0.83816886 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87009114 | ‚àö(Val Loss) = 0.91551560 | Current Learning Rate: 0.0002\nEpoch 413/1000 | Train Loss=1602.19592285 | Val Loss=0.83822179 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87010980 | ‚àö(Val Loss) = 0.91554451 | Current Learning Rate: 0.0002\nEpoch 414/1000 | Train Loss=1602.19580078 | Val Loss=0.83820331 | Data=15.99770164 | Physics=2.42568616 | Val RMSE: 0.87012649 | ‚àö(Val Loss) = 0.91553444 | Current Learning Rate: 0.0002\nEpoch 415/1000 | Train Loss=1602.19592285 | Val Loss=0.83812541 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87014145 | ‚àö(Val Loss) = 0.91549188 | Current Learning Rate: 0.0002\nEpoch 416/1000 | Train Loss=1602.19592285 | Val Loss=0.83800340 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87015492 | ‚àö(Val Loss) = 0.91542524 | Current Learning Rate: 0.0002\nEpoch 417/1000 | Train Loss=1602.19592285 | Val Loss=0.83784384 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87016690 | ‚àö(Val Loss) = 0.91533810 | Current Learning Rate: 0.0002\nEpoch 418/1000 | Train Loss=1602.19592285 | Val Loss=0.83765548 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87017787 | ‚àö(Val Loss) = 0.91523522 | Current Learning Rate: 0.0002\nEpoch 419/1000 | Train Loss=1602.19580078 | Val Loss=0.83744675 | Data=15.99770164 | Physics=2.42568616 | Val RMSE: 0.87018782 | ‚àö(Val Loss) = 0.91512114 | Current Learning Rate: 0.0002\nEpoch 420/1000 | Train Loss=1602.19567871 | Val Loss=0.83722097 | Data=15.99769974 | Physics=2.42568640 | Val RMSE: 0.87019664 | ‚àö(Val Loss) = 0.91499782 | Current Learning Rate: 0.0002\nEpoch 421/1000 | Train Loss=1602.19592285 | Val Loss=0.83698422 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87020469 | ‚àö(Val Loss) = 0.91486841 | Current Learning Rate: 0.0002\n\n Epoch :  420 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 422/1000 | Train Loss=1602.19592285 | Val Loss=0.83673930 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87021142 | ‚àö(Val Loss) = 0.91473454 | Current Learning Rate: 0.0002\nEpoch 423/1000 | Train Loss=1602.19567871 | Val Loss=0.83649004 | Data=15.99769974 | Physics=2.42568640 | Val RMSE: 0.87021810 | ‚àö(Val Loss) = 0.91459829 | Current Learning Rate: 0.0002\nEpoch 424/1000 | Train Loss=1602.19580078 | Val Loss=0.83623862 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87022376 | ‚àö(Val Loss) = 0.91446084 | Current Learning Rate: 0.0002\nEpoch 425/1000 | Train Loss=1602.19592285 | Val Loss=0.83598727 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87022907 | ‚àö(Val Loss) = 0.91432339 | Current Learning Rate: 0.0002\nEpoch 426/1000 | Train Loss=1602.19592285 | Val Loss=0.83573824 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87023348 | ‚àö(Val Loss) = 0.91418719 | Current Learning Rate: 0.0002\nEpoch 427/1000 | Train Loss=1602.19580078 | Val Loss=0.83549184 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87023783 | ‚àö(Val Loss) = 0.91405243 | Current Learning Rate: 0.0002\nEpoch 428/1000 | Train Loss=1602.19592285 | Val Loss=0.83525103 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87024146 | ‚àö(Val Loss) = 0.91392070 | Current Learning Rate: 0.0002\nEpoch 429/1000 | Train Loss=1602.19592285 | Val Loss=0.83501297 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87024522 | ‚àö(Val Loss) = 0.91379046 | Current Learning Rate: 0.0002\nEpoch 430/1000 | Train Loss=1602.19580078 | Val Loss=0.83478171 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87024814 | ‚àö(Val Loss) = 0.91366386 | Current Learning Rate: 0.0002\nEpoch 431/1000 | Train Loss=1602.19592285 | Val Loss=0.83455712 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87025082 | ‚àö(Val Loss) = 0.91354096 | Current Learning Rate: 0.0002\nEpoch 432/1000 | Train Loss=1602.19616699 | Val Loss=0.83433801 | Data=15.99770451 | Physics=2.42568640 | Val RMSE: 0.87025326 | ‚àö(Val Loss) = 0.91342103 | Current Learning Rate: 0.0002\nEpoch 433/1000 | Train Loss=1602.19592285 | Val Loss=0.83412564 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87025559 | ‚àö(Val Loss) = 0.91330481 | Current Learning Rate: 0.0002\nEpoch 434/1000 | Train Loss=1602.19580078 | Val Loss=0.83391947 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87025702 | ‚àö(Val Loss) = 0.91319191 | Current Learning Rate: 0.0002\nEpoch 435/1000 | Train Loss=1602.19592285 | Val Loss=0.83371967 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87025917 | ‚àö(Val Loss) = 0.91308248 | Current Learning Rate: 0.0002\nEpoch 436/1000 | Train Loss=1602.19592285 | Val Loss=0.83352786 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87026078 | ‚àö(Val Loss) = 0.91297746 | Current Learning Rate: 0.0002\nEpoch 437/1000 | Train Loss=1602.19592285 | Val Loss=0.83334088 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87026209 | ‚àö(Val Loss) = 0.91287506 | Current Learning Rate: 0.0002\nEpoch 438/1000 | Train Loss=1602.19580078 | Val Loss=0.83316177 | Data=15.99770164 | Physics=2.42568616 | Val RMSE: 0.87026340 | ‚àö(Val Loss) = 0.91277695 | Current Learning Rate: 0.0002\nEpoch 439/1000 | Train Loss=1602.19592285 | Val Loss=0.83298838 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87026429 | ‚àö(Val Loss) = 0.91268200 | Current Learning Rate: 0.0002\nEpoch 440/1000 | Train Loss=1602.19592285 | Val Loss=0.83282131 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87026519 | ‚àö(Val Loss) = 0.91259044 | Current Learning Rate: 0.0002\nEpoch 441/1000 | Train Loss=1602.19580078 | Val Loss=0.83265972 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87026650 | ‚àö(Val Loss) = 0.91250193 | Current Learning Rate: 0.0002\n\n Epoch :  440 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 442/1000 | Train Loss=1602.19580078 | Val Loss=0.83250409 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87026697 | ‚àö(Val Loss) = 0.91241664 | Current Learning Rate: 0.0002\nEpoch 443/1000 | Train Loss=1602.19580078 | Val Loss=0.83235443 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87026751 | ‚àö(Val Loss) = 0.91233462 | Current Learning Rate: 0.0002\nEpoch 444/1000 | Train Loss=1602.19580078 | Val Loss=0.83221090 | Data=15.99770164 | Physics=2.42568616 | Val RMSE: 0.87026864 | ‚àö(Val Loss) = 0.91225594 | Current Learning Rate: 0.0002\nEpoch 445/1000 | Train Loss=1602.19592285 | Val Loss=0.83207172 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.91217965 | Current Learning Rate: 0.0002\nEpoch 446/1000 | Train Loss=1602.19580078 | Val Loss=0.83193690 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87026936 | ‚àö(Val Loss) = 0.91210574 | Current Learning Rate: 0.0002\nEpoch 447/1000 | Train Loss=1602.19592285 | Val Loss=0.83180863 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87026966 | ‚àö(Val Loss) = 0.91203541 | Current Learning Rate: 0.0002\nEpoch 448/1000 | Train Loss=1602.19616699 | Val Loss=0.83168346 | Data=15.99770451 | Physics=2.42568640 | Val RMSE: 0.87027013 | ‚àö(Val Loss) = 0.91196680 | Current Learning Rate: 0.0002\nEpoch 449/1000 | Train Loss=1602.19616699 | Val Loss=0.83156353 | Data=15.99770451 | Physics=2.42568640 | Val RMSE: 0.87027097 | ‚àö(Val Loss) = 0.91190106 | Current Learning Rate: 0.0002\nEpoch 450/1000 | Train Loss=1602.19567871 | Val Loss=0.83144814 | Data=15.99769974 | Physics=2.42568640 | Val RMSE: 0.87027103 | ‚àö(Val Loss) = 0.91183776 | Current Learning Rate: 0.0002\nEpoch 451/1000 | Train Loss=1602.19592285 | Val Loss=0.83133608 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027138 | ‚àö(Val Loss) = 0.91177630 | Current Learning Rate: 0.0002\nEpoch 452/1000 | Train Loss=1602.19592285 | Val Loss=0.83122861 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027138 | ‚àö(Val Loss) = 0.91171741 | Current Learning Rate: 0.0002\nEpoch 453/1000 | Train Loss=1602.19616699 | Val Loss=0.83112448 | Data=15.99770451 | Physics=2.42568616 | Val RMSE: 0.87027162 | ‚àö(Val Loss) = 0.91166031 | Current Learning Rate: 0.0002\nEpoch 454/1000 | Train Loss=1602.19580078 | Val Loss=0.83102447 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027186 | ‚àö(Val Loss) = 0.91160542 | Current Learning Rate: 0.0002\nEpoch 455/1000 | Train Loss=1602.19580078 | Val Loss=0.83092642 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027180 | ‚àö(Val Loss) = 0.91155165 | Current Learning Rate: 0.0002\nEpoch 456/1000 | Train Loss=1602.19592285 | Val Loss=0.83083361 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027186 | ‚àö(Val Loss) = 0.91150075 | Current Learning Rate: 0.0002\nEpoch 457/1000 | Train Loss=1602.19592285 | Val Loss=0.83074313 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027210 | ‚àö(Val Loss) = 0.91145110 | Current Learning Rate: 0.0002\nEpoch 458/1000 | Train Loss=1602.19592285 | Val Loss=0.83065563 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027276 | ‚àö(Val Loss) = 0.91140312 | Current Learning Rate: 0.0002\nEpoch 459/1000 | Train Loss=1602.19592285 | Val Loss=0.83057117 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027276 | ‚àö(Val Loss) = 0.91135675 | Current Learning Rate: 0.0002\nEpoch 460/1000 | Train Loss=1602.19592285 | Val Loss=0.83048898 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027276 | ‚àö(Val Loss) = 0.91131169 | Current Learning Rate: 0.0002\nEpoch 461/1000 | Train Loss=1602.19592285 | Val Loss=0.83041018 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027276 | ‚àö(Val Loss) = 0.91126847 | Current Learning Rate: 0.0002\n\n Epoch :  460 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 462/1000 | Train Loss=1602.19592285 | Val Loss=0.83033419 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027276 | ‚àö(Val Loss) = 0.91122675 | Current Learning Rate: 0.0002\nEpoch 463/1000 | Train Loss=1602.19592285 | Val Loss=0.83025980 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027287 | ‚àö(Val Loss) = 0.91118592 | Current Learning Rate: 0.0002\nEpoch 464/1000 | Train Loss=1602.19592285 | Val Loss=0.83018786 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027299 | ‚àö(Val Loss) = 0.91114646 | Current Learning Rate: 0.0002\nEpoch 465/1000 | Train Loss=1602.19580078 | Val Loss=0.83011836 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027299 | ‚àö(Val Loss) = 0.91110831 | Current Learning Rate: 0.0002\nEpoch 466/1000 | Train Loss=1602.19592285 | Val Loss=0.83005089 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027299 | ‚àö(Val Loss) = 0.91107130 | Current Learning Rate: 0.0002\nEpoch 467/1000 | Train Loss=1602.19592285 | Val Loss=0.82998586 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027293 | ‚àö(Val Loss) = 0.91103560 | Current Learning Rate: 0.0002\nEpoch 468/1000 | Train Loss=1602.19592285 | Val Loss=0.82992250 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027293 | ‚àö(Val Loss) = 0.91100085 | Current Learning Rate: 0.0002\nEpoch 469/1000 | Train Loss=1602.19580078 | Val Loss=0.82986033 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027293 | ‚àö(Val Loss) = 0.91096669 | Current Learning Rate: 0.0002\nEpoch 470/1000 | Train Loss=1602.19567871 | Val Loss=0.82980198 | Data=15.99769974 | Physics=2.42568640 | Val RMSE: 0.87027293 | ‚àö(Val Loss) = 0.91093469 | Current Learning Rate: 0.0002\nEpoch 471/1000 | Train Loss=1602.19592285 | Val Loss=0.82974398 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027287 | ‚àö(Val Loss) = 0.91090286 | Current Learning Rate: 0.0002\nEpoch 472/1000 | Train Loss=1602.19592285 | Val Loss=0.82968777 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027287 | ‚àö(Val Loss) = 0.91087198 | Current Learning Rate: 0.0002\nEpoch 473/1000 | Train Loss=1602.19616699 | Val Loss=0.82963270 | Data=15.99770451 | Physics=2.42568640 | Val RMSE: 0.87027287 | ‚àö(Val Loss) = 0.91084176 | Current Learning Rate: 0.0002\nEpoch 474/1000 | Train Loss=1602.19592285 | Val Loss=0.82957953 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027287 | ‚àö(Val Loss) = 0.91081256 | Current Learning Rate: 0.0002\nEpoch 475/1000 | Train Loss=1602.19592285 | Val Loss=0.82952809 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027282 | ‚àö(Val Loss) = 0.91078430 | Current Learning Rate: 0.0002\nEpoch 476/1000 | Train Loss=1602.19580078 | Val Loss=0.82947803 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027282 | ‚àö(Val Loss) = 0.91075683 | Current Learning Rate: 0.0002\nEpoch 477/1000 | Train Loss=1602.19580078 | Val Loss=0.82942867 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027264 | ‚àö(Val Loss) = 0.91072977 | Current Learning Rate: 0.0002\nEpoch 478/1000 | Train Loss=1602.19580078 | Val Loss=0.82938153 | Data=15.99770164 | Physics=2.42568616 | Val RMSE: 0.87027264 | ‚àö(Val Loss) = 0.91070384 | Current Learning Rate: 0.0002\nEpoch 479/1000 | Train Loss=1602.19592285 | Val Loss=0.82933605 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027264 | ‚àö(Val Loss) = 0.91067892 | Current Learning Rate: 0.0002\nEpoch 480/1000 | Train Loss=1602.19616699 | Val Loss=0.82929099 | Data=15.99770451 | Physics=2.42568640 | Val RMSE: 0.87027252 | ‚àö(Val Loss) = 0.91065413 | Current Learning Rate: 0.0002\nEpoch 481/1000 | Train Loss=1602.19580078 | Val Loss=0.82924753 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027246 | ‚àö(Val Loss) = 0.91063029 | Current Learning Rate: 0.0002\n\n Epoch :  480 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 482/1000 | Train Loss=1602.19592285 | Val Loss=0.82920474 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027246 | ‚àö(Val Loss) = 0.91060680 | Current Learning Rate: 0.0002\nEpoch 483/1000 | Train Loss=1602.19592285 | Val Loss=0.82916260 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027246 | ‚àö(Val Loss) = 0.91058367 | Current Learning Rate: 0.0002\nEpoch 484/1000 | Train Loss=1602.19592285 | Val Loss=0.82912225 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027234 | ‚àö(Val Loss) = 0.91056150 | Current Learning Rate: 0.0002\nEpoch 485/1000 | Train Loss=1602.19580078 | Val Loss=0.82908255 | Data=15.99770164 | Physics=2.42568616 | Val RMSE: 0.87027234 | ‚àö(Val Loss) = 0.91053969 | Current Learning Rate: 0.0002\nEpoch 486/1000 | Train Loss=1602.19580078 | Val Loss=0.82904369 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027234 | ‚àö(Val Loss) = 0.91051835 | Current Learning Rate: 0.0002\nEpoch 487/1000 | Train Loss=1602.19592285 | Val Loss=0.82900733 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027234 | ‚àö(Val Loss) = 0.91049838 | Current Learning Rate: 0.0002\nEpoch 488/1000 | Train Loss=1602.19592285 | Val Loss=0.82897043 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027234 | ‚àö(Val Loss) = 0.91047812 | Current Learning Rate: 0.0002\nEpoch 489/1000 | Train Loss=1602.19592285 | Val Loss=0.82893485 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027234 | ‚àö(Val Loss) = 0.91045856 | Current Learning Rate: 0.0002\nEpoch 490/1000 | Train Loss=1602.19592285 | Val Loss=0.82890010 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027234 | ‚àö(Val Loss) = 0.91043949 | Current Learning Rate: 0.0002\nEpoch 491/1000 | Train Loss=1602.19580078 | Val Loss=0.82886618 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027222 | ‚àö(Val Loss) = 0.91042089 | Current Learning Rate: 0.0002\nEpoch 492/1000 | Train Loss=1602.19592285 | Val Loss=0.82883221 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027222 | ‚àö(Val Loss) = 0.91040224 | Current Learning Rate: 0.0002\nEpoch 493/1000 | Train Loss=1602.19592285 | Val Loss=0.82879949 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027222 | ‚àö(Val Loss) = 0.91038424 | Current Learning Rate: 0.0002\nEpoch 494/1000 | Train Loss=1602.19592285 | Val Loss=0.82876879 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027222 | ‚àö(Val Loss) = 0.91036737 | Current Learning Rate: 0.0002\nEpoch 495/1000 | Train Loss=1602.19592285 | Val Loss=0.82873756 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027222 | ‚àö(Val Loss) = 0.91035026 | Current Learning Rate: 0.0002\nEpoch 496/1000 | Train Loss=1602.19592285 | Val Loss=0.82870758 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027204 | ‚àö(Val Loss) = 0.91033375 | Current Learning Rate: 0.0002\nEpoch 497/1000 | Train Loss=1602.19616699 | Val Loss=0.82867759 | Data=15.99770451 | Physics=2.42568640 | Val RMSE: 0.87027198 | ‚àö(Val Loss) = 0.91031730 | Current Learning Rate: 0.0002\nEpoch 498/1000 | Train Loss=1602.19592285 | Val Loss=0.82864815 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027192 | ‚àö(Val Loss) = 0.91030115 | Current Learning Rate: 0.0002\nEpoch 499/1000 | Train Loss=1602.19592285 | Val Loss=0.82862031 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027192 | ‚àö(Val Loss) = 0.91028583 | Current Learning Rate: 0.0002\nEpoch 500/1000 | Train Loss=1602.19592285 | Val Loss=0.82859272 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027192 | ‚àö(Val Loss) = 0.91027069 | Current Learning Rate: 0.0002\nEpoch 501/1000 | Train Loss=1602.19580078 | Val Loss=0.82856619 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027180 | ‚àö(Val Loss) = 0.91025609 | Current Learning Rate: 0.0002\n\n Epoch :  500 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 502/1000 | Train Loss=1602.19567871 | Val Loss=0.82853925 | Data=15.99769974 | Physics=2.42568616 | Val RMSE: 0.87027180 | ‚àö(Val Loss) = 0.91024131 | Current Learning Rate: 0.0002\nEpoch 503/1000 | Train Loss=1602.19592285 | Val Loss=0.82851315 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027180 | ‚àö(Val Loss) = 0.91022700 | Current Learning Rate: 0.0002\nEpoch 504/1000 | Train Loss=1602.19592285 | Val Loss=0.82848799 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027180 | ‚àö(Val Loss) = 0.91021317 | Current Learning Rate: 0.0002\nEpoch 505/1000 | Train Loss=1602.19580078 | Val Loss=0.82846266 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027180 | ‚àö(Val Loss) = 0.91019922 | Current Learning Rate: 0.0002\nEpoch 506/1000 | Train Loss=1602.19580078 | Val Loss=0.82843804 | Data=15.99770164 | Physics=2.42568616 | Val RMSE: 0.87027180 | ‚àö(Val Loss) = 0.91018569 | Current Learning Rate: 0.0002\nEpoch 507/1000 | Train Loss=1602.19592285 | Val Loss=0.82841444 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027180 | ‚àö(Val Loss) = 0.91017276 | Current Learning Rate: 0.0002\nEpoch 508/1000 | Train Loss=1602.19580078 | Val Loss=0.82839191 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027174 | ‚àö(Val Loss) = 0.91016036 | Current Learning Rate: 0.0002\nEpoch 509/1000 | Train Loss=1602.19592285 | Val Loss=0.82836884 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027174 | ‚àö(Val Loss) = 0.91014773 | Current Learning Rate: 0.0002\nEpoch 510/1000 | Train Loss=1602.19592285 | Val Loss=0.82834637 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027168 | ‚àö(Val Loss) = 0.91013539 | Current Learning Rate: 0.0002\nEpoch 511/1000 | Train Loss=1602.19592285 | Val Loss=0.82832402 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027174 | ‚àö(Val Loss) = 0.91012305 | Current Learning Rate: 0.0002\nEpoch 512/1000 | Train Loss=1602.19592285 | Val Loss=0.82830203 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027174 | ‚àö(Val Loss) = 0.91011101 | Current Learning Rate: 0.0002\nEpoch 513/1000 | Train Loss=1602.19592285 | Val Loss=0.82828116 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027156 | ‚àö(Val Loss) = 0.91009951 | Current Learning Rate: 0.0002\nEpoch 514/1000 | Train Loss=1602.19592285 | Val Loss=0.82826132 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027156 | ‚àö(Val Loss) = 0.91008866 | Current Learning Rate: 0.0002\nEpoch 515/1000 | Train Loss=1602.19592285 | Val Loss=0.82824051 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027138 | ‚àö(Val Loss) = 0.91007721 | Current Learning Rate: 0.0002\nEpoch 516/1000 | Train Loss=1602.19580078 | Val Loss=0.82822025 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027138 | ‚àö(Val Loss) = 0.91006607 | Current Learning Rate: 0.0002\nEpoch 517/1000 | Train Loss=1602.19592285 | Val Loss=0.82820117 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027138 | ‚àö(Val Loss) = 0.91005558 | Current Learning Rate: 0.0002\nEpoch 518/1000 | Train Loss=1602.19592285 | Val Loss=0.82818156 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027144 | ‚àö(Val Loss) = 0.91004479 | Current Learning Rate: 0.0002\nEpoch 519/1000 | Train Loss=1602.19580078 | Val Loss=0.82816219 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027133 | ‚àö(Val Loss) = 0.91003418 | Current Learning Rate: 0.0002\nEpoch 520/1000 | Train Loss=1602.19592285 | Val Loss=0.82814449 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027133 | ‚àö(Val Loss) = 0.91002446 | Current Learning Rate: 0.0002\nEpoch 521/1000 | Train Loss=1602.19592285 | Val Loss=0.82812625 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027133 | ‚àö(Val Loss) = 0.91001445 | Current Learning Rate: 0.0002\n\n Epoch :  520 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 522/1000 | Train Loss=1602.19592285 | Val Loss=0.82810855 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027138 | ‚àö(Val Loss) = 0.91000468 | Current Learning Rate: 0.0002\nEpoch 523/1000 | Train Loss=1602.19580078 | Val Loss=0.82809120 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027138 | ‚àö(Val Loss) = 0.90999514 | Current Learning Rate: 0.0002\nEpoch 524/1000 | Train Loss=1602.19567871 | Val Loss=0.82807380 | Data=15.99769974 | Physics=2.42568640 | Val RMSE: 0.87027138 | ‚àö(Val Loss) = 0.90998560 | Current Learning Rate: 0.0002\nEpoch 525/1000 | Train Loss=1602.19580078 | Val Loss=0.82805717 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027138 | ‚àö(Val Loss) = 0.90997648 | Current Learning Rate: 0.0002\nEpoch 526/1000 | Train Loss=1602.19592285 | Val Loss=0.82804024 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027121 | ‚àö(Val Loss) = 0.90996718 | Current Learning Rate: 0.0002\nEpoch 527/1000 | Train Loss=1602.19580078 | Val Loss=0.82802331 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027121 | ‚àö(Val Loss) = 0.90995789 | Current Learning Rate: 0.0002\nEpoch 528/1000 | Train Loss=1602.19580078 | Val Loss=0.82800829 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027127 | ‚àö(Val Loss) = 0.90994960 | Current Learning Rate: 0.0002\nEpoch 529/1000 | Train Loss=1602.19580078 | Val Loss=0.82799286 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027127 | ‚àö(Val Loss) = 0.90994114 | Current Learning Rate: 0.0002\nEpoch 530/1000 | Train Loss=1602.19592285 | Val Loss=0.82797688 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027127 | ‚àö(Val Loss) = 0.90993237 | Current Learning Rate: 0.0002\nEpoch 531/1000 | Train Loss=1602.19592285 | Val Loss=0.82796174 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027127 | ‚àö(Val Loss) = 0.90992403 | Current Learning Rate: 0.0002\nEpoch 532/1000 | Train Loss=1602.19592285 | Val Loss=0.82794619 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027109 | ‚àö(Val Loss) = 0.90991551 | Current Learning Rate: 0.0002\nEpoch 533/1000 | Train Loss=1602.19580078 | Val Loss=0.82793218 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027109 | ‚àö(Val Loss) = 0.90990776 | Current Learning Rate: 0.0002\nEpoch 534/1000 | Train Loss=1602.19567871 | Val Loss=0.82791859 | Data=15.99769974 | Physics=2.42568640 | Val RMSE: 0.87027097 | ‚àö(Val Loss) = 0.90990031 | Current Learning Rate: 0.0002\nEpoch 535/1000 | Train Loss=1602.19592285 | Val Loss=0.82790387 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027097 | ‚àö(Val Loss) = 0.90989220 | Current Learning Rate: 0.0002\nEpoch 536/1000 | Train Loss=1602.19592285 | Val Loss=0.82788950 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027097 | ‚àö(Val Loss) = 0.90988433 | Current Learning Rate: 0.0002\nEpoch 537/1000 | Train Loss=1602.19592285 | Val Loss=0.82787591 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027097 | ‚àö(Val Loss) = 0.90987688 | Current Learning Rate: 0.0002\nEpoch 538/1000 | Train Loss=1602.19616699 | Val Loss=0.82786226 | Data=15.99770451 | Physics=2.42568640 | Val RMSE: 0.87027091 | ‚àö(Val Loss) = 0.90986937 | Current Learning Rate: 0.0002\nEpoch 539/1000 | Train Loss=1602.19592285 | Val Loss=0.82784891 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027091 | ‚àö(Val Loss) = 0.90986204 | Current Learning Rate: 0.0002\nEpoch 540/1000 | Train Loss=1602.19580078 | Val Loss=0.82783526 | Data=15.99770164 | Physics=2.42568616 | Val RMSE: 0.87027091 | ‚àö(Val Loss) = 0.90985453 | Current Learning Rate: 0.0002\nEpoch 541/1000 | Train Loss=1602.19592285 | Val Loss=0.82782328 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027091 | ‚àö(Val Loss) = 0.90984792 | Current Learning Rate: 0.0002\n\n Epoch :  540 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 542/1000 | Train Loss=1602.19592285 | Val Loss=0.82781053 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027091 | ‚àö(Val Loss) = 0.90984094 | Current Learning Rate: 0.0002\nEpoch 543/1000 | Train Loss=1602.19592285 | Val Loss=0.82779866 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027091 | ‚àö(Val Loss) = 0.90983444 | Current Learning Rate: 0.0002\nEpoch 544/1000 | Train Loss=1602.19592285 | Val Loss=0.82778615 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027097 | ‚àö(Val Loss) = 0.90982753 | Current Learning Rate: 0.0002\nEpoch 545/1000 | Train Loss=1602.19580078 | Val Loss=0.82777351 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027091 | ‚àö(Val Loss) = 0.90982062 | Current Learning Rate: 0.0002\nEpoch 546/1000 | Train Loss=1602.19580078 | Val Loss=0.82776207 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027085 | ‚àö(Val Loss) = 0.90981430 | Current Learning Rate: 0.0002\nEpoch 547/1000 | Train Loss=1602.19616699 | Val Loss=0.82774985 | Data=15.99770451 | Physics=2.42568640 | Val RMSE: 0.87027085 | ‚àö(Val Loss) = 0.90980756 | Current Learning Rate: 0.0002\nEpoch 548/1000 | Train Loss=1602.19592285 | Val Loss=0.82773912 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027085 | ‚àö(Val Loss) = 0.90980172 | Current Learning Rate: 0.0002\nEpoch 549/1000 | Train Loss=1602.19592285 | Val Loss=0.82772774 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027085 | ‚àö(Val Loss) = 0.90979546 | Current Learning Rate: 0.0002\nEpoch 550/1000 | Train Loss=1602.19592285 | Val Loss=0.82771641 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027085 | ‚àö(Val Loss) = 0.90978920 | Current Learning Rate: 0.0002\nEpoch 551/1000 | Train Loss=1602.19592285 | Val Loss=0.82770520 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027091 | ‚àö(Val Loss) = 0.90978307 | Current Learning Rate: 0.0002\nEpoch 552/1000 | Train Loss=1602.19580078 | Val Loss=0.82769382 | Data=15.99770164 | Physics=2.42568616 | Val RMSE: 0.87027085 | ‚àö(Val Loss) = 0.90977681 | Current Learning Rate: 0.0002\nEpoch 553/1000 | Train Loss=1602.19567871 | Val Loss=0.82768387 | Data=15.99769974 | Physics=2.42568640 | Val RMSE: 0.87027073 | ‚àö(Val Loss) = 0.90977132 | Current Learning Rate: 0.0002\nEpoch 554/1000 | Train Loss=1602.19580078 | Val Loss=0.82767314 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027073 | ‚àö(Val Loss) = 0.90976542 | Current Learning Rate: 0.0002\nEpoch 555/1000 | Train Loss=1602.19592285 | Val Loss=0.82766342 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027073 | ‚àö(Val Loss) = 0.90976012 | Current Learning Rate: 0.0002\nEpoch 556/1000 | Train Loss=1602.19580078 | Val Loss=0.82765317 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027061 | ‚àö(Val Loss) = 0.90975446 | Current Learning Rate: 0.0002\nEpoch 557/1000 | Train Loss=1602.19580078 | Val Loss=0.82764304 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027061 | ‚àö(Val Loss) = 0.90974891 | Current Learning Rate: 0.0002\nEpoch 558/1000 | Train Loss=1602.19616699 | Val Loss=0.82763314 | Data=15.99770451 | Physics=2.42568640 | Val RMSE: 0.87027061 | ‚àö(Val Loss) = 0.90974343 | Current Learning Rate: 0.0002\nEpoch 559/1000 | Train Loss=1602.19580078 | Val Loss=0.82762277 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027061 | ‚àö(Val Loss) = 0.90973777 | Current Learning Rate: 0.0002\nEpoch 560/1000 | Train Loss=1602.19592285 | Val Loss=0.82761341 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027049 | ‚àö(Val Loss) = 0.90973258 | Current Learning Rate: 0.0002\nEpoch 561/1000 | Train Loss=1602.19592285 | Val Loss=0.82760358 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027049 | ‚àö(Val Loss) = 0.90972722 | Current Learning Rate: 0.0002\n\n Epoch :  560 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 562/1000 | Train Loss=1602.19592285 | Val Loss=0.82759523 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027055 | ‚àö(Val Loss) = 0.90972263 | Current Learning Rate: 0.0002\nEpoch 563/1000 | Train Loss=1602.19592285 | Val Loss=0.82758570 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027055 | ‚àö(Val Loss) = 0.90971738 | Current Learning Rate: 0.0002\nEpoch 564/1000 | Train Loss=1602.19592285 | Val Loss=0.82757652 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027055 | ‚àö(Val Loss) = 0.90971231 | Current Learning Rate: 0.0002\nEpoch 565/1000 | Train Loss=1602.19592285 | Val Loss=0.82756722 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027055 | ‚àö(Val Loss) = 0.90970719 | Current Learning Rate: 0.0002\nEpoch 566/1000 | Train Loss=1602.19592285 | Val Loss=0.82755834 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027061 | ‚àö(Val Loss) = 0.90970236 | Current Learning Rate: 0.0002\nEpoch 567/1000 | Train Loss=1602.19592285 | Val Loss=0.82754970 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027061 | ‚àö(Val Loss) = 0.90969759 | Current Learning Rate: 0.0002\nEpoch 568/1000 | Train Loss=1602.19580078 | Val Loss=0.82754230 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027061 | ‚àö(Val Loss) = 0.90969354 | Current Learning Rate: 0.0002\nEpoch 569/1000 | Train Loss=1602.19592285 | Val Loss=0.82753313 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027049 | ‚àö(Val Loss) = 0.90968847 | Current Learning Rate: 0.0002\nEpoch 570/1000 | Train Loss=1602.19592285 | Val Loss=0.82752419 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027049 | ‚àö(Val Loss) = 0.90968359 | Current Learning Rate: 0.0002\nEpoch 571/1000 | Train Loss=1602.19580078 | Val Loss=0.82751673 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027055 | ‚àö(Val Loss) = 0.90967947 | Current Learning Rate: 0.0002\nEpoch 572/1000 | Train Loss=1602.19580078 | Val Loss=0.82750815 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027049 | ‚àö(Val Loss) = 0.90967476 | Current Learning Rate: 0.0002\nEpoch 573/1000 | Train Loss=1602.19592285 | Val Loss=0.82750005 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027049 | ‚àö(Val Loss) = 0.90967029 | Current Learning Rate: 0.0002\nEpoch 574/1000 | Train Loss=1602.19580078 | Val Loss=0.82749176 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027049 | ‚àö(Val Loss) = 0.90966576 | Current Learning Rate: 0.0002\nEpoch 575/1000 | Train Loss=1602.19592285 | Val Loss=0.82748532 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027055 | ‚àö(Val Loss) = 0.90966219 | Current Learning Rate: 0.0002\nEpoch 576/1000 | Train Loss=1602.19592285 | Val Loss=0.82747704 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027055 | ‚àö(Val Loss) = 0.90965766 | Current Learning Rate: 0.0002\nEpoch 577/1000 | Train Loss=1602.19580078 | Val Loss=0.82746941 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027043 | ‚àö(Val Loss) = 0.90965343 | Current Learning Rate: 0.0002\nEpoch 578/1000 | Train Loss=1602.19592285 | Val Loss=0.82746178 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027037 | ‚àö(Val Loss) = 0.90964925 | Current Learning Rate: 0.0002\nEpoch 579/1000 | Train Loss=1602.19592285 | Val Loss=0.82745427 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027025 | ‚àö(Val Loss) = 0.90964514 | Current Learning Rate: 0.0002\nEpoch 580/1000 | Train Loss=1602.19592285 | Val Loss=0.82744652 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027025 | ‚àö(Val Loss) = 0.90964085 | Current Learning Rate: 0.0002\nEpoch 581/1000 | Train Loss=1602.19580078 | Val Loss=0.82743919 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027025 | ‚àö(Val Loss) = 0.90963686 | Current Learning Rate: 0.0002\n\n Epoch :  580 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 582/1000 | Train Loss=1602.19592285 | Val Loss=0.82743305 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027031 | ‚àö(Val Loss) = 0.90963346 | Current Learning Rate: 0.0002\nEpoch 583/1000 | Train Loss=1602.19592285 | Val Loss=0.82742608 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027031 | ‚àö(Val Loss) = 0.90962964 | Current Learning Rate: 0.0002\nEpoch 584/1000 | Train Loss=1602.19592285 | Val Loss=0.82741886 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90962565 | Current Learning Rate: 0.0002\nEpoch 585/1000 | Train Loss=1602.19592285 | Val Loss=0.82741213 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90962195 | Current Learning Rate: 0.0002\nEpoch 586/1000 | Train Loss=1602.19592285 | Val Loss=0.82740515 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90961814 | Current Learning Rate: 0.0002\nEpoch 587/1000 | Train Loss=1602.19580078 | Val Loss=0.82739848 | Data=15.99770164 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90961444 | Current Learning Rate: 0.0002\nEpoch 588/1000 | Train Loss=1602.19592285 | Val Loss=0.82739151 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90961063 | Current Learning Rate: 0.0002\nEpoch 589/1000 | Train Loss=1602.19592285 | Val Loss=0.82738596 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027025 | ‚àö(Val Loss) = 0.90960759 | Current Learning Rate: 0.0002\nEpoch 590/1000 | Train Loss=1602.19592285 | Val Loss=0.82737958 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027025 | ‚àö(Val Loss) = 0.90960407 | Current Learning Rate: 0.0002\nEpoch 591/1000 | Train Loss=1602.19592285 | Val Loss=0.82737315 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027025 | ‚àö(Val Loss) = 0.90960056 | Current Learning Rate: 0.0002\nEpoch 592/1000 | Train Loss=1602.19592285 | Val Loss=0.82736677 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027025 | ‚àö(Val Loss) = 0.90959704 | Current Learning Rate: 0.0002\nEpoch 593/1000 | Train Loss=1602.19592285 | Val Loss=0.82736057 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027025 | ‚àö(Val Loss) = 0.90959364 | Current Learning Rate: 0.0002\nEpoch 594/1000 | Train Loss=1602.19592285 | Val Loss=0.82735419 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90959013 | Current Learning Rate: 0.0002\nEpoch 595/1000 | Train Loss=1602.19592285 | Val Loss=0.82734805 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90958673 | Current Learning Rate: 0.0002\nEpoch 596/1000 | Train Loss=1602.19592285 | Val Loss=0.82734245 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90958369 | Current Learning Rate: 0.0002\nEpoch 597/1000 | Train Loss=1602.19592285 | Val Loss=0.82733667 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90958047 | Current Learning Rate: 0.0002\nEpoch 598/1000 | Train Loss=1602.19592285 | Val Loss=0.82733095 | Data=15.99770260 | Physics=2.42568616 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90957737 | Current Learning Rate: 0.0002\nEpoch 599/1000 | Train Loss=1602.19616699 | Val Loss=0.82732487 | Data=15.99770451 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90957403 | Current Learning Rate: 0.0002\nEpoch 600/1000 | Train Loss=1602.19592285 | Val Loss=0.82731897 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027019 | ‚àö(Val Loss) = 0.90957075 | Current Learning Rate: 0.0002\nEpoch 601/1000 | Train Loss=1602.19592285 | Val Loss=0.82731348 | Data=15.99770260 | Physics=2.42568640 | Val RMSE: 0.87027025 | ‚àö(Val Loss) = 0.90956777 | Current Learning Rate: 0.0002\n\n Epoch :  600 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 602/1000 | Train Loss=2391.06713867 | Val Loss=0.82730848 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027007 | ‚àö(Val Loss) = 0.90956497 | Current Learning Rate: 0.0002\nEpoch 603/1000 | Train Loss=2391.06713867 | Val Loss=0.82730335 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027001 | ‚àö(Val Loss) = 0.90956217 | Current Learning Rate: 0.0002\nEpoch 604/1000 | Train Loss=2391.06713867 | Val Loss=0.82729763 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026983 | ‚àö(Val Loss) = 0.90955901 | Current Learning Rate: 0.0002\nEpoch 605/1000 | Train Loss=2391.06713867 | Val Loss=0.82729226 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026989 | ‚àö(Val Loss) = 0.90955609 | Current Learning Rate: 0.0002\nEpoch 606/1000 | Train Loss=2391.06713867 | Val Loss=0.82728690 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026989 | ‚àö(Val Loss) = 0.90955311 | Current Learning Rate: 0.0002\nEpoch 607/1000 | Train Loss=2391.06713867 | Val Loss=0.82728165 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026989 | ‚àö(Val Loss) = 0.90955025 | Current Learning Rate: 0.0002\nEpoch 608/1000 | Train Loss=2391.06713867 | Val Loss=0.82727605 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026978 | ‚àö(Val Loss) = 0.90954715 | Current Learning Rate: 0.0002\nEpoch 609/1000 | Train Loss=2391.06713867 | Val Loss=0.82727170 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026978 | ‚àö(Val Loss) = 0.90954477 | Current Learning Rate: 0.0002\nEpoch 610/1000 | Train Loss=2391.06713867 | Val Loss=0.82726657 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026978 | ‚àö(Val Loss) = 0.90954196 | Current Learning Rate: 0.0002\nEpoch 611/1000 | Train Loss=2391.06713867 | Val Loss=0.82726187 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026978 | ‚àö(Val Loss) = 0.90953934 | Current Learning Rate: 0.0002\nEpoch 612/1000 | Train Loss=2391.06713867 | Val Loss=0.82725638 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026960 | ‚àö(Val Loss) = 0.90953636 | Current Learning Rate: 0.0002\nEpoch 613/1000 | Train Loss=2391.06713867 | Val Loss=0.82725149 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026954 | ‚àö(Val Loss) = 0.90953368 | Current Learning Rate: 0.0002\nEpoch 614/1000 | Train Loss=2391.06713867 | Val Loss=0.82724655 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026954 | ‚àö(Val Loss) = 0.90953094 | Current Learning Rate: 0.0002\nEpoch 615/1000 | Train Loss=2391.06713867 | Val Loss=0.82724184 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90952837 | Current Learning Rate: 0.0002\nEpoch 616/1000 | Train Loss=2391.06665039 | Val Loss=0.82723767 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90952605 | Current Learning Rate: 0.0002\nEpoch 617/1000 | Train Loss=2391.06713867 | Val Loss=0.82723296 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90952349 | Current Learning Rate: 0.0002\nEpoch 618/1000 | Train Loss=2391.06713867 | Val Loss=0.82722813 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90952080 | Current Learning Rate: 0.0002\nEpoch 619/1000 | Train Loss=2391.06713867 | Val Loss=0.82722384 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90951848 | Current Learning Rate: 0.0002\nEpoch 620/1000 | Train Loss=2391.06713867 | Val Loss=0.82721877 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90951568 | Current Learning Rate: 0.0002\nEpoch 621/1000 | Train Loss=2391.06665039 | Val Loss=0.82721454 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90951335 | Current Learning Rate: 0.0002\n\n Epoch :  620 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 622/1000 | Train Loss=2391.06713867 | Val Loss=0.82720977 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90951073 | Current Learning Rate: 0.0002\nEpoch 623/1000 | Train Loss=2391.06713867 | Val Loss=0.82720685 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026954 | ‚àö(Val Loss) = 0.90950912 | Current Learning Rate: 0.0002\nEpoch 624/1000 | Train Loss=2391.06713867 | Val Loss=0.82720172 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026936 | ‚àö(Val Loss) = 0.90950632 | Current Learning Rate: 0.0002\nEpoch 625/1000 | Train Loss=2391.06713867 | Val Loss=0.82719827 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026936 | ‚àö(Val Loss) = 0.90950441 | Current Learning Rate: 0.0002\nEpoch 626/1000 | Train Loss=2391.06713867 | Val Loss=0.82719344 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026936 | ‚àö(Val Loss) = 0.90950173 | Current Learning Rate: 0.0002\nEpoch 627/1000 | Train Loss=2391.06713867 | Val Loss=0.82718933 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026936 | ‚àö(Val Loss) = 0.90949947 | Current Learning Rate: 0.0002\nEpoch 628/1000 | Train Loss=2391.06713867 | Val Loss=0.82718480 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026936 | ‚àö(Val Loss) = 0.90949702 | Current Learning Rate: 0.0002\nEpoch 629/1000 | Train Loss=2391.06665039 | Val Loss=0.82718098 | Data=23.88640976 | Physics=2.42568616 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90949488 | Current Learning Rate: 0.0002\nEpoch 630/1000 | Train Loss=2391.06713867 | Val Loss=0.82717782 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90949315 | Current Learning Rate: 0.0002\nEpoch 631/1000 | Train Loss=2391.06713867 | Val Loss=0.82717365 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026948 | ‚àö(Val Loss) = 0.90949088 | Current Learning Rate: 0.0002\nEpoch 632/1000 | Train Loss=2391.06713867 | Val Loss=0.82716954 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026936 | ‚àö(Val Loss) = 0.90948862 | Current Learning Rate: 0.0002\nEpoch 633/1000 | Train Loss=2391.06713867 | Val Loss=0.82716560 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026936 | ‚àö(Val Loss) = 0.90948647 | Current Learning Rate: 0.0002\nEpoch 634/1000 | Train Loss=2391.06713867 | Val Loss=0.82716155 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90948421 | Current Learning Rate: 0.0002\nEpoch 635/1000 | Train Loss=2391.06665039 | Val Loss=0.82715762 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90948206 | Current Learning Rate: 0.0002\nEpoch 636/1000 | Train Loss=2391.06713867 | Val Loss=0.82715404 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90948009 | Current Learning Rate: 0.0002\nEpoch 637/1000 | Train Loss=2391.06713867 | Val Loss=0.82715064 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90947825 | Current Learning Rate: 0.0002\nEpoch 638/1000 | Train Loss=2391.06665039 | Val Loss=0.82714713 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90947628 | Current Learning Rate: 0.0002\nEpoch 639/1000 | Train Loss=2391.06713867 | Val Loss=0.82714325 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90947413 | Current Learning Rate: 0.0002\nEpoch 640/1000 | Train Loss=2391.06665039 | Val Loss=0.82713985 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026918 | ‚àö(Val Loss) = 0.90947229 | Current Learning Rate: 0.0002\nEpoch 641/1000 | Train Loss=2391.06665039 | Val Loss=0.82713562 | Data=23.88640976 | Physics=2.42568616 | Val RMSE: 0.87026918 | ‚àö(Val Loss) = 0.90946996 | Current Learning Rate: 0.0002\n\n Epoch :  640 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 642/1000 | Train Loss=2391.06665039 | Val Loss=0.82713264 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026918 | ‚àö(Val Loss) = 0.90946835 | Current Learning Rate: 0.0002\nEpoch 643/1000 | Train Loss=2391.06713867 | Val Loss=0.82712841 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026918 | ‚àö(Val Loss) = 0.90946603 | Current Learning Rate: 0.0002\nEpoch 644/1000 | Train Loss=2391.06713867 | Val Loss=0.82712597 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026918 | ‚àö(Val Loss) = 0.90946466 | Current Learning Rate: 0.0002\nEpoch 645/1000 | Train Loss=2391.06713867 | Val Loss=0.82712257 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90946281 | Current Learning Rate: 0.0002\nEpoch 646/1000 | Train Loss=2391.06665039 | Val Loss=0.82711929 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90946096 | Current Learning Rate: 0.0002\nEpoch 647/1000 | Train Loss=2391.06713867 | Val Loss=0.82711548 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90945888 | Current Learning Rate: 0.0002\nEpoch 648/1000 | Train Loss=2391.06713867 | Val Loss=0.82711214 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90945703 | Current Learning Rate: 0.0002\nEpoch 649/1000 | Train Loss=2391.06713867 | Val Loss=0.82710868 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90945518 | Current Learning Rate: 0.0002\nEpoch 650/1000 | Train Loss=2391.06713867 | Val Loss=0.82710505 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90945315 | Current Learning Rate: 0.0002\nEpoch 651/1000 | Train Loss=2391.06665039 | Val Loss=0.82710314 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026918 | ‚àö(Val Loss) = 0.90945208 | Current Learning Rate: 0.0002\nEpoch 652/1000 | Train Loss=2391.06713867 | Val Loss=0.82709992 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026918 | ‚àö(Val Loss) = 0.90945035 | Current Learning Rate: 0.0002\nEpoch 653/1000 | Train Loss=2391.06713867 | Val Loss=0.82709652 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026918 | ‚àö(Val Loss) = 0.90944844 | Current Learning Rate: 0.0002\nEpoch 654/1000 | Train Loss=2391.06713867 | Val Loss=0.82709283 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026918 | ‚àö(Val Loss) = 0.90944642 | Current Learning Rate: 0.0002\nEpoch 655/1000 | Train Loss=2391.06713867 | Val Loss=0.82709044 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026918 | ‚àö(Val Loss) = 0.90944511 | Current Learning Rate: 0.0002\nEpoch 656/1000 | Train Loss=2391.06713867 | Val Loss=0.82708716 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90944332 | Current Learning Rate: 0.0002\nEpoch 657/1000 | Train Loss=2391.06713867 | Val Loss=0.82708347 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90944129 | Current Learning Rate: 0.0002\nEpoch 658/1000 | Train Loss=2391.06738281 | Val Loss=0.82708210 | Data=23.88641548 | Physics=2.42568616 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90944052 | Current Learning Rate: 0.0002\nEpoch 659/1000 | Train Loss=2391.06713867 | Val Loss=0.82707876 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90943873 | Current Learning Rate: 0.0002\nEpoch 660/1000 | Train Loss=2391.06713867 | Val Loss=0.82707572 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026924 | ‚àö(Val Loss) = 0.90943706 | Current Learning Rate: 0.0002\nEpoch 661/1000 | Train Loss=2391.06665039 | Val Loss=0.82707274 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026912 | ‚àö(Val Loss) = 0.90943539 | Current Learning Rate: 0.0002\n\n Epoch :  660 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 662/1000 | Train Loss=2391.06713867 | Val Loss=0.82706964 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026912 | ‚àö(Val Loss) = 0.90943366 | Current Learning Rate: 0.0002\nEpoch 663/1000 | Train Loss=2391.06713867 | Val Loss=0.82706666 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90943205 | Current Learning Rate: 0.0002\nEpoch 664/1000 | Train Loss=2391.06713867 | Val Loss=0.82706475 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026900 | ‚àö(Val Loss) = 0.90943098 | Current Learning Rate: 0.0002\nEpoch 665/1000 | Train Loss=2391.06713867 | Val Loss=0.82706165 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026900 | ‚àö(Val Loss) = 0.90942931 | Current Learning Rate: 0.0002\nEpoch 666/1000 | Train Loss=2391.06713867 | Val Loss=0.82705891 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90942782 | Current Learning Rate: 0.0002\nEpoch 667/1000 | Train Loss=2391.06713867 | Val Loss=0.82705617 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90942627 | Current Learning Rate: 0.0002\nEpoch 668/1000 | Train Loss=2391.06713867 | Val Loss=0.82705337 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90942472 | Current Learning Rate: 0.0002\nEpoch 669/1000 | Train Loss=2391.06713867 | Val Loss=0.82705051 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90942317 | Current Learning Rate: 0.0002\nEpoch 670/1000 | Train Loss=2391.06713867 | Val Loss=0.82704771 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026894 | ‚àö(Val Loss) = 0.90942162 | Current Learning Rate: 0.0002\nEpoch 671/1000 | Train Loss=2391.06713867 | Val Loss=0.82704592 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026894 | ‚àö(Val Loss) = 0.90942067 | Current Learning Rate: 0.0002\nEpoch 672/1000 | Train Loss=2391.06713867 | Val Loss=0.82704312 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026894 | ‚àö(Val Loss) = 0.90941912 | Current Learning Rate: 0.0002\nEpoch 673/1000 | Train Loss=2391.06713867 | Val Loss=0.82704031 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026900 | ‚àö(Val Loss) = 0.90941757 | Current Learning Rate: 0.0002\nEpoch 674/1000 | Train Loss=2391.06713867 | Val Loss=0.82703775 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026900 | ‚àö(Val Loss) = 0.90941614 | Current Learning Rate: 0.0002\nEpoch 675/1000 | Train Loss=2391.06713867 | Val Loss=0.82703555 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026900 | ‚àö(Val Loss) = 0.90941495 | Current Learning Rate: 0.0002\nEpoch 676/1000 | Train Loss=2391.06713867 | Val Loss=0.82703292 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90941352 | Current Learning Rate: 0.0002\nEpoch 677/1000 | Train Loss=2391.06713867 | Val Loss=0.82703006 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90941191 | Current Learning Rate: 0.0002\nEpoch 678/1000 | Train Loss=2391.06713867 | Val Loss=0.82702845 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90941107 | Current Learning Rate: 0.0002\nEpoch 679/1000 | Train Loss=2391.06713867 | Val Loss=0.82702595 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90940964 | Current Learning Rate: 0.0002\nEpoch 680/1000 | Train Loss=2391.06713867 | Val Loss=0.82702392 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90940857 | Current Learning Rate: 0.0002\nEpoch 681/1000 | Train Loss=2391.06713867 | Val Loss=0.82702076 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90940684 | Current Learning Rate: 0.0002\n\n Epoch :  680 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 682/1000 | Train Loss=2391.06713867 | Val Loss=0.82701838 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90940553 | Current Learning Rate: 0.0002\nEpoch 683/1000 | Train Loss=2391.06713867 | Val Loss=0.82701576 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026900 | ‚àö(Val Loss) = 0.90940404 | Current Learning Rate: 0.0002\nEpoch 684/1000 | Train Loss=2391.06713867 | Val Loss=0.82701373 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026900 | ‚àö(Val Loss) = 0.90940297 | Current Learning Rate: 0.0002\nEpoch 685/1000 | Train Loss=2391.06738281 | Val Loss=0.82701290 | Data=23.88641548 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90940249 | Current Learning Rate: 0.0002\nEpoch 686/1000 | Train Loss=2391.06713867 | Val Loss=0.82700992 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90940088 | Current Learning Rate: 0.0002\nEpoch 687/1000 | Train Loss=2391.06713867 | Val Loss=0.82700801 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90939981 | Current Learning Rate: 0.0002\nEpoch 688/1000 | Train Loss=2391.06713867 | Val Loss=0.82700491 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90939808 | Current Learning Rate: 0.0002\nEpoch 689/1000 | Train Loss=2391.06713867 | Val Loss=0.82700288 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90939701 | Current Learning Rate: 0.0002\nEpoch 690/1000 | Train Loss=2391.06713867 | Val Loss=0.82700086 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90939587 | Current Learning Rate: 0.0002\nEpoch 691/1000 | Train Loss=2391.06713867 | Val Loss=0.82699907 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90939492 | Current Learning Rate: 0.0002\nEpoch 692/1000 | Train Loss=2391.06665039 | Val Loss=0.82699686 | Data=23.88640976 | Physics=2.42568616 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90939367 | Current Learning Rate: 0.0002\nEpoch 693/1000 | Train Loss=2391.06713867 | Val Loss=0.82699478 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026906 | ‚àö(Val Loss) = 0.90939254 | Current Learning Rate: 0.0002\nEpoch 694/1000 | Train Loss=2391.06665039 | Val Loss=0.82699329 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026894 | ‚àö(Val Loss) = 0.90939170 | Current Learning Rate: 0.0002\nEpoch 695/1000 | Train Loss=2391.06713867 | Val Loss=0.82699037 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026894 | ‚àö(Val Loss) = 0.90939009 | Current Learning Rate: 0.0002\nEpoch 696/1000 | Train Loss=2391.06738281 | Val Loss=0.82698888 | Data=23.88641548 | Physics=2.42568640 | Val RMSE: 0.87026900 | ‚àö(Val Loss) = 0.90938926 | Current Learning Rate: 0.0002\nEpoch 697/1000 | Train Loss=2391.06713867 | Val Loss=0.82698649 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87026900 | ‚àö(Val Loss) = 0.90938801 | Current Learning Rate: 0.0002\nEpoch 698/1000 | Train Loss=2391.06665039 | Val Loss=0.82698393 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026882 | ‚àö(Val Loss) = 0.90938658 | Current Learning Rate: 0.0002\nEpoch 699/1000 | Train Loss=2391.06713867 | Val Loss=0.82698250 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026888 | ‚àö(Val Loss) = 0.90938580 | Current Learning Rate: 0.0002\nEpoch 700/1000 | Train Loss=2391.06713867 | Val Loss=0.82698083 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87026888 | ‚àö(Val Loss) = 0.90938485 | Current Learning Rate: 0.0002\nEpoch 701/1000 | Train Loss=2391.06665039 | Val Loss=0.82697934 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87026888 | ‚àö(Val Loss) = 0.90938407 | Current Learning Rate: 0.0002\n‚úÖ Learning Rate updated to 0.001\n\n Epoch :  700 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 702/1000 | Train Loss=2391.06713867 | Val Loss=0.87786144 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021452 | ‚àö(Val Loss) = 0.93694258 | Current Learning Rate: 0.001\nEpoch 703/1000 | Train Loss=2391.06665039 | Val Loss=0.87777305 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87021446 | ‚àö(Val Loss) = 0.93689543 | Current Learning Rate: 0.001\nEpoch 704/1000 | Train Loss=2391.06713867 | Val Loss=0.87768310 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021446 | ‚àö(Val Loss) = 0.93684745 | Current Learning Rate: 0.001\nEpoch 705/1000 | Train Loss=2391.06713867 | Val Loss=0.87759572 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021440 | ‚àö(Val Loss) = 0.93680078 | Current Learning Rate: 0.001\nEpoch 706/1000 | Train Loss=2391.06665039 | Val Loss=0.87750882 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87021440 | ‚àö(Val Loss) = 0.93675441 | Current Learning Rate: 0.001\nEpoch 707/1000 | Train Loss=2391.06713867 | Val Loss=0.87742251 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021416 | ‚àö(Val Loss) = 0.93670833 | Current Learning Rate: 0.001\nEpoch 708/1000 | Train Loss=2391.06665039 | Val Loss=0.87733489 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87021416 | ‚àö(Val Loss) = 0.93666154 | Current Learning Rate: 0.001\nEpoch 709/1000 | Train Loss=2391.06713867 | Val Loss=0.87725085 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021422 | ‚àö(Val Loss) = 0.93661672 | Current Learning Rate: 0.001\nEpoch 710/1000 | Train Loss=2391.06713867 | Val Loss=0.87716544 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021416 | ‚àö(Val Loss) = 0.93657112 | Current Learning Rate: 0.001\nEpoch 711/1000 | Train Loss=2391.06713867 | Val Loss=0.87707990 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021422 | ‚àö(Val Loss) = 0.93652546 | Current Learning Rate: 0.001\nEpoch 712/1000 | Train Loss=2391.06713867 | Val Loss=0.87699741 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021410 | ‚àö(Val Loss) = 0.93648142 | Current Learning Rate: 0.001\nEpoch 713/1000 | Train Loss=2391.06713867 | Val Loss=0.87691551 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021416 | ‚àö(Val Loss) = 0.93643767 | Current Learning Rate: 0.001\nEpoch 714/1000 | Train Loss=2391.06713867 | Val Loss=0.87683004 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021405 | ‚àö(Val Loss) = 0.93639201 | Current Learning Rate: 0.001\nEpoch 715/1000 | Train Loss=2391.06713867 | Val Loss=0.87674850 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021399 | ‚àö(Val Loss) = 0.93634850 | Current Learning Rate: 0.001\nEpoch 716/1000 | Train Loss=2391.06713867 | Val Loss=0.87666684 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021399 | ‚àö(Val Loss) = 0.93630487 | Current Learning Rate: 0.001\nEpoch 717/1000 | Train Loss=2391.06713867 | Val Loss=0.87658596 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021405 | ‚àö(Val Loss) = 0.93626171 | Current Learning Rate: 0.001\nEpoch 718/1000 | Train Loss=2391.06665039 | Val Loss=0.87650418 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87021399 | ‚àö(Val Loss) = 0.93621802 | Current Learning Rate: 0.001\nEpoch 719/1000 | Train Loss=2391.06713867 | Val Loss=0.87642539 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021399 | ‚àö(Val Loss) = 0.93617594 | Current Learning Rate: 0.001\nEpoch 720/1000 | Train Loss=2391.06713867 | Val Loss=0.87634736 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021387 | ‚àö(Val Loss) = 0.93613428 | Current Learning Rate: 0.001\nEpoch 721/1000 | Train Loss=2391.06713867 | Val Loss=0.87626797 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021393 | ‚àö(Val Loss) = 0.93609184 | Current Learning Rate: 0.001\n\n Epoch :  720 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 722/1000 | Train Loss=2391.06713867 | Val Loss=0.87618804 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021369 | ‚àö(Val Loss) = 0.93604916 | Current Learning Rate: 0.001\nEpoch 723/1000 | Train Loss=2391.06713867 | Val Loss=0.87611014 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021375 | ‚àö(Val Loss) = 0.93600756 | Current Learning Rate: 0.001\nEpoch 724/1000 | Train Loss=2391.06713867 | Val Loss=0.87603408 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021375 | ‚àö(Val Loss) = 0.93596691 | Current Learning Rate: 0.001\nEpoch 725/1000 | Train Loss=2391.06713867 | Val Loss=0.87595642 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021375 | ‚àö(Val Loss) = 0.93592542 | Current Learning Rate: 0.001\nEpoch 726/1000 | Train Loss=2391.06665039 | Val Loss=0.87588000 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87021375 | ‚àö(Val Loss) = 0.93588459 | Current Learning Rate: 0.001\nEpoch 727/1000 | Train Loss=2391.06713867 | Val Loss=0.87580395 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021387 | ‚àö(Val Loss) = 0.93584400 | Current Learning Rate: 0.001\nEpoch 728/1000 | Train Loss=2391.06713867 | Val Loss=0.87572885 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021369 | ‚àö(Val Loss) = 0.93580383 | Current Learning Rate: 0.001\nEpoch 729/1000 | Train Loss=2391.06713867 | Val Loss=0.87565321 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021363 | ‚àö(Val Loss) = 0.93576342 | Current Learning Rate: 0.001\nEpoch 730/1000 | Train Loss=2391.06665039 | Val Loss=0.87558007 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87021351 | ‚àö(Val Loss) = 0.93572438 | Current Learning Rate: 0.001\nEpoch 731/1000 | Train Loss=2391.06713867 | Val Loss=0.87550575 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021357 | ‚àö(Val Loss) = 0.93568462 | Current Learning Rate: 0.001\nEpoch 732/1000 | Train Loss=2391.06713867 | Val Loss=0.87543130 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021363 | ‚àö(Val Loss) = 0.93564487 | Current Learning Rate: 0.001\nEpoch 733/1000 | Train Loss=2391.06713867 | Val Loss=0.87535721 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021357 | ‚àö(Val Loss) = 0.93560529 | Current Learning Rate: 0.001\nEpoch 734/1000 | Train Loss=2391.06713867 | Val Loss=0.87528616 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021363 | ‚àö(Val Loss) = 0.93556732 | Current Learning Rate: 0.001\nEpoch 735/1000 | Train Loss=2391.06713867 | Val Loss=0.87521398 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021345 | ‚àö(Val Loss) = 0.93552870 | Current Learning Rate: 0.001\nEpoch 736/1000 | Train Loss=2391.06713867 | Val Loss=0.87514251 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021351 | ‚àö(Val Loss) = 0.93549055 | Current Learning Rate: 0.001\nEpoch 737/1000 | Train Loss=2391.06713867 | Val Loss=0.87507105 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021345 | ‚àö(Val Loss) = 0.93545234 | Current Learning Rate: 0.001\nEpoch 738/1000 | Train Loss=2391.06713867 | Val Loss=0.87500036 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021333 | ‚àö(Val Loss) = 0.93541455 | Current Learning Rate: 0.001\nEpoch 739/1000 | Train Loss=2391.06713867 | Val Loss=0.87493020 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021333 | ‚àö(Val Loss) = 0.93537706 | Current Learning Rate: 0.001\nEpoch 740/1000 | Train Loss=2391.06665039 | Val Loss=0.87486064 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87021345 | ‚àö(Val Loss) = 0.93533987 | Current Learning Rate: 0.001\nEpoch 741/1000 | Train Loss=2391.06713867 | Val Loss=0.87479204 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021333 | ‚àö(Val Loss) = 0.93530315 | Current Learning Rate: 0.001\n\n Epoch :  740 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 742/1000 | Train Loss=2391.06713867 | Val Loss=0.87472212 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021345 | ‚àö(Val Loss) = 0.93526578 | Current Learning Rate: 0.001\nEpoch 743/1000 | Train Loss=2391.06713867 | Val Loss=0.87465274 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021327 | ‚àö(Val Loss) = 0.93522871 | Current Learning Rate: 0.001\nEpoch 744/1000 | Train Loss=2391.06713867 | Val Loss=0.87458682 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021333 | ‚àö(Val Loss) = 0.93519348 | Current Learning Rate: 0.001\nEpoch 745/1000 | Train Loss=2391.06713867 | Val Loss=0.87451786 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021327 | ‚àö(Val Loss) = 0.93515658 | Current Learning Rate: 0.001\nEpoch 746/1000 | Train Loss=2391.06713867 | Val Loss=0.87444979 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021321 | ‚àö(Val Loss) = 0.93512022 | Current Learning Rate: 0.001\nEpoch 747/1000 | Train Loss=2391.06713867 | Val Loss=0.87438256 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021321 | ‚àö(Val Loss) = 0.93508422 | Current Learning Rate: 0.001\nEpoch 748/1000 | Train Loss=2391.06713867 | Val Loss=0.87431598 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021327 | ‚àö(Val Loss) = 0.93504864 | Current Learning Rate: 0.001\nEpoch 749/1000 | Train Loss=2391.06713867 | Val Loss=0.87425089 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021321 | ‚àö(Val Loss) = 0.93501383 | Current Learning Rate: 0.001\nEpoch 750/1000 | Train Loss=2391.06713867 | Val Loss=0.87418431 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021321 | ‚àö(Val Loss) = 0.93497825 | Current Learning Rate: 0.001\nEpoch 751/1000 | Train Loss=2391.06713867 | Val Loss=0.87411988 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021315 | ‚àö(Val Loss) = 0.93494380 | Current Learning Rate: 0.001\nEpoch 752/1000 | Train Loss=2391.06713867 | Val Loss=0.87405443 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021321 | ‚àö(Val Loss) = 0.93490881 | Current Learning Rate: 0.001\nEpoch 753/1000 | Train Loss=2391.06713867 | Val Loss=0.87399173 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021309 | ‚àö(Val Loss) = 0.93487525 | Current Learning Rate: 0.001\nEpoch 754/1000 | Train Loss=2391.06713867 | Val Loss=0.87392527 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021303 | ‚àö(Val Loss) = 0.93483973 | Current Learning Rate: 0.001\nEpoch 755/1000 | Train Loss=2391.06713867 | Val Loss=0.87386221 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021303 | ‚àö(Val Loss) = 0.93480599 | Current Learning Rate: 0.001\nEpoch 756/1000 | Train Loss=2391.06713867 | Val Loss=0.87379861 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021309 | ‚àö(Val Loss) = 0.93477196 | Current Learning Rate: 0.001\nEpoch 757/1000 | Train Loss=2391.06713867 | Val Loss=0.87373674 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021303 | ‚àö(Val Loss) = 0.93473887 | Current Learning Rate: 0.001\nEpoch 758/1000 | Train Loss=2391.06713867 | Val Loss=0.87367243 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021303 | ‚àö(Val Loss) = 0.93470448 | Current Learning Rate: 0.001\nEpoch 759/1000 | Train Loss=2391.06665039 | Val Loss=0.87361169 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87021297 | ‚àö(Val Loss) = 0.93467200 | Current Learning Rate: 0.001\nEpoch 760/1000 | Train Loss=2391.06738281 | Val Loss=0.87355042 | Data=23.88641548 | Physics=2.42568640 | Val RMSE: 0.87021303 | ‚àö(Val Loss) = 0.93463922 | Current Learning Rate: 0.001\nEpoch 761/1000 | Train Loss=2391.06713867 | Val Loss=0.87348825 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021291 | ‚àö(Val Loss) = 0.93460596 | Current Learning Rate: 0.001\n\n Epoch :  760 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 762/1000 | Train Loss=2391.06665039 | Val Loss=0.87342608 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87021279 | ‚àö(Val Loss) = 0.93457270 | Current Learning Rate: 0.001\nEpoch 763/1000 | Train Loss=2391.06713867 | Val Loss=0.87336659 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021285 | ‚àö(Val Loss) = 0.93454087 | Current Learning Rate: 0.001\nEpoch 764/1000 | Train Loss=2391.06713867 | Val Loss=0.87330467 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021291 | ‚àö(Val Loss) = 0.93450773 | Current Learning Rate: 0.001\nEpoch 765/1000 | Train Loss=2391.06713867 | Val Loss=0.87324512 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021285 | ‚àö(Val Loss) = 0.93447584 | Current Learning Rate: 0.001\nEpoch 766/1000 | Train Loss=2391.06713867 | Val Loss=0.87318641 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021291 | ‚àö(Val Loss) = 0.93444443 | Current Learning Rate: 0.001\nEpoch 767/1000 | Train Loss=2391.06713867 | Val Loss=0.87312704 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021297 | ‚àö(Val Loss) = 0.93441266 | Current Learning Rate: 0.001\nEpoch 768/1000 | Train Loss=2391.06713867 | Val Loss=0.87306684 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021279 | ‚àö(Val Loss) = 0.93438047 | Current Learning Rate: 0.001\nEpoch 769/1000 | Train Loss=2391.06713867 | Val Loss=0.87300909 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021279 | ‚àö(Val Loss) = 0.93434954 | Current Learning Rate: 0.001\nEpoch 770/1000 | Train Loss=2391.06713867 | Val Loss=0.87295109 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021267 | ‚àö(Val Loss) = 0.93431854 | Current Learning Rate: 0.001\nEpoch 771/1000 | Train Loss=2391.06713867 | Val Loss=0.87289238 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021267 | ‚àö(Val Loss) = 0.93428707 | Current Learning Rate: 0.001\nEpoch 772/1000 | Train Loss=2391.06713867 | Val Loss=0.87283450 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021279 | ‚àö(Val Loss) = 0.93425614 | Current Learning Rate: 0.001\nEpoch 773/1000 | Train Loss=2391.06713867 | Val Loss=0.87277693 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021273 | ‚àö(Val Loss) = 0.93422532 | Current Learning Rate: 0.001\nEpoch 774/1000 | Train Loss=2391.06713867 | Val Loss=0.87272066 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021279 | ‚àö(Val Loss) = 0.93419522 | Current Learning Rate: 0.001\nEpoch 775/1000 | Train Loss=2391.06713867 | Val Loss=0.87266380 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021279 | ‚àö(Val Loss) = 0.93416476 | Current Learning Rate: 0.001\nEpoch 776/1000 | Train Loss=2391.06738281 | Val Loss=0.87260747 | Data=23.88641548 | Physics=2.42568640 | Val RMSE: 0.87021267 | ‚àö(Val Loss) = 0.93413460 | Current Learning Rate: 0.001\nEpoch 777/1000 | Train Loss=2391.06713867 | Val Loss=0.87255377 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021273 | ‚àö(Val Loss) = 0.93410587 | Current Learning Rate: 0.001\nEpoch 778/1000 | Train Loss=2391.06713867 | Val Loss=0.87249470 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021261 | ‚àö(Val Loss) = 0.93407422 | Current Learning Rate: 0.001\nEpoch 779/1000 | Train Loss=2391.06713867 | Val Loss=0.87244046 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021255 | ‚àö(Val Loss) = 0.93404520 | Current Learning Rate: 0.001\nEpoch 780/1000 | Train Loss=2391.06713867 | Val Loss=0.87238622 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021261 | ‚àö(Val Loss) = 0.93401617 | Current Learning Rate: 0.001\nEpoch 781/1000 | Train Loss=2391.06713867 | Val Loss=0.87233227 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021267 | ‚àö(Val Loss) = 0.93398732 | Current Learning Rate: 0.001\n\n Epoch :  780 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 782/1000 | Train Loss=2391.06713867 | Val Loss=0.87227523 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021261 | ‚àö(Val Loss) = 0.93395674 | Current Learning Rate: 0.001\nEpoch 783/1000 | Train Loss=2391.06713867 | Val Loss=0.87222284 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021267 | ‚àö(Val Loss) = 0.93392873 | Current Learning Rate: 0.001\nEpoch 784/1000 | Train Loss=2391.06713867 | Val Loss=0.87216842 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021255 | ‚àö(Val Loss) = 0.93389958 | Current Learning Rate: 0.001\nEpoch 785/1000 | Train Loss=2391.06713867 | Val Loss=0.87211591 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021261 | ‚àö(Val Loss) = 0.93387145 | Current Learning Rate: 0.001\nEpoch 786/1000 | Train Loss=2391.06713867 | Val Loss=0.87206268 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021250 | ‚àö(Val Loss) = 0.93384296 | Current Learning Rate: 0.001\nEpoch 787/1000 | Train Loss=2391.06713867 | Val Loss=0.87200898 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021250 | ‚àö(Val Loss) = 0.93381423 | Current Learning Rate: 0.001\nEpoch 788/1000 | Train Loss=2391.06713867 | Val Loss=0.87195629 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021250 | ‚àö(Val Loss) = 0.93378597 | Current Learning Rate: 0.001\nEpoch 789/1000 | Train Loss=2391.06738281 | Val Loss=0.87190413 | Data=23.88641548 | Physics=2.42568640 | Val RMSE: 0.87021250 | ‚àö(Val Loss) = 0.93375808 | Current Learning Rate: 0.001\nEpoch 790/1000 | Train Loss=2391.06713867 | Val Loss=0.87185192 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021255 | ‚àö(Val Loss) = 0.93373013 | Current Learning Rate: 0.001\nEpoch 791/1000 | Train Loss=2391.06713867 | Val Loss=0.87180066 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021255 | ‚àö(Val Loss) = 0.93370265 | Current Learning Rate: 0.001\nEpoch 792/1000 | Train Loss=2391.06713867 | Val Loss=0.87174886 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021255 | ‚àö(Val Loss) = 0.93367493 | Current Learning Rate: 0.001\nEpoch 793/1000 | Train Loss=2391.06713867 | Val Loss=0.87169689 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021250 | ‚àö(Val Loss) = 0.93364710 | Current Learning Rate: 0.001\nEpoch 794/1000 | Train Loss=2391.06665039 | Val Loss=0.87164843 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87021250 | ‚àö(Val Loss) = 0.93362117 | Current Learning Rate: 0.001\nEpoch 795/1000 | Train Loss=2391.06713867 | Val Loss=0.87159681 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021250 | ‚àö(Val Loss) = 0.93359351 | Current Learning Rate: 0.001\nEpoch 796/1000 | Train Loss=2391.06713867 | Val Loss=0.87154651 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021238 | ‚àö(Val Loss) = 0.93356657 | Current Learning Rate: 0.001\nEpoch 797/1000 | Train Loss=2391.06713867 | Val Loss=0.87149531 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021238 | ‚àö(Val Loss) = 0.93353915 | Current Learning Rate: 0.001\nEpoch 798/1000 | Train Loss=2391.06713867 | Val Loss=0.87144607 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021250 | ‚àö(Val Loss) = 0.93351275 | Current Learning Rate: 0.001\nEpoch 799/1000 | Train Loss=2391.06713867 | Val Loss=0.87139767 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87021238 | ‚àö(Val Loss) = 0.93348682 | Current Learning Rate: 0.001\nEpoch 800/1000 | Train Loss=2391.06713867 | Val Loss=0.87134588 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87021238 | ‚àö(Val Loss) = 0.93345910 | Current Learning Rate: 0.0001\nEpoch 801/1000 | Train Loss=2391.06713867 | Val Loss=0.82574296 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870398 | Current Learning Rate: 0.0001\n\n Epoch :  800 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 802/1000 | Train Loss=2391.06738281 | Val Loss=0.82574302 | Data=23.88641548 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870404 | Current Learning Rate: 0.0001\nEpoch 803/1000 | Train Loss=2391.06665039 | Val Loss=0.82574254 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870374 | Current Learning Rate: 0.0001\nEpoch 804/1000 | Train Loss=2391.06713867 | Val Loss=0.82574368 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870440 | Current Learning Rate: 0.0001\nEpoch 805/1000 | Train Loss=2391.06713867 | Val Loss=0.82574356 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870434 | Current Learning Rate: 0.0001\nEpoch 806/1000 | Train Loss=2391.06713867 | Val Loss=0.82574368 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870440 | Current Learning Rate: 0.0001\nEpoch 807/1000 | Train Loss=2391.06665039 | Val Loss=0.82574427 | Data=23.88640976 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870470 | Current Learning Rate: 0.0001\nEpoch 808/1000 | Train Loss=2391.06713867 | Val Loss=0.82574522 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870523 | Current Learning Rate: 0.0001\nEpoch 809/1000 | Train Loss=2391.06713867 | Val Loss=0.82574493 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870506 | Current Learning Rate: 0.0001\nEpoch 810/1000 | Train Loss=2391.06713867 | Val Loss=0.82574505 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870517 | Current Learning Rate: 0.0001\nEpoch 811/1000 | Train Loss=2391.06738281 | Val Loss=0.82574564 | Data=23.88641548 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870547 | Current Learning Rate: 0.0001\nEpoch 812/1000 | Train Loss=2391.06738281 | Val Loss=0.82574582 | Data=23.88641548 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870559 | Current Learning Rate: 0.0001\nEpoch 813/1000 | Train Loss=2391.06713867 | Val Loss=0.82574528 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870529 | Current Learning Rate: 0.0001\nEpoch 814/1000 | Train Loss=2391.06713867 | Val Loss=0.82574624 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870583 | Current Learning Rate: 0.0001\nEpoch 815/1000 | Train Loss=2391.06713867 | Val Loss=0.82574648 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870595 | Current Learning Rate: 0.0001\nEpoch 816/1000 | Train Loss=2391.06665039 | Val Loss=0.82574612 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870571 | Current Learning Rate: 0.0001\nEpoch 817/1000 | Train Loss=2391.06713867 | Val Loss=0.82574630 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870583 | Current Learning Rate: 0.0001\nEpoch 818/1000 | Train Loss=2391.06713867 | Val Loss=0.82574719 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870631 | Current Learning Rate: 0.0001\nEpoch 819/1000 | Train Loss=2391.06713867 | Val Loss=0.82574719 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870631 | Current Learning Rate: 0.0001\nEpoch 820/1000 | Train Loss=2391.06713867 | Val Loss=0.82574683 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870613 | Current Learning Rate: 0.0001\nEpoch 821/1000 | Train Loss=2391.06713867 | Val Loss=0.82574868 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90870714 | Current Learning Rate: 0.0001\n\n Epoch :  820 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 822/1000 | Train Loss=2391.06713867 | Val Loss=0.82574868 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90870714 | Current Learning Rate: 0.0001\nEpoch 823/1000 | Train Loss=2391.06713867 | Val Loss=0.82574868 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90870714 | Current Learning Rate: 0.0001\nEpoch 824/1000 | Train Loss=2391.06665039 | Val Loss=0.82574916 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90870744 | Current Learning Rate: 0.0001\nEpoch 825/1000 | Train Loss=2391.06713867 | Val Loss=0.82574934 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90870750 | Current Learning Rate: 0.0001\nEpoch 826/1000 | Train Loss=2391.06713867 | Val Loss=0.82574904 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870732 | Current Learning Rate: 0.0001\nEpoch 827/1000 | Train Loss=2391.06713867 | Val Loss=0.82574898 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870732 | Current Learning Rate: 0.0001\nEpoch 828/1000 | Train Loss=2391.06713867 | Val Loss=0.82574970 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870768 | Current Learning Rate: 0.0001\nEpoch 829/1000 | Train Loss=2391.06713867 | Val Loss=0.82575017 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870798 | Current Learning Rate: 0.0001\nEpoch 830/1000 | Train Loss=2391.06713867 | Val Loss=0.82575017 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870798 | Current Learning Rate: 0.0001\nEpoch 831/1000 | Train Loss=2391.06713867 | Val Loss=0.82575101 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870845 | Current Learning Rate: 0.0001\nEpoch 832/1000 | Train Loss=2391.06713867 | Val Loss=0.82575053 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870816 | Current Learning Rate: 0.0001\nEpoch 833/1000 | Train Loss=2391.06713867 | Val Loss=0.82575053 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870816 | Current Learning Rate: 0.0001\nEpoch 834/1000 | Train Loss=2391.06713867 | Val Loss=0.82575125 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90870857 | Current Learning Rate: 0.0001\nEpoch 835/1000 | Train Loss=2391.06713867 | Val Loss=0.82575238 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90870917 | Current Learning Rate: 0.0001\nEpoch 836/1000 | Train Loss=2391.06713867 | Val Loss=0.82575238 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90870917 | Current Learning Rate: 0.0001\nEpoch 837/1000 | Train Loss=2391.06665039 | Val Loss=0.82575226 | Data=23.88640976 | Physics=2.42568616 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90870911 | Current Learning Rate: 0.0001\nEpoch 838/1000 | Train Loss=2391.06713867 | Val Loss=0.82575291 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90870947 | Current Learning Rate: 0.0001\nEpoch 839/1000 | Train Loss=2391.06713867 | Val Loss=0.82575321 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90870965 | Current Learning Rate: 0.0001\nEpoch 840/1000 | Train Loss=2391.06713867 | Val Loss=0.82575297 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90870953 | Current Learning Rate: 0.0001\nEpoch 841/1000 | Train Loss=2391.06738281 | Val Loss=0.82575387 | Data=23.88641548 | Physics=2.42568616 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90871000 | Current Learning Rate: 0.0001\n\n Epoch :  840 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 842/1000 | Train Loss=2391.06713867 | Val Loss=0.82575381 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90870994 | Current Learning Rate: 0.0001\nEpoch 843/1000 | Train Loss=2391.06713867 | Val Loss=0.82575393 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90871006 | Current Learning Rate: 0.0001\nEpoch 844/1000 | Train Loss=2391.06713867 | Val Loss=0.82575428 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90871024 | Current Learning Rate: 0.0001\nEpoch 845/1000 | Train Loss=2391.06713867 | Val Loss=0.82575446 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90871030 | Current Learning Rate: 0.0001\nEpoch 846/1000 | Train Loss=2391.06665039 | Val Loss=0.82575452 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90871036 | Current Learning Rate: 0.0001\nEpoch 847/1000 | Train Loss=2391.06713867 | Val Loss=0.82575446 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90871030 | Current Learning Rate: 0.0001\nEpoch 848/1000 | Train Loss=2391.06713867 | Val Loss=0.82575530 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90871078 | Current Learning Rate: 0.0001\nEpoch 849/1000 | Train Loss=2391.06713867 | Val Loss=0.82575625 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90871131 | Current Learning Rate: 0.0001\nEpoch 850/1000 | Train Loss=2391.06713867 | Val Loss=0.82575607 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90871119 | Current Learning Rate: 0.0001\nEpoch 851/1000 | Train Loss=2391.06713867 | Val Loss=0.82575697 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90871173 | Current Learning Rate: 0.0001\nEpoch 852/1000 | Train Loss=2391.06713867 | Val Loss=0.82575738 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90871191 | Current Learning Rate: 0.0001\nEpoch 853/1000 | Train Loss=2391.06665039 | Val Loss=0.82575697 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90871173 | Current Learning Rate: 0.0001\nEpoch 854/1000 | Train Loss=2391.06713867 | Val Loss=0.82575750 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90871203 | Current Learning Rate: 0.0001\nEpoch 855/1000 | Train Loss=2391.06713867 | Val Loss=0.82575774 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90871215 | Current Learning Rate: 0.0001\nEpoch 856/1000 | Train Loss=2391.06665039 | Val Loss=0.82575774 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90871215 | Current Learning Rate: 0.0001\nEpoch 857/1000 | Train Loss=2391.06713867 | Val Loss=0.82575756 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90871203 | Current Learning Rate: 0.0001\nEpoch 858/1000 | Train Loss=2391.06713867 | Val Loss=0.82575893 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90871280 | Current Learning Rate: 0.0001\nEpoch 859/1000 | Train Loss=2391.06713867 | Val Loss=0.82575834 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90871245 | Current Learning Rate: 0.0001\nEpoch 860/1000 | Train Loss=2391.06713867 | Val Loss=0.82575834 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90871245 | Current Learning Rate: 0.0001\nEpoch 861/1000 | Train Loss=2391.06713867 | Val Loss=0.82575935 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871304 | Current Learning Rate: 0.0001\n\n Epoch :  860 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 862/1000 | Train Loss=2391.06713867 | Val Loss=0.82575911 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871286 | Current Learning Rate: 0.0001\nEpoch 863/1000 | Train Loss=2391.06713867 | Val Loss=0.82576048 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871364 | Current Learning Rate: 0.0001\nEpoch 864/1000 | Train Loss=2391.06713867 | Val Loss=0.82576048 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90871364 | Current Learning Rate: 0.0001\nEpoch 865/1000 | Train Loss=2391.06713867 | Val Loss=0.82576078 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90871382 | Current Learning Rate: 0.0001\nEpoch 866/1000 | Train Loss=2391.06713867 | Val Loss=0.82576114 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90871400 | Current Learning Rate: 0.0001\nEpoch 867/1000 | Train Loss=2391.06713867 | Val Loss=0.82576078 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027675 | ‚àö(Val Loss) = 0.90871382 | Current Learning Rate: 0.0001\nEpoch 868/1000 | Train Loss=2391.06713867 | Val Loss=0.82576156 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027681 | ‚àö(Val Loss) = 0.90871423 | Current Learning Rate: 0.0001\nEpoch 869/1000 | Train Loss=2391.06665039 | Val Loss=0.82576150 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027681 | ‚àö(Val Loss) = 0.90871418 | Current Learning Rate: 0.0001\nEpoch 870/1000 | Train Loss=2391.06713867 | Val Loss=0.82576156 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027681 | ‚àö(Val Loss) = 0.90871423 | Current Learning Rate: 0.0001\nEpoch 871/1000 | Train Loss=2391.06713867 | Val Loss=0.82576233 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027681 | ‚àö(Val Loss) = 0.90871465 | Current Learning Rate: 0.0001\nEpoch 872/1000 | Train Loss=2391.06713867 | Val Loss=0.82576233 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871465 | Current Learning Rate: 0.0001\nEpoch 873/1000 | Train Loss=2391.06665039 | Val Loss=0.82576233 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871465 | Current Learning Rate: 0.0001\nEpoch 874/1000 | Train Loss=2391.06713867 | Val Loss=0.82576317 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871513 | Current Learning Rate: 0.0001\nEpoch 875/1000 | Train Loss=2391.06713867 | Val Loss=0.82576305 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871507 | Current Learning Rate: 0.0001\nEpoch 876/1000 | Train Loss=2391.06713867 | Val Loss=0.82576305 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871507 | Current Learning Rate: 0.0001\nEpoch 877/1000 | Train Loss=2391.06665039 | Val Loss=0.82576394 | Data=23.88640976 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871555 | Current Learning Rate: 0.0001\nEpoch 878/1000 | Train Loss=2391.06713867 | Val Loss=0.82576472 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871596 | Current Learning Rate: 0.0001\nEpoch 879/1000 | Train Loss=2391.06713867 | Val Loss=0.82576466 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871596 | Current Learning Rate: 0.0001\nEpoch 880/1000 | Train Loss=2391.06713867 | Val Loss=0.82576478 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871602 | Current Learning Rate: 0.0001\nEpoch 881/1000 | Train Loss=2391.06713867 | Val Loss=0.82576543 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871638 | Current Learning Rate: 0.0001\n\n Epoch :  880 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 882/1000 | Train Loss=2391.06713867 | Val Loss=0.82576543 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871638 | Current Learning Rate: 0.0001\nEpoch 883/1000 | Train Loss=2391.06713867 | Val Loss=0.82576537 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027681 | ‚àö(Val Loss) = 0.90871632 | Current Learning Rate: 0.0001\nEpoch 884/1000 | Train Loss=2391.06713867 | Val Loss=0.82576597 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027681 | ‚àö(Val Loss) = 0.90871668 | Current Learning Rate: 0.0001\nEpoch 885/1000 | Train Loss=2391.06713867 | Val Loss=0.82576638 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027681 | ‚àö(Val Loss) = 0.90871692 | Current Learning Rate: 0.0001\nEpoch 886/1000 | Train Loss=2391.06713867 | Val Loss=0.82576621 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027681 | ‚àö(Val Loss) = 0.90871680 | Current Learning Rate: 0.0001\nEpoch 887/1000 | Train Loss=2391.06738281 | Val Loss=0.82576621 | Data=23.88641548 | Physics=2.42568640 | Val RMSE: 0.87027681 | ‚àö(Val Loss) = 0.90871680 | Current Learning Rate: 0.0001\nEpoch 888/1000 | Train Loss=2391.06713867 | Val Loss=0.82576698 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871722 | Current Learning Rate: 0.0001\nEpoch 889/1000 | Train Loss=2391.06713867 | Val Loss=0.82576698 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871722 | Current Learning Rate: 0.0001\nEpoch 890/1000 | Train Loss=2391.06713867 | Val Loss=0.82576698 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871722 | Current Learning Rate: 0.0001\nEpoch 891/1000 | Train Loss=2391.06713867 | Val Loss=0.82576865 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871811 | Current Learning Rate: 0.0001\nEpoch 892/1000 | Train Loss=2391.06713867 | Val Loss=0.82576859 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871811 | Current Learning Rate: 0.0001\nEpoch 893/1000 | Train Loss=2391.06713867 | Val Loss=0.82576889 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871829 | Current Learning Rate: 0.0001\nEpoch 894/1000 | Train Loss=2391.06713867 | Val Loss=0.82576919 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871841 | Current Learning Rate: 0.0001\nEpoch 895/1000 | Train Loss=2391.06713867 | Val Loss=0.82576954 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871865 | Current Learning Rate: 0.0001\nEpoch 896/1000 | Train Loss=2391.06713867 | Val Loss=0.82576984 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871876 | Current Learning Rate: 0.0001\nEpoch 897/1000 | Train Loss=2391.06713867 | Val Loss=0.82576936 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871853 | Current Learning Rate: 0.0001\nEpoch 898/1000 | Train Loss=2391.06713867 | Val Loss=0.82577020 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871900 | Current Learning Rate: 0.0001\nEpoch 899/1000 | Train Loss=2391.06713867 | Val Loss=0.82577038 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871906 | Current Learning Rate: 0.0001\nEpoch 900/1000 | Train Loss=2391.06713867 | Val Loss=0.82577026 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871900 | Current Learning Rate: 0.0001\nEpoch 901/1000 | Train Loss=2391.06713867 | Val Loss=0.82577157 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871972 | Current Learning Rate: 0.0001\n\n Epoch :  900 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 902/1000 | Train Loss=2391.06713867 | Val Loss=0.82577103 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871942 | Current Learning Rate: 0.0001\nEpoch 903/1000 | Train Loss=2391.06713867 | Val Loss=0.82577115 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90871954 | Current Learning Rate: 0.0001\nEpoch 904/1000 | Train Loss=2391.06713867 | Val Loss=0.82577187 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90871990 | Current Learning Rate: 0.0001\nEpoch 905/1000 | Train Loss=2391.06713867 | Val Loss=0.82577312 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872061 | Current Learning Rate: 0.0001\nEpoch 906/1000 | Train Loss=2391.06713867 | Val Loss=0.82577258 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872031 | Current Learning Rate: 0.0001\nEpoch 907/1000 | Train Loss=2391.06713867 | Val Loss=0.82577318 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872061 | Current Learning Rate: 0.0001\nEpoch 908/1000 | Train Loss=2391.06665039 | Val Loss=0.82577395 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872103 | Current Learning Rate: 0.0001\nEpoch 909/1000 | Train Loss=2391.06665039 | Val Loss=0.82577336 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872073 | Current Learning Rate: 0.0001\nEpoch 910/1000 | Train Loss=2391.06713867 | Val Loss=0.82577354 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872085 | Current Learning Rate: 0.0001\nEpoch 911/1000 | Train Loss=2391.06713867 | Val Loss=0.82577473 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872151 | Current Learning Rate: 0.0001\nEpoch 912/1000 | Train Loss=2391.06713867 | Val Loss=0.82577413 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872115 | Current Learning Rate: 0.0001\nEpoch 913/1000 | Train Loss=2391.06713867 | Val Loss=0.82577437 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872127 | Current Learning Rate: 0.0001\nEpoch 914/1000 | Train Loss=2391.06738281 | Val Loss=0.82577503 | Data=23.88641548 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872163 | Current Learning Rate: 0.0001\nEpoch 915/1000 | Train Loss=2391.06713867 | Val Loss=0.82577515 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872169 | Current Learning Rate: 0.0001\nEpoch 916/1000 | Train Loss=2391.06713867 | Val Loss=0.82577556 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872192 | Current Learning Rate: 0.0001\nEpoch 917/1000 | Train Loss=2391.06665039 | Val Loss=0.82577580 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872204 | Current Learning Rate: 0.0001\nEpoch 918/1000 | Train Loss=2391.06665039 | Val Loss=0.82577634 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872234 | Current Learning Rate: 0.0001\nEpoch 919/1000 | Train Loss=2391.06713867 | Val Loss=0.82577670 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872258 | Current Learning Rate: 0.0001\nEpoch 920/1000 | Train Loss=2391.06713867 | Val Loss=0.82577705 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872276 | Current Learning Rate: 0.0001\nEpoch 921/1000 | Train Loss=2391.06713867 | Val Loss=0.82577759 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872306 | Current Learning Rate: 0.0001\n\n Epoch :  920 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 922/1000 | Train Loss=2391.06713867 | Val Loss=0.82577795 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872324 | Current Learning Rate: 0.0001\nEpoch 923/1000 | Train Loss=2391.06713867 | Val Loss=0.82577759 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872306 | Current Learning Rate: 0.0001\nEpoch 924/1000 | Train Loss=2391.06713867 | Val Loss=0.82577878 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872371 | Current Learning Rate: 0.0001\nEpoch 925/1000 | Train Loss=2391.06665039 | Val Loss=0.82577848 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872353 | Current Learning Rate: 0.0001\nEpoch 926/1000 | Train Loss=2391.06713867 | Val Loss=0.82577878 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872371 | Current Learning Rate: 0.0001\nEpoch 927/1000 | Train Loss=2391.06713867 | Val Loss=0.82577890 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872377 | Current Learning Rate: 0.0001\nEpoch 928/1000 | Train Loss=2391.06713867 | Val Loss=0.82577950 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872413 | Current Learning Rate: 0.0001\nEpoch 929/1000 | Train Loss=2391.06713867 | Val Loss=0.82577920 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872395 | Current Learning Rate: 0.0001\nEpoch 930/1000 | Train Loss=2391.06713867 | Val Loss=0.82577908 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872389 | Current Learning Rate: 0.0001\nEpoch 931/1000 | Train Loss=2391.06665039 | Val Loss=0.82578039 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872461 | Current Learning Rate: 0.0001\nEpoch 932/1000 | Train Loss=2391.06713867 | Val Loss=0.82577997 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90872437 | Current Learning Rate: 0.0001\nEpoch 933/1000 | Train Loss=2391.06713867 | Val Loss=0.82578081 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872484 | Current Learning Rate: 0.0001\nEpoch 934/1000 | Train Loss=2391.06713867 | Val Loss=0.82578212 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872556 | Current Learning Rate: 0.0001\nEpoch 935/1000 | Train Loss=2391.06713867 | Val Loss=0.82578182 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872538 | Current Learning Rate: 0.0001\nEpoch 936/1000 | Train Loss=2391.06713867 | Val Loss=0.82578176 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872532 | Current Learning Rate: 0.0001\nEpoch 937/1000 | Train Loss=2391.06713867 | Val Loss=0.82578295 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872598 | Current Learning Rate: 0.0001\nEpoch 938/1000 | Train Loss=2391.06713867 | Val Loss=0.82578260 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872580 | Current Learning Rate: 0.0001\nEpoch 939/1000 | Train Loss=2391.06713867 | Val Loss=0.82578248 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872574 | Current Learning Rate: 0.0001\nEpoch 940/1000 | Train Loss=2391.06713867 | Val Loss=0.82578349 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872627 | Current Learning Rate: 0.0001\nEpoch 941/1000 | Train Loss=2391.06665039 | Val Loss=0.82578379 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872645 | Current Learning Rate: 0.0001\n\n Epoch :  940 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 942/1000 | Train Loss=2391.06713867 | Val Loss=0.82578337 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872622 | Current Learning Rate: 0.0001\nEpoch 943/1000 | Train Loss=2391.06713867 | Val Loss=0.82578325 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872616 | Current Learning Rate: 0.0001\nEpoch 944/1000 | Train Loss=2391.06713867 | Val Loss=0.82578450 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872687 | Current Learning Rate: 0.0001\nEpoch 945/1000 | Train Loss=2391.06713867 | Val Loss=0.82578444 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872681 | Current Learning Rate: 0.0001\nEpoch 946/1000 | Train Loss=2391.06665039 | Val Loss=0.82578421 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90872669 | Current Learning Rate: 0.0001\nEpoch 947/1000 | Train Loss=2391.06713867 | Val Loss=0.82578599 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027711 | ‚àö(Val Loss) = 0.90872771 | Current Learning Rate: 0.0001\nEpoch 948/1000 | Train Loss=2391.06713867 | Val Loss=0.82578588 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027711 | ‚àö(Val Loss) = 0.90872759 | Current Learning Rate: 0.0001\nEpoch 949/1000 | Train Loss=2391.06713867 | Val Loss=0.82578623 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027711 | ‚àö(Val Loss) = 0.90872782 | Current Learning Rate: 0.0001\nEpoch 950/1000 | Train Loss=2391.06713867 | Val Loss=0.82578707 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027711 | ‚àö(Val Loss) = 0.90872824 | Current Learning Rate: 0.0001\nEpoch 951/1000 | Train Loss=2391.06713867 | Val Loss=0.82578671 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027711 | ‚àö(Val Loss) = 0.90872806 | Current Learning Rate: 0.0001\nEpoch 952/1000 | Train Loss=2391.06713867 | Val Loss=0.82578677 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027711 | ‚àö(Val Loss) = 0.90872812 | Current Learning Rate: 0.0001\nEpoch 953/1000 | Train Loss=2391.06713867 | Val Loss=0.82578737 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027711 | ‚àö(Val Loss) = 0.90872842 | Current Learning Rate: 0.0001\nEpoch 954/1000 | Train Loss=2391.06713867 | Val Loss=0.82578743 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027711 | ‚àö(Val Loss) = 0.90872848 | Current Learning Rate: 0.0001\nEpoch 955/1000 | Train Loss=2391.06665039 | Val Loss=0.82578743 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027711 | ‚àö(Val Loss) = 0.90872848 | Current Learning Rate: 0.0001\nEpoch 956/1000 | Train Loss=2391.06713867 | Val Loss=0.82578778 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027711 | ‚àö(Val Loss) = 0.90872866 | Current Learning Rate: 0.0001\nEpoch 957/1000 | Train Loss=2391.06713867 | Val Loss=0.82578874 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027717 | ‚àö(Val Loss) = 0.90872920 | Current Learning Rate: 0.0001\nEpoch 958/1000 | Train Loss=2391.06713867 | Val Loss=0.82578874 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027717 | ‚àö(Val Loss) = 0.90872920 | Current Learning Rate: 0.0001\nEpoch 959/1000 | Train Loss=2391.06713867 | Val Loss=0.82578874 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027717 | ‚àö(Val Loss) = 0.90872920 | Current Learning Rate: 0.0001\nEpoch 960/1000 | Train Loss=2391.06713867 | Val Loss=0.82579035 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027717 | ‚àö(Val Loss) = 0.90873009 | Current Learning Rate: 0.0001\nEpoch 961/1000 | Train Loss=2391.06713867 | Val Loss=0.82579035 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873009 | Current Learning Rate: 0.0001\n\n Epoch :  960 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 962/1000 | Train Loss=2391.06738281 | Val Loss=0.82579005 | Data=23.88641548 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90872991 | Current Learning Rate: 0.0001\nEpoch 963/1000 | Train Loss=2391.06713867 | Val Loss=0.82579064 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873021 | Current Learning Rate: 0.0001\nEpoch 964/1000 | Train Loss=2391.06713867 | Val Loss=0.82579100 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873045 | Current Learning Rate: 0.0001\nEpoch 965/1000 | Train Loss=2391.06665039 | Val Loss=0.82579076 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873033 | Current Learning Rate: 0.0001\nEpoch 966/1000 | Train Loss=2391.06713867 | Val Loss=0.82579094 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873039 | Current Learning Rate: 0.0001\nEpoch 967/1000 | Train Loss=2391.06713867 | Val Loss=0.82579178 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90873086 | Current Learning Rate: 0.0001\nEpoch 968/1000 | Train Loss=2391.06713867 | Val Loss=0.82579178 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90873086 | Current Learning Rate: 0.0001\nEpoch 969/1000 | Train Loss=2391.06713867 | Val Loss=0.82579172 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90873080 | Current Learning Rate: 0.0001\nEpoch 970/1000 | Train Loss=2391.06665039 | Val Loss=0.82579297 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90873152 | Current Learning Rate: 0.0001\nEpoch 971/1000 | Train Loss=2391.06713867 | Val Loss=0.82579285 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027687 | ‚àö(Val Loss) = 0.90873146 | Current Learning Rate: 0.0001\nEpoch 972/1000 | Train Loss=2391.06713867 | Val Loss=0.82579285 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873146 | Current Learning Rate: 0.0001\nEpoch 973/1000 | Train Loss=2391.06665039 | Val Loss=0.82579368 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873194 | Current Learning Rate: 0.0001\nEpoch 974/1000 | Train Loss=2391.06713867 | Val Loss=0.82579458 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873241 | Current Learning Rate: 0.0001\nEpoch 975/1000 | Train Loss=2391.06713867 | Val Loss=0.82579458 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873241 | Current Learning Rate: 0.0001\nEpoch 976/1000 | Train Loss=2391.06665039 | Val Loss=0.82579511 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873271 | Current Learning Rate: 0.0001\nEpoch 977/1000 | Train Loss=2391.06713867 | Val Loss=0.82579535 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873283 | Current Learning Rate: 0.0001\nEpoch 978/1000 | Train Loss=2391.06713867 | Val Loss=0.82579517 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873271 | Current Learning Rate: 0.0001\nEpoch 979/1000 | Train Loss=2391.06713867 | Val Loss=0.82579517 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873271 | Current Learning Rate: 0.0001\nEpoch 980/1000 | Train Loss=2391.06713867 | Val Loss=0.82579589 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873313 | Current Learning Rate: 0.0001\nEpoch 981/1000 | Train Loss=2391.06713867 | Val Loss=0.82579631 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873337 | Current Learning Rate: 0.0001\n\n Epoch :  980 \n Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n        [  0.9908,  -3.6061, -10.8017]]) \n Prediction :  [[ 0.99507636 -3.3374639  -9.56136   ]\n [ 0.99508965 -3.3374808  -9.561314  ]] \n\nFinal Test RMSE:  0.5951478481292725\nEpoch 982/1000 | Train Loss=2391.06713867 | Val Loss=0.82579631 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873337 | Current Learning Rate: 0.0001\nEpoch 983/1000 | Train Loss=2391.06713867 | Val Loss=0.82579708 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873379 | Current Learning Rate: 0.0001\nEpoch 984/1000 | Train Loss=2391.06713867 | Val Loss=0.82579690 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873367 | Current Learning Rate: 0.0001\nEpoch 985/1000 | Train Loss=2391.06713867 | Val Loss=0.82579690 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90873367 | Current Learning Rate: 0.0001\nEpoch 986/1000 | Train Loss=2391.06713867 | Val Loss=0.82579798 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027705 | ‚àö(Val Loss) = 0.90873426 | Current Learning Rate: 0.0001\nEpoch 987/1000 | Train Loss=2391.06713867 | Val Loss=0.82579798 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873426 | Current Learning Rate: 0.0001\nEpoch 988/1000 | Train Loss=2391.06665039 | Val Loss=0.82579881 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873474 | Current Learning Rate: 0.0001\nEpoch 989/1000 | Train Loss=2391.06713867 | Val Loss=0.82579917 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873492 | Current Learning Rate: 0.0001\nEpoch 990/1000 | Train Loss=2391.06713867 | Val Loss=0.82579929 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873498 | Current Learning Rate: 0.0001\nEpoch 991/1000 | Train Loss=2391.06713867 | Val Loss=0.82579958 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873516 | Current Learning Rate: 0.0001\nEpoch 992/1000 | Train Loss=2391.06665039 | Val Loss=0.82579958 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873516 | Current Learning Rate: 0.0001\nEpoch 993/1000 | Train Loss=2391.06713867 | Val Loss=0.82580030 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873557 | Current Learning Rate: 0.0001\nEpoch 994/1000 | Train Loss=2391.06665039 | Val Loss=0.82580024 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873551 | Current Learning Rate: 0.0001\nEpoch 995/1000 | Train Loss=2391.06713867 | Val Loss=0.82580066 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027693 | ‚àö(Val Loss) = 0.90873575 | Current Learning Rate: 0.0001\nEpoch 996/1000 | Train Loss=2391.06665039 | Val Loss=0.82580137 | Data=23.88640976 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873617 | Current Learning Rate: 0.0001\nEpoch 997/1000 | Train Loss=2391.06713867 | Val Loss=0.82580125 | Data=23.88641357 | Physics=2.42568616 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873605 | Current Learning Rate: 0.0001\nEpoch 998/1000 | Train Loss=2391.06713867 | Val Loss=0.82580137 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873617 | Current Learning Rate: 0.0001\nEpoch 999/1000 | Train Loss=2391.06738281 | Val Loss=0.82580221 | Data=23.88641548 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873659 | Current Learning Rate: 0.0001\n‚úÖ Saved last model at epoch 1000 \nEpoch 1000/1000 | Train Loss=2391.06713867 | Val Loss=0.82580197 | Data=23.88641357 | Physics=2.42568640 | Val RMSE: 0.87027699 | ‚àö(Val Loss) = 0.90873647 | Current Learning Rate: 0.0001\n‚úÖ Metrics saved successfully!\nPlot losses after training 3:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Test RMSE :  0.5951478481292725\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# help(TensorDataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:26.821927Z","iopub.execute_input":"2026-02-03T21:07:26.822194Z","iopub.status.idle":"2026-02-03T21:07:26.826719Z","shell.execute_reply.started":"2026-02-03T21:07:26.822171Z","shell.execute_reply":"2026-02-03T21:07:26.825887Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# # Use X_batch.shape[1] to dynamically get the correct window size (e.g., 6)\n# current_batch_size = X_batch.size(0)\n# current_window_size = X_batch.size(1) \n\n# x_seq = torch.arange(current_window_size, dtype=torch.float32, device=X_batch.device)\n# x_seq = x_seq.view(1, -1, 1).repeat(current_batch_size, 1, 1) # Shape: [32, 6, 1]\n\n# loss, data_loss, phys_loss = criterion(y_pred, y_batch, x_seq, alpha=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:26.827741Z","iopub.execute_input":"2026-02-03T21:07:26.827980Z","iopub.status.idle":"2026-02-03T21:07:26.838747Z","shell.execute_reply.started":"2026-02-03T21:07:26.827959Z","shell.execute_reply":"2026-02-03T21:07:26.837995Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def df_to_X_y_tensor(df, window_size=5,output_size=5):\n    '''\n    Converts a time series into (X, y) tensors for LSTM training.\n    \n    X shape: (num_samples, window_size, 1)\n    y shape: (num_samples, 1)\n    '''\n    # if isinstance(df, (pd.DataFrame, pd.Series)):\n    #     df_as_np = df.to_numpy()\n    # else:\n    #     df_as_np = df  # Assume already numpy\n\n    X, y , y2 = [], [], []\n    #for i in range(len(df_as_np) - window_size):\n    X.append(list(df['SoH'])[:window_size+1])\n    #y.append([df_as_np[i + window_size:i + window_size+output_size]])\n    y.append([list(df['k'])[-1],list(df['a'])[-1],list(df['b'])[-1]])\n    #append([[val] for val in df_as_np[i + window_size:i + window_size+1]]) #next cycle\n    y2.append(list(df['rul'])[:1])\n    X,y,y2 = np.array(X),np.array(y), np.array(y2)\n    X_tensor = torch.tensor(X, dtype=torch.float32)#.squeeze()\n    y_tensor = torch.tensor(y, dtype=torch.float32)#.squeeze()\n    y_2_tensor = torch.tensor(y2, dtype=torch.float32)  #bug was here written y instead of y2\n    return X_tensor, y_tensor, y_2_tensor\n\ndef get_x_y_lists(paths):\n    X_list,y_list,y_target = [],[],[]\n    for path in paths:\n        print(path)\n        df = pd.read_csv(path)\n        df['Cycle number'] = df['Cycle number']/10000\n        df['rul'] = df['rul']/10000\n        #normalize SoH\n        df['SoH'] =  df['SoH']/soh_normalization_constant\n        df.index = df['Cycle number']\n        #SoH = df[model_columns]\n        X, y , y1 = df_to_X_y_tensor(df, window_size=WINDOW_SIZE,output_size=OUTPUT_SIZE)\n        X_list.append(X)\n        y_list.append(y)\n        y_target.append(y1) #RUL\n    return X_list,y_list, y_target\n\n\n\ndef give_paths_get_loaders(paths,data_type,shuffle=False):\n    X_list, y_list, y_target = get_x_y_lists(paths)\n\n    batch_size = torch.cat(X_list, dim=0).shape[0]\n    \n    if INPUT_SIZE == 1:\n        # Concatenate all X and y\n        X_1,y_1,y_2 = torch.cat(X_list, dim=0).unsqueeze(-1),torch.cat(y_list, dim=0).view(batch_size,-1),torch.cat(y_target, dim=0).view(batch_size,-1)\n    else:\n        X_1,y_1,y_2 = torch.cat(X_list, dim=0).squeeze(2),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE),torch.cat(y_target, dim=0).view(-1,INPUT_SIZE)\n    \n    print(f\" X_{data_type} shape : {X_1.shape} , y_{data_type} shape : {y_1.shape} Ôºåy_2{data_type} shape: {y_2.shape}\" )\n    \n    #DataLoader\n    print(\"load : \")\n    loader = DataLoader(TensorDataset(X_1, y_1, y_2), batch_size=32, shuffle=shuffle)\n    print(f\"{data_type}loader lengths : \",loader.__len__())\n    return loader,X_1,y_1, y_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:26.839542Z","iopub.execute_input":"2026-02-03T21:07:26.839740Z","iopub.status.idle":"2026-02-03T21:07:26.851543Z","shell.execute_reply.started":"2026-02-03T21:07:26.839720Z","shell.execute_reply":"2026-02-03T21:07:26.850963Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def inverse_gompertz_func(y,k,a,b):\n    return (a - np.log(np.log(k/y)))/b\n\nfor data_range in data_ranges:\n    for WINDOW_SIZE in WINDOW_SIZES:\n        WINDOW_SIZE = WINDOW_SIZE - 1\n        train_paths,val_paths,test_paths = datasets[data_range]\n        train_loader,X_train,y_train,y_train_target= give_paths_get_loaders(train_paths,data_range,shuffle=True)\n        val_loader,X_val,y_val,y_val_target= give_paths_get_loaders(val_paths,data_range)\n        test_loader,X_test,y_test,y_test_target = give_paths_get_loaders(test_paths,data_range)\n    # print(float(torch.mean(y_train, dim=0)[0]),float(torch.mean(y_train, dim=0)[1]),float(torch.mean(y_train, dim=0)[2]))\n    # print()\n    # print()\n    model = PhysicsInformedLSTM(input_size=INPUT_SIZE).to(device)\n    model.load_state_dict(torch.load(f'/kaggle/working/best_lstm_model-window-100_model_pinn_data_{data_range}.pth'))\n    test_rmse = 0 \n    for X_batch, y_batch,y_target in test_loader:\n        # Calculate RMSE directly\n        \n        y_pred = model(X_batch.to(device)).cpu().detach().numpy()\n        test_rmse += root_mean_squared_error(y_batch, y_pred)\n    print('Test RMSE : ',test_rmse)    \n    \n\n    for i, pred in enumerate(y_pred):\n        k,a,b,target_rul = y_pred[i][0],y_pred[i][1],y_pred[i][2],y_target[i].item()*10000\n        \n        print(k,a,b)\n        \n        print(f'RUL : {inverse_gompertz_func(y=0.7,k=k,a=a,b=b)*10000:.0f} cycles vs Target RUL : {target_rul}')\n    print('-'*30)\n    print()\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:26.852416Z","iopub.execute_input":"2026-02-03T21:07:26.852742Z","iopub.status.idle":"2026-02-03T21:07:28.014926Z","shell.execute_reply.started":"2026-02-03T21:07:26.852721Z","shell.execute_reply":"2026-02-03T21:07:28.014304Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/generate-hust-data-gompertz-k-a-b/1-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-8-hust_gompertz_params.csv\n X_all shape : torch.Size([55, 100, 1]) , y_all shape : torch.Size([55, 3]) Ôºåy_2all shape: torch.Size([55, 1])\nload : \nallloader lengths :  2\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-5-hust_gompertz_params.csv\n X_all shape : torch.Size([11, 100, 1]) , y_all shape : torch.Size([11, 3]) Ôºåy_2all shape: torch.Size([11, 1])\nload : \nallloader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\n X_all shape : torch.Size([11, 100, 1]) , y_all shape : torch.Size([11, 3]) Ôºåy_2all shape: torch.Size([11, 1])\nload : \nallloader lengths :  1\nTest RMSE :  1.6645255088806152\n0.9997545 -3.6670835 -14.181376\nRUL : 1858 cycles vs Target RUL : 1975.0000536441803\n1.017321 -3.6561015 -14.1789875\nRUL : 1885 cycles vs Target RUL : 2284.9999368190765\n0.98335093 -3.677263 -14.183639\nRUL : 1832 cycles vs Target RUL : 1875.0\n0.9918498 -3.6719964 -14.182465\nRUL : 1846 cycles vs Target RUL : 1938.0000233650208\n0.99420863 -3.6705313 -14.182139\nRUL : 1850 cycles vs Target RUL : 1678.9999604225159\n0.97456807 -3.6826885 -14.184859\nRUL : 1817 cycles vs Target RUL : 1561.0000491142273\n0.95600367 -3.6940708 -14.187476\nRUL : 1782 cycles vs Target RUL : 1307.9999387264252\n1.0027694 -3.6652079 -14.180963\nRUL : 1863 cycles vs Target RUL : 2364.9999499320984\n0.9554059 -3.6944363 -14.187561\nRUL : 1781 cycles vs Target RUL : 1419.0000295639038\n0.9701287 -3.685412 -14.185486\nRUL : 1809 cycles vs Target RUL : 1386.0000669956207\n0.9533667 -3.6956801 -14.187851\nRUL : 1777 cycles vs Target RUL : 1503.9999783039093\n------------------------------\n\n\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-8-hust_gompertz_params.csv\n X_low shape : torch.Size([14, 100, 1]) , y_low shape : torch.Size([14, 3]) Ôºåy_2low shape: torch.Size([14, 1])\nload : \nlowloader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-5-hust_gompertz_params.csv\n X_low shape : torch.Size([2, 100, 1]) , y_low shape : torch.Size([2, 3]) Ôºåy_2low shape: torch.Size([2, 1])\nload : \nlowloader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\n X_low shape : torch.Size([5, 100, 1]) , y_low shape : torch.Size([5, 3]) Ôºåy_2low shape: torch.Size([5, 1])\nload : \nlowloader lengths :  1\nTest RMSE :  1.2676979303359985\n0.9554488 -3.9225414 -19.531675\nRUL : 1410 cycles vs Target RUL : 1561.0000491142273\n0.9553731 -3.92253 -19.532564\nRUL : 1410 cycles vs Target RUL : 1307.9999387264252\n0.9553694 -3.9225295 -19.532608\nRUL : 1410 cycles vs Target RUL : 1419.0000295639038\n0.9554471 -3.922541 -19.531694\nRUL : 1410 cycles vs Target RUL : 1386.0000669956207\n0.95536 -3.9225283 -19.532717\nRUL : 1410 cycles vs Target RUL : 1503.9999783039093\n------------------------------\n\n\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-5-hust_gompertz_params.csv\n X_mid shape : torch.Size([26, 100, 1]) , y_mid shape : torch.Size([26, 3]) Ôºåy_2mid shape: torch.Size([26, 1])\nload : \nmidloader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-1-hust_gompertz_params.csv\n X_mid shape : torch.Size([5, 100, 1]) , y_mid shape : torch.Size([5, 3]) Ôºåy_2mid shape: torch.Size([5, 1])\nload : \nmidloader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n X_mid shape : torch.Size([4, 100, 1]) , y_mid shape : torch.Size([4, 3]) Ôºåy_2mid shape: torch.Size([4, 1])\nload : \nmidloader lengths :  1\nTest RMSE :  0.4138225018978119\n0.97449857 -3.7601511 -13.948477\nRUL : 1903 cycles vs Target RUL : 1975.0000536441803\n0.97446585 -3.7601473 -13.948813\nRUL : 1903 cycles vs Target RUL : 1875.0\n0.9744858 -3.7601497 -13.948608\nRUL : 1903 cycles vs Target RUL : 1938.0000233650208\n0.9744909 -3.7601502 -13.948557\nRUL : 1903 cycles vs Target RUL : 1678.9999604225159\n------------------------------\n\n\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-8-hust_gompertz_params.csv\n X_high shape : torch.Size([15, 100, 1]) , y_high shape : torch.Size([15, 3]) Ôºåy_2high shape: torch.Size([15, 1])\nload : \nhighloader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n X_high shape : torch.Size([4, 100, 1]) , y_high shape : torch.Size([4, 3]) Ôºåy_2high shape: torch.Size([4, 1])\nload : \nhighloader lengths :  1\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n X_high shape : torch.Size([2, 100, 1]) , y_high shape : torch.Size([2, 3]) Ôºåy_2high shape: torch.Size([2, 1])\nload : \nhighloader lengths :  1\nTest RMSE :  0.5951478481292725\n0.99507636 -3.3374639 -9.56136\nRUL : 2398 cycles vs Target RUL : 2284.9999368190765\n0.99508965 -3.3374808 -9.561314\nRUL : 2398 cycles vs Target RUL : 2364.9999499320984\n------------------------------\n\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# for sample in train_loader:\n#     print(sample)\n#     break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.015884Z","iopub.execute_input":"2026-02-03T21:07:28.016258Z","iopub.status.idle":"2026-02-03T21:07:28.019816Z","shell.execute_reply.started":"2026-02-03T21:07:28.016227Z","shell.execute_reply":"2026-02-03T21:07:28.018897Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# sample[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.020906Z","iopub.execute_input":"2026-02-03T21:07:28.021608Z","iopub.status.idle":"2026-02-03T21:07:28.032355Z","shell.execute_reply.started":"2026-02-03T21:07:28.021584Z","shell.execute_reply":"2026-02-03T21:07:28.031767Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# # help(model)\n# for X_batch, y_batch, y_target in train_loader:\n#     #optimizer.zero_grad()\n#     #Set computing environment\n#     X_batch, y_batch = X_batch.to(device), y_batch.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.035648Z","iopub.execute_input":"2026-02-03T21:07:28.036268Z","iopub.status.idle":"2026-02-03T21:07:28.045314Z","shell.execute_reply.started":"2026-02-03T21:07:28.036244Z","shell.execute_reply":"2026-02-03T21:07:28.044502Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# model(X_batch)","metadata":{"_uuid":"ab16f4e0-1acc-499b-8d8e-43e88dc79030","_cell_guid":"35554e79-62f0-483e-95f8-aab488cb1af6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-03T21:07:28.046224Z","iopub.execute_input":"2026-02-03T21:07:28.046502Z","iopub.status.idle":"2026-02-03T21:07:28.055813Z","shell.execute_reply.started":"2026-02-03T21:07:28.046472Z","shell.execute_reply":"2026-02-03T21:07:28.055148Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# X_batch.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.056708Z","iopub.execute_input":"2026-02-03T21:07:28.056887Z","iopub.status.idle":"2026-02-03T21:07:28.066597Z","shell.execute_reply.started":"2026-02-03T21:07:28.056869Z","shell.execute_reply":"2026-02-03T21:07:28.065877Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# y_batch.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.067497Z","iopub.execute_input":"2026-02-03T21:07:28.067756Z","iopub.status.idle":"2026-02-03T21:07:28.078027Z","shell.execute_reply.started":"2026-02-03T21:07:28.067725Z","shell.execute_reply":"2026-02-03T21:07:28.077208Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# X_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.079046Z","iopub.execute_input":"2026-02-03T21:07:28.079366Z","iopub.status.idle":"2026-02-03T21:07:28.089768Z","shell.execute_reply.started":"2026-02-03T21:07:28.079336Z","shell.execute_reply":"2026-02-03T21:07:28.089131Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# for X_batch, y_batch, y_target in test_loader:\n#     print(model(X_batch.to(device)))\n#     print(y_batch)\n#     break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.090670Z","iopub.execute_input":"2026-02-03T21:07:28.090944Z","iopub.status.idle":"2026-02-03T21:07:28.101153Z","shell.execute_reply.started":"2026-02-03T21:07:28.090913Z","shell.execute_reply":"2026-02-03T21:07:28.100469Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# test_rmse = 0 \n\n# for X_batch, y_batch,y_target in test_loader:\n#     # Calculate RMSE directly\n#     y_pred = model(X_batch.to(device)).cpu().detach().numpy()\n#     test_rmse += root_mean_squared_error(y_batch, y_pred)\n# print('Test RMSE : ',test_rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.102040Z","iopub.execute_input":"2026-02-03T21:07:28.102439Z","iopub.status.idle":"2026-02-03T21:07:28.112506Z","shell.execute_reply.started":"2026-02-03T21:07:28.102406Z","shell.execute_reply":"2026-02-03T21:07:28.111828Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# X_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.113346Z","iopub.execute_input":"2026-02-03T21:07:28.113585Z","iopub.status.idle":"2026-02-03T21:07:28.126888Z","shell.execute_reply.started":"2026-02-03T21:07:28.113541Z","shell.execute_reply":"2026-02-03T21:07:28.126277Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# #['6-6', '8-6', '10-1', '6-8', '8-8', '10-7', '10-6', '7-6', '4-5', '10-4', '3-1', '8-1', '9-6', '1-2', '6-1', '6-2', '8-5', '5-3', '2-5', '9-4', '7-5', '1-1']\n# y_batch, y_pred, y_target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.127741Z","iopub.execute_input":"2026-02-03T21:07:28.128344Z","iopub.status.idle":"2026-02-03T21:07:28.138062Z","shell.execute_reply.started":"2026-02-03T21:07:28.128315Z","shell.execute_reply":"2026-02-03T21:07:28.137486Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# def inverse_gompertz_func(y,k,a,b):\n#     return (a - np.log(np.log(k/y)))/b\n\n# for i, pred in enumerate(y_pred):\n#     k,a,b,target_rul = y_pred[i][0],y_pred[i][1],y_pred[i][2],y_target[i].item()*10000\n    \n#     print(k,a,b)\n    \n#     print(f'RUL : {inverse_gompertz_func(y=0.7,k=k,a=a,b=b)*10000:.0f} cycles vs Target RUL : {target_rul}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.138916Z","iopub.execute_input":"2026-02-03T21:07:28.139147Z","iopub.status.idle":"2026-02-03T21:07:28.150458Z","shell.execute_reply.started":"2026-02-03T21:07:28.139122Z","shell.execute_reply":"2026-02-03T21:07:28.149745Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Define the function\ndef gompertz_func(x,k,a,b):\n    return k*np.exp(-np.exp(a-(b*x)))\n\ndef gompertz_exponent_func(x,k,a,b):\n    return a-(b*x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.151395Z","iopub.execute_input":"2026-02-03T21:07:28.151700Z","iopub.status.idle":"2026-02-03T21:07:28.160813Z","shell.execute_reply.started":"2026-02-03T21:07:28.151673Z","shell.execute_reply":"2026-02-03T21:07:28.160154Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Check if k,a and b are the correct k, a and b\npath = \"../input/generate-hust-data-gompertz-k-a-b/\"\nfiles = os.listdir(path)\nk_s, a_s, b_s ,e_s = [], [],[],[]\n#print(files)\nfiles = [f for f in files if re.match(r'^\\d', f) and f.endswith('-hust_gompertz_params.csv')]\nfor file in files:\n    df = pd.read_csv(path+file)\n    #print(df.head())\n    df['exponent'] =  gompertz_exponent_func(x=df['rul']/10000,k=df['k'],a=df['a'],b=df['b'])\n    \n    answers = file[:3],list(df['k'])[-1], list(df['a'])[-1], list(df['b'])[-1], list(df['exponent'])[-1]\n    k_s.append(answers[1]), a_s.append(answers[2]), b_s.append(answers[3]) , e_s.append(answers[4])\n    print(answers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.161622Z","iopub.execute_input":"2026-02-03T21:07:28.161816Z","iopub.status.idle":"2026-02-03T21:07:28.705124Z","shell.execute_reply.started":"2026-02-03T21:07:28.161797Z","shell.execute_reply":"2026-02-03T21:07:28.704305Z"}},"outputs":[{"name":"stdout","text":"('6-6', 0.999946950696718, -3.0463019901673665, -8.058702298233401, -3.045496119937543)\n('8-7', 0.960536968518218, -3.8912779384551865, -13.448726573064285, -3.88993306579788)\n('8-6', 0.9907509429027922, -3.6060510716281655, -10.801653050097192, -3.604970906323156)\n('9-1', 0.9891766657708968, -3.6505840630566007, -12.21416655437278, -3.6493626464011633)\n('10-', 0.9481215769904175, -4.211693705003083, -17.547137063751773, -4.209938991296708)\n('6-8', 1.0107420560063778, -2.9030997786747967, -7.691465017103098, -2.9023306321730864)\n('8-8', 0.98413059165122, -3.7445201053751354, -15.65627371654888, -3.7429544780034805)\n('10-', 0.9786268880932516, -3.700592087534084, -14.445176742202348, -3.6991475698598637)\n('3-5', 1.0099262656709476, -3.2992647017070773, -8.637530260413152, -3.298400948681036)\n('5-1', 0.9967531041266844, -3.1470462327694158, -8.325162160953383, -3.1462137165533206)\n('5-5', 0.9897916363777954, -3.3170924362716403, -14.18101023291364, -3.315674335248349)\n('7-1', 0.9711572804774884, -3.575039239519331, -14.258499516057528, -3.573613389567725)\n('2-7', 0.9816085817779644, -3.690469938888635, -11.465814604260702, -3.689323357428209)\n('10-', 1.001915470696929, -3.599474816014408, -11.307329465009976, -3.5983440830679068)\n('4-7', 1.0052259003082558, -3.407105896520339, -10.876646876210822, -3.406018231832718)\n('7-7', 0.946147991876142, -4.208655935983245, -17.71497672411127, -4.2068844383108335)\n('7-6', 0.9417780156098636, -4.459192427373631, -22.763029699141367, -4.456916124403717)\n('4-5', 0.9645748300540335, -4.011222592503926, -18.31586703008384, -4.009391005800918)\n('9-2', 0.9868162660323028, -3.5523454627640505, -11.455413817936458, -3.5511999213822567)\n('10-', 0.9660291957197452, -4.116644983133188, -16.335190867850276, -4.115011464046403)\n('3-1', 0.9821717134588616, -3.672732167031856, -13.070265487410865, -3.6714251404831146)\n('9-7', 0.9775381856439505, -3.744888811894645, -12.948923061859276, -3.743593919588459)\n('8-1', 0.9456276972788872, -4.333303161056529, -23.960673470163748, -4.330907093709513)\n('10-', 0.9593479646405142, -3.935482671348799, -19.795025034420167, -3.9335031688453572)\n('8-4', 0.951491728160938, -4.194247424452098, -15.876148349374354, -4.192659809617161)\n('4-6', 0.9556436039053372, -3.675950935516932, -17.921465786644312, -3.674158788938268)\n('4-4', 0.9446541576167672, -4.116515985488998, -19.43706976660556, -4.1145722785123375)\n('3-8', 0.998790862672299, -3.489105881736752, -10.105225673784416, -3.4880953591693733)\n('5-4', 0.9575242339770756, -3.869757189316574, -13.903639295285016, -3.8683668253870453)\n('9-6', 0.9894867250978762, -3.514648337342772, -13.6811165735078, -3.5132802256854214)\n('10-', 0.9748932220917949, -3.8520124764630586, -13.306799932569524, -3.8506817964698015)\n('7-8', 0.97426043797264, -3.848156231659844, -14.075337896306312, -3.8467486978702135)\n('5-2', 0.9737189970425392, -3.786715934788834, -13.759517077949551, -3.785339983081039)\n('9-8', 0.9793520149404712, -3.5667178792469256, -10.5803751100897, -3.5656598417359167)\n('1-2', 0.9878393112287064, -3.2905567616226774, -8.286660537755745, -3.289728095568902)\n('5-6', 0.992199632791357, -3.234753679404763, -8.78836361122989, -3.23387484304364)\n('10-', 0.9810927453141476, -3.696831703548425, -14.90488840010583, -3.695341214708414)\n('2-6', 0.9741816586297092, -3.929944284676785, -17.607317697644717, -3.928183552907021)\n('6-1', 0.9588366998743444, -3.640233365160378, -15.379144993044608, -3.6386954506610736)\n('2-4', 0.9666667211568614, -3.6660744576841258, -16.745222017238643, -3.6643999354824017)\n('1-4', 0.9673059424220348, -3.925555364526829, -18.654933380454832, -3.9236898711887833)\n('4-1', 0.974821967446186, -3.372626606036675, -9.707446711479887, -3.371655861365527)\n('1-6', 0.9929696191801812, -3.072267260322839, -17.486425834693904, -3.0705186177393697)\n('6-2', 0.996469739189045, -3.4288210477352727, -12.41892394491297, -3.4275791553407813)\n('8-5', 0.9443346786607972, -4.248933227509226, -22.64083448113504, -4.246669144061112)\n('5-7', 0.9509452700127012, -4.168676257617839, -20.346791664402623, -4.166641578451398)\n('1-5', 1.0269961764888282, -2.5394838913108764, -7.762704719391589, -2.538707620838937)\n('1-8', 1.176959439132687, -1.76136905371235, -4.594320481289704, -1.760909621664221)\n('5-3', 0.9844119628138948, -3.370905329648145, -8.364134928392389, -3.3700689161553057)\n('6-5', 0.9712489726304112, -3.576215979830769, -11.063467716199494, -3.575109633059149)\n('9-5', 0.9814640731936904, -3.618466236145764, -11.45437482678623, -3.6173207986630853)\n('4-8', 0.9758667820481126, -3.952479028448344, -16.603497593198004, -3.950818678689024)\n('7-2', 0.9830362000868677, -3.6081140523844697, -11.879995409053606, -3.6069260528435643)\n('2-5', 0.9918741565499856, -3.1225846814739864, -14.754183927612011, -3.1211092630812254)\n('7-3', 0.9380190201332748, -4.421884133200824, -24.77026606455475, -4.4194071065943685)\n('9-3', 0.9829966800119196, -3.780097805501982, -13.953569509172684, -3.7787024485510647)\n('9-4', 0.9828479237657588, -3.803723861433833, -13.514995536394917, -3.8023723618801935)\n('8-2', 0.9700145507747884, -3.770245304757344, -12.93207578012185, -3.7689520971793318)\n('10-', 0.9587643506561674, -3.966746560065031, -14.856415967509578, -3.96526091846828)\n('6-3', 0.9806756263864168, -3.726280764417439, -14.527732773997478, -3.7248279911400393)\n('3-2', 1.0002698989405732, -3.217233095172596, -9.342928016212378, -3.2162988023709747)\n('7-5', 0.9679687433672456, -3.982004648141131, -14.98298186355295, -3.9805063499547755)\n('3-7', 0.9668157454051522, -3.434256467160316, -9.241872331646816, -3.4333322799271513)\n('2-3', 0.9668372715267, -3.95717090564515, -16.188588649938964, -3.955552046780156)\n('1-3', 0.9665676417676108, -3.831582591469573, -14.469462795195676, -3.8301356451900532)\n('8-3', 0.9778512025135716, -3.686571756642753, -11.118306872408995, -3.685459925955512)\n('2-8', 0.950967890890836, -4.177833850384779, -20.089537392760224, -4.175824896645503)\n('7-4', 0.9387359424428108, -4.519553698685354, -23.65230300554172, -4.5171884683848)\n('4-2', 0.9682300546367292, -3.931617464161364, -15.636234694434467, -3.9300538406919205)\n('6-4', 0.966222823987164, -3.5907074328107025, -14.105482777800818, -3.5892968845329225)\n('1-1', 0.9476435264638574, -4.022110744135286, -18.819087759290067, -4.020228835359357)\n('3-3', 0.9740832906865424, -3.754155073650343, -15.56499408085347, -3.7525985742422576)\n('4-3', 0.943336599803412, -4.355165630424821, -27.431058619993472, -4.352422524562821)\n('3-4', 0.9616245947685256, -3.6340887875536914, -13.679658656239406, -3.6327208216880673)\n('2-2', 0.9833277323259622, -3.811998089978892, -10.046908660312472, -3.810993399112861)\n('1-7', 0.9584982828776633, -3.978633021713848, -16.753315677650253, -3.976957690146083)\n('3-6', 0.9851126867563308, -3.474965978645603, -9.437832325931804, -3.4740221954130095)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"min(k_s), max(k_s), min(a_s), max(a_s), min(b_s), max(b_s), min(e_s), max(e_s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.706335Z","iopub.execute_input":"2026-02-03T21:07:28.706818Z","iopub.status.idle":"2026-02-03T21:07:28.712447Z","shell.execute_reply.started":"2026-02-03T21:07:28.706783Z","shell.execute_reply":"2026-02-03T21:07:28.711714Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(0.9380190201332748,\n 1.176959439132687,\n -4.519553698685354,\n -1.76136905371235,\n -27.431058619993472,\n -4.594320481289704,\n -4.5171884683848,\n -1.760909621664221)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"df = pd.read_csv('../input/generate-hust-data-gompertz-k-a-b/hust_gompertz_params.csv')\ndf['exponent'] = df['a'] - df['b']\nprint(df[['file','k','a','b','rul']].head(50))\nprint(df[['file','k','a','b','rul']].tail(27))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.713263Z","iopub.execute_input":"2026-02-03T21:07:28.713510Z","iopub.status.idle":"2026-02-03T21:07:28.737450Z","shell.execute_reply.started":"2026-02-03T21:07:28.713476Z","shell.execute_reply":"2026-02-03T21:07:28.736833Z"}},"outputs":[{"name":"stdout","text":"        file         k         a          b   rul\n0    6-6.csv  0.999947 -3.046302  -8.058702  2468\n1    7-8.csv  0.974260 -3.848156 -14.075338  1938\n2    5-3.csv  0.984412 -3.370905  -8.364135  2689\n3    6-8.csv  1.010742 -2.903100  -7.691465  2450\n4    5-5.csv  0.989792 -3.317092 -14.181010  1583\n5    2-4.csv  0.966667 -3.666074 -16.745222  1499\n6   10-3.csv  0.958764 -3.966747 -14.856416  1848\n7    1-4.csv  0.967306 -3.925555 -18.654933  1500\n8    6-3.csv  0.980676 -3.726281 -14.527733  1804\n9    5-6.csv  0.992200 -3.234754  -8.788364  2460\n10   8-3.csv  0.977851 -3.686572 -11.118307  2290\n11   3-6.csv  0.985113 -3.474966  -9.437832  2491\n12  10-4.csv  0.966029 -4.116645 -16.335191  1811\n13   9-6.csv  0.989487 -3.514648 -13.681117  1742\n14   3-2.csv  1.000270 -3.217233  -9.342928  2283\n15   9-2.csv  0.986816 -3.552345 -11.455414  2143\n16   9-7.csv  0.977538 -3.744889 -12.948923  2012\n17   6-5.csv  0.971249 -3.576216 -11.063468  2178\n18   4-8.csv  0.975867 -3.952479 -16.603498  1706\n19   5-7.csv  0.950945 -4.168676 -20.346792  1448\n20   1-5.csv  1.026996 -2.539484  -7.762705  1971\n21   1-7.csv  0.958498 -3.978633 -16.753316  1678\n22   1-8.csv  1.176959 -1.761369  -4.594320  2285\n23   8-7.csv  0.960537 -3.891278 -13.448727  2047\n24   6-2.csv  0.996470 -3.428821 -12.418924  1908\n25   3-3.csv  0.974083 -3.754155 -15.564994  1649\n26   6-1.csv  0.958837 -3.640233 -15.379145  1609\n27   2-7.csv  0.981609 -3.690470 -11.465815  2202\n28   1-2.csv  0.987839 -3.290557  -8.286661  2678\n29  10-7.csv  0.978627 -3.700592 -14.445177  1783\n30  10-1.csv  0.948122 -4.211694 -17.547137  1702\n31   9-1.csv  0.989177 -3.650584 -12.214167  2057\n32   8-5.csv  0.944335 -4.248933 -22.640834  1348\n33   9-4.csv  0.982848 -3.803724 -13.514996  1975\n34   4-4.csv  0.944654 -4.116516 -19.437070  1491\n35   3-4.csv  0.961625 -3.634089 -13.679659  1766\n36  10-6.csv  1.001915 -3.599475 -11.307329  2285\n37   5-2.csv  0.973719 -3.786716 -13.759517  1926\n38   7-5.csv  0.967969 -3.982005 -14.982982  1875\n39   4-3.csv  0.943337 -4.355166 -27.431059  1142\n40   5-4.csv  0.957524 -3.869757 -13.903639  1962\n41   3-5.csv  1.009926 -3.299265  -8.637530  2657\n42   7-2.csv  0.983036 -3.608114 -11.879995  2030\n43   3-7.csv  0.966816 -3.434256  -9.241872  2479\n44   1-3.csv  0.966568 -3.831583 -14.469463  1858\n45   9-5.csv  0.981464 -3.618466 -11.454375  2168\n46   3-1.csv  0.982172 -3.672732 -13.070265  1938\n47   4-1.csv  0.974822 -3.372627  -9.707447  2217\n48   9-3.csv  0.982997 -3.780098 -13.953570  1905\n49   7-1.csv  0.971157 -3.575039 -14.258500  1690\n        file         k         a          b   rul\n50   8-8.csv  0.984131 -3.744520 -15.656274  1679\n51   6-4.csv  0.966223 -3.590707 -14.105483  1717\n52   4-2.csv  0.968230 -3.931617 -15.636235  1782\n53   5-1.csv  0.996753 -3.147046  -8.325162  2507\n54   2-3.csv  0.966837 -3.957171 -16.188589  1751\n55   4-5.csv  0.964575 -4.011223 -18.315867  1561\n56   4-6.csv  0.955644 -3.675951 -17.921466  1380\n57   2-8.csv  0.950968 -4.177834 -20.089537  1481\n58   8-1.csv  0.945628 -4.333303 -23.960673  1308\n59  10-8.csv  0.959348 -3.935483 -19.795025  1400\n60   9-8.csv  0.979352 -3.566718 -10.580375  2308\n61   3-8.csv  0.998791 -3.489106 -10.105226  2342\n62   7-3.csv  0.938019 -4.421884 -24.770266  1295\n63   8-2.csv  0.970015 -3.770245 -12.932076  2041\n64   8-6.csv  0.990751 -3.606051 -10.801653  2365\n65   7-6.csv  0.941778 -4.459192 -22.763030  1419\n66  10-5.csv  0.974893 -3.852012 -13.306800  2030\n67   7-7.csv  0.946148 -4.208656 -17.714977  1685\n68   7-4.csv  0.938736 -4.519554 -23.652303  1393\n69   4-7.csv  1.005226 -3.407106 -10.876647  2216\n70   2-6.csv  0.974182 -3.929944 -17.607318  1572\n71  10-2.csv  0.981093 -3.696832 -14.904888  1697\n72   1-6.csv  0.992970 -3.072267 -17.486426  1143\n73   8-4.csv  0.951492 -4.194247 -15.876148  1885\n74   2-5.csv  0.991874 -3.122585 -14.754184  1386\n75   1-1.csv  0.947644 -4.022111 -18.819088  1504\n76   2-2.csv  0.983328 -3.811998 -10.046909  2651\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# Mixture of Experts Ensemble","metadata":{}},{"cell_type":"code","source":"class GompertzPINN(nn.Module):\n    def __init__(self, base_model):\n        super().__init__()\n        self.base_model =  base_model\n        \n        # 1. Freeze the base model\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n        self.base_model.eval()\n        \n    def inverse_gompertz_layer(self, y, k, a, b):\n        # Formula: (a - ln(ln(k/y))) / b\n        # Safety: Ensure k/y > 1 to avoid NaN in first log, and ln(k/y) > 0 for second log\n        # We clamp ratio to be slightly > 1.0 + epsilon\n        ratio = torch.clamp(k / y, min=1.00001)\n        \n        term1 = torch.log(ratio)\n        term2 = torch.log(term1 + 1e-8) # Safety epsilon\n        \n        rul = (a - term2) / b\n        return rul\n\n    def forward(self, x, current_soh=None):\n        # 1. Get Parameters (k, a, b) from frozen model\n        # Assuming model output is shape (Batch, 3) -> [k, a, b]\n        params = self.base_model(x)\n        k, a, b = params[:, 0:1], params[:, 1:2], params[:, 2:3]\n        \n        # # 2. Get Current SoH (y)\n        # # If not provided, grab the last value from the input window\n        # # Assuming x shape is (Batch, Window, Features) and SoH is Feature 0\n        # if current_soh is None:\n        #     current_soh = x[:, -1, 0].unsqueeze(1)\n\n        # 3. Predict RUL\n        rul = self.inverse_gompertz_layer(0.7, k, a, b)*10000\n        return rul, k, a, b","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.738263Z","iopub.execute_input":"2026-02-03T21:07:28.738540Z","iopub.status.idle":"2026-02-03T21:07:28.745047Z","shell.execute_reply.started":"2026-02-03T21:07:28.738518Z","shell.execute_reply":"2026-02-03T21:07:28.744232Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# # Create 3 independent PINNs\n# pinn_low  = GompertzPINN(f'best_lstm_model-window-100_model_pinn_data_{data_ranges[1]}.pth')\n# pinn_mid  = GompertzPINN(f'best_lstm_model-window-100_model_pinn_data_{data_ranges[2]}.pth')\n# pinn_high = GompertzPINN(f'best_lstm_model-window-100_model_pinn_data_{data_ranges[3]}.pth')\nWINDOW_SIZE = 99\ndef load_and_wrap_model(weight_path):\n    # 1. Initialize the empty architecture\n    # Make sure INPUT_SIZE matches what you used during training\n    model = PhysicsInformedLSTM(input_size=INPUT_SIZE).to('cpu')\n    \n    # 2. Load the weights from the file\n    # We use map_location='cpu' to be safe, then move to device later\n    model.load_state_dict(torch.load(weight_path, map_location='cpu'))\n    \n    # 3. Wrap it in the Physics/Gompertz layer\n    pinn_wrapper = GompertzPINN(base_model=model)\n    \n    return pinn_wrapper.to('cpu')\n\n# --- Usage ---\n\n# 1. Define your paths\npath_low  = f'best_lstm_model-window-100_model_pinn_data_{data_ranges[1]}.pth'\npath_mid  = f'best_lstm_model-window-100_model_pinn_data_{data_ranges[2]}.pth'\npath_high = f'best_lstm_model-window-100_model_pinn_data_{data_ranges[3]}.pth'\n\n# 2. Create the 3 independent PINNs correctly\npinn_low  = load_and_wrap_model(path_low)\npinn_mid  = load_and_wrap_model(path_mid)\npinn_high = load_and_wrap_model(path_high)\n\nprint(\"‚úÖ Models loaded and wrapped successfully!\")\n\ntrain_paths,val_paths,test_paths = datasets[data_ranges[0]]\ntest_loader,X_test,y_test,y_test_target = give_paths_get_loaders(test_paths,data_ranges[0])\n\nfor X_batch, y_batch,y_target in test_loader:\n    break\n\n# Usage\nwith torch.no_grad():\n    rul_l, k_l, a_l, b_l = pinn_low(X_batch[0].unsqueeze(0))\n    rul_m, k_m, a_m, b_m = pinn_mid(X_batch[0].unsqueeze(0))\n    rul_h, k_h, a_h, b_h = pinn_high(X_batch[0].unsqueeze(0))\nprint(y_batch[0],y_target[0])\nprint(f\"Low Model says: {rul_l.item():.2f}--------------{k_l, a_l, b_l}\")\nprint(f\"Mid Model says: {rul_m.item():.2f}--------------{k_m, a_m, b_m}\")\nprint(f\"High Model says: {rul_h.item():.2f}--------------{k_h, a_h, b_h}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.746476Z","iopub.execute_input":"2026-02-03T21:07:28.747196Z","iopub.status.idle":"2026-02-03T21:07:28.854818Z","shell.execute_reply.started":"2026-02-03T21:07:28.747158Z","shell.execute_reply":"2026-02-03T21:07:28.853995Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Models loaded and wrapped successfully!\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\n X_all shape : torch.Size([11, 100, 1]) , y_all shape : torch.Size([11, 3]) Ôºåy_2all shape: torch.Size([11, 1])\nload : \nallloader lengths :  1\ntensor([  0.9828,  -3.8037, -13.5150]) tensor([0.1975])\nLow Model says: 1410.75--------------(tensor([[0.9556]]), tensor([[-3.9226]]), tensor([[-19.5305]]))\nMid Model says: 1902.74--------------(tensor([[0.9745]]), tensor([[-3.7602]]), tensor([[-13.9485]]))\nHigh Model says: 2397.85--------------(tensor([[0.9951]]), tensor([[-3.3375]]), tensor([[-9.5613]]))\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"model = PhysicsInformedLSTM(input_size=INPUT_SIZE).to(device)\nmodel.load_state_dict(torch.load(f'/kaggle/working/best_lstm_model-window-100_model_pinn_data_{data_ranges[1]}.pth'))\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\nmodel(X_batch.to(device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:28.855772Z","iopub.execute_input":"2026-02-03T21:07:28.856000Z","iopub.status.idle":"2026-02-03T21:07:28.938491Z","shell.execute_reply.started":"2026-02-03T21:07:28.855978Z","shell.execute_reply":"2026-02-03T21:07:28.937833Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor([[  0.9556,  -3.9226, -19.5305],\n        [  0.9556,  -3.9226, -19.5299],\n        [  0.9555,  -3.9225, -19.5311],\n        [  0.9555,  -3.9226, -19.5307],\n        [  0.9555,  -3.9226, -19.5306],\n        [  0.9554,  -3.9225, -19.5317],\n        [  0.9554,  -3.9225, -19.5326],\n        [  0.9556,  -3.9226, -19.5305],\n        [  0.9554,  -3.9225, -19.5326],\n        [  0.9554,  -3.9225, -19.5317],\n        [  0.9554,  -3.9225, -19.5327]], device='cuda:0')"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Mixture of Experts Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\n\n# --- 1. The MoE Model Architecture ---\nclass TrainableGompertzMoE(nn.Module):\n    def __init__(self, model_low, model_mid, model_high, input_dim=1, window_size=100):\n        super().__init__()\n        \n        # Load Experts\n        self.expert_low = model_low\n        self.expert_mid = model_mid\n        self.expert_high = model_high\n        \n        # # Freeze Experts (We only train the Gate)\n        # self.freeze_expert(self.expert_low)\n        # self.freeze_expert(self.expert_mid)\n        # self.freeze_expert(self.expert_high)\n        # Partially Freeze Experts\n        # We freeze the LSTM (the \"Brain\") but train the FC (the \"Mouth\")\n        self.partial_freeze(self.expert_low)\n        self.partial_freeze(self.expert_mid)\n        self.partial_freeze(self.expert_high)\n        \n        # Gating Network (The Router)\n        # Decides which expert to trust based on the input curve\n        self.gate = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(window_size * input_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 3), \n        )\n        \n    def freeze_expert(self, model):\n        for param in model.parameters():\n            param.requires_grad = False\n        model.eval()\n\n    def partial_freeze(self, model):\n        # 1. Freeze EVERYTHING first\n        for param in model.parameters():\n            param.requires_grad = False\n            \n        # 2. Unfreeze ONLY the Final Layer (self.fc)\n        # This assumes your expert class has a layer named 'fc'\n        for param in model.fc.parameters():\n            param.requires_grad = True\n            \n        # 3. Set mode (Keep LSTM in eval mode to disable Dropout randomness in the feature extractor)\n        model.eval()\n\n    def inverse_gompertz_layer(self, y, k, a, b):\n        # RUL = (a - ln(ln(k/y))) / b\n        # Safety Clamping\n        ratio = torch.clamp(k / y, min=1.0001)\n        term1 = torch.log(ratio)\n        term2 = torch.log(torch.clamp(term1, min=1e-6)) \n        rul = (a - term2) / (b - 1e-6)\n        return rul\n\n    def forward(self, x):\n        # x: [Batch, Window, Features]\n        \n        # 1. Get Expert Predictions\n        out_l = self.expert_low(x) # [Batch, 3]\n        out_m = self.expert_mid(x)\n        out_h = self.expert_high(x)\n        \n        expert_outputs = torch.stack([out_l, out_m, out_h], dim=1) # [Batch, 3, 3]\n        \n        # 2. Get Gate Weights\n        logits = self.gate(x) # [Batch, 3]\n        temperature = 5.0 # <--- The Magic Number. Start with 2.0 or 5.0\n        weights = F.softmax(logits * temperature, dim=1)\n        # 3. Weighted Average of Parameters\n        # bmm: Batch Matrix Multiply\n        weighted_params = torch.bmm(weights.unsqueeze(1), expert_outputs).squeeze(1)\n        \n        k_pred = weighted_params[:, 0:1]\n        a_pred = weighted_params[:, 1:2]\n        b_pred = weighted_params[:, 2:3]\n\n        # 4. Calculate RUL\n        rul_pred = self.inverse_gompertz_layer(0.7, k_pred, a_pred, b_pred)\n        \n        return rul_pred, k_pred, a_pred, b_pred, weights\n\n# --- 2. The MoE Loss Function ---\ndef direct_rul_loss(pred_rul, true_rul, weights, pred_params, true_params):\n    \"\"\"\n    Optimizes for RUL accuracy while keeping parameters sane.\n    \"\"\"\n    # 1. RUL Error (Primary Goal) - Robust Huber Loss\n    rul_loss = F.smooth_l1_loss(pred_rul, true_rul)\n    \n    # 2. Parameter Consistency (Secondary Goal)\n    # Ensures the weighted k,a,b aren't wildly different from the single-model targets\n    param_loss = F.mse_loss(pred_params, true_params)\n    \n    # 3. Entropy Regularization\n    # Encourages the gate to be decisive (pick one expert) rather than averaging all 3\n    entropy_loss = -torch.mean(torch.sum(weights * torch.log(weights + 1e-8), dim=1))\n    \n    # Weights: RUL is king (1.0), Params help guide (0.1), Entropy for sharpness (0.01)\n    total_loss = (10000.0 * rul_loss) + (0.01 * param_loss) + (0.001 * entropy_loss)\n    \n    return total_loss, rul_loss.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:21:53.814596Z","iopub.execute_input":"2026-02-03T21:21:53.815220Z","iopub.status.idle":"2026-02-03T21:21:53.829603Z","shell.execute_reply.started":"2026-02-03T21:21:53.815188Z","shell.execute_reply":"2026-02-03T21:21:53.828977Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def train_moe_model(WINDOW_SIZES, train_paths, val_paths, test_paths, path_low, path_mid, path_high):\n    \n    # --- Helper: Data Preparation (Your Original Code) ---\n    def df_to_X_y_tensor(df, window_size=5, output_size=5):\n        X, y , y2 = [], [], []\n        # Create sequences\n        # Note: Ideally iterate carefully to avoid index errors, assuming your data is pre-padded or sufficient\n        df_vals = df['SoH'].values\n        df_k = df['k'].values\n        df_a = df['a'].values\n        df_b = df['b'].values\n        df_rul = df['rul'].values\n        \n        # Optimized loop\n        # for i in range(len(df) - window_size):\n        X.append(df_vals[i : i + window_size])\n        # Target params at the END of the window\n        y.append([df_k[-1], df_a[-1], df_b[-1]])\n        # Target RUL at the END of the window\n        y2.append(df_rul[:1]) #list(df['rul'])[:1]\n\n        X = np.array(X)[..., np.newaxis] # Add feature dim\n        y = np.array(y)\n        y2 = np.array(y2)\n        \n        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), torch.tensor(y2, dtype=torch.float32)\n\n    def get_x_y_lists(paths, w_size):\n        X_list, y_list, y_target = [], [], []\n        for path in paths:\n            df = pd.read_csv(path)\n            # Normalization (ensure these match your previous training!)\n            df['Cycle number'] = df['Cycle number'] / 10000\n            df['rul'] = df['rul'] / 10000 \n            df['SoH'] = df['SoH'] # Assuming SoH is already 0-1 or normalized externally\n            \n            X, y, y1 = df_to_X_y_tensor(df, window_size=w_size)\n            X_list.append(X)\n            y_list.append(y)\n            y_target.append(y1)\n        return X_list, y_list, y_target\n\n    def give_paths_get_loaders(paths, w_size, shuffle=False):\n        X_list, y_list, y_target = get_x_y_lists(paths, w_size)\n        \n        # Concatenate\n        X_cat = torch.cat(X_list, dim=0)\n        y_cat = torch.cat(y_list, dim=0)\n        y_tar_cat = torch.cat(y_target, dim=0)\n        \n        print(f\"Dataset Shape: X={X_cat.shape}, Param_Y={y_cat.shape}, RUL_Y={y_tar_cat.shape}\")\n        \n        loader = DataLoader(TensorDataset(X_cat, y_cat, y_tar_cat), batch_size=32, shuffle=shuffle)\n        return loader\n\n    # --- Helper: Load Base Model ---\n    def load_base_model(path, input_size):\n        # Ensure this matches your generic PhysicsInformedLSTM class definition\n        model = PhysicsInformedLSTM(input_size=input_size) \n        model.load_state_dict(torch.load(path, map_location=device))\n        return model\n\n    # --- MAIN LOOP ---\n    for WINDOW_SIZE in WINDOW_SIZES:\n        #WINDOW_SIZE = WINDOW_SIZE + 1 \n        print(f\"\\nüöÄ STARTING TRAINING FOR WINDOW SIZE: {WINDOW_SIZE}\")\n        \n        # 1. Prepare Data\n        train_loader = give_paths_get_loaders(train_paths, WINDOW_SIZE, shuffle=True)\n        val_loader   = give_paths_get_loaders(val_paths, WINDOW_SIZE, shuffle=False)\n        test_loader  = give_paths_get_loaders(test_paths, WINDOW_SIZE, shuffle=False)\n        \n        # 2. Initialize Experts & MoE\n        print(\"Loading Experts...\")\n        INPUT_SIZE = 1 # Assuming SoH only\n        expert_low  = load_base_model(path_low, INPUT_SIZE)\n        expert_mid  = load_base_model(path_mid, INPUT_SIZE)\n        expert_high = load_base_model(path_high, INPUT_SIZE)\n        \n        model = TrainableGompertzMoE(\n            expert_low, expert_mid, expert_high, \n            input_dim=INPUT_SIZE, window_size=WINDOW_SIZE\n        ).to(device)\n        \n        # 3. Optimizer (Only train the Gate!)\n        optimizer = torch.optim.Adam(model.gate.parameters(), lr=1e-3)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n        \n        # 4. Metrics Setup\n        best_val_loss = float('inf')\n        train_history = []\n        val_history = []\n        \n        # 5. Training Loop\n        num_epochs = 1000 # MoE converges faster than raw LSTM\n        \n        for epoch in range(num_epochs):\n            model.train()\n            total_train_loss = 0\n            total_rul_error = 0\n            \n            for X_batch, y_params_batch, y_rul_batch in train_loader:\n                X_batch = X_batch.to(device)\n                y_params_batch = y_params_batch.to(device)\n                y_rul_batch = y_rul_batch.to(device)\n                \n                optimizer.zero_grad()\n                \n                # Forward Pass\n                pred_rul, pred_k, pred_a, pred_b, gate_weights = model(X_batch)\n                \n                # Combined Params for loss calculation\n                pred_params = torch.cat([pred_k, pred_a, pred_b], dim=1)\n                \n                # Loss Calculation\n                loss, rul_l1 = direct_rul_loss(pred_rul, y_rul_batch, gate_weights, pred_params, y_params_batch)\n                \n                loss.backward()\n                optimizer.step()\n                \n                total_train_loss += loss.item()\n                total_rul_error += rul_l1\n                \n            avg_train_loss = total_train_loss / len(train_loader)\n            avg_rul_error = total_rul_error / len(train_loader)\n            \n            # --- Validation ---\n            model.eval()\n            total_val_loss = 0\n            val_rmse_accum = 0\n            \n            with torch.no_grad():\n                for X_val, y_params_val, y_rul_val in val_loader:\n                    X_val = X_val.to(device)\n                    y_params_val = y_params_val.to(device)\n                    y_rul_val = y_rul_val.to(device)\n                    \n                    pred_rul, k, a, b, w = model(X_val)\n                    pred_params = torch.cat([k, a, b], dim=1)\n                    \n                    loss, _ = direct_rul_loss(pred_rul, y_rul_val, w, pred_params, y_params_val)\n                    \n                    # RMSE Calculation\n                    mse = F.mse_loss(pred_rul, y_rul_val)\n                    val_rmse_accum += torch.sqrt(mse).item()\n                    total_val_loss += loss.item()\n            \n            avg_val_loss = total_val_loss / len(val_loader)\n            avg_val_rmse = val_rmse_accum / len(val_loader)\n            \n            # Adjust LR\n            scheduler.step(avg_val_loss)\n            \n            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | RUL L1: {avg_rul_error:.4f} | Val Loss: {avg_val_loss:.4f} | Val RMSE: {avg_val_rmse:.4f}\")\n            \n            # Save Best\n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                torch.save(model.state_dict(), f\"best_moe_model_window_{WINDOW_SIZE}.pth\")\n                print(\"‚úÖ Saved Best MoE Model\")\n                \n        # 6. Final Test\n        print(\"\\n--- Running Final Test ---\")\n        model.load_state_dict(torch.load(f\"best_moe_model_window_{WINDOW_SIZE}.pth\"))\n        model.eval()\n        test_rmse_total = 0\n        with torch.no_grad():\n            for X_test, _, y_rul_test in test_loader:\n                X_test = X_test.to(device)\n                y_rul_test = y_rul_test.to(device)\n                \n                pred_rul, _, _, _, _ = model(X_test)\n                mse = F.mse_loss(pred_rul, y_rul_test)\n                test_rmse_total += torch.sqrt(mse).item()\n        \n        print(f\"üèÜ Final Test RMSE for Window {WINDOW_SIZE}: {test_rmse_total / len(test_loader):.4f}\")\n        # return test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:21:56.854348Z","iopub.execute_input":"2026-02-03T21:21:56.854874Z","iopub.status.idle":"2026-02-03T21:21:56.873004Z","shell.execute_reply.started":"2026-02-03T21:21:56.854843Z","shell.execute_reply":"2026-02-03T21:21:56.872360Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"data_ranges = ['all','low','mid','high']\n\ntrain_moe_model(WINDOW_SIZES,*datasets['all'],'best_lstm_model-window-100_model_pinn_data_low.pth','best_lstm_model-window-100_model_pinn_data_mid.pth','best_lstm_model-window-100_model_pinn_data_high.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:22:00.273820Z","iopub.execute_input":"2026-02-03T21:22:00.274157Z","iopub.status.idle":"2026-02-03T21:22:13.864466Z","shell.execute_reply.started":"2026-02-03T21:22:00.274126Z","shell.execute_reply":"2026-02-03T21:22:13.863706Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ STARTING TRAINING FOR WINDOW SIZE: 100\nDataset Shape: X=torch.Size([55, 100, 1]), Param_Y=torch.Size([55, 3]), RUL_Y=torch.Size([55, 1])\nDataset Shape: X=torch.Size([11, 100, 1]), Param_Y=torch.Size([11, 3]), RUL_Y=torch.Size([11, 1])\nDataset Shape: X=torch.Size([11, 100, 1]), Param_Y=torch.Size([11, 3]), RUL_Y=torch.Size([11, 1])\nLoading Experts...\nEpoch 1/1000 | Train Loss: 7.8861 | RUL L1: 0.0008 | Val Loss: 10.4543 | Val RMSE: 0.0456\n‚úÖ Saved Best MoE Model\nEpoch 2/1000 | Train Loss: 7.4270 | RUL L1: 0.0007 | Val Loss: 10.7068 | Val RMSE: 0.0461\nEpoch 3/1000 | Train Loss: 6.7819 | RUL L1: 0.0007 | Val Loss: 10.6636 | Val RMSE: 0.0460\nEpoch 4/1000 | Train Loss: 7.9098 | RUL L1: 0.0008 | Val Loss: 10.6630 | Val RMSE: 0.0460\nEpoch 5/1000 | Train Loss: 7.2536 | RUL L1: 0.0007 | Val Loss: 10.6662 | Val RMSE: 0.0460\nEpoch 6/1000 | Train Loss: 6.9743 | RUL L1: 0.0007 | Val Loss: 10.6512 | Val RMSE: 0.0460\nEpoch 7/1000 | Train Loss: 6.8735 | RUL L1: 0.0007 | Val Loss: 10.6461 | Val RMSE: 0.0460\nEpoch 8/1000 | Train Loss: 6.9644 | RUL L1: 0.0007 | Val Loss: 10.6471 | Val RMSE: 0.0460\nEpoch 9/1000 | Train Loss: 6.7214 | RUL L1: 0.0007 | Val Loss: 10.6429 | Val RMSE: 0.0460\nEpoch 10/1000 | Train Loss: 7.0853 | RUL L1: 0.0007 | Val Loss: 10.6513 | Val RMSE: 0.0460\nEpoch 11/1000 | Train Loss: 7.1089 | RUL L1: 0.0007 | Val Loss: 10.6668 | Val RMSE: 0.0460\nEpoch 12/1000 | Train Loss: 7.3634 | RUL L1: 0.0007 | Val Loss: 10.6824 | Val RMSE: 0.0461\nEpoch 13/1000 | Train Loss: 7.2097 | RUL L1: 0.0007 | Val Loss: 10.6858 | Val RMSE: 0.0461\nEpoch 14/1000 | Train Loss: 6.9857 | RUL L1: 0.0007 | Val Loss: 10.6867 | Val RMSE: 0.0461\nEpoch 15/1000 | Train Loss: 7.1308 | RUL L1: 0.0007 | Val Loss: 10.6874 | Val RMSE: 0.0461\nEpoch 16/1000 | Train Loss: 7.2246 | RUL L1: 0.0007 | Val Loss: 10.6866 | Val RMSE: 0.0461\nEpoch 17/1000 | Train Loss: 7.0272 | RUL L1: 0.0007 | Val Loss: 10.6817 | Val RMSE: 0.0461\nEpoch 18/1000 | Train Loss: 7.2814 | RUL L1: 0.0007 | Val Loss: 10.6788 | Val RMSE: 0.0461\nEpoch 19/1000 | Train Loss: 7.1219 | RUL L1: 0.0007 | Val Loss: 10.6778 | Val RMSE: 0.0461\nEpoch 20/1000 | Train Loss: 7.1328 | RUL L1: 0.0007 | Val Loss: 10.6741 | Val RMSE: 0.0460\nEpoch 21/1000 | Train Loss: 7.2305 | RUL L1: 0.0007 | Val Loss: 10.6720 | Val RMSE: 0.0460\nEpoch 22/1000 | Train Loss: 7.1422 | RUL L1: 0.0007 | Val Loss: 10.6633 | Val RMSE: 0.0460\nEpoch 23/1000 | Train Loss: 7.0452 | RUL L1: 0.0007 | Val Loss: 10.6550 | Val RMSE: 0.0460\nEpoch 24/1000 | Train Loss: 7.2655 | RUL L1: 0.0007 | Val Loss: 10.6520 | Val RMSE: 0.0460\nEpoch 25/1000 | Train Loss: 6.9065 | RUL L1: 0.0007 | Val Loss: 10.6498 | Val RMSE: 0.0460\nEpoch 26/1000 | Train Loss: 6.7759 | RUL L1: 0.0007 | Val Loss: 10.6478 | Val RMSE: 0.0460\nEpoch 27/1000 | Train Loss: 6.8981 | RUL L1: 0.0007 | Val Loss: 10.6449 | Val RMSE: 0.0460\nEpoch 28/1000 | Train Loss: 7.0241 | RUL L1: 0.0007 | Val Loss: 10.6427 | Val RMSE: 0.0460\nEpoch 29/1000 | Train Loss: 6.7977 | RUL L1: 0.0007 | Val Loss: 10.6402 | Val RMSE: 0.0460\nEpoch 30/1000 | Train Loss: 7.0956 | RUL L1: 0.0007 | Val Loss: 10.6379 | Val RMSE: 0.0460\nEpoch 31/1000 | Train Loss: 7.3128 | RUL L1: 0.0007 | Val Loss: 10.6351 | Val RMSE: 0.0460\nEpoch 32/1000 | Train Loss: 6.9126 | RUL L1: 0.0007 | Val Loss: 10.6322 | Val RMSE: 0.0460\nEpoch 33/1000 | Train Loss: 6.8617 | RUL L1: 0.0007 | Val Loss: 10.6297 | Val RMSE: 0.0460\nEpoch 34/1000 | Train Loss: 7.0120 | RUL L1: 0.0007 | Val Loss: 10.6279 | Val RMSE: 0.0459\nEpoch 35/1000 | Train Loss: 7.0660 | RUL L1: 0.0007 | Val Loss: 10.6275 | Val RMSE: 0.0459\nEpoch 36/1000 | Train Loss: 7.1618 | RUL L1: 0.0007 | Val Loss: 10.6272 | Val RMSE: 0.0459\nEpoch 37/1000 | Train Loss: 7.0602 | RUL L1: 0.0007 | Val Loss: 10.6275 | Val RMSE: 0.0459\nEpoch 38/1000 | Train Loss: 7.5464 | RUL L1: 0.0007 | Val Loss: 10.6280 | Val RMSE: 0.0459\nEpoch 39/1000 | Train Loss: 7.2355 | RUL L1: 0.0007 | Val Loss: 10.6284 | Val RMSE: 0.0459\nEpoch 40/1000 | Train Loss: 7.1515 | RUL L1: 0.0007 | Val Loss: 10.6285 | Val RMSE: 0.0459\nEpoch 41/1000 | Train Loss: 6.9045 | RUL L1: 0.0007 | Val Loss: 10.6281 | Val RMSE: 0.0459\nEpoch 42/1000 | Train Loss: 6.8752 | RUL L1: 0.0007 | Val Loss: 10.6279 | Val RMSE: 0.0459\nEpoch 43/1000 | Train Loss: 6.9705 | RUL L1: 0.0007 | Val Loss: 10.6281 | Val RMSE: 0.0459\nEpoch 44/1000 | Train Loss: 7.1075 | RUL L1: 0.0007 | Val Loss: 10.6284 | Val RMSE: 0.0459\nEpoch 45/1000 | Train Loss: 7.0516 | RUL L1: 0.0007 | Val Loss: 10.6285 | Val RMSE: 0.0459\nEpoch 46/1000 | Train Loss: 7.1873 | RUL L1: 0.0007 | Val Loss: 10.6286 | Val RMSE: 0.0459\nEpoch 47/1000 | Train Loss: 6.9345 | RUL L1: 0.0007 | Val Loss: 10.6285 | Val RMSE: 0.0459\nEpoch 48/1000 | Train Loss: 7.3363 | RUL L1: 0.0007 | Val Loss: 10.6285 | Val RMSE: 0.0459\nEpoch 49/1000 | Train Loss: 7.2054 | RUL L1: 0.0007 | Val Loss: 10.6284 | Val RMSE: 0.0459\nEpoch 50/1000 | Train Loss: 7.1268 | RUL L1: 0.0007 | Val Loss: 10.6282 | Val RMSE: 0.0459\nEpoch 51/1000 | Train Loss: 7.0731 | RUL L1: 0.0007 | Val Loss: 10.6278 | Val RMSE: 0.0459\nEpoch 52/1000 | Train Loss: 6.7834 | RUL L1: 0.0007 | Val Loss: 10.6270 | Val RMSE: 0.0459\nEpoch 53/1000 | Train Loss: 7.2303 | RUL L1: 0.0007 | Val Loss: 10.6264 | Val RMSE: 0.0459\nEpoch 54/1000 | Train Loss: 6.7872 | RUL L1: 0.0007 | Val Loss: 10.6259 | Val RMSE: 0.0459\nEpoch 55/1000 | Train Loss: 7.2003 | RUL L1: 0.0007 | Val Loss: 10.6259 | Val RMSE: 0.0459\nEpoch 56/1000 | Train Loss: 6.9464 | RUL L1: 0.0007 | Val Loss: 10.6262 | Val RMSE: 0.0459\nEpoch 57/1000 | Train Loss: 6.8873 | RUL L1: 0.0007 | Val Loss: 10.6262 | Val RMSE: 0.0459\nEpoch 58/1000 | Train Loss: 6.8738 | RUL L1: 0.0007 | Val Loss: 10.6261 | Val RMSE: 0.0459\nEpoch 59/1000 | Train Loss: 7.1797 | RUL L1: 0.0007 | Val Loss: 10.6260 | Val RMSE: 0.0459\nEpoch 60/1000 | Train Loss: 7.6212 | RUL L1: 0.0008 | Val Loss: 10.6258 | Val RMSE: 0.0459\nEpoch 61/1000 | Train Loss: 7.0252 | RUL L1: 0.0007 | Val Loss: 10.6256 | Val RMSE: 0.0459\nEpoch 62/1000 | Train Loss: 7.0727 | RUL L1: 0.0007 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 63/1000 | Train Loss: 6.8378 | RUL L1: 0.0007 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 64/1000 | Train Loss: 6.9956 | RUL L1: 0.0007 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 65/1000 | Train Loss: 6.8144 | RUL L1: 0.0007 | Val Loss: 10.6257 | Val RMSE: 0.0459\nEpoch 66/1000 | Train Loss: 6.8080 | RUL L1: 0.0007 | Val Loss: 10.6258 | Val RMSE: 0.0459\nEpoch 67/1000 | Train Loss: 7.0606 | RUL L1: 0.0007 | Val Loss: 10.6259 | Val RMSE: 0.0459\nEpoch 68/1000 | Train Loss: 7.0407 | RUL L1: 0.0007 | Val Loss: 10.6259 | Val RMSE: 0.0459\nEpoch 69/1000 | Train Loss: 6.9983 | RUL L1: 0.0007 | Val Loss: 10.6259 | Val RMSE: 0.0459\nEpoch 70/1000 | Train Loss: 6.9960 | RUL L1: 0.0007 | Val Loss: 10.6259 | Val RMSE: 0.0459\nEpoch 71/1000 | Train Loss: 7.2166 | RUL L1: 0.0007 | Val Loss: 10.6260 | Val RMSE: 0.0459\nEpoch 72/1000 | Train Loss: 6.9868 | RUL L1: 0.0007 | Val Loss: 10.6260 | Val RMSE: 0.0459\nEpoch 73/1000 | Train Loss: 7.0239 | RUL L1: 0.0007 | Val Loss: 10.6260 | Val RMSE: 0.0459\nEpoch 74/1000 | Train Loss: 7.0164 | RUL L1: 0.0007 | Val Loss: 10.6259 | Val RMSE: 0.0459\nEpoch 75/1000 | Train Loss: 7.1346 | RUL L1: 0.0007 | Val Loss: 10.6259 | Val RMSE: 0.0459\nEpoch 76/1000 | Train Loss: 6.6980 | RUL L1: 0.0007 | Val Loss: 10.6257 | Val RMSE: 0.0459\nEpoch 77/1000 | Train Loss: 6.8356 | RUL L1: 0.0007 | Val Loss: 10.6256 | Val RMSE: 0.0459\nEpoch 78/1000 | Train Loss: 6.9063 | RUL L1: 0.0007 | Val Loss: 10.6256 | Val RMSE: 0.0459\nEpoch 79/1000 | Train Loss: 7.2366 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 80/1000 | Train Loss: 6.9226 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 81/1000 | Train Loss: 6.7904 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 82/1000 | Train Loss: 7.0977 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 83/1000 | Train Loss: 6.8886 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 84/1000 | Train Loss: 6.9977 | RUL L1: 0.0007 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 85/1000 | Train Loss: 7.7395 | RUL L1: 0.0008 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 86/1000 | Train Loss: 6.7723 | RUL L1: 0.0007 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 87/1000 | Train Loss: 7.3016 | RUL L1: 0.0007 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 88/1000 | Train Loss: 7.3124 | RUL L1: 0.0007 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 89/1000 | Train Loss: 7.2059 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 90/1000 | Train Loss: 6.8077 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 91/1000 | Train Loss: 7.1194 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 92/1000 | Train Loss: 7.1487 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 93/1000 | Train Loss: 7.0409 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 94/1000 | Train Loss: 7.1769 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 95/1000 | Train Loss: 6.6739 | RUL L1: 0.0007 | Val Loss: 10.6255 | Val RMSE: 0.0459\nEpoch 96/1000 | Train Loss: 7.1252 | RUL L1: 0.0007 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 97/1000 | Train Loss: 7.2172 | RUL L1: 0.0007 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 98/1000 | Train Loss: 6.8626 | RUL L1: 0.0007 | Val Loss: 10.6254 | Val RMSE: 0.0459\nEpoch 99/1000 | Train Loss: 6.8380 | RUL L1: 0.0007 | Val Loss: 10.6253 | Val RMSE: 0.0459\nEpoch 100/1000 | Train Loss: 7.0747 | RUL L1: 0.0007 | Val Loss: 10.6253 | Val RMSE: 0.0459\nEpoch 101/1000 | Train Loss: 7.2181 | RUL L1: 0.0007 | Val Loss: 10.6253 | Val RMSE: 0.0459\nEpoch 102/1000 | Train Loss: 6.9114 | RUL L1: 0.0007 | Val Loss: 10.6252 | Val RMSE: 0.0459\nEpoch 103/1000 | Train Loss: 7.2509 | RUL L1: 0.0007 | Val Loss: 10.6252 | Val RMSE: 0.0459\nEpoch 104/1000 | Train Loss: 6.9847 | RUL L1: 0.0007 | Val Loss: 10.6252 | Val RMSE: 0.0459\nEpoch 105/1000 | Train Loss: 7.2674 | RUL L1: 0.0007 | Val Loss: 10.6252 | Val RMSE: 0.0459\nEpoch 106/1000 | Train Loss: 7.0790 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 107/1000 | Train Loss: 6.9743 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 108/1000 | Train Loss: 7.0591 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 109/1000 | Train Loss: 6.6466 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 110/1000 | Train Loss: 7.1757 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 111/1000 | Train Loss: 6.6846 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 112/1000 | Train Loss: 7.1145 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 113/1000 | Train Loss: 6.9838 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 114/1000 | Train Loss: 7.2056 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 115/1000 | Train Loss: 6.9593 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 116/1000 | Train Loss: 6.8991 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 117/1000 | Train Loss: 6.9253 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 118/1000 | Train Loss: 7.0743 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 119/1000 | Train Loss: 7.0495 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 120/1000 | Train Loss: 6.9895 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 121/1000 | Train Loss: 7.0577 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 122/1000 | Train Loss: 7.0783 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 123/1000 | Train Loss: 6.8412 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 124/1000 | Train Loss: 6.6308 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 125/1000 | Train Loss: 6.6422 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 126/1000 | Train Loss: 6.8072 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 127/1000 | Train Loss: 7.2021 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 128/1000 | Train Loss: 6.9681 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 129/1000 | Train Loss: 7.0164 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 130/1000 | Train Loss: 6.9197 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 131/1000 | Train Loss: 7.1756 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 132/1000 | Train Loss: 7.2386 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 133/1000 | Train Loss: 7.2604 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 134/1000 | Train Loss: 7.4967 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 135/1000 | Train Loss: 6.8779 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 136/1000 | Train Loss: 7.0554 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 137/1000 | Train Loss: 7.2734 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 138/1000 | Train Loss: 6.9333 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 139/1000 | Train Loss: 7.2322 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 140/1000 | Train Loss: 7.1777 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 141/1000 | Train Loss: 7.1390 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 142/1000 | Train Loss: 6.9178 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 143/1000 | Train Loss: 7.1390 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 144/1000 | Train Loss: 7.0013 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 145/1000 | Train Loss: 7.1921 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 146/1000 | Train Loss: 7.3073 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 147/1000 | Train Loss: 7.1066 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 148/1000 | Train Loss: 7.1697 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 149/1000 | Train Loss: 7.0306 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 150/1000 | Train Loss: 6.7789 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 151/1000 | Train Loss: 6.8710 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 152/1000 | Train Loss: 7.1807 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 153/1000 | Train Loss: 6.9905 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 154/1000 | Train Loss: 6.8491 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 155/1000 | Train Loss: 6.9970 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 156/1000 | Train Loss: 6.8994 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 157/1000 | Train Loss: 6.9671 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 158/1000 | Train Loss: 7.5454 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 159/1000 | Train Loss: 7.1258 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 160/1000 | Train Loss: 6.9178 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 161/1000 | Train Loss: 6.8039 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 162/1000 | Train Loss: 7.0352 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 163/1000 | Train Loss: 7.6650 | RUL L1: 0.0008 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 164/1000 | Train Loss: 7.1534 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 165/1000 | Train Loss: 7.4587 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 166/1000 | Train Loss: 6.7957 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 167/1000 | Train Loss: 7.5276 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 168/1000 | Train Loss: 7.1092 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 169/1000 | Train Loss: 7.1754 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 170/1000 | Train Loss: 7.1445 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 171/1000 | Train Loss: 6.9096 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 172/1000 | Train Loss: 7.0734 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 173/1000 | Train Loss: 7.0367 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 174/1000 | Train Loss: 7.0699 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 175/1000 | Train Loss: 7.0983 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 176/1000 | Train Loss: 7.0029 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 177/1000 | Train Loss: 7.2010 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 178/1000 | Train Loss: 6.7627 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 179/1000 | Train Loss: 6.9988 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 180/1000 | Train Loss: 6.9440 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 181/1000 | Train Loss: 7.0374 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 182/1000 | Train Loss: 7.3396 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 183/1000 | Train Loss: 6.8298 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 184/1000 | Train Loss: 6.8983 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 185/1000 | Train Loss: 6.7415 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 186/1000 | Train Loss: 7.2215 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 187/1000 | Train Loss: 7.0738 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 188/1000 | Train Loss: 7.1326 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 189/1000 | Train Loss: 7.2412 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 190/1000 | Train Loss: 7.0532 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 191/1000 | Train Loss: 7.1396 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 192/1000 | Train Loss: 7.0247 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 193/1000 | Train Loss: 7.4687 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 194/1000 | Train Loss: 6.7927 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 195/1000 | Train Loss: 6.8746 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 196/1000 | Train Loss: 6.8083 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 197/1000 | Train Loss: 6.6541 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 198/1000 | Train Loss: 6.9631 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 199/1000 | Train Loss: 7.0469 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 200/1000 | Train Loss: 7.0502 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 201/1000 | Train Loss: 6.9844 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 202/1000 | Train Loss: 7.0618 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 203/1000 | Train Loss: 7.1490 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 204/1000 | Train Loss: 7.2189 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 205/1000 | Train Loss: 6.9847 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 206/1000 | Train Loss: 7.2364 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 207/1000 | Train Loss: 6.7103 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 208/1000 | Train Loss: 7.2582 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 209/1000 | Train Loss: 7.3387 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 210/1000 | Train Loss: 7.1064 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 211/1000 | Train Loss: 6.7296 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 212/1000 | Train Loss: 7.2159 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 213/1000 | Train Loss: 6.8510 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 214/1000 | Train Loss: 7.4320 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 215/1000 | Train Loss: 7.1505 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 216/1000 | Train Loss: 6.9786 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 217/1000 | Train Loss: 6.9444 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 218/1000 | Train Loss: 7.0065 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 219/1000 | Train Loss: 7.0329 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 220/1000 | Train Loss: 6.9857 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 221/1000 | Train Loss: 6.8633 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 222/1000 | Train Loss: 6.9200 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 223/1000 | Train Loss: 6.9297 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 224/1000 | Train Loss: 6.9273 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 225/1000 | Train Loss: 6.8461 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 226/1000 | Train Loss: 7.1456 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 227/1000 | Train Loss: 7.0523 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 228/1000 | Train Loss: 7.1464 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 229/1000 | Train Loss: 7.3256 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 230/1000 | Train Loss: 7.1303 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 231/1000 | Train Loss: 7.0030 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 232/1000 | Train Loss: 7.0144 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 233/1000 | Train Loss: 6.7116 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 234/1000 | Train Loss: 7.3435 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 235/1000 | Train Loss: 7.1667 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 236/1000 | Train Loss: 7.0137 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 237/1000 | Train Loss: 7.0288 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 238/1000 | Train Loss: 7.0498 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 239/1000 | Train Loss: 6.8635 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 240/1000 | Train Loss: 7.0287 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 241/1000 | Train Loss: 6.9166 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 242/1000 | Train Loss: 7.1427 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 243/1000 | Train Loss: 6.8368 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 244/1000 | Train Loss: 7.2679 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 245/1000 | Train Loss: 6.8176 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 246/1000 | Train Loss: 7.3682 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 247/1000 | Train Loss: 7.2371 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 248/1000 | Train Loss: 6.7192 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 249/1000 | Train Loss: 7.0210 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 250/1000 | Train Loss: 7.1061 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 251/1000 | Train Loss: 7.1992 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 252/1000 | Train Loss: 6.8222 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 253/1000 | Train Loss: 6.6479 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 254/1000 | Train Loss: 6.7920 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 255/1000 | Train Loss: 6.7878 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 256/1000 | Train Loss: 6.8651 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 257/1000 | Train Loss: 7.4570 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 258/1000 | Train Loss: 7.0511 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 259/1000 | Train Loss: 7.1082 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 260/1000 | Train Loss: 6.6455 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 261/1000 | Train Loss: 6.8451 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 262/1000 | Train Loss: 7.1476 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 263/1000 | Train Loss: 7.1545 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 264/1000 | Train Loss: 7.3882 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 265/1000 | Train Loss: 6.7788 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 266/1000 | Train Loss: 6.9222 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 267/1000 | Train Loss: 7.2564 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 268/1000 | Train Loss: 7.2199 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 269/1000 | Train Loss: 7.0724 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 270/1000 | Train Loss: 7.0990 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 271/1000 | Train Loss: 7.1929 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 272/1000 | Train Loss: 6.8453 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 273/1000 | Train Loss: 7.1729 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 274/1000 | Train Loss: 6.9430 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 275/1000 | Train Loss: 6.9126 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 276/1000 | Train Loss: 6.9684 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 277/1000 | Train Loss: 7.2533 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 278/1000 | Train Loss: 7.1526 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 279/1000 | Train Loss: 6.8380 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 280/1000 | Train Loss: 7.4774 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 281/1000 | Train Loss: 6.9690 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 282/1000 | Train Loss: 6.8294 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 283/1000 | Train Loss: 7.0411 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 284/1000 | Train Loss: 7.1566 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 285/1000 | Train Loss: 7.4394 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 286/1000 | Train Loss: 6.7328 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 287/1000 | Train Loss: 6.8494 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 288/1000 | Train Loss: 7.0255 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 289/1000 | Train Loss: 7.0202 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 290/1000 | Train Loss: 6.9325 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 291/1000 | Train Loss: 7.0383 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 292/1000 | Train Loss: 7.0682 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 293/1000 | Train Loss: 6.9612 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 294/1000 | Train Loss: 6.6350 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 295/1000 | Train Loss: 7.0789 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 296/1000 | Train Loss: 7.1744 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 297/1000 | Train Loss: 7.3063 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 298/1000 | Train Loss: 6.8132 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 299/1000 | Train Loss: 7.1654 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 300/1000 | Train Loss: 6.9968 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 301/1000 | Train Loss: 6.9196 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 302/1000 | Train Loss: 7.2970 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 303/1000 | Train Loss: 7.2506 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 304/1000 | Train Loss: 7.1019 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 305/1000 | Train Loss: 6.7991 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 306/1000 | Train Loss: 7.1601 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 307/1000 | Train Loss: 7.2419 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 308/1000 | Train Loss: 7.2071 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 309/1000 | Train Loss: 7.1214 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 310/1000 | Train Loss: 6.7354 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 311/1000 | Train Loss: 6.7945 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 312/1000 | Train Loss: 6.8379 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 313/1000 | Train Loss: 7.3909 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 314/1000 | Train Loss: 7.2583 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 315/1000 | Train Loss: 7.1728 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 316/1000 | Train Loss: 7.0360 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 317/1000 | Train Loss: 7.0286 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 318/1000 | Train Loss: 7.0828 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 319/1000 | Train Loss: 7.0239 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 320/1000 | Train Loss: 7.2830 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 321/1000 | Train Loss: 7.0545 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 322/1000 | Train Loss: 7.1368 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 323/1000 | Train Loss: 7.3417 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 324/1000 | Train Loss: 6.9327 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 325/1000 | Train Loss: 6.8016 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 326/1000 | Train Loss: 7.1318 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 327/1000 | Train Loss: 6.9872 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 328/1000 | Train Loss: 6.5793 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 329/1000 | Train Loss: 7.3000 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 330/1000 | Train Loss: 6.9875 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 331/1000 | Train Loss: 6.7982 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 332/1000 | Train Loss: 6.8653 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 333/1000 | Train Loss: 6.8987 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 334/1000 | Train Loss: 7.1347 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 335/1000 | Train Loss: 7.0636 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 336/1000 | Train Loss: 7.1315 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 337/1000 | Train Loss: 7.0599 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 338/1000 | Train Loss: 7.1322 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 339/1000 | Train Loss: 7.1554 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 340/1000 | Train Loss: 7.0011 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 341/1000 | Train Loss: 7.0149 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 342/1000 | Train Loss: 6.9974 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 343/1000 | Train Loss: 7.3407 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 344/1000 | Train Loss: 6.9421 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 345/1000 | Train Loss: 7.1315 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 346/1000 | Train Loss: 7.0351 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 347/1000 | Train Loss: 7.2143 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 348/1000 | Train Loss: 6.9689 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 349/1000 | Train Loss: 6.7700 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 350/1000 | Train Loss: 7.1287 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 351/1000 | Train Loss: 7.2823 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 352/1000 | Train Loss: 6.9188 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 353/1000 | Train Loss: 6.9818 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 354/1000 | Train Loss: 6.8385 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 355/1000 | Train Loss: 7.4218 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 356/1000 | Train Loss: 7.3209 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 357/1000 | Train Loss: 7.1104 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 358/1000 | Train Loss: 6.8874 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 359/1000 | Train Loss: 7.1818 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 360/1000 | Train Loss: 6.8617 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 361/1000 | Train Loss: 7.1215 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 362/1000 | Train Loss: 7.0066 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 363/1000 | Train Loss: 7.0706 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 364/1000 | Train Loss: 6.9096 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 365/1000 | Train Loss: 7.2936 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 366/1000 | Train Loss: 7.0676 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 367/1000 | Train Loss: 6.8551 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 368/1000 | Train Loss: 7.3465 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 369/1000 | Train Loss: 7.1224 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 370/1000 | Train Loss: 7.1636 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 371/1000 | Train Loss: 7.2693 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 372/1000 | Train Loss: 7.1597 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 373/1000 | Train Loss: 6.9878 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 374/1000 | Train Loss: 6.9131 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 375/1000 | Train Loss: 7.2584 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 376/1000 | Train Loss: 6.9857 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 377/1000 | Train Loss: 7.2854 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 378/1000 | Train Loss: 7.1222 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 379/1000 | Train Loss: 7.2809 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 380/1000 | Train Loss: 7.1833 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 381/1000 | Train Loss: 6.8322 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 382/1000 | Train Loss: 7.1706 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 383/1000 | Train Loss: 7.0587 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 384/1000 | Train Loss: 7.3869 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 385/1000 | Train Loss: 7.1891 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 386/1000 | Train Loss: 7.5157 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 387/1000 | Train Loss: 7.1456 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 388/1000 | Train Loss: 6.7833 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 389/1000 | Train Loss: 7.1213 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 390/1000 | Train Loss: 7.0098 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 391/1000 | Train Loss: 6.7965 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 392/1000 | Train Loss: 7.1906 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 393/1000 | Train Loss: 7.0562 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 394/1000 | Train Loss: 7.1832 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 395/1000 | Train Loss: 7.0099 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 396/1000 | Train Loss: 7.2279 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 397/1000 | Train Loss: 7.3159 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 398/1000 | Train Loss: 6.8922 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 399/1000 | Train Loss: 7.1186 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 400/1000 | Train Loss: 6.9258 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 401/1000 | Train Loss: 7.1865 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 402/1000 | Train Loss: 7.0684 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 403/1000 | Train Loss: 7.0348 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 404/1000 | Train Loss: 7.1639 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 405/1000 | Train Loss: 7.1445 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 406/1000 | Train Loss: 6.9473 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 407/1000 | Train Loss: 7.2605 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 408/1000 | Train Loss: 6.7656 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 409/1000 | Train Loss: 6.8948 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 410/1000 | Train Loss: 7.1239 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 411/1000 | Train Loss: 7.0887 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 412/1000 | Train Loss: 7.2220 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 413/1000 | Train Loss: 6.6709 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 414/1000 | Train Loss: 7.0916 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 415/1000 | Train Loss: 7.0156 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 416/1000 | Train Loss: 7.1204 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 417/1000 | Train Loss: 7.1950 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 418/1000 | Train Loss: 6.8605 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 419/1000 | Train Loss: 7.1585 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 420/1000 | Train Loss: 7.1371 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 421/1000 | Train Loss: 7.2572 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 422/1000 | Train Loss: 7.2949 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 423/1000 | Train Loss: 6.9899 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 424/1000 | Train Loss: 7.2670 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 425/1000 | Train Loss: 6.9375 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 426/1000 | Train Loss: 6.6456 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 427/1000 | Train Loss: 7.0630 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 428/1000 | Train Loss: 7.0959 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 429/1000 | Train Loss: 6.8463 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 430/1000 | Train Loss: 7.2302 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 431/1000 | Train Loss: 7.0480 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 432/1000 | Train Loss: 7.1217 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 433/1000 | Train Loss: 6.9131 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 434/1000 | Train Loss: 7.3373 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 435/1000 | Train Loss: 7.2176 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 436/1000 | Train Loss: 7.3695 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 437/1000 | Train Loss: 7.1039 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 438/1000 | Train Loss: 7.2583 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 439/1000 | Train Loss: 7.1719 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 440/1000 | Train Loss: 7.2108 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 441/1000 | Train Loss: 7.1834 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 442/1000 | Train Loss: 7.0428 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 443/1000 | Train Loss: 6.7583 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 444/1000 | Train Loss: 6.8143 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 445/1000 | Train Loss: 7.0559 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 446/1000 | Train Loss: 6.9517 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 447/1000 | Train Loss: 6.8743 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 448/1000 | Train Loss: 7.4243 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 449/1000 | Train Loss: 7.1160 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 450/1000 | Train Loss: 7.2078 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 451/1000 | Train Loss: 7.0210 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 452/1000 | Train Loss: 7.1138 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 453/1000 | Train Loss: 7.2453 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 454/1000 | Train Loss: 7.3053 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 455/1000 | Train Loss: 6.8864 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 456/1000 | Train Loss: 6.9448 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 457/1000 | Train Loss: 7.0870 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 458/1000 | Train Loss: 6.9040 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 459/1000 | Train Loss: 6.8001 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 460/1000 | Train Loss: 6.9859 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 461/1000 | Train Loss: 7.2343 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 462/1000 | Train Loss: 7.0220 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 463/1000 | Train Loss: 6.9690 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 464/1000 | Train Loss: 6.9583 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 465/1000 | Train Loss: 6.7455 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 466/1000 | Train Loss: 7.3179 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 467/1000 | Train Loss: 6.9686 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 468/1000 | Train Loss: 6.7897 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 469/1000 | Train Loss: 7.1015 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 470/1000 | Train Loss: 7.4396 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 471/1000 | Train Loss: 7.0277 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 472/1000 | Train Loss: 7.4801 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 473/1000 | Train Loss: 6.8264 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 474/1000 | Train Loss: 7.2627 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 475/1000 | Train Loss: 7.0781 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 476/1000 | Train Loss: 6.8577 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 477/1000 | Train Loss: 7.0295 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 478/1000 | Train Loss: 6.9185 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 479/1000 | Train Loss: 7.0782 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 480/1000 | Train Loss: 6.9560 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 481/1000 | Train Loss: 7.1936 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 482/1000 | Train Loss: 6.9723 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 483/1000 | Train Loss: 7.4189 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 484/1000 | Train Loss: 7.3675 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 485/1000 | Train Loss: 7.0255 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 486/1000 | Train Loss: 7.2353 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 487/1000 | Train Loss: 6.9801 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 488/1000 | Train Loss: 6.9819 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 489/1000 | Train Loss: 7.0102 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 490/1000 | Train Loss: 6.9956 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 491/1000 | Train Loss: 6.8680 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 492/1000 | Train Loss: 7.0875 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 493/1000 | Train Loss: 7.1410 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 494/1000 | Train Loss: 6.6463 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 495/1000 | Train Loss: 7.0376 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 496/1000 | Train Loss: 7.1813 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 497/1000 | Train Loss: 7.0401 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 498/1000 | Train Loss: 6.8511 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 499/1000 | Train Loss: 7.0428 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 500/1000 | Train Loss: 7.0488 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 501/1000 | Train Loss: 7.1762 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 502/1000 | Train Loss: 7.0660 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 503/1000 | Train Loss: 7.3482 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 504/1000 | Train Loss: 7.1186 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 505/1000 | Train Loss: 7.0643 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 506/1000 | Train Loss: 6.6404 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 507/1000 | Train Loss: 6.9503 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 508/1000 | Train Loss: 7.0593 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 509/1000 | Train Loss: 7.5449 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 510/1000 | Train Loss: 6.9564 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 511/1000 | Train Loss: 6.7307 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 512/1000 | Train Loss: 6.8827 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 513/1000 | Train Loss: 7.0404 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 514/1000 | Train Loss: 6.9477 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 515/1000 | Train Loss: 6.7511 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 516/1000 | Train Loss: 6.8474 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 517/1000 | Train Loss: 7.2259 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 518/1000 | Train Loss: 6.9381 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 519/1000 | Train Loss: 7.4009 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 520/1000 | Train Loss: 6.8316 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 521/1000 | Train Loss: 6.7693 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 522/1000 | Train Loss: 7.0069 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 523/1000 | Train Loss: 6.8437 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 524/1000 | Train Loss: 7.0672 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 525/1000 | Train Loss: 6.9887 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 526/1000 | Train Loss: 7.0907 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 527/1000 | Train Loss: 6.9651 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 528/1000 | Train Loss: 6.9376 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 529/1000 | Train Loss: 7.2055 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 530/1000 | Train Loss: 7.1416 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 531/1000 | Train Loss: 7.3028 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 532/1000 | Train Loss: 7.2340 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 533/1000 | Train Loss: 7.1512 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 534/1000 | Train Loss: 7.0656 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 535/1000 | Train Loss: 6.9154 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 536/1000 | Train Loss: 6.9123 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 537/1000 | Train Loss: 6.9665 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 538/1000 | Train Loss: 6.8440 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 539/1000 | Train Loss: 6.8115 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 540/1000 | Train Loss: 7.1575 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 541/1000 | Train Loss: 6.8998 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 542/1000 | Train Loss: 6.8080 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 543/1000 | Train Loss: 7.1388 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 544/1000 | Train Loss: 7.1497 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 545/1000 | Train Loss: 6.8773 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 546/1000 | Train Loss: 7.3769 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 547/1000 | Train Loss: 6.7834 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 548/1000 | Train Loss: 6.9695 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 549/1000 | Train Loss: 6.9387 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 550/1000 | Train Loss: 6.7262 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 551/1000 | Train Loss: 7.1783 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 552/1000 | Train Loss: 6.9498 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 553/1000 | Train Loss: 6.9124 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 554/1000 | Train Loss: 7.2490 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 555/1000 | Train Loss: 7.0019 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 556/1000 | Train Loss: 6.9961 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 557/1000 | Train Loss: 6.9966 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 558/1000 | Train Loss: 6.8322 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 559/1000 | Train Loss: 7.0203 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 560/1000 | Train Loss: 7.2242 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 561/1000 | Train Loss: 6.8301 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 562/1000 | Train Loss: 7.1911 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 563/1000 | Train Loss: 6.9785 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 564/1000 | Train Loss: 6.7358 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 565/1000 | Train Loss: 7.1607 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 566/1000 | Train Loss: 7.0775 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 567/1000 | Train Loss: 7.0536 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 568/1000 | Train Loss: 6.7694 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 569/1000 | Train Loss: 6.8862 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 570/1000 | Train Loss: 6.8018 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 571/1000 | Train Loss: 7.0706 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 572/1000 | Train Loss: 7.0868 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 573/1000 | Train Loss: 6.7194 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 574/1000 | Train Loss: 7.1136 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 575/1000 | Train Loss: 6.9988 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 576/1000 | Train Loss: 7.2679 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 577/1000 | Train Loss: 6.7896 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 578/1000 | Train Loss: 6.9368 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 579/1000 | Train Loss: 6.8574 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 580/1000 | Train Loss: 7.2071 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 581/1000 | Train Loss: 7.1127 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 582/1000 | Train Loss: 7.0776 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 583/1000 | Train Loss: 6.9287 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 584/1000 | Train Loss: 7.3223 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 585/1000 | Train Loss: 7.0730 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 586/1000 | Train Loss: 6.9970 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 587/1000 | Train Loss: 7.0564 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 588/1000 | Train Loss: 7.1823 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 589/1000 | Train Loss: 7.1963 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 590/1000 | Train Loss: 6.7740 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 591/1000 | Train Loss: 7.0273 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 592/1000 | Train Loss: 6.8472 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 593/1000 | Train Loss: 6.9498 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 594/1000 | Train Loss: 7.0975 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 595/1000 | Train Loss: 6.8629 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 596/1000 | Train Loss: 7.0139 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 597/1000 | Train Loss: 6.8137 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 598/1000 | Train Loss: 6.7231 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 599/1000 | Train Loss: 6.9005 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 600/1000 | Train Loss: 6.8522 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 601/1000 | Train Loss: 6.7690 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 602/1000 | Train Loss: 7.2064 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 603/1000 | Train Loss: 6.9019 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 604/1000 | Train Loss: 7.1228 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 605/1000 | Train Loss: 6.7058 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 606/1000 | Train Loss: 6.8613 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 607/1000 | Train Loss: 7.1163 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 608/1000 | Train Loss: 6.7459 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 609/1000 | Train Loss: 6.7425 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 610/1000 | Train Loss: 7.0705 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 611/1000 | Train Loss: 6.9863 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 612/1000 | Train Loss: 6.9914 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 613/1000 | Train Loss: 7.2115 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 614/1000 | Train Loss: 6.8755 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 615/1000 | Train Loss: 7.0597 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 616/1000 | Train Loss: 7.2303 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 617/1000 | Train Loss: 7.2160 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 618/1000 | Train Loss: 6.7978 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 619/1000 | Train Loss: 7.0884 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 620/1000 | Train Loss: 6.6841 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 621/1000 | Train Loss: 7.1584 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 622/1000 | Train Loss: 7.0641 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 623/1000 | Train Loss: 6.8772 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 624/1000 | Train Loss: 6.7051 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 625/1000 | Train Loss: 7.5338 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 626/1000 | Train Loss: 6.9825 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 627/1000 | Train Loss: 6.8089 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 628/1000 | Train Loss: 6.6394 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 629/1000 | Train Loss: 6.9956 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 630/1000 | Train Loss: 7.1739 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 631/1000 | Train Loss: 7.0828 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 632/1000 | Train Loss: 7.0764 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 633/1000 | Train Loss: 7.3531 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 634/1000 | Train Loss: 6.8975 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 635/1000 | Train Loss: 6.9796 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 636/1000 | Train Loss: 6.8027 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 637/1000 | Train Loss: 7.1326 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 638/1000 | Train Loss: 7.2858 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 639/1000 | Train Loss: 7.1049 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 640/1000 | Train Loss: 6.9826 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 641/1000 | Train Loss: 7.3285 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 642/1000 | Train Loss: 7.1207 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 643/1000 | Train Loss: 7.2779 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 644/1000 | Train Loss: 7.1484 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 645/1000 | Train Loss: 6.7142 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 646/1000 | Train Loss: 6.9599 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 647/1000 | Train Loss: 7.0318 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 648/1000 | Train Loss: 7.1928 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 649/1000 | Train Loss: 6.6648 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 650/1000 | Train Loss: 7.2935 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 651/1000 | Train Loss: 6.7665 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 652/1000 | Train Loss: 7.1642 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 653/1000 | Train Loss: 6.8840 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 654/1000 | Train Loss: 6.8216 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 655/1000 | Train Loss: 7.0367 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 656/1000 | Train Loss: 6.9238 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 657/1000 | Train Loss: 6.8696 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 658/1000 | Train Loss: 7.1622 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 659/1000 | Train Loss: 6.9466 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 660/1000 | Train Loss: 6.9926 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 661/1000 | Train Loss: 6.8053 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 662/1000 | Train Loss: 7.1417 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 663/1000 | Train Loss: 6.7919 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 664/1000 | Train Loss: 7.1714 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 665/1000 | Train Loss: 6.8974 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 666/1000 | Train Loss: 7.1579 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 667/1000 | Train Loss: 7.2114 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 668/1000 | Train Loss: 6.7911 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 669/1000 | Train Loss: 7.0662 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 670/1000 | Train Loss: 7.1349 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 671/1000 | Train Loss: 7.4222 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 672/1000 | Train Loss: 6.9177 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 673/1000 | Train Loss: 6.8655 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 674/1000 | Train Loss: 7.0793 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 675/1000 | Train Loss: 7.3167 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 676/1000 | Train Loss: 6.6881 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 677/1000 | Train Loss: 6.8046 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 678/1000 | Train Loss: 7.2152 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 679/1000 | Train Loss: 7.1473 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 680/1000 | Train Loss: 7.1513 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 681/1000 | Train Loss: 6.8711 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 682/1000 | Train Loss: 6.9910 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 683/1000 | Train Loss: 6.8598 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 684/1000 | Train Loss: 6.9651 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 685/1000 | Train Loss: 7.3111 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 686/1000 | Train Loss: 7.2293 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 687/1000 | Train Loss: 7.3969 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 688/1000 | Train Loss: 6.7452 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 689/1000 | Train Loss: 7.0818 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 690/1000 | Train Loss: 6.9784 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 691/1000 | Train Loss: 6.8973 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 692/1000 | Train Loss: 7.1709 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 693/1000 | Train Loss: 7.1153 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 694/1000 | Train Loss: 6.9710 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 695/1000 | Train Loss: 6.9173 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 696/1000 | Train Loss: 7.0233 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 697/1000 | Train Loss: 6.6658 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 698/1000 | Train Loss: 7.1408 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 699/1000 | Train Loss: 7.0352 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 700/1000 | Train Loss: 6.7911 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 701/1000 | Train Loss: 7.2716 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 702/1000 | Train Loss: 7.0959 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 703/1000 | Train Loss: 6.9640 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 704/1000 | Train Loss: 7.0766 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 705/1000 | Train Loss: 7.0549 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 706/1000 | Train Loss: 6.9048 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 707/1000 | Train Loss: 6.5828 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 708/1000 | Train Loss: 7.1205 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 709/1000 | Train Loss: 6.9624 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 710/1000 | Train Loss: 6.5158 | RUL L1: 0.0006 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 711/1000 | Train Loss: 7.0343 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 712/1000 | Train Loss: 7.0504 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 713/1000 | Train Loss: 6.8178 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 714/1000 | Train Loss: 7.4145 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 715/1000 | Train Loss: 7.0500 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 716/1000 | Train Loss: 7.1770 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 717/1000 | Train Loss: 7.1995 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 718/1000 | Train Loss: 7.1389 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 719/1000 | Train Loss: 7.1030 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 720/1000 | Train Loss: 6.9607 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 721/1000 | Train Loss: 6.5411 | RUL L1: 0.0006 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 722/1000 | Train Loss: 7.2560 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 723/1000 | Train Loss: 6.8848 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 724/1000 | Train Loss: 7.1471 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 725/1000 | Train Loss: 7.0924 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 726/1000 | Train Loss: 6.8764 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 727/1000 | Train Loss: 7.0879 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 728/1000 | Train Loss: 7.1397 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 729/1000 | Train Loss: 6.8935 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 730/1000 | Train Loss: 7.1889 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 731/1000 | Train Loss: 7.0434 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 732/1000 | Train Loss: 7.0856 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 733/1000 | Train Loss: 7.4832 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 734/1000 | Train Loss: 7.1199 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 735/1000 | Train Loss: 6.9727 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 736/1000 | Train Loss: 7.2455 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 737/1000 | Train Loss: 7.0781 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 738/1000 | Train Loss: 7.1425 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 739/1000 | Train Loss: 7.0814 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 740/1000 | Train Loss: 7.3538 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 741/1000 | Train Loss: 7.0515 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 742/1000 | Train Loss: 6.7494 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 743/1000 | Train Loss: 6.8450 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 744/1000 | Train Loss: 7.1020 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 745/1000 | Train Loss: 7.3070 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 746/1000 | Train Loss: 7.0501 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 747/1000 | Train Loss: 7.5543 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 748/1000 | Train Loss: 7.0152 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 749/1000 | Train Loss: 6.9960 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 750/1000 | Train Loss: 6.7862 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 751/1000 | Train Loss: 6.9673 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 752/1000 | Train Loss: 7.2592 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 753/1000 | Train Loss: 7.0870 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 754/1000 | Train Loss: 7.0694 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 755/1000 | Train Loss: 6.9180 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 756/1000 | Train Loss: 7.3184 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 757/1000 | Train Loss: 7.2205 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 758/1000 | Train Loss: 7.2978 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 759/1000 | Train Loss: 7.0317 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 760/1000 | Train Loss: 7.0381 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 761/1000 | Train Loss: 7.2352 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 762/1000 | Train Loss: 7.3036 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 763/1000 | Train Loss: 7.1832 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 764/1000 | Train Loss: 6.9284 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 765/1000 | Train Loss: 7.0612 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 766/1000 | Train Loss: 7.1828 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 767/1000 | Train Loss: 7.1317 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 768/1000 | Train Loss: 7.0938 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 769/1000 | Train Loss: 7.1610 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 770/1000 | Train Loss: 7.0903 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 771/1000 | Train Loss: 6.7830 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 772/1000 | Train Loss: 7.4179 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 773/1000 | Train Loss: 7.1609 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 774/1000 | Train Loss: 7.0687 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 775/1000 | Train Loss: 7.1898 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 776/1000 | Train Loss: 7.0437 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 777/1000 | Train Loss: 6.9125 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 778/1000 | Train Loss: 7.3582 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 779/1000 | Train Loss: 7.1159 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 780/1000 | Train Loss: 6.9158 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 781/1000 | Train Loss: 7.0302 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 782/1000 | Train Loss: 6.8969 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 783/1000 | Train Loss: 6.9813 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 784/1000 | Train Loss: 7.1354 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 785/1000 | Train Loss: 6.7465 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 786/1000 | Train Loss: 7.4374 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 787/1000 | Train Loss: 7.3296 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 788/1000 | Train Loss: 7.1533 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 789/1000 | Train Loss: 6.9676 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 790/1000 | Train Loss: 6.9723 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 791/1000 | Train Loss: 7.1964 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 792/1000 | Train Loss: 6.9600 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 793/1000 | Train Loss: 7.2000 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 794/1000 | Train Loss: 6.9458 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 795/1000 | Train Loss: 6.9539 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 796/1000 | Train Loss: 7.4500 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 797/1000 | Train Loss: 6.7796 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 798/1000 | Train Loss: 7.0999 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 799/1000 | Train Loss: 7.0633 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 800/1000 | Train Loss: 6.6591 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 801/1000 | Train Loss: 7.2469 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 802/1000 | Train Loss: 7.3210 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 803/1000 | Train Loss: 6.9552 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 804/1000 | Train Loss: 7.1182 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 805/1000 | Train Loss: 6.9755 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 806/1000 | Train Loss: 7.1749 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 807/1000 | Train Loss: 6.8787 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 808/1000 | Train Loss: 6.9210 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 809/1000 | Train Loss: 6.9483 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 810/1000 | Train Loss: 7.1093 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 811/1000 | Train Loss: 6.8164 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 812/1000 | Train Loss: 6.9953 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 813/1000 | Train Loss: 6.9664 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 814/1000 | Train Loss: 7.3194 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 815/1000 | Train Loss: 6.7342 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 816/1000 | Train Loss: 6.9777 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 817/1000 | Train Loss: 6.8872 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 818/1000 | Train Loss: 7.0263 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 819/1000 | Train Loss: 6.8088 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 820/1000 | Train Loss: 6.8838 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 821/1000 | Train Loss: 6.9053 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 822/1000 | Train Loss: 6.9773 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 823/1000 | Train Loss: 7.1660 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 824/1000 | Train Loss: 7.0511 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 825/1000 | Train Loss: 7.1617 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 826/1000 | Train Loss: 7.0762 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 827/1000 | Train Loss: 7.0473 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 828/1000 | Train Loss: 7.2384 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 829/1000 | Train Loss: 7.0433 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 830/1000 | Train Loss: 6.7317 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 831/1000 | Train Loss: 6.7790 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 832/1000 | Train Loss: 7.2767 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 833/1000 | Train Loss: 7.2021 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 834/1000 | Train Loss: 6.9993 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 835/1000 | Train Loss: 6.9393 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 836/1000 | Train Loss: 7.1107 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 837/1000 | Train Loss: 6.7186 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 838/1000 | Train Loss: 7.2907 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 839/1000 | Train Loss: 6.8247 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 840/1000 | Train Loss: 6.8731 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 841/1000 | Train Loss: 6.9497 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 842/1000 | Train Loss: 7.2794 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 843/1000 | Train Loss: 6.7530 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 844/1000 | Train Loss: 7.1321 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 845/1000 | Train Loss: 6.6752 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 846/1000 | Train Loss: 7.1844 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 847/1000 | Train Loss: 7.3609 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 848/1000 | Train Loss: 7.0073 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 849/1000 | Train Loss: 6.9716 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 850/1000 | Train Loss: 7.1979 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 851/1000 | Train Loss: 7.0587 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 852/1000 | Train Loss: 7.2147 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 853/1000 | Train Loss: 7.2063 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 854/1000 | Train Loss: 6.9008 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 855/1000 | Train Loss: 7.0964 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 856/1000 | Train Loss: 7.3636 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 857/1000 | Train Loss: 7.2086 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 858/1000 | Train Loss: 6.8377 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 859/1000 | Train Loss: 6.9652 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 860/1000 | Train Loss: 7.3840 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 861/1000 | Train Loss: 7.0593 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 862/1000 | Train Loss: 7.3173 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 863/1000 | Train Loss: 7.0961 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 864/1000 | Train Loss: 7.5173 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 865/1000 | Train Loss: 7.2022 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 866/1000 | Train Loss: 7.1079 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 867/1000 | Train Loss: 6.9915 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 868/1000 | Train Loss: 7.2258 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 869/1000 | Train Loss: 6.9183 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 870/1000 | Train Loss: 7.2350 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 871/1000 | Train Loss: 7.2679 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 872/1000 | Train Loss: 7.0759 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 873/1000 | Train Loss: 6.7308 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 874/1000 | Train Loss: 7.2362 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 875/1000 | Train Loss: 6.9012 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 876/1000 | Train Loss: 7.5056 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 877/1000 | Train Loss: 6.8863 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 878/1000 | Train Loss: 6.8901 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 879/1000 | Train Loss: 7.3830 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 880/1000 | Train Loss: 7.1238 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 881/1000 | Train Loss: 6.9913 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 882/1000 | Train Loss: 7.0406 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 883/1000 | Train Loss: 7.0290 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 884/1000 | Train Loss: 7.3004 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 885/1000 | Train Loss: 6.7958 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 886/1000 | Train Loss: 7.1054 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 887/1000 | Train Loss: 7.0472 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 888/1000 | Train Loss: 7.1869 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 889/1000 | Train Loss: 7.1949 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 890/1000 | Train Loss: 7.1085 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 891/1000 | Train Loss: 6.9008 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 892/1000 | Train Loss: 7.0904 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 893/1000 | Train Loss: 7.0519 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 894/1000 | Train Loss: 6.8435 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 895/1000 | Train Loss: 7.2179 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 896/1000 | Train Loss: 7.4925 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 897/1000 | Train Loss: 6.7118 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 898/1000 | Train Loss: 7.0349 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 899/1000 | Train Loss: 6.5821 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 900/1000 | Train Loss: 7.2766 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 901/1000 | Train Loss: 6.9854 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 902/1000 | Train Loss: 7.1135 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 903/1000 | Train Loss: 7.2525 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 904/1000 | Train Loss: 6.8288 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 905/1000 | Train Loss: 7.2017 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 906/1000 | Train Loss: 7.0212 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 907/1000 | Train Loss: 7.0541 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 908/1000 | Train Loss: 6.8758 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 909/1000 | Train Loss: 7.0322 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 910/1000 | Train Loss: 7.2508 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 911/1000 | Train Loss: 6.9692 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 912/1000 | Train Loss: 7.1136 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 913/1000 | Train Loss: 6.7476 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 914/1000 | Train Loss: 7.0976 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 915/1000 | Train Loss: 7.1536 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 916/1000 | Train Loss: 7.1064 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 917/1000 | Train Loss: 6.8275 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 918/1000 | Train Loss: 7.1566 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 919/1000 | Train Loss: 7.1510 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 920/1000 | Train Loss: 6.8783 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 921/1000 | Train Loss: 7.3919 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 922/1000 | Train Loss: 6.6686 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 923/1000 | Train Loss: 7.0391 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 924/1000 | Train Loss: 7.1211 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 925/1000 | Train Loss: 7.2630 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 926/1000 | Train Loss: 7.1089 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 927/1000 | Train Loss: 6.8192 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 928/1000 | Train Loss: 6.9655 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 929/1000 | Train Loss: 6.9935 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 930/1000 | Train Loss: 6.8438 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 931/1000 | Train Loss: 6.8039 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 932/1000 | Train Loss: 7.3643 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 933/1000 | Train Loss: 6.9468 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 934/1000 | Train Loss: 6.8432 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 935/1000 | Train Loss: 7.1238 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 936/1000 | Train Loss: 7.0723 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 937/1000 | Train Loss: 6.8566 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 938/1000 | Train Loss: 7.2061 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 939/1000 | Train Loss: 6.8184 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 940/1000 | Train Loss: 6.7870 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 941/1000 | Train Loss: 7.1494 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 942/1000 | Train Loss: 6.8339 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 943/1000 | Train Loss: 7.2389 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 944/1000 | Train Loss: 7.2896 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 945/1000 | Train Loss: 7.3663 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 946/1000 | Train Loss: 7.0674 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 947/1000 | Train Loss: 7.3876 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 948/1000 | Train Loss: 6.9007 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 949/1000 | Train Loss: 7.0115 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 950/1000 | Train Loss: 6.8292 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 951/1000 | Train Loss: 7.4305 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 952/1000 | Train Loss: 7.0238 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 953/1000 | Train Loss: 6.8920 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 954/1000 | Train Loss: 7.0675 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 955/1000 | Train Loss: 6.8299 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 956/1000 | Train Loss: 7.2026 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 957/1000 | Train Loss: 7.2931 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 958/1000 | Train Loss: 6.9544 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 959/1000 | Train Loss: 7.1734 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 960/1000 | Train Loss: 6.8137 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 961/1000 | Train Loss: 7.0970 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 962/1000 | Train Loss: 7.0958 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 963/1000 | Train Loss: 7.0141 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 964/1000 | Train Loss: 7.5083 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 965/1000 | Train Loss: 6.8346 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 966/1000 | Train Loss: 6.8838 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 967/1000 | Train Loss: 7.2165 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 968/1000 | Train Loss: 6.8872 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 969/1000 | Train Loss: 7.3209 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 970/1000 | Train Loss: 7.0519 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 971/1000 | Train Loss: 7.2575 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 972/1000 | Train Loss: 7.1834 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 973/1000 | Train Loss: 7.1113 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 974/1000 | Train Loss: 6.7157 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 975/1000 | Train Loss: 7.1840 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 976/1000 | Train Loss: 7.1569 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 977/1000 | Train Loss: 7.1124 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 978/1000 | Train Loss: 6.9856 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 979/1000 | Train Loss: 7.0067 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 980/1000 | Train Loss: 6.8511 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 981/1000 | Train Loss: 6.9404 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 982/1000 | Train Loss: 6.8350 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 983/1000 | Train Loss: 7.2144 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 984/1000 | Train Loss: 6.8397 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 985/1000 | Train Loss: 7.4453 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 986/1000 | Train Loss: 7.2032 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 987/1000 | Train Loss: 7.2264 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 988/1000 | Train Loss: 7.2719 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 989/1000 | Train Loss: 7.1506 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 990/1000 | Train Loss: 7.5428 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 991/1000 | Train Loss: 7.1575 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 992/1000 | Train Loss: 6.9354 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 993/1000 | Train Loss: 7.2812 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 994/1000 | Train Loss: 7.0678 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 995/1000 | Train Loss: 6.9206 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 996/1000 | Train Loss: 7.1200 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 997/1000 | Train Loss: 7.3690 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 998/1000 | Train Loss: 6.5949 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 999/1000 | Train Loss: 7.0453 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\nEpoch 1000/1000 | Train Loss: 6.8435 | RUL L1: 0.0007 | Val Loss: 10.6251 | Val RMSE: 0.0459\n\n--- Running Final Test ---\nüèÜ Final Test RMSE for Window 100: 0.0376\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n\ndef load_base_model(path, input_size):\n    # Ensure this matches your generic PhysicsInformedLSTM class definition\n    model = PhysicsInformedLSTM(input_size=input_size) \n    model.load_state_dict(torch.load(path, map_location=device))\n    return model\n        \ndef test_and_visualize(model, test_loader, device):\n    model.eval()\n    \n    # Storage for analysis\n    all_true_rul = []\n    all_pred_rul = []\n    all_weights = [] # To store [w_low, w_mid, w_high]\n    all_soh = []     # To see at what SoH the model switches experts\n    \n    print(\"\\nüîç Starting Testing Inference...\")\n    \n    with torch.no_grad():\n        for X_batch, _, y_rul_batch in test_loader:\n            X_batch = X_batch.to(device)\n            y_rul_batch = y_rul_batch.to(device)\n            \n            # Forward pass\n            pred_rul, k, a, b, weights = model(X_batch)\n            \n            # Store results (Move to CPU for plotting)\n            all_true_rul.extend(y_rul_batch.cpu().numpy().flatten())\n            all_pred_rul.extend(pred_rul.cpu().numpy().flatten())\n            all_weights.extend(weights.cpu().numpy())\n            \n            # Store the current SoH (last step of input window)\n            # Assuming SoH is feature 0\n            current_soh = X_batch[:, -1, 0].cpu().numpy()\n            all_soh.extend(current_soh)\n\n    # Convert to numpy arrays\n    all_true_rul = np.array(all_true_rul)\n    all_pred_rul = np.array(all_pred_rul)\n    all_weights = np.array(all_weights) # Shape: [N_samples, 3]\n    all_soh = np.array(all_soh)\n\n    # --- 1. Calculate Metrics ---\n    rmse = np.sqrt(mean_squared_error(all_true_rul, all_pred_rul))\n    mae = mean_absolute_error(all_true_rul, all_pred_rul)\n    print(f\"\\nüèÜ Final Test RMSE: {rmse:.4f}\")\n    print(f\"üìâ Final Test MAE:  {mae:.4f}\")\n\n    # --- 2. Visualization: Prediction vs Actual ---\n    plt.figure(figsize=(10, 5))\n    plt.scatter(all_true_rul, all_pred_rul, alpha=0.5, color='blue', label='Predictions')\n    plt.plot([min(all_true_rul), max(all_true_rul)], [min(all_true_rul), max(all_true_rul)], 'r--', lw=2, label='Ideal')\n    plt.xlabel(\"True RUL\")\n    plt.ylabel(\"Predicted RUL\")\n    plt.title(f\"RUL Prediction Accuracy (RMSE: {rmse:.2f})\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    # --- 3. Visualization: Expert Selection (The \"Brain\" of the MoE) ---\n    # We sort by RUL to see how expert usage changes as battery degrades\n    sort_idx = np.argsort(all_true_rul)\n    sorted_rul = all_true_rul[sort_idx]\n    sorted_weights = all_weights[sort_idx]\n\n    plt.figure(figsize=(12, 6))\n    plt.stackplot(sorted_rul, \n                  sorted_weights[:, 0], # Low Expert\n                  sorted_weights[:, 1], # Mid Expert\n                  sorted_weights[:, 2], # High Expert\n                  labels=['Low Expert', 'Mid Expert', 'High Expert'],\n                  colors=['#ff9999', '#66b3ff', '#99ff99'], \n                  alpha=0.8)\n    \n    plt.xlabel(\"Remaining Useful Life (RUL)\")\n    plt.ylabel(\"Gate Weight (Confidence)\")\n    plt.title(\"Mixture of Experts: Which Model is Active?\")\n    plt.legend(loc='upper left')\n    plt.gca().invert_xaxis() # High RUL on left (Fresh), Low RUL on right (Dead)\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n    # --- 4. Deep Dive: Inspect 5 Random Samples ---\n    print(\"\\nüî¨ Sample Breakdown (Interpretability Check):\")\n    indices = np.random.choice(len(all_true_rul), 5, replace=True)\n    \n    print(f\"{'True RUL':<10} | {'Pred RUL':<10} | {'Low W':<8} | {'Mid W':<8} | {'High W':<8} | {'Decision'}\")\n    print(\"-\" * 75)\n    \n    for idx in indices:\n        t_rul = all_true_rul[idx]\n        p_rul = all_pred_rul[idx]\n        w = all_weights[idx]\n        \n        # Determine dominant expert\n        expert_names = ['Low', 'Mid', 'High']\n        dominant = expert_names[np.argmax(w)]\n        \n        print(f\"{t_rul:<10.2f} | {p_rul:<10.2f} | {w[0]:<8.2f} | {w[1]:<8.2f} | {w[2]:<8.2f} | {dominant}\")\n\n# --- How to Run It ---\n# 1. Load your best saved model structure\n# (Ensure expert_low/mid/high are loaded as done in training)\nWINDOW_SIZE =100\nexpert_low, expert_mid, expert_high = load_base_model('best_lstm_model-window-100_model_pinn_data_low.pth', INPUT_SIZE),load_base_model('best_lstm_model-window-100_model_pinn_data_mid.pth', INPUT_SIZE),load_base_model('best_lstm_model-window-100_model_pinn_data_high.pth', INPUT_SIZE)\nmodel = TrainableGompertzMoE(expert_low, expert_mid, expert_high, input_dim=1, window_size=WINDOW_SIZE).to(device)\nmodel.load_state_dict(torch.load(f\"best_moe_model_window_{WINDOW_SIZE}.pth\"))\n\n\n\n# test_loader  = give_paths_get_loaders(test_paths, WINDOW_SIZE, shuffle=False)\n# 2. Run the test\ntest_and_visualize(model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:22:16.996606Z","iopub.execute_input":"2026-02-03T21:22:16.997327Z","iopub.status.idle":"2026-02-03T21:22:17.321910Z","shell.execute_reply.started":"2026-02-03T21:22:16.997297Z","shell.execute_reply":"2026-02-03T21:22:17.321160Z"}},"outputs":[{"name":"stdout","text":"\nüîç Starting Testing Inference...\n\nüèÜ Final Test RMSE: 0.0376\nüìâ Final Test MAE:  0.0321\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA1oAAAHWCAYAAABuT/gUAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAe3FJREFUeJzt3Xl8DPcfx/HXJnJH3FcIcV9V1FV6aAnRkxYtVUJb2qqiQVEl7rtKtaWlpQdFW5QeSJW2VFFBqfuMo84iCLl2fn/MLxubBElssjnez8cjD9nvzM5+Zveb3X2b73zHYhiGgYiIiIiIiDiMi7MLEBERERERyW0UtERERERERBxMQUtERERERMTBFLREREREREQcTEFLRERERETEwRS0REREREREHExBS0RERERExMEUtERERERERBxMQUtERERERMTBFLRERHKJ4cOHY7FY7NoCAwPp2rWrwx6ja9euBAYGOmx7kvWOHTuGp6cn69evd3Yped758+fx8fHhxx9/dHYpIpIJFLREJEeZO3cuFovF9pMvXz5Kly5N165dOXHiRIr1AwMDefzxx1Pd1l9//YXFYmHu3Lm2tsSwcu7cuXTXdmNdLi4u+Pv707JlS9auXZvubTnTyZMnGT58ONu2bXN2KanavXs3FosFT09PLl686OxycpyRI0fSqFEj7rvvPltb165d7fqvh4cHVapUYdiwYVy/fj3FNhLXe+mll1J9jCFDhtjWSf63tHz5cpo2bUrx4sXx9vamQoUKPPPMM6xYscK2zpEjR+zqSf4zfvz4DO//7t27adWqFb6+vhQuXJjOnTtz9uzZNN9/2bJl3HPPPXh6elK2bFnCwsKIj4+/5X26d++OxWJJ8V5UpEgRXnrpJYYOHZqhfRGR7C2fswsQEcmIkSNHUr58ea5fv86ff/7J3LlzWbduHTt37sTT09NpdbVo0YIuXbpgGAaHDx/mww8/pFmzZvzwww888sgjWV7P3r17cXFJ3/+pnTx5khEjRhAYGEidOnXsls2aNQur1erACtPvyy+/pGTJkly4cIFvvvnmpl/2JaWzZ8/y2Wef8dlnn6VY5uHhwezZswG4dOkS3333HaNGjeLgwYPMmzcvxfqenp58++23fPjhh7i7u9st++qrr/D09EwR0iZPnsyAAQNo2rQpgwcPxtvbmwMHDvDzzz+zYMECWrVqZbd+x44defTRR1M8dt26ddO97wDHjx/nwQcfpECBAowdO5YrV64wefJkduzYwaZNm1LsR3I//fQTbdq04aGHHmL69Ons2LGD0aNHc+bMGWbMmJHqff766y/mzp170/elV155hffee49ffvmFZs2aZWi/RCSbMkREcpA5c+YYgLF582a79oEDBxqAsXDhQrv2cuXKGY899liq29q8ebMBGHPmzLG1hYWFGYBx9uzZdNcGGK+99ppd299//20ARsuWLW96v2vXrhkJCQnpfrzkEmu/U6k9L9mF1Wo1AgMDjdDQUOOpp54yHnroIWeXdFNXrlxxdgkpTJkyxfDy8jIuX75s1x4SEmL4+PjYtVmtVuPee+81LBaLcerUKbtlgNGmTRvDxcXFWLp0qd2y9evXG4DRtm1bu7+luLg4w8/Pz2jRokWqtZ0+fdr2++HDhw3AmDRpUob3NTWvvvqq4eXlZRw9etTWFh4ebgDGRx99dNv716hRw6hdu7YRFxdnaxsyZIhhsViM3bt3p1jfarUajRs3Nl544YVbvhfdddddRufOnTOwRyKSnWnooIjkCg888AAABw8edHIl9mrVqkXRokU5fPgwAGvXrsVisbBgwQLefvttSpcujbe3N1FRUQBs3LiRVq1aUaBAAby9vWnatGmq59KsW7eOBg0a4OnpScWKFfnoo49SffzUztG6ePEib7zxBoGBgXh4eFCmTBm6dOnCuXPnWLt2LQ0aNACgW7dutqFaicMrUztH6+rVq/Tr14+AgAA8PDyoWrUqkydPxjAMu/UsFgu9evVi6dKl3HXXXXh4eFCzZk27IWO3s379eo4cOUKHDh3o0KEDv/32G8ePH0+xntVqZdq0adSqVQtPT0+KFStGq1at+Ouvv+zW+/LLL2nYsCHe3t4UKlSIBx98kFWrVtnVPHz48BTbT/68Jg5p/fXXX+nZsyfFixenTJkyABw9epSePXtStWpVvLy8KFKkCO3bt+fIkSMptnur1+bKlSv4+PjQp0+fFPc7fvw4rq6ujBs37pbP39KlS2nUqBG+vr63XC9x3++//34Mw+DQoUMplpcuXZoHH3yQ+fPn27XPmzePWrVqcdddd9m1nzt3jqioKLshizcqXrz4bWtKzaVLl9izZw+XLl267brffvstjz/+OGXLlrW1BQUFUaVKFRYtWnTL++7atYtdu3bRo0cP8uVLGhDUs2dPDMPgm2++SXGfL774gp07dzJmzJhbbrtFixYsX748xd+MiORsCloikiskfmktVKiQcwtJ5sKFC1y4cIEiRYrYtY8aNYoffviB/v37M3bsWNzd3fnll1948MEHiYqKIiwsjLFjx3Lx4kWaNWvGpk2bbPfdsWMHLVu25MyZMwwfPpxu3boRFhbGkiVLblvPlStXeOCBB5g+fTotW7Zk2rRpvPLKK+zZs4fjx49TvXp1Ro4cCUCPHj344osv+OKLL3jwwQdT3Z5hGDz55JO8++67tGrViilTplC1alUGDBhAaGhoivXXrVtHz5496dChAxMnTuT69eu0bduW8+fPp+n5nDdvHhUrVqRBgwY88cQTeHt789VXX6VY78UXX6Rv374EBAQwYcIEBg0ahKenJ3/++adtnREjRtC5c2fc3NwYOXIkI0aMICAggF9++SVNtaSmZ8+e7Nq1i2HDhjFo0CAANm/ezB9//EGHDh147733eOWVV1i9ejUPPfQQ0dHRtvve7rXx9fXlqaeeYuHChSQkJNg97ldffYVhGHTq1OmmtcXFxbF582buueeeNO/P7f6unnvuOZYvX86VK1cAiI+P5+uvv+a5555LsW7x4sXx8vJi+fLl/Pfff2l6/OjoaM6dO5fi58ZzopYsWUL16tVv2/9PnDjBmTNnqF+/foplDRs2ZOvWrbe8f+Ly5Pf39/enTJkyKe5/+fJlBg4cyFtvvUXJkiVvue169epx8eJF/vnnn1uuJyI5jDMPp4mIpFfi0MGff/7ZOHv2rHHs2DHjm2++MYoVK2Z4eHgYx44ds1s/q4cOvvjii8bZs2eNM2fOGBs3bjSaN29uAMY777xjGIZhrFmzxgCMChUqGNHR0bb7Wq1Wo3LlykZwcLBhtVpt7dHR0Ub58uXthlu1adPG8PT0tBv+tGvXLsPV1TXF0MFy5coZISEhttvDhg0zAGPx4sUp6k983FsNHQwJCTHKlStnu7106VIDMEaPHm23Xrt27QyLxWIcOHDA7vlxd3e3a9u+fbsBGNOnT0/xWMnFxsYaRYoUMYYMGWJre+6554zatWvbrffLL78YgNG7d++b7uP+/fsNFxcX46mnnkoxbPPG5x8wwsLCUmwn+fOa2C/vv/9+Iz4+3m7dG1/nRBs2bDAA4/PPP7e1peW1WblypQEYP/30k93yu+++22jatGmK+93owIEDN32uE4cOnj171jh79qxx4MABY/LkyYbFYjHuuusuu+fEMJKGyf7333+Gu7u78cUXXxiGYRg//PCDYbFYjCNHjqT6t5S4jz4+PsYjjzxijBkzxtiyZUuKehKHDt7sZ8OGDbZ1E5/72w11TezXNz7niQYMGGAAxvXr1296/0mTJhmAERkZmWJZgwYNjHvvvdeurX///kb58uVt27zVe9Eff/yR6tBnEcnZdERLRHKkoKAgihUrRkBAAO3atcPHx4dly5bZhms5yyeffEKxYsUoXrw4jRo1Yv369YSGhtK3b1+79UJCQvDy8rLd3rZtG/v37+e5557j/Pnztv+5v3r1Ks2bN+e3337DarWSkJDAypUradOmjd3wp+rVqxMcHHzb+r799ltq167NU089lWJZ8qnh0+LHH3/E1dWV3r1727X369cPwzD46aef7NqDgoKoWLGi7fbdd9+Nn59fqkPTkvvpp584f/48HTt2tLV17NiR7du32x0J+Pbbb7FYLISFhaXYRuI+Ll26FKvVyrBhw1JMFpKR5yFR9+7dcXV1tWu78XWOi4vj/PnzVKpUiYIFCxIREWFX9+1em6CgIPz9/e0mp9i5cyd///03zz///C1rSzxqeLOjU1evXqVYsWIUK1aMSpUq0b9/f+677z6+++67mz4nhQoVolWrVrajivPnz6dJkyaUK1cu1fVHjBjB/PnzqVu3LitXrmTIkCHUq1ePe+65h927d6dYv0ePHoSHh6f4qVGjhm2drl27YhjGbS9jcO3aNcCc9CO5xIkqEtfJyP1vvO++ffuYNm0akyZNSnX95BJfk4zMdioi2ZdmHRSRHOmDDz6gSpUqXLp0iU8//ZTffvstTV9oUnMnX6yTa926Nb169cJisZA/f35q1qyJj49PivXKly9vd3v//v2AGcBu5tKlS8TExHDt2jUqV66cYnnVqlVvez2egwcP0rZt27TsSpocPXoUf39/8ufPb9devXp12/Ib3RgOExUqVIgLFy7c9rG+/PJLypcvj4eHBwcOHACgYsWKeHt7M2/ePMaOHQuY++jv70/hwoVvuq2DBw/i4uJi94XdEZK/rmB+QR83bhxz5szhxIkTdufh3HheUVpeGxcXFzp16sSMGTOIjo627bunpyft27dPU43GTc4D8vT0ZPny5YB5ztfEiRM5c+aMXVBMzXPPPUfnzp2JjIxk6dKlTJw48Zbrd+zYkY4dOxIVFcXGjRuZO3cu8+fP54knnkgxa2jlypUJCgpK037dTuJ+xMTEpFiWODvirfb1dve/8b59+vShSZMmaf5bS3xNHPleJCLOp6AlIjlSw4YNbedKtGnThvvvv5/nnnuOvXv32p3on/x/mm+UeH6MI6eDL1OmTJq+GCb/Qpc4ZfqkSZNSTKmeyNfXN9UveTlJ8qM9iW725T9RVFQUy5cv5/r166mGzPnz5zNmzJgs+6Ka/BypRKl9UX/99deZM2cOffv2pXHjxhQoUACLxUKHDh0yNFV+ly5dmDRpEkuXLqVjx47Mnz+fxx9/nAIFCtzyfonnCd4s1Lq6utr13eDgYKpVq8bLL7/MsmXLbrrdJ598Eg8PD0JCQoiJieGZZ55J0374+fnRokULWrRogZubG5999hkbN26kadOmabp/epUqVQqAf//9N8Wyf//9l8KFC9/yP2tuvH9AQECK+zds2BCAX375hRUrVrB48WK7CU/i4+O5du0aR44coXDhwvj5+dmWJb4mRYsWzdjOiUi2pKAlIjle4mxrDz/8MO+//75tEgKAcuXKsWvXrlTvt3fvXts6zpY4nM7Pz++WQa1YsWJ4eXnZjoDdKHF/bvc4O3fuvOU66Qkr5cqV4+eff+by5ct2R7X27NljW+4Iixcv5vr168yYMSPFl9G9e/fy9ttvs379eu6//34qVqzIypUr+e+//256VKtixYpYrVZ27dp102AL5tG25BdFjo2NTfXL+s188803hISE8M4779jarl+/nmK7aXltAO666y7q1q3LvHnzKFOmDJGRkUyfPv229ytbtixeXl62GTBvp1SpUrzxxhuMGDGCP//8k3vvvTfV9by8vGjTpg1ffvkljzzySIbCQv369fnss8/S9bymV+nSpSlWrFiKmScBNm3adMt+ANiW//XXX7ZQBeZ1544fP06PHj0AiIyMBODpp59OsY0TJ05Qvnx53n33XbvhxImvSeKRYBHJHXSOlojkCg899BANGzZk6tSpdhdJffTRRzl+/DhLly61Wz8mJobZs2dTvHjxdM3Cllnq1atHxYoVmTx5sm0GtxudPXsWMENlcHAwS5cutX2hA9i9ezcrV6687eO0bduW7du3pzpDW+JRpcShjsmDQGoeffRREhISeP/99+3a3333XSwWi8Mu0vzll19SoUIFXnnlFdq1a2f3079/f3x9fW3nLbVt2xbDMBgxYkSK7STuY5s2bXBxcWHkyJEpjirdeHStYsWK/Pbbb3bLP/7445se0UqNq6triiN206dPT7GNtLw2iTp37syqVauYOnUqRYoUSdPz7ObmRv369VMNGjfz+uuv4+3tzfjx42+5Xv/+/QkLC2Po0KE3XSc6OpoNGzakuizxXL6qVaumubZE6ZnevW3btnz//fccO3bM1rZ69Wr27dtnN/QyLi6OPXv22AW/mjVrUq1atRSv/4wZM7BYLLRr1w6AZs2asWTJkhQ/xYoVo379+ixZsoQnnnjCrq4tW7ZQoEABatasme79F5HsS0e0RCTXGDBgAO3bt2fu3Lm88sorgHky/aeffkr79u154YUXqFu3LufPn2fhwoXs3LmTzz//HHd39xTbmjJlCt7e3nZtLi4uvPXWW5lSu4uLC7Nnz+aRRx6hZs2adOvWjdKlS3PixAnWrFmDn5+f7fyZESNGsGLFCh544AF69uxJfHw806dPp2bNmvz999+3fJwBAwbwzTff2J6PevXq8d9//7Fs2TJmzpxJ7dq1qVixIgULFmTmzJnkz58fHx8fGjVqlOr5R0888QQPP/wwQ4YM4ciRI9SuXZtVq1bx3Xff0bdvX7uJLzLq5MmTrFmzJsWEG4k8PDwIDg7m66+/5r333uPhhx+mc+fOvPfee+zfv59WrVphtVr5/fffefjhh+nVqxeVKlViyJAhjBo1igceeICnn34aDw8PNm/ejL+/v+16VC+99BKvvPIKbdu2pUWLFmzfvp2VK1em66jN448/zhdffEGBAgWoUaMGGzZs4Oeff04x5X9aXptEzz33HG+++SZLlizh1Vdfxc3NLU21tG7dmiFDhhAVFWU3dO1mihQpQrdu3fjwww/ZvXv3TY+41K5d266+1ERHR9OkSRPuvfdeWrVqRUBAABcvXmTp0qX8/vvvtGnThrp169rdJyIigi+//DLFtipWrEjjxo0Bc3r3bt26MWfOnNtOiPHWW2/x9ddf8/DDD9OnTx+uXLnCpEmTqFWrFt26dbOtd+LECapXr05ISIjtGnJgDu198sknadmyJR06dGDnzp28//77vPTSS7bnpmzZsqmei9i3b19KlChBmzZtUiwLDw/niSee0DlaIrmNcyY7FBHJmMSpnDdv3pxiWUJCglGxYkWjYsWKdlNsX7hwwXjjjTeM8uXLG25uboafn5/x8MMPp5gi2zCSpndP7cfV1fWWtfH/Ka9vJXF696+//jrV5Vu3bjWefvppo0iRIoaHh4dRrlw545lnnjFWr15tt96vv/5q1KtXz3B3dzcqVKhgzJw501b7jZJPQ24YhnH+/HmjV69eRunSpQ13d3ejTJkyRkhIiHHu3DnbOt99951Ro0YNI1++fHZTZyef3t0wDOPy5cvGG2+8Yfj7+xtubm5G5cqVjUmTJt10SvDkUqvxRu+8844BpHgObjR37lwDML777jvDMAwjPj7emDRpklGtWjXD3d3dKFasmPHII4+kmEr8008/NerWrWt4eHgYhQoVMpo2bWqEh4fblickJBgDBw40ihYtanh7exvBwcHGgQMHbjq9e2r98sKFC0a3bt2MokWLGr6+vkZwcLCxZ8+eDL82iR599FEDMP7444+bPi/JnT592siXL59tOvZEidO7p+bgwYOGq6urXa1p6evJp3ePi4szZs2aZbRp08YoV66c4eHhYXh7ext169Y1Jk2aZMTExNjue7vp3VN77m83vXuinTt3Gi1btjS8vb2NggULGp06dTJOnTplt07i46fWL5csWWLUqVPH8PDwMMqUKWO8/fbbRmxs7G0f92bTu+/evdt2yQoRyV0shqHLkIuIiOQ0Tz31FDt27LDNwJhWL774Ivv27eP333/PpMokPfr27ctvv/3Gli1bdERLJJfROVoiIiI5zL///ssPP/xA586d033fsLAwNm/ezPr16zOhMkmP8+fPM3v2bEaPHq2QJZIL6YiWiIhIDnH48GHWr1/P7Nmz2bx5MwcPHqRkyZLOLktERFKhI1oiIiI5xK+//krnzp05fPgwn332mUKWiEg2piNaIiIiIiIiDqYjWiIiIiIiIg6moCUiIiIiIuJgumBxKqxWKydPniR//vyaBUhEREREJA8zDIPLly/j7++Pi0vaj1MpaKXi5MmTBAQEOLsMERERERHJJo4dO0aZMmXSvL6CViry588PmE+mn5+fk6vJm+Li4li1ahUtW7bEzc3N2eVINqA+IcmpT0hq1C8kOfUJSS69fSIqKoqAgABbRkirbBG0PvjgAyZNmsSpU6eoXbs206dPp2HDhqmuO2vWLD7//HN27twJQL169Rg7duxN13/llVf46KOPePfdd+nbt2+a6kkcLujn56eg5SRxcXF4e3vj5+enN0UB1CckJfUJSY36hSSnPiHJZbRPpPeUIqdPhrFw4UJCQ0MJCwsjIiKC2rVrExwczJkzZ1Jdf+3atXTs2JE1a9awYcMGAgICaNmyJSdOnEix7pIlS/jzzz/x9/fP7N0QERERERGxcXrQmjJlCt27d6dbt27UqFGDmTNn4u3tzaeffprq+vPmzaNnz57UqVOHatWqMXv2bKxWK6tXr7Zb78SJE7z++uvMmzdP/3shIiIiIiJZyqlDB2NjY9myZQuDBw+2tbm4uBAUFMSGDRvStI3o6Gji4uIoXLiwrc1qtdK5c2cGDBhAzZo1b7uNmJgYYmJibLejoqIA87BiXFxcWndHHCjxedfzL4nUJyQ59QlJjfqFJKc+Icmlt09ktO84NWidO3eOhIQESpQoYddeokQJ9uzZk6ZtDBw4EH9/f4KCgmxtEyZMIF++fPTu3TtN2xg3bhwjRoxI0b5q1Sq8vb1vej8XF5d0TfEo6ZMvXz7WrFnj7DIASEhIwDAMZ5chQHh4uLNLkGxGfUJSo34hyalPSHJp7RPR0dEZ2n62mAwjo8aPH8+CBQtYu3Ytnp6eAGzZsoVp06YRERGR5hPWBg8eTGhoqO124swiLVu2THUyjLi4OE6fPs21a9ccsyOSgmEYXL9+HU9Pz2xxLTOLxUKpUqXw8fFxdil5VlxcHOHh4bRo0ULDgQVQn5DUqV9IcuoTklx6+0TiaLf0cmrQKlq0KK6urpw+fdqu/fTp05QsWfKW9508eTLjx4/n559/5u6777a1//7775w5c4ayZcva2hISEujXrx9Tp07lyJEjKbbl4eGBh4dHinY3N7cUT77VauXQoUO4urpSunRp3N3ds0UQyG2sVitXrlzB19fX6UcNDcPg7NmznDp1isqVK+Pq6urUevK61P4uJW9Tn5DUqF9IcuoTklxa+0RG+41Tg5a7uzv16tVj9erVtGnTBsA2sUWvXr1uer+JEycyZswYVq5cSf369e2Wde7c2W4YIUBwcDCdO3emW7dud1xzbGwsVquVgICAWw4rlDtjtVqJjY3F09PT6UELoFixYhw5coS4uDgFLRERERG5LacPHQwNDSUkJIT69evTsGFDpk6dytWrV22hqEuXLpQuXZpx48YB5vlXw4YNY/78+QQGBnLq1CkAfH198fX1pUiRIhQpUsTuMdzc3ChZsiRVq1Z1WN3Z4cu/ZB0dtRQRERGR9HB60Hr22Wc5e/Ysw4YN49SpU9SpU4cVK1bYJsiIjIy0CzUzZswgNjaWdu3a2W0nLCyM4cOHZ2XpIiIiIiIiqXJ60ALo1avXTYcKrl271u52audY3U5G7iMiIiIiIpJRGv8mDte1a1fbOXcADz30EH379r2jbTpiGyIiIiIiWUVBKw/p2rUrFosFi8WCu7s7lSpVYuTIkcTHx2fq4y5evJhRo0alad21a9disVi4ePFihrchIiIiIuJs2WLoYF5ltUJkJFy+DPnzQ9mykNlzbLRq1Yo5c+YQExPDjz/+yGuvvYabmxuDBw+2Wy82NhZ3d3eHPGbhwoWzxTZEREREJIcwDMjhk5HpiJaT7N4N48fDsGEwapT57/jxZntm8vDwoGTJkpQrV45XX32VoKAgli1bZhvuN2bMGPz9/W0zNB47doxnnnmGggULUrhwYVq3bm13zltCQgKhoaEULFiQIkWK8Oabb2IYht1jJh/2FxMTw8CBAwkICMDDw4NKlSrxySefcOTIER5++GEAihQpQqFChWyzTybfxoULF+jSpQuFChXC29ubRx55hP3799uWz507l4IFC7Jy5UqqV6+Or68vrVq14t9//7Wts3btWho2bIiPjw8FCxbkvvvu4+jRo456qkVEREQkvfbvh65doX9/Z1dyxxS0nGD3bnjvPdi6FYoWhapVzX+3bjXbMzts3cjLy4vY2FgAVq9ezd69ewkPD+f7778nLi6O4OBg8ufPz++//8769ettgSXxPu+88w5z587l008/Zd26dfz3338sWbLklo/ZpUsXvvrqK9577z12797NRx99hK+vLwEBAXz77bcA7N69mz179jB16tRUt9G1a1f++usvli1bxoYNGzAMg0cffZS4uDjbOtHR0UyePJkvvviC3377jcjISPr//482Pj6eNm3a0LRpU/7++282bNhAjx49NI27iIiIiDPs2wddukC1avDZZ/Dhh3DDf5DnRBo6mMWsVliyBM6dgxo1ko6I+vmZt3ftgqVLzfCVmcMIDcNg9erVrFy5ktdff52zZ8/i4+PD7NmzbUMGv/zyS6xWK7Nnz7YFkDlz5lCwYEHWrl1Ly5YtmTp1KoMHD+bpp58GYObMmaxcufKmj7tv3z4WLVpEeHi47cLSFSpUsC1PHCJYvHhxXFxc8PPzS7GN/fv3s2zZMtavX0+TJk0AmDdvHgEBASxdupT27dsDEBcXx8yZM6lYsSJgzm45cuRIAKKiorh06RKPP/64bXn16tUz+GyKiIiISIZERsKQITB/vvlFOZGXF+zYAaVKOa+2O6QjWlksMhL27IGAgJTDTi0WKFPGPKIVGZk5j//999/j6+uLp6cnjzzyCM8++6zt+mO1atWyOy9r+/btHDhwgPz589suCF24cGGuX7/OwYMHuXTpEv/++y+NGjWy3SdfvnzUr1//po+/bds2XF1dadq0aYb3Yffu3eTLl8/ucYsUKULVqlXZfcPhQG9vb1uIAihVqhRnzpwBzEDXtWtXgoODeeKJJ5g2bZrdsEIRERERyQIJCbBgQVLIKlwYRo+GI0egZUunlnandEQri12+DNevg49P6st9fODECXO9zPDwww8zY8YM3N3d8ff3J1++pC7gk6yoK1euUK9ePebNm5diO8WKFcvQ43t5eWXofhnh5uZmd9tisdidPzZnzhx69+7NihUrWLhwIW+//Tbh4eHce++9WVajiIiISJ5y5Qr4+ibdLl8eQkLMIV39+kGvXuYscbmAjmhlsfz5wdMTrl5NffnVq+byzOpfPj4+VKpUibJly9qFrNTcc8897N+/n+LFi1OpUiW7nwIFClCgQAFKlSrFxo0bbfeJj49ny5YtN91mrVq1sFqt/Prrr6kuTzyilpCQcNNtVK9enfj4eLvHPX/+PHv37qVGjRq33Kfk6taty+DBg/njjz+46667mD9/frruLyIiIiJp8M8/0KEDVK9uHnW40bhxcPgwDB6ca0IWKGhlubJlzXP8jh0zZ628kWHA8eNm/ytb1jn13ahTp04ULVqU1q1b8/vvv3P48GHWrl1L7969OX78OAB9+vRh/PjxLF26lD179tCzZ88U18C6UWBgICEhIbzwwgssXbrUts1FixYBUK5cOSwWC99//z3nzp3jypUrKbZRuXJlWrduTffu3Vm3bh3bt2/n+eefp3Tp0rRu3TpN+3b48GEGDx7Mhg0bOHr0KKtWrWL//v06T0tERETEkXbuhGeegVq1YOFC88vu7Nn26xQrlqsCViIFrSzm4gJPPWXOMrhrF1y6BPHx5r+7dpntbdpk/vW00sLb25vffvuNsmXL8vTTT1O9enVefPFFrl+/bpukol+/fnTu3JmQkBAaN25M/vz5eeqpp2653RkzZtCuXTt69uxJtWrV6N69O1f/f4ivdOnSjBgxgrfeeosqVarw+uuvp7qNOXPmUK9ePR5//HEaN26MYRj8+OOPKYYL3mrf9uzZQ9u2balSpQo9evTgtdde4+WXX07HMyQiIiIiqdqxA9q3NwPW118nHWEoVswcvpUHWIzkFz0SoqKiKFCgAJcuXUox693169c5fPgw5cuXx/MOOsnu3ebsg3v2mEdPPT3NI1lt2pj/5nVWq5WoqCj8/PxwyQap01Gvu2RcXFwcP/74I48++miaA7XkbuoTkhr1C0lOfSKLbd8OI0fC4sX27cWLw5tvwiuv3HyygiyS3j5xq2xwK5oMw0mqVzencI+MNCe+yJ/fHC6YDTKFiIiIiEj6ffIJvPSSfVuJEjBwILz8Mnh7O6cuJ1HQciIXFwgMdHYVIiIiIiIO8Mgj4OEBMTFQsqQZsHr0yHMBK5GCloiIiIiIpM+WLXD0KDz9dFKbv7958eECBaB7d/Oiw3mYgpaIiIiIiKTN5s0wYgT88AMUKWJeVPjG62INHeq82rIZnREkIiIiIiK3tmkTPPYYNGxohiyA8+fN87IkVQpaIiIiIiKSuj//NM+9atQIfvwxqT0gAD780JxFUFKloYMiIiIiImLvzz9h+HBYudK+vWxZeOst6NrVnPhCbkpBS0RERERE7H33nX3ICgw0A1ZICLi7O62snERDB0VERERE8rqEBPvb/fqZFxYODITZs2HfPnMmQYWsNFPQEpuHHnqIvn37ZvttioiIiIiD/PorNGtmziR4o6JFYe1aM2C9+CK4uTmlvJxMQSsP6dq1K23atHF2GSIiIiLibGvXwsMPw0MPwZo1MG0aXLhgv079+gpYd0BBS0REREQkLzAM+OUXaNrUDFlr1yYtK14cjhxxVmW5koJWHnX16lW6dOmCr68vpUqV4p133kmxTkxMDP3796d06dL4+PjQqFEj1t7wB3n+/Hk6duxI6dKl8fb2platWnz11VdZuBciIiIicluGAT//DA8+CM2bw2+/JS2rXBk+/xx274a6dZ1XYy6kWQcdacoU8+d27rkHli2zb3vySYiIuP19Q0PNnzs0YMAAfv31V7777juKFy/OW2+9RUREBHXq1LGt06tXL3bt2sWCBQvw9/dnyZIltGrVih07dlC5cmWuX79OvXr1GDhwIH5+fvzwww907tyZihUr0rBhwzuuUUREREQcoHVrWL7cvq1qVXj7bejQAfIpEmQGPauOFBUFJ07cfr2AgJRtZ8+m7b5RUemvK5krV67wySef8OWXX9K8eXMAPvvsM8qUKWNbJzIykjlz5hAZGYm/vz8A/fv3Z8WKFcyZM4exY8dSunRp+vfvb7vP66+/zsqVK1m0aJGCloiIiEh20aBBUtCqVg2GDoVnnwVXV+fWlcspaDmSnx+ULn379YoVS70tLff180t/XckcPHiQ2NhYGjVqZGsrXLgwVatWtd3esWMHCQkJVKlSxe6+MTExFClSBICEhATGjh3LokWLOHHiBLGxscTExODt7X3HNYqIiIhIOhmGee2runWhRImk9t69YcUKeP11aN9eASuLKGg50p0M60s+lNDJrly5gqurK1u2bME12R+jr68vAJMmTWLatGlMnTqVWrVq4ePjQ9++fYmNjXVGySIiIiJ5k2HATz/ByJGwcSP07w+TJiUtL1AA1q93Xn15lCbDyIMqVqyIm5sbGzdutLVduHCBffv22W7XrVuXhIQEzpw5Q6VKlex+SpYsCcD69etp3bo1zz//PLVr16ZChQp22xARERGRTGQY8MMP0KgRPPaYGbIAPvjAPC1FnEpBKw/y9fXlxRdfZMCAAfzyyy/s3LmTrl274uKS1B2qVKlCp06d6NKlC4sXL+bw4cNs2rSJcePG8cMPPwBQuXJlwsPD+eOPP9i9ezcvv/wyp0+fdtZuiYiIiOQNhmGec9WwITz+OGzenLTs7rvhiy/g/6d6iPNo6GAeNWnSJK5cucITTzxB/vz56devH5cuXbJbZ86cOYwePZp+/fpx4sQJihYtyr333svjjz8OwNtvv82hQ4cIDg7G29ubHj160KZNmxTbEREREREHMAzzdJORI1POVl27NoSFmTMMuuhYSnagoJWHzJ071/a7r68vX3zxBV988YWtbcCAAXbru7m5MWLECEaMGJHq9goXLszSpUtv+Zg3XndLRERERO7AtWvw8stw4wiiunVh2DDzUkEKWNmKXg0RERERkZzA2xsS/2P8nnvgu+9gyxZo00YhKxvSKyIiIiIikp1YrfDNN+b1ryIj7Ze98op5ftZff5lHsSwW59Qot6WgJSIiIiKSHVitsGiReb5V+/ZmmJowwX4dHx9zAgwFrGxP52iJiIiIiDhTQoJ5BGvkSNi1y37Zzp1mANPQwBxHr1gGGYbh7BIkC+n1FhEREYdLSICvvoJataBDB/uQde+9sGIFrF2rkJVD6YhWOrm5uQEQHR2Nl5eXk6uRrBIbGwuAq6urkysRERGRXGH/fvMcqz177NubNDGnaW/RQsMDczgFrXRydXWlYMGCnDlzBgBvb28s+iNwOKvVSmxsLNevX7e7kLKzajl79ize3t7ky6c/GREREXGAsmXhypWk2/fdB8OHQ/PmCli5hL41ZkDJkiUBbGFLHM8wDK5du4aXl1e2CLIuLi6ULVs2W9QiIiIiOUx8PPz5J9x/f1Kbhwe89ZY5dDAsDJo1U8DKZRS0MsBisVCqVCmKFy9OXFycs8vJleLi4vjtt9948MEHbcM1ncnd3d3pR9ZEREQkh4mPhy+/hDFj4PBhc5hgpUpJy19+2ZyuXQErV1LQugOurq46ZyeTuLq6Eh8fj6enZ7YIWiIiIiJpFhdnBqzRo+HQoaT2MWNgzpyk2/pP3FxNQUtERERExBHi4uDzz5OOYN2oWTPo1s05dYlTKGiJiIiIiNyJ2NikgHXkiP2yoCDzHKwbz8+SPEFBS0RERETkTixeDN2727e1aGEGrPvuc05N4nQaGCoiIiIicifatUua5CI4GP74A1atUsjK43RES0REREQkLWJi4JNPYO9emDYtqT1fPpgxA3x94d57nVefZCsKWiIiIiIit3L9uhmwxo2DEyfMtu7d4a67ktYJCnJObZJtaeigiIiIiEhqrl+H6dOhYkXo1SspZAH8+KPz6pIcQUe0RERERERudO0afPwxTJgA//5rv+yJJ2DYMKhf3zm1SY6hoCUiIiIikmjBAnjjDTh1yr69dWszYN1zj3PqkhxHQUtEREREJFG+fPYh66mnzIBVp47TSpKcSUFLRERERPKmq1fhwgUoUyap7emnoVYtqFIFhg6F2rWdV5/kaApaIiIiIpK3XL0KH34IkyZBgwbwww9Jy1xc4M8/wdvbefVJrpAtZh384IMPCAwMxNPTk0aNGrFp06abrjtr1iweeOABChUqRKFChQgKCrJbPy4ujoEDB1KrVi18fHzw9/enS5cunDx5Mit2RURERESyqytXzAkuAgPhzTfh7Flz9sDNm+3XU8gSB3B60Fq4cCGhoaGEhYURERFB7dq1CQ4O5syZM6muv3btWjp27MiaNWvYsGEDAQEBtGzZkhP/n24zOjqaiIgIhg4dSkREBIsXL2bv3r08+eSTWblbIiIiIpJN5Lt2DZfEgDVoEJw7Zy6wWODZZ6FQIafWJ7mT04cOTpkyhe7du9OtWzcAZs6cyQ8//MCnn37KoEGDUqw/b948u9uzZ8/m22+/ZfXq1XTp0oUCBQoQHh5ut877779Pw4YNiYyMpGzZspm3MyIiIiKSfURF4TJ1Ki0mT8b18uWkdosFOnSAt9+GGjWcV5/kak4NWrGxsWzZsoXBgwfb2lxcXAgKCmLDhg1p2kZ0dDRxcXEULlz4putcunQJi8VCwYIFU10eExNDTEyM7XZUVBRgDkOMi4tLUx3iWInPu55/SaQ+IcmpT0hq1C/kRq7t2+O6ahWu/79tuLhgPPssCYMHQ7VqZqP6Sp6T3veJjL6fODVonTt3joSEBEqUKGHXXqJECfbs2ZOmbQwcOBB/f3+CgoJSXX79+nUGDhxIx44d8fPzS3WdcePGMWLEiBTtq1atwltjdJ0q+dFJEfUJSU59QlKjfiEAxRs2pPGqVRguLhx/4AH2PfMMV0qXhkOHzB/J09L6PhEdHZ2h7Tt96OCdGD9+PAsWLGDt2rV4enqmWB4XF8czzzyDYRjMmDHjptsZPHgwoaGhtttRUVG2c79uFs4kc8XFxREeHk6LFi1wc3NzdjmSDahPSHLqE5Ia9Ys86uJFXN57D6NZM4z7709qf+QR4hIS+LVMGZp07cqD6hNC+t8nEke7pZdTg1bRokVxdXXl9OnTdu2nT5+mZMmSt7zv5MmTGT9+PD///DN33313iuWJIevo0aP88ssvtwxMHh4eeHh4pGh3c3PTm7ST6TWQ5NQnJDn1CUmN+kUeceECTJ0K06bBpUuwbh2sWWO3StyIEVz98Uf1CUkhrX0io/3GqbMOuru7U69ePVavXm1rs1qtrF69msaNG9/0fhMnTmTUqFGsWLGC+vXrp1ieGLL279/Pzz//TJEiRTKlfhERERFxgv/+My8mHBgII0eaIQvg999h/36nliaSyOlDB0NDQwkJCaF+/fo0bNiQqVOncvXqVdsshF26dKF06dKMGzcOgAkTJjBs2DDmz59PYGAgp06dAsDX1xdfX1/i4uJo164dERERfP/99yQkJNjWKVy4MO7u7s7ZURERERG5M+fPw7vvwnvvwY2zCObLByEh8NZbUKGC8+oTuYHTg9azzz7L2bNnGTZsGKdOnaJOnTqsWLHCNkFGZGQkLi5JB95mzJhBbGws7dq1s9tOWFgYw4cP58SJEyxbtgyAOnXq2K2zZs0aHnrooUzdHxERERFxsPh4CAszA9aVK0nt+fJBt25mwAoMdFp5IqlxetAC6NWrF7169Up12dq1a+1uHzly5JbbCgwMxDAMB1UmIiIiIk6XLx+sX58Ustzc4IUXYPBgKFfOubWJ3IRTz9ESEREREUnh/HlI/h/nYWFmwHrlFThwAGbOVMiSbE1BS0RERESyhzNnYMAAKFsWVqywX/bQQxAZCTNmmMtFsjkFLRERERFxrtOnoX9/KF8eJk+G6GgYPtz+qJbFAre5/I9IdpItztESERERkTzo1CmYONEcBnjtWlK7hwc0agQxMeDp6bz6RO6AgpaIiIiIZK1//00KWNevJ7V7ekKPHjBwIPj7O68+EQdQ0BIRERGRrHPiBFSqlDJgvfIKvPkmlCrlvNpEHEjnaImIiIhI1ildGh5+2PzdywtCQ+HwYfNCxApZkovoiJaIiIiIZI7jx2HOHBgyBFxu+P/94cOhRg1zhsESJZxWnkhmUtASEREREcc6dgzGjYNPPoHYWDNUtW2btLxhQ/NHJBfT0EERERERcYzISHj1VahY0bzeVWys2T5linPrEnECHdESERERkTtz5Ih5BGvOHIiLS2r39YXXXzfPwxLJYxS0RERERCRjjh+HESNg7lyIj09qz58/KWAVKeK08kScSUFLRERERDLmwgWYPTvptp8f9O4Nb7wBhQs7ry6RbEDnaImIiIhI2sTE2N+uVQvatTMD1rBh5hDCUaMUskTQES0RERERuZ0DB2D0aNi0Cf7+G/Ld8BXy3XfBxwcKFXJefSLZkI5oiYiIiEjq9u2DLl2galX47DPYvRu++sp+nTJlFLJEUqEjWiIiIiJib+9e8wjW/PlgtSa1FyoE1687ry6RHERBS0RERERMu3ebAWvBAvuAVbiwOYPg66+b52OJyG0paImIiIgIfPSRebFhw0hqK1IE+vWDXr3MKdtFJM0UtEREREQEmjUDi8UMWkWKQP/+8NprClgiGaSgJSIiIpLX7NwJ//4LLVoktVWuDH36QMmS0LMn+Po6rz6RXEBBS0RERCSv2LEDRo6Eb76BgADYvx88PJKWT5nivNpEchlN7y4iIiKS223fDm3bwt13myEL4Ngx+OIL59YlkospaImIiIjkVlu3wlNPQZ06sHhxUnvJkuaFhp97zmmlieR2GjooIiIikttERJhDBL/7zr69VCkYOBB69AAvL+fUJpJHKGiJiIiI5DazZ9uHLH9/GDQIXnpJAUski2jooIiIiEhOd+O1r8AMVW5uULo0TJ8OBw+aFxtWyBLJMgpaIiIiIjnVxo3w6KMpZwssWxZWrYIDB8yLDXt6Oqc+kTxMQUtEREQkp/nzT3jkEbj3XvjpJ5g4EaKj7dd56CEFLBEnUtASERERySn++AOCg6FxY1ixIqnd09M8eiUi2YaCloiIiEh2t24dtGgB991nDglMVK4cfPyxeeHhu+92Xn0ikoJmHRQRERHJrgwDWreG5cvt2wMDYcgQ6NIF3N2dUpqI3JqCloiIiEh2ZbFAlSpJt8uXh7ffhs6dzVkFRSTb0tBBERERkexi7Vq4dMm+bcAAqFULPv0U9u6FF15QyBLJARS0RERERJzJMOCXX6BpU3j4YfO6VzcqUQK2b4du3RSwRHIQBS0RERERZzAMWL0aHnwQmjeH334z26dMgago+3UtlqyvT0TuiIKWiIiISFYyDAgPhwcegKAgc0bBRFWrmke0fHycV5+IOIQmwxARERHJCokBa/hw2LDBflm1ajB0KDz7LLi6OqU8EXEsBS0RERGRrBAVBe3b2w8LrF4dhg0z2xWwRHIVDR0UERERyQoFCkDv3ubvNWvCggWwYwd06KCQJZILKWiJiIiIOJJhwA8/mDMInjtnv+yNN2DRIvj7bw0TFMnlFLREREREHMEwYPlyaNgQHn/cvCbWlCn26xQubA4TdNFXMJHcTudoiYiIiNwJw4Bly2DkSIiIsF/2++/mck3PLpLn6L9TRERERDLCMGDpUrjnHmjTxj5k1a0LS5bAr78qZInkUTqiJSIiIpJee/ea51ht327ffs89EBYGTzyhgCWSxyloiYiIiKRX6dJw/HjS7fr1zYD12GMKWCICaOigiIiIyK1ZrbBtm32bry/07w8NGpgzDG7aZE6AoZAlIv+noCUiIiKSmoQEWLgQ7r4b7r0XTp60X96vH2zcCI8+qoAlIikoaImIiIjcKCEBvvoKatUyLyb8zz8QEwMTJ9qv5+amgCUiN6VztERERETADFgLFsDo0bBnj/2yxo3NoYEiImmkoCUiIiJ5W3y8eQRr9GjYt89+2X33wfDh0Ly5jl6JSLooaImIiEje9tln8NJL9m0PPGDOItismQKWiGSIztESERGRvK1TJyhVyvy9aVP45RfzQsM6iiUid0BHtERERCRviIuDL7+EyEjzaFUiT0/48EMoWBAeeshZ1YlILqOgJSIiIrlbXBx8/jmMHQuHDkG+fBASAoGBSeu0aeOs6kQkl8oWQwc/+OADAgMD8fT0pFGjRmzatOmm686aNYsHHniAQoUKUahQIYKCglKsbxgGw4YNo1SpUnh5eREUFMT+/fszezdEREQkO4mNhdmzoUoV8xysQ4fM9vh4WLzYubWJSK7n9KC1cOFCQkNDCQsLIyIigtq1axMcHMyZM2dSXX/t2rV07NiRNWvWsGHDBgICAmjZsiUnTpywrTNx4kTee+89Zs6cycaNG/Hx8SE4OJjr169n1W6JiIiIk1ji4rAkBqzu3eHIkaSFLVvC+vUQGuq0+kQkb3B60JoyZQrdu3enW7du1KhRg5kzZ+Lt7c2nn36a6vrz5s2jZ8+e1KlTh2rVqjF79mysViurV68GzKNZU6dO5e2336Z169bcfffdfP7555w8eZKlS5dm4Z6JiIhIVrN8/TVBPXuSr2dPOHo0aUFwMPzxB6xcCU2aOK9AEckznHqOVmxsLFu2bGHw4MG2NhcXF4KCgtiwYUOathEdHU1cXByFCxcG4PDhw5w6dYqgoCDbOgUKFKBRo0Zs2LCBDh06pNhGTEwMMTExtttRUVEAxMXFERcXl6F9kzuT+Lzr+ZdE6hOSnPqEpMZ66RLeZ88m3W7VCuuQIRiNGpkN6i95jt4rJLn09omM9h2nBq1z586RkJBAiRIl7NpLlCjBnuRXZL+JgQMH4u/vbwtWp06dsm0j+TYTlyU3btw4RowYkaJ91apVeHt7p6kOyRzh4eHOLkGyGfUJSU59Iu9yiY0l37VrxBYoYGuzFC9O8+LFuVy2LHufeYaLVarA+fPw449OrFSyA71XSHJp7RPR0dEZ2n6OnnVw/PjxLFiwgLVr1+Lp6Znh7QwePJjQG8ZqR0VF2c798vPzc0Spkk5xcXGEh4fTokUL3NzcnF2OZAPqE5Kc+kQedv06Lp98gsukSRgPPEDCF1/YFsXFxbFmyhQefuopmqhfCHqvkJTS2ycSR7ull1ODVtGiRXF1deX06dN27adPn6ZkyZK3vO/kyZMZP348P//8M3fffbetPfF+p0+fplTixQf/f7tOnTqpbsvDwwMPD48U7W5ubvqDdDK9BpKc+oQkpz6Rh1y7Bh9/DBMmwL//AmBZtAiX4cOhWjXbavG+vuoXkoL6hCSX1j6R0X7j1Mkw3N3dqVevnm0iC8A2sUXjxo1ver+JEycyatQoVqxYQf369e2WlS9fnpIlS9ptMyoqio0bN95ymyIiIpJNXbsGU6dChQrQt68tZAHw5JPg4vS5vUREUnD60MHQ0FBCQkKoX78+DRs2ZOrUqVy9epVu3boB0KVLF0qXLs24ceMAmDBhAsOGDWP+/PkEBgbazrvy9fXF19cXi8VC3759GT16NJUrV6Z8+fIMHToUf39/2uhihCIiIjlHdDTMnAkTJ0Ky0S889RQMGwY3Ga0iIuJsTg9azz77LGfPnmXYsGGcOnWKOnXqsGLFCttkFpGRkbjc8D9VM2bMIDY2lnbt2tltJywsjOHDhwPw5ptvcvXqVXr06MHFixe5//77WbFixR2dxyUiIiJZ7LHHYO1a+7a2bWHoUKhd2ykliYiklcOCVkJCAqdPn8bf3z/d9+3Vqxe9evVKddnaZG+wR2686OBNWCwWRo4cyciRI9Ndi4iIiGQTPXokBa127cyAdcN52SIi2ZnDBjXv3LmTgIAAR21ORERE8oorV8wJLrZts29/5hno0wd27ICvv1bIEpEcxelDB0VERCSPunwZPvgAJk82r3W1YQMsXZq03NXVnARDRCQHUtASERGRrBUVBe+/D++8A//9l9S+fDlERkLZss6rTUTEQTQfqoiIiGSNqCgYPRrKl4chQ5JClosLdOoE//yjkCUiuUaaj2j9/ffft1y+d+/eOy5GREREcqHYWPMcrHffhQsXktpdXOC55+Dtt6FqVefVJyKSCdIctOrUqYPFYsEwjBTLEtstFotDixMREZFcwM0NlixJClkuLvD88+ZRrSpVnFubiEgmSXPQOnz4cGbWISIiIrnFlSvg65t022KBsDDzGliJAatyZefVJyKSBdIctMqVK5eZdYiIiEhO999/5iyB06bBTz9BkyZJy558Eg4cgMBAZ1UnIpKl0hy0li1blmp7gQIFqFKlCqVKlXJYUSIiIpKDnD9vnn/13nvmlO0AI0bAypVJ61gsClkikqekOWi1adPmpsssFgsdOnRg1qxZeHt7O6IuERERye7OnYMpU2D6dHO4YKJ8+czZA+PizPOzRETyoDRP7261WlP9uXDhAuHh4URERDB69OjMrFVERESyg7NnYdAg8wjVuHFJIcvNDXr0gP37YdYshSwRydPu+DpaBQoUoFmzZrz77rssXrzYETWJiIhIdnXokHkdrAkT4OpVs83NDV55xTwH66OPNERQRAQHXrC4WrVqHD9+3FGbExERkeyofHmoXdv83d0devaEgwdhxgxdbFhE5AYOC1qHDh3C39/fUZsTERERZzt92pzg4sZraFosMHIk9OplBqwPPoCAAOfVKCKSTaV5Moxb2bZtG/379+exxx5zxOZERETEmU6dgokTYeZMuHYNatSAoKCk5c2bmz8iInJTaQ5ahQoVwmKxpGi/evUq8fHxtGjRghEjRji0OBEREclC//6bFLCuX09qHz/ePmiJSLZhtUJkpHllhfz5zRG8Lg4bsyZ3Is1Ba+rUqam2+/n5UbVqVWrUqOGomkRERCQrnTxpTm7x8cf2AcvLy5zkYsCATC9BXxZF0m/3bliyBPbsMf90PT2hWjV46imoXt3Z1WVcbnk/SHPQCgkJycw6RETylNzyISJ3xun94ORJc3r2WbMgJiap3csLXn3VDFglS5p1Hsm8OnPrl0WRzLR7t3kK5blz5mmSPj7mRKBbt8KxY9C7d878+8lN7wcOOUcLICIigmHDhvH99987apOSyzj9C4U4nF7TjMlNHyKScdmiHxw5Au+/n3Tb29ucRbB/fyhRIkvqzK1fFkUyk9Vq/l2eO2eeQpl4do+fn3l71y5YuhSqVs1Zn8u57f0gXUFr5cqVhIeH4+7uzksvvUSFChXYs2cPgwYNYvny5QQHB2dWnXlSbvoSm5kf1LnpebqZ7LiP2eJLYg6U2z5EJGOc1g8SEsDVNel2kybmuVd//AGvvWYGrOLFs6zO3PplUSSzRUaan78BAUl/N4ksFihTxvz7jYzMOZe1y43vB2kOWp988gndu3encOHCXLhwgdmzZzNlyhRef/11nn32WXbu3El1fTtwmNz0JTYzP6hz0/N0M9lxHxUWMiY3fohI+jmlHxw9ag4R3LYNNmyw/2Y2YwYUKADFimV5nbnxy6JIVrh82fxO4OOT+nIfHzhxwlwvp8iN7wdpfmucNm0aEyZM4Ny5cyxatIhz587x4YcfsmPHDmbOnKmQ5UCJX2K3boWiRc0PsaJFzdvvvWcuzymSf1D7+Zn/mZr4QX3unPlBbbWmf9u56Xm6mey4j5n5muZ26fkQkdwrS/vBkSPw8stQuTJ89BFs3AjLl9uvU6lSipCVVXWm5cvi9es568uiSFbIn9/8j9erV1NffvWquTx//qyt607kxveDNAetgwcP0r59ewCefvpp8uXLx6RJkyhTpkymFZcX5bYvsZn1QZ3bnqfUZNd9VFjIuNz4ISLplyX94PBh6N7dDFgffwxxcWZ7/vzmNbKySZ258cuiSFYoW9Yc3XLsmP31xMG8ffy4ObKkbFnn1JcRufH9IM1B69q1a3h7ewNgsVjw8PCgVKlSmVZYXpXbvsRm1gd1bnueUpNd91FhIeNy44eIpF+m9oNDh+DFF6FKFZg9G+Ljkx707bfNI1w9eji/zv/LjV8WRbKCi4t5CkHRouYw3kuXzD/3S5fM20WLQps2OWsYem58P0jXZBizZ8/G19cXgPj4eObOnUvRokXt1undu7fjqsuDctuY2xs/qP38Ui7P6Ad1bnueUpNd9zGzXtO8IPFDZOtW+3NeIOlD5J57ctaHiKRfpvWDDz80T5BMSEhq8/ODPn2gb18oXDh71HmDxC+Lx46ZXw7LlEk65/P48Zz5ZVEkq1Svbv7JJ57HfeKE+fl7zz3m301OO6snN74fpDlolS1bllmzZtlulyxZki+++MJuHYvFoqB1h3Lbl9jM+qDObc9TarLrPiosZFxu/BCR9Mu0ftC4cVLIKlDADFd9+kChQtmrzmRy25dFkaxUvbp5/nZ2m5k4o3Lb+0Gag9aRI0cysQxJlNu+xGbWB3Vue55Sk133UWHhzuS2DxHJmDvuB/v2mSdqNmmS1Fa3LoSEQIUK5sYLFnR+nel4nNz0ZVEkK7m45JxZ+NIiN70fOOyCxeIYufFLbGZ8UOfG5ym57LyPCgt3Jjd9iEjGZagf7N0Lo0fD/PnmeVg7d9pfF2vu3OxRZwbkti+LIpJxueX9QEErG8qNX2Iz44M6Nz5PyWXnfVRYuDO55UNE7kya+8GePTBqFCxYkDTV6J498PXX0KFDZpYIqL+KiGSEglY2lRu/xGbGB3VufJ6Sy877qC9fIpls9+6kgHXjNFxFikC/fvDYY86rTUREbklBKxvTl9i0yQvPU17YRxG5wT//mAFr0aKUAat/f3jttZw924+ISB6goCUiIpLdjB0LCxcm3S5aFAYMgJ494f+XWRERkewtTUErKioqzRv0S20eahEREUm7t9+Gr76CYsXMgPXqqze/qJ6IiGRLaQpaBQsWxHLj3NK3kHDjhRJFRETk5rZvh5EjzXOtXnghqb16dfjuO2jWTAFLRCSHSlPQWrNmje33I0eOMGjQILp27Urjxo0B2LBhA5999hnjxo3LnCpFRERyk61bzYC1dGnS7c6dwc0taZ0nnnBKaSIi4hhpClpNmza1/T5y5EimTJlCx44dbW1PPvkktWrV4uOPPyYkJMTxVYqIiOQGERFmwPruO/v2a9dg/37z6uQiIpIrpHuC6A0bNlC/fv0U7fXr12fTpk0OKUpERCRX2bIFnnwS6tWzD1n+/jBtGhw6pJAlIpLLpDtoBQQEMGvWrBTts2fPJiAgwCFFiYiI5ApWKzz9NNSvD8uXJ7WXLg3Tp8PBg+ZVyb28nFejiIhkinRP7/7uu+/Stm1bfvrpJxo1agTApk2b2L9/P99++63DCxQREcmxXFzMa18lKlMGBg82J77w9HReXSIikunSfUTr0UcfZd++fTzxxBP8999//PfffzzxxBPs27ePRx99NDNqFBERyRk2bjTPt7rRW29BhQowYwYcOGBeC0shS0Qk18vQBYsDAgIYO3aso2sRERHJmf74A0aMgFWrzHOuevdOWla+vDnRhUu6/29TRERysAy96//+++88//zzNGnShBMnTgDwxRdfsG7dOocWJyIikq2tWwctWsB995khC2DCBLh+3X49hSwRkTwn3e/83377LcHBwXh5eREREUFMTAwAly5d0lEuERHJG37/HYKC4IEH4Oefk9oDA80jW66uTitNRESyh3QHrdGjRzNz5kxmzZqF2w0XVrzvvvuIiIhwaHEiIiLZyq+/QrNm8OCDsHp1Unv58vDJJ7BvH7z0kv2Fh0VEJE9K9zlae/fu5cEHH0zRXqBAAS5evOiImkRERLKfc+cgOBj+P5IDgIoVYcgQeP55hSsREbGT7iNaJUuW5MCBAyna161bR4UKFRxSlIiISLZTtCi8/LL5e6VKMHcu7NkD3bopZImISArpDlrdu3enT58+bNy4EYvFwsmTJ5k3bx79+/fn1VdfzYwaRUREso5hmMMCW7eGK1fslw0cCJ9/Drt3Q0gI5MvQ5L0iIpIHpPsTYtCgQVitVpo3b050dDQPPvggHh4e9O/fn9dffz0zahQREcl8hmFObDFiBKxfb7Z98IEZrhL5+0Pnzs6pT0REcpR0H9GyWCwMGTKE//77j507d/Lnn39y9uxZRo0alRn1iYiIZC7DgJUrzSnaW7ZMClkAP/xgLhcREUmndAetF154gcuXL+Pu7k6NGjVo2LAhvr6+XL16lRdeeCEzahQREXE8w4AVK6BxY2jVCjZsSFpWowZ89RWsWQMWi/NqFBGRHCvdQeuzzz7j2rVrKdqvXbvG559/7pCiREREMtXu3XDvvfDII7BxY1J7zZqwcCHs2AEdOuh6WCIikmFpPkcrKioKwzAwDIPLly/j6elpW5aQkMCPP/5I8eLFM6VIERERhypWDP75J+n2XXfBsGHQti24pPv/IEVERFJIc9AqWLAgFosFi8VClSpVUiy3WCyMGDHCocWJiIjcMcOA/fvhxs+uokWhVy/46SczYD31lAKWiIg4VJqD1po1azAMg2bNmvHtt99SuHBh2zJ3d3fKlSuHv79/phQpIiKSboYBy5aZswgePAhHjkChQknLhw+HsWMVsEREJFOkOWg1bdoUgMOHD1O2bFksOjlYRESyI6sVvvsORo6EbduS2qdONUNXohuGwIuIiDhauv8b75dffuGbb75J0f7111/z2WefpbuADz74gMDAQDw9PWnUqBGbNm266br//PMPbdu2JTAwEIvFwtSpU1Osk5CQwNChQylfvjxeXl5UrFiRUaNGYWh6XhGR3M1qhcWLoW5dePpp+5B1zz3m7IIiIiJZJN1Ba9y4cRQtWjRFe/HixRk7dmy6trVw4UJCQ0MJCwsjIiKC2rVrExwczJkzZ1JdPzo6mgoVKjB+/HhKliyZ6joTJkxgxowZvP/+++zevZsJEyYwceJEpk+fnq7aREQkh7Ba4ZtvoE4dczKLv/9OWlavHixfDn/9ZU7hLiIikkXSHbQiIyMpX758ivZy5coRGRmZrm1NmTKF7t27061bN2rUqMHMmTPx9vbm008/TXX9Bg0aMGnSJDp06ICHh0eq6/zxxx+0bt2axx57jMDAQNq1a0fLli1veaRMRERyLpePPoL27c0p2RM1aADffw+bN8Pjj+taWCIikuXSfI5WouLFi/P3338TGBho1759+3aKFCmS5u3ExsayZcsWBg8ebGtzcXEhKCiIDTdeNDKdmjRpwscff8y+ffuoUqUK27dvZ926dUyZMuWm94mJiSEmJsZ2OyoqCoC4uDji4uIyXItkXOLzrudfEqlPSHKJfSGmfXu8wsKwXLyItUEDrEOHYgQHm+EqPt7JVUpW03uFJKc+Icmlt09ktO+kO2h17NiR3r17kz9/fh588EEAfv31V/r06UOHDh3SvJ1z586RkJBAiRIl7NpLlCjBnj170luWzaBBg4iKiqJatWq4urqSkJDAmDFj6NSp003vM27cuFSnpl+1ahXe3t4ZrkXuXHh4uLNLkGxGfSIPS0ig9Pr1eFy8yKEnn7Q1h2/aREBICDEFCnCmbl1zKOFPPzmxUMkO9F4hyalPSHJp7RPR0dEZ2n66g9aoUaM4cuQIzZs3J18+8+5Wq5UuXbqk+xytzLBo0SLmzZvH/PnzqVmzJtu2baNv3774+/sTEhKS6n0GDx5MaGio7XZUVBQBAQG0bNkSPz+/rCpdbhAXF0d4eDgtWrTAzc3N2eVINqA+kYclJGBZtAjXsWOx7N2L4eVFtREjiCtcOKlPPPqos6uUbELvFZKc+oQkl94+kTjaLb3SHbTc3d1ZuHAho0aNYvv27Xh5eVGrVi3KlSuXru0ULVoUV1dXTp8+bdd++vTpm050kRYDBgxg0KBBtqNrtWrV4ujRo4wbN+6mQcvDwyPVc77c3Nz0B+lkeg0kOfWJPCQ+HhYsgNGjYe9eW7Pl2jXcFi+GV18F1CckdeoXkpz6hCSX1j6R0X6T7qCVqEqVKlSpUiWjd8fd3Z169eqxevVq2rRpA5hHxlavXk2vXr0yvN3o6Ghckl180tXVFavVmuFtiohIFoqPh/nzzYC1f7/9sgcegLAwaNZM51+JiEi2lqagFRoayqhRo/Dx8bEbYpeaW006kdp2Q0JCqF+/Pg0bNmTq1KlcvXqVbt26AdClSxdKly7NuHHjAHMCjV27dtl+P3HiBNu2bcPX15dKlSoB8MQTTzBmzBjKli1LzZo12bp1K1OmTOGFF15Ic10iIuIk334LgwbBgQP27U2bmgHroYc0g6CIiOQIaQpaW7dutc22sXXr1puuZ0nnh9+zzz7L2bNnGTZsGKdOnaJOnTqsWLHCNkFGZGSk3dGpkydPUrduXdvtyZMnM3nyZJo2bcratWsBmD59OkOHDqVnz56cOXMGf39/Xn75ZYYNG5au2kRExAmOHrUPWQ89lBSwREREcpA0Ba01a9ak+rsj9OrV66ZDBRPDU6LAwEAMw7jl9vLnz8/UqVOZOnWqgyoUEZFMERcH0dFQoEBS2yuvwIQJcNddZsD6/+y2IiIiOU26L1gsIiJyR2JjYdYsqFIFbriWIgDe3rB9O6xerZAlIiI5WpqOaD399NNp3uDixYszXIyIiORisbEwdy6MHWsOEQSYPdsMWwEBSevdwcyzIiIi2UWaglaBG4Z1GIbBkiVLKFCgAPXr1wdgy5YtXLx4MV2BTERE8oiYGJgzB8aNg8hI+2UPP2wOHxQREcll0hS05syZY/t94MCBPPPMM8ycORNXV1cAEhIS6Nmzpy7uKyIiSWJi4JNPzIB1/Lj9slatzHOw7r3XObWJiIhksnRfR+vTTz9l3bp1tpAF5nWqQkNDadKkCZMmTXJogSIikkM1bw7r19u3PfqoGbAaNnROTSIiIlkk3ZNhxMfHs2fPnhTte/bs0UWBRUQkyfPPJ/3++OOwaRP88INCloiI5AnpPqLVrVs3XnzxRQ4ePEjD/39Ybty4kfHjx9suNCwiInnItWvmLIKPPAKVKye1d+sGmzdDz55Qr57z6hMREXGCdAetyZMnU7JkSd555x3+/fdfAEqVKsWAAQPo16+fwwsUEZFs6to1+Ogj87pXp05BRIQ5q2AiDw/zHC0REZE8KN1By8XFhTfffJM333yTqKgoAE2CISKSl0RHw8yZMHEinD6d1D5vHowfr+nZRUREyOAFi+Pj4/n555/56quvsFgsAJw8eZIrV644tDgREclGrl6Fd96B8uWhXz/7kPX00/DXXwpZIiIi/5fuI1pHjx6lVatWREZGEhMTQ4sWLcifPz8TJkwgJiaGmTNnZkadIiLiLDEx8N57MGkSnD1rv6xdOxg6FO6+2zm1iYiIZFPpPqLVp08f6tevz4ULF/Dy8rK1P/XUU6xevdqhxYmISDbg6mpOdpEYsiwWeOYZ2LEDvv5aIUtERCQV6T6i9fvvv/PHH3/g7u5u1x4YGMiJEyccVpiIiDhJTIw5kUWifPng7beha1d49lnz95o1nVaeiIhITpDuI1pWq5WEhIQU7cePHyd//vwOKUpERJwgKgrGjoUyZeDvv+2XPfcc7NoFX32lkCUiIpIG6Q5aLVu2ZOrUqbbbFouFK1euEBYWxqOPPurI2kREJCtcugSjR0NgIAwZAufOwahR9uvkywfVqjmlPBERkZwoQ9fRatWqFTVq1OD69es899xz7N+/n6JFi/LVV19lRo0iIpIZLl2CadPg3Xfh4sWkdhcX8PGBhATz/CwRERFJt3QHrYCAALZv387ChQvZvn07V65c4cUXX6RTp052k2OIiEg2dfGiGbCmTrUPWK6u8Pzz5lGtypWdVJyIiEjukK6gFRcXR7Vq1fj+++/p1KkTnTp1yqy6REQkM+zZA/feax7NSuTqCp07mwGrUiXn1SYiIpKLpOscLTc3N65fv55ZtYiISGarUgXKljV/d3WFF16AvXthzhyFLBEREQdK92QYr732GhMmTCA+Pj4z6hEREUc5fx7mzrVvc3GBESPgxRdh3z745BOoWNEp5YmIiORm6T5Ha/PmzaxevZpVq1ZRq1YtfHx87JYvXrzYYcWJiEgGnDsHU6bA9Olw5QrUqAENGyYtf+op80dEREQyTbqDVsGCBWnbtm1m1CIiInfi3Dl45x14/30zYCUaPRqWLXNeXSIiInlQuoPWnDlzMqMOERHJqLNnYfJk+OADuHo1qd3NzRwiOHiw82oTERHJo9IctKxWK5MmTWLZsmXExsbSvHlzwsLCNKW7iIiznDmTFLCio5Pa3d3hpZdg0CAICHBefSIiInlYmifDGDNmDG+99Ra+vr6ULl2aadOm8dprr2VmbSIicitbtsCkSUkhy90dXnsNDh40w5dCloiIiNOkOWh9/vnnfPjhh6xcuZKlS5eyfPly5s2bh9Vqzcz6REQkkWHY327VCho0AA8PeP11OHTIPD+rTBnn1CciIiI2aR46GBkZyaOPPmq7HRQUhMVi4eTJk5TRh7qISOb591+YMMGcjv3HH5PaLRZzevYiRcDf33n1iYiISAppDlrx8fF4enratbm5uREXF+fwokREBDh50gxYH38MiReL//VXaNo0aZ1atZxTm4iIiNxSmoOWYRh07doVDw8PW9v169d55ZVX7K6lpetoiYjcoRMnYPx4mDULYmKS2r28zKNaNwYtERERyZbSHLRCQkJStD3//PMOLUZEJE87fjwpYMXGJrV7e0PPntC/P5Qo4bz6REREJM3SHLR0/SwRkUz0wQcQGpoyYL32mhmwihd3Xm0iIiKSbum+YLGIiGSCu+5KClk+PtCrF/TrB8WKObcuERERyRAFLRGRrHb0KFy4AHXqJLU1bQpPPGEGrtBQKFrUaeWJiIjInVPQEhHJKocPw9ixMHcu1K0LGzeaU7Qn+u47+9siIiKSY6X5gsUiIpJBhw7Biy9ClSowezbEx8PmzbBihf16ClkiIiK5ho5oiYhkloMHYcwY+PxzSEhIavfzgz59oFEj59UmIiIimUpBS0TE0fbvNwPWl1/aB6wCBaBvXzNkFSrktPJEREQk8yloiYg4Wr9+sHx50u2CBeGNN6B3b/N3ERERyfV0jpaIiKMNHWr+W6gQjBwJR47AsGEKWSIiInmIgpaISEbt2QOdOsG339q3N2gA8+aZAWvoUHPIoIiIiOQpGjooIpJeu3fDqFGwYAEYBvz9Nzz1FLjc8H9Xzz3nvPpERETE6XRES0Qkrf75Bzp0gJo14auvzJAF8O+/5gyDIiIiIv+noCUicjs7d8Izz0CtWrBwYVLAKloUJkwwhwhWruzUEkVERCR70dBBEZGbSUgwhwAuWmTfXrw4DBgAr74KPj7OqU1ERESyNQUtEZGbcXVNOnoFUKIEvPkmvPIKeHs7ry4RERHJ9jR0UEQk0fbtEB9v3zZ0KJQqBVOmwKFDEBqqkCUiIiK3paAlIhIRAa1bQ5065rTsN6pVC44eNS84rIAlIiIiaaSgJSJ515Yt8OSTUK8eLFtmto0enfKolptb1tcmIiIiOZrO0RKRvGfzZhgxAn74wb69dGno08f+vCwRERGRDFDQEpG8Y9MmM2D9+KN9e5kyMHgwvPACeHo6pzYRERHJVRS0RCRv+PdfaNLEnLI9UUBAUsDy8HBebSIiIpLr6BwtEckbSpWC5583fy9bFmbOhP37zWthKWSJiIiIgyloiUjus369GapiYuzb334bPvrIDFgvv6yAJSIiIplGQwdFJPf4/XfzHKzVq83b991nHrFKVKmS+SMiIiKSyXRES0Ryvl9/hWbN4MEHk0IWwKJFzqtJRERE8jQFLRHJudauhYcfhocegjVrktorVoQ5c2DVKmdVJiIiInmc04PWBx98QGBgIJ6enjRq1IhNmzbddN1//vmHtm3bEhgYiMViYerUqamud+LECZ5//nmKFCmCl5cXtWrV4q+//sqkPRCRLLdrFzRtaoastWuT2itVgrlzYc8e6NpVFxoWERERp3Fq0Fq4cCGhoaGEhYURERFB7dq1CQ4O5syZM6muHx0dTYUKFRg/fjwlS5ZMdZ0LFy5w33334ebmxk8//cSuXbt45513KFSoUGbuiohkpfz5YcOGpNuVK8Pnn8Pu3RASAvl0+qmIiIg4l1O/jUyZMoXu3bvTrVs3AGbOnMkPP/zAp59+yqBBg1Ks36BBAxo0aACQ6nKACRMmEBAQwJw5c2xt5cuXz4TqRSRLGAacOAElSiS1BQTASy+Z52MNHQodOihciYiISLbitG8msbGxbNmyhcGDB9vaXFxcCAoKYsON/1OdTsuWLSM4OJj27dvz66+/Urp0aXr27En37t1vep+YmBhibpgGOioqCoC4uDji4uIyXItkXOLzruc/DzMMLD//jMuoUVgOHSLun3+AG/rE6NEwZQq4upphTH0lz9H7hKRG/UKSU5+Q5NLbJzLad5wWtM6dO0dCQgIlbvxfaqBEiRLs2bMnw9s9dOgQM2bMIDQ0lLfeeovNmzfTu3dv3N3dCQkJSfU+48aNY8SIESnaV61ahbe3d4ZrkTsXHh7u7BIkqxkGxbduperChRTeu9fWfOjNN6F1a/UJSUF9QlKjfiHJqU9IcmntE9HR0Rnafq4ba2O1Wqlfvz5jx44FoG7duuzcuZOZM2feNGgNHjyY0NBQ2+2oqCgCAgJo2bIlfn5+WVK32IuLiyM8PJwWLVrgpgkN8gbDwLJiBS5jxuCyebP9ourVqfzooxwE9Qmx0fuEpEb9QpJTn5Dk0tsnEke7pZfTglbRokVxdXXl9OnTdu2nT5++6UQXaVGqVClq1Khh11a9enW+/fbbm97Hw8MDDw+PFO1ubm76g3QyvQZ5gGHAjz+aFxpOFrC46y4YNgxL27a4JCTAjz+qT0gK6hOSGvULSU59QpJLa5/IaL9x2qyD7u7u1KtXj9U3XFzUarWyevVqGjdunOHt3nfffey9YbgRwL59+yhXrlyGtykimejdd+Hxx+1D1t13wzffwPbt0L49uDj9ShQiIiIi6eLUby+hoaHMmjWLzz77jN27d/Pqq69y9epV2yyEXbp0sZssIzY2lm3btrFt2zZiY2M5ceIE27Zt48CBA7Z13njjDf7880/Gjh3LgQMHmD9/Ph9//DGvvfZalu+fiKRBp07g5WX+XqcOLF4MW7dC27YKWCIiIpJjOfUcrWeffZazZ88ybNgwTp06RZ06dVixYoVtgozIyEhcbviidfLkSerWrWu7PXnyZCZPnkzTpk1Z+/+LljZo0IAlS5YwePBgRo4cSfny5Zk6dSqdOnXK0n0TkWQMA777Di5cgP//ZwpgTts+caI5ZfuTT4LF4rwaRURERBzE6ZNh9OrVi169eqW6LDE8JQoMDMQwjNtu8/HHH+fxxx93RHkicqesVli6FEaONIcCFiwITz8NBQokrXOT9wARERGRnErjckQkc1it5nlWdeqYwwC3bzfbL16EBQucWZmIiIhIplPQEhHHslph0SKoXducyGLHjqRlDRrA999Djx7Oq09EREQkCzh96KCI5CLffQdDhsA//9i3N2wIw4dDq1Y6B0tERETyBAUtEXGciAj7kHXvvRAWBsHBClgiIiKSp2jooIhkTEICREfbt/XtC35+0KQJrFwJf/yho1giIiKSJyloiUj6xMfDl19CjRrmTII3KlTInPRi3Tpo2VIBS0RERPIsBS0RSZv4ePj8czNgde4M+/bB++/DuXP26wUGKmCJiIhInqegJSK3Fh8Pc+dC9eoQEgL79yctq18f/vvPaaWJiIiIZFcKWiKSurg4mDMHqlaFbt3gwIGkZQ8/DGvXmj9VqjirQhEREZFsS7MOikhKhgEPPAAbN9q3N2tmziL44IPOqUtEREQkh9ARLRFJyWKBp55Kuh0UBL//DqtXK2SJiIiIpIGClkheFxsLH38MJ07Yt7/2GrRrZ84gGB4O99/vnPpEREREciAFLZG8KiYGZs6EypXh5ZdhwgT75b6+8PXXcN99zqlPREREJAdT0BLJa2Ji4MMPoVIlePVViIw022fNgvPnnVubiIiISC6hoCWSV1y/Dh98ABUrmsMCjx9PWvboo/Drr1CkiPPqExEREclFNOugSG4XE2MerRo3Dk6etF/2+OMwbBg0aOCc2kRERERyKQUtkdzOMFKGrCefNANWvXrOq0tEREQkF9PQQZHcJiHB/ranJwwcaP7eujVs2QLffaeQJSIiIpKJFLREcovoaJgyBQID4fBh+2Xdu0NEBCxdCvfc44zqRERERPIUBS2RnO7qVXjnHShfHvr1Mye5GDvWfh0vL6hb1zn1iYiIiORBOkdLJKe6etWcpn3SJDh71n5ZdLR5bpbF4pzaRERERPI4BS2RnObKFTNgTZ5sH7AsFmjfHoYOhbvucl59IiIiIqKgJZKj7NwJDz8M584ltVks8Oyz8PbbULOm82oTERERERudoyWSk1StCvnzm79bLPDcc/DPP/DVVwpZIiIiItmIgpZIdhUVBYsX27e5uZnXv+rUCXbtgnnzoHp159QnIiIiIjeloCWS3Vy6BKNGmdO0t2sHu3fbL+/aFb78EqpVc0Z1IiIiIpIGCloi2cXFizBihBmwhg2DCxfMmQNHj3Z2ZSIiIiKSTpoMQ8TZLlyAqVNh2jTzaFYiV1fo3BmGDHFaaSIiIiKSMQpaIs7y33/w7rvw3nvm+ViJXF2hSxczYFWs6Lz6RERERCTDFLREnCU83H5YYL58EBICb70FFSo4ry4RERERuWM6R0vEWdq1M2cMzJcPuneHfftg9myFLBEREZFcQEe0RDLbuXPwzjtw7Jg5W2AiV1eYMwdKlDAnwBARERGRXENBSySznD0LkyfDBx/A1atmW79+ULdu0jqNGjmnNhERERHJVBo6KOJoZ87Am2+aR6kmTkwKWe7u8NdfTi1NRERERLKGjmiJOMrp0zBpEsyYAdHRSe3u7uY5WIMGQZkyzqtPRERERLKMgpaII7z/vnkU69q1pDYPD+jRAwYOhNKlnVebiIiIiGQ5BS0RRyhbNilkeXrCyy+bwcvf37l1iYiIiIhTKGiJpNfJk3DlClSpktT2xBNw333QoIEZsEqVcl59IiIiIuJ0CloiaXXiBIwfD7NmwQMPmBccTmSxwG+/gYvmlxERERERzToocnvHjsFrr5kXEn7/fYiJgZ9/hj/+sF9PIUtERERE/k9HtERuJjLSPIL1yScQG5vU7u1tBq/KlZ1Xm4iIiIhkawpaIskdPQrjxsGnn0JcXFK7jw/06mVedLhYMefVJyIiIiLZnoKWSHLdusGaNUm3fX3h9dchNBSKFnVeXSIiIiKSY+ikEpHk3nrL/Dd/fvP3I0dg7FiFLBERERFJMwUtybsOHYKXXrKfPRCgeXOYMcMMWGPGQJEiTilPRERERHIuDR2UvOfgQTNAff45JCTA7t0QFGRO0Q7mv6+84twaRURERCRH0xEtyTsOHICuXaFqVZgzxwxZAP/8Y06AISIiIiLiIApakvvt3w8hIWbA+uyzpIBVsCCMGGEOEQwMdGKBIiIiIpLbaOig5F6xsfDiizB/PlitSe2FCsEbb0Dv3lCggPPqExEREZFcS0FLci93dzhzJilkFS5sTtH++uvg5+fc2kREREQkV9PQQck99u0Dw7BvCwszA9aYMXD4MAwZopAlIiIiIplOQUtyvn/+gQ4doFo1WLbMflmTJnDsmHk9LAUsEREREckiClqSc+3cCc88A7VqwcKF5tGsESNSHtXy9nZOfSIiIiKSZ+kcLcl5duyAkSPhm2/s24sVg44dIT4e3NycU5uIiIiICApakpNs324GrMWL7duLF4c33zQvMuzj45zaRERERERukC2GDn7wwQcEBgbi6elJo0aN2LRp003X/eeff2jbti2BgYFYLBamTp16y22PHz8ei8VC3759HVu0ZK0jR6BuXfuQVbIkTJliTnLRr59CloiIiIhkG04PWgsXLiQ0NJSwsDAiIiKoXbs2wcHBnDlzJtX1o6OjqVChAuPHj6dkyZK33PbmzZv56KOPuPvuuzOjdMlKgYHQurX5e6lSMHUqHDpkXg9L52CJiIiISDbj9KA1ZcoUunfvTrdu3ahRowYzZ87E29ubTz/9NNX1GzRowKRJk+jQoQMeHh433e6VK1fo1KkTs2bNolChQplVvmSGLVtwef11SEiwbx8+HN57Dw4ehD59wMvLKeWJiIiIiNyOU8/Rio2NZcuWLQwePNjW5uLiQlBQEBs2bLijbb/22ms89thjBAUFMXr06FuuGxMTQ0xMjO12VFQUAHFxccTFxd1RHZJ2lr/+wmX0aFx+/BFXoLSPD3GtWiWtUKOG+QOg1yXPSfxb1N+kJFKfkNSoX0hy6hOSXHr7REb7jlOD1rlz50hISKBEiRJ27SVKlGDPnj0Z3u6CBQuIiIhg8+bNaVp/3LhxjBgxIkX7qlWr8NawtExXaN8+qi5YQImICLv2sqtXE/7gg06qSrKr8PBwZ5cg2Yz6hKRG/UKSU5+Q5NLaJ6KjozO0/Vw36+CxY8fo06cP4eHheHp6puk+gwcPJjQ01HY7KiqKgIAAWrZsiZ8ucptpLH/+aR7BWrXKrt0ICCCuXz82li5NixYtcNNU7YL5v0nh4eHqE2KjPiGpUb+Q5NQnJLn09onE0W7p5dSgVbRoUVxdXTl9+rRd++nTp2870cXNbNmyhTNnznDPPffY2hISEvjtt994//33iYmJwdXV1e4+Hh4eqZ7v5ebmpj/IzLBrlzmJRbKARdmy8NZbWLp2xeLigvXHH/UaSArqE5Kc+oSkRv1CklOfkOTS2icy2m+cOhmGu7s79erVY/Xq1bY2q9XK6tWrady4cYa22bx5c3bs2MG2bdtsP/Xr16dTp05s27YtRcgSJ3BxgRsP1ZYrBx9/DPv3w8svwy0mORERERERyQmcPnQwNDSUkJAQ6tevT8OGDZk6dSpXr16lW7duAHTp0oXSpUszbtw4wJxAY9euXbbfT5w4wbZt2/D19aVSpUrkz5+fu+66y+4xfHx8KFKkSIp2ySLnz0ORIkm3q1WDjh3hjz9gyBDo0gXc3Z1Xn4iIiIiIgzk9aD377LOcPXuWYcOGcerUKerUqcOKFStsE2RERkbi4pJ04O3kyZPUrVvXdnvy5MlMnjyZpk2bsnbt2qwuX27l119hxAjzgsL79sGNh12nT4f8+e3bRERERERyCacHLYBevXrRq1evVJclD0+BgYEYhpGu7SuAZbG1a82AdePz/vnn8OKLSbcLF87qqkREREREsky2CFqSCxgGrFljBqzffrNfVqmS/dBBEREREZFcTkFL7oxhwC+/wPDhsG6d/bIqVeDtt83zsfKpq4mIiIhI3qFvv3JnJk6EQYPs26pWhaFDoUMH0CyPIiIiIpIHOXV6d8kFnn026WhVtWowbx788w906qSQJSIiIiJ5lo5oSdoYBqxYAVevQrt2Se2BgRAWZp6H1b69wpWIiIiICApacjuGAT/9ZE5ysWkTlCoFjz8Onp5J67z9tvPqExERERHJhjR0UFJnGPD999CwITz2mBmyAP79FxYscG5tIiIiIiLZnIKW2DMMWL4cGjSAJ56Av/5KWlarFnzzDXTp4rz6RERERERyAA0dlCQ//ADDhkFEhH177dpme5s24KJsLiIiIiJyOwpakiQ83D5k1aljTnTx5JMKWCIiIiIi6aBvz3mV1QqxsfZtb74JHh5Qty58950ZunQUS0REREQk3fQNOq+xWs3zrOrUgXfesV/m7w9btpg/Tz4JFotTShQRERERyekUtPIKqxUWLTLPt2rfHnbsMIPWlSv269WsqYAlIiIiInKHFLRyu4QEWLjQnDHw2Wdh586kZRUrwqlTzqtNRERERCSXUtDKrRIS4KuvzIDVoQPs2pW07N57YcUK+PNPqFTJeTWKiIiIiORSmnUwNzIMaNIk6SLDiZo0MWcRbNFCwwNFRERERDKRjmjlRhYLNG+edPu++8yp29etg5YtFbJERERERDKZglZOFx8PX3wB//1n3x4aCsHB8PPP8PvvEBSkgCUiIiIikkUUtHKq+HiYOxeqV4cuXWDqVPvlRYua52E1b66AJSIiIiKSxRS0cpq4OPj0U6haFbp1gwMHzPZp0yAqyrm1iYiIiIgIoKCVc8TFwSefmAHrxRfh0KGkZc2awfLl4OfnvPpERERERMRGsw5md7Gx8NlnMHYsHDlivywoyJxF8P77nVKaiIiIiIikTkEru7t+HQYOhAsXktpatDAD1n33Oa8uERERERG5KQ0dzO78/OCNN8zfg4Phjz9g1SqFLBERERGRbExHtHKC3r3N6181auTsSkREREREJA10RCsnKFBAIUtEREREJAdR0BIREREREXEwBS0REREREREHU9ASERERERFxMAUtERERERERB1PQEhERERERcTAFLREREREREQdT0BIREREREXEwBS0REREREREHU9ASERERERFxMAUtERERERERB1PQEhERERERcTAFLREREREREQdT0BIREREREXEwBS0REREREREHy+fsArIjwzAAiIqKcnIleVdcXBzR0dFERUXh5ubm7HIkG1CfkOTUJyQ16heSnPqEJJfePpGYCRIzQlopaKXi8uXLAAQEBDi5EhERERERyQ4uX75MgQIF0ry+xUhvNMsDrFYrJ0+eJH/+/FgsFmeXkydFRUUREBDAsWPH8PPzc3Y5kg2oT0hy6hOSGvULSU59QpJLb58wDIPLly/j7++Pi0vaz7zSEa1UuLi4UKZMGWeXIYCfn5/eFMWO+oQkpz4hqVG/kOTUJyS59PSJ9BzJSqTJMERERERERBxMQUtERERERMTBFLQkW/Lw8CAsLAwPDw9nlyLZhPqEJKc+IalRv5Dk1CckuazqE5oMQ0RERERExMF0REtERERERMTBFLREREREREQcTEFLRERERETEwRS0REREREREHExBS7LEBx98QGBgIJ6enjRq1IhNmzbddN1//vmHtm3bEhgYiMViYerUqbfc9vjx47FYLPTt29exRUumyow+ceLECZ5//nmKFCmCl5cXtWrV4q+//sqkPZDM4Oh+kZCQwNChQylfvjxeXl5UrFiRUaNGoXmgco709IlZs2bxwAMPUKhQIQoVKkRQUFCK9Q3DYNiwYZQqVQovLy+CgoLYv39/Zu+GOJgj+0VcXBwDBw6kVq1a+Pj44O/vT5cuXTh58mRW7Io4iKPfK270yiuvpOk7aXIKWpLpFi5cSGhoKGFhYURERFC7dm2Cg4M5c+ZMqutHR0dToUIFxo8fT8mSJW+57c2bN/PRRx9x9913Z0bpkkkyo09cuHCB++67Dzc3N3766Sd27drFO++8Q6FChTJzV8SBMqNfTJgwgRkzZvD++++ze/duJkyYwMSJE5k+fXpm7oo4SHr7xNq1a+nYsSNr1qxhw4YNBAQE0LJlS06cOGFbZ+LEibz33nvMnDmTjRs34uPjQ3BwMNevX8+q3ZI75Oh+ER0dTUREBEOHDiUiIoLFixezd+9ennzyyazcLbkDmfFekWjJkiX8+eef+Pv7p78wQySTNWzY0HjttddstxMSEgx/f39j3Lhxt71vuXLljHfffTfVZZcvXzYqV65shIeHG02bNjX69OnjoIols2VGnxg4cKBx//33O7JMyWKZ0S8ee+wx44UXXrBre/rpp41OnTrdcb2S+e6kTxiGYcTHxxv58+c3PvvsM8MwDMNqtRolS5Y0Jk2aZFvn4sWLhoeHh/HVV185tnjJNI7uF6nZtGmTARhHjx6943ol82VWnzh+/LhRunRpY+fOnbf8TnozOqIlmSo2NpYtW7YQFBRka3NxcSEoKIgNGzbc0bZfe+01HnvsMbttS/aXWX1i2bJl1K9fn/bt21O8eHHq1q3LrFmzHFGyZIHM6hdNmjRh9erV7Nu3D4Dt27ezbt06HnnkkTuuWTKXI/pEdHQ0cXFxFC5cGIDDhw9z6tQpu20WKFCARo0a3fFnkmSNzOgXqbl06RIWi4WCBQveacmSyTKrT1itVjp37syAAQOoWbNmhmrLl6F7iaTRuXPnSEhIoESJEnbtJUqUYM+ePRne7oIFC4iIiGDz5s13WqJksczqE4cOHWLGjBmEhoby1ltvsXnzZnr37o27uzshISF3WrZksszqF4MGDSIqKopq1arh6upKQkICY8aMoVOnTndasmQyR/SJgQMH4u/vb/sCdurUKds2km8zcZlkb5nRL5K7fv06AwcOpGPHjvj5+d1xzZK5MqtPTJgwgXz58tG7d+8M16agJTnOsWPH6NOnD+Hh4Xh6ejq7HMkmrFYr9evXZ+zYsQDUrVuXnTt3MnPmTAWtPGzRokXMmzeP+fPnU7NmTbZt20bfvn3x9/dXv8jlxo8fz4IFC1i7dq0+K8Tmdv0iLi6OZ555BsMwmDFjhhMqlKyWWp/YsmUL06ZNIyIiAovFkuFta+igZKqiRYvi6urK6dOn7dpPnz5924kubmbLli2cOXOGe+65h3z58pEvXz5+/fVX3nvvPfLly0dCQoIjSpdMkhl9AqBUqVLUqFHDrq169epERkZmeJuSdTKrXwwYMIBBgwbRoUMHatWqRefOnXnjjTcYN27cnZYsmexO+sTkyZMZP348q1atspssKfF+ju5nknUyo18kSgxZR48eJTw8XEezcojM6BO///47Z86coWzZsrbvmkePHqVfv34EBgamuTYFLclU7u7u1KtXj9WrV9varFYrq1evpnHjxhnaZvPmzdmxYwfbtm2z/dSvX59OnTqxbds2XF1dHVW+ZILM6BMA9913H3v37rVr27dvH+XKlcvwNiXrZFa/iI6OxsXF/qPO1dUVq9Wa4W1K1shon5g4cSKjRo1ixYoV1K9f325Z+fLlKVmypN02o6Ki2Lhx4x31M8k6mdEvIClk7d+/n59//pkiRYpkSv3ieJnRJzp37szff/9t913T39+fAQMGsHLlyrQXl66pM0QyYMGCBYaHh4cxd+5cY9euXUaPHj2MggULGqdOnTIMwzA6d+5sDBo0yLZ+TEyMsXXrVmPr1q1GqVKljP79+xtbt2419u/ff9PH0KyDOUtm9IlNmzYZ+fLlM8aMGWPs37/fmDdvnuHt7W18+eWXWb5/kjGZ0S9CQkKM0qVLG99//71x+PBhY/HixUbRokWNN998M8v3T9IvvX1i/Pjxhru7u/HNN98Y//77r+3n8uXLdusULFjQ+O6774y///7baN26tVG+fHnj2rVrWb5/kjGO7hexsbHGk08+aZQpU8bYtm2b3ToxMTFO2UdJn8x4r0guI7MOKmhJlpg+fbpRtmxZw93d3WjYsKHx559/2pY1bdrUCAkJsd0+fPiwAaT4adq06U23r6CV82RGn1i+fLlx1113GR4eHka1atWMjz/+OIv2RhzF0f0iKirK6NOnj1G2bFnD09PTqFChgjFkyBB9ecpB0tMnypUrl2qfCAsLs61jtVqNoUOHGiVKlDA8PDyM5s2bG3v37s3CPRJHcGS/uNl7CWCsWbMma3dMMszR7xXJZSRoWQzDMNJ+/EtERERERERuR+doiYiIiIiIOJiCloiIiIiIiIMpaImIiIiIiDiYgpaIiIiIiIiDKWiJiIiIiIg4mIKWiIiIiIiIgyloiYiIiIiIOJiCloiIiIiIiIMpaImIiIiIiDiYgpaIiGR7Fovllj/Dhw/Psloeeugh2+N6enpSpUoVxo0bh2EYtnXWrl2LxWLh4sWLKe4fGBjI1KlTbbctFgtLly7N/MJFRCRL5XN2ASIiIrfz77//2n5fuHAhw4YNY+/evbY2X19f2++GYZCQkEC+fJn3Ede9e3dGjhxJTEwMv/zyCz169KBgwYK8+uqrmfaYIiKSs+iIloiIZHslS5a0/RQoUACLxWK7vWfPHvLnz89PP/1EvXr18PDwYN26dXTt2pU2bdrYbadv37489NBDtttWq5Vx48ZRvnx5vLy8qF27Nt98881t6/H29qZkyZKUK1eObt26cffddxMeHu7gvRYRkZxMR7RERCRXGDRoEJMnT6ZChQoUKlQoTfcZN24cX375JTNnzqRy5cr89ttvPP/88xQrVoymTZve9v6GYbBu3Tr27NlD5cqV73QXREQkF1HQEhGRXGHkyJG0aNEizevHxMQwduxYfv75Zxo3bgxAhQoVWLduHR999NEtg9aHH37I7NmziY2NJS4uDk9PT3r37n3H+yAiIrmHgpaIiOQK9evXT9f6Bw4cIDo6OkU4i42NpW7dure8b6dOnRgyZAgXLlwgLCyMJk2a0KRJk3TXLCIiuZeCloiI5Ao+Pj52t11cXOxmAgSIi4uz/X7lyhUAfvjhB0qXLm23noeHxy0fq0CBAlSqVAmARYsWUalSJe69916CgoIA8PPzA+DSpUsULFjQ7r4XL16kQIECadwrERHJqRS0REQkVypWrBg7d+60a9u2bRtubm4A1KhRAw8PDyIjI9N0PtbN+Pr60qdPH/r378/WrVuxWCxUrlwZFxcXtmzZQrly5WzrHjp0iEuXLlGlSpUMP56IiOQMCloiIpIrNWvWjEmTJvH555/TuHFjvvzyS3bu3GkbFpg/f3769+/PG2+8gdVq5f777+fSpUusX78ePz8/QkJC0vxYL7/8MqNGjeLbb7+lXbt25M+fn5deeol+/fqRL18+atWqxbFjxxg4cCD33ntvimGGhw8fZtu2bXZtlStXTnGUTkREcg4FLRERyZWCg4MZOnQob775JtevX+eFF16gS5cu7Nixw7bOqFGjKFasGOPGjePQoUMULFiQe+65h7feeitdj1W4cGG6dOnC8OHDefrpp3FxcWHatGmMHz+egQMHcvToUUqWLEmLFi0YM2YMFovF7v6hoaEptvn7779z//33Z2znRUTE6SxG8gHsIiIiIiIickd0wWIREREREREHU9ASERERERFxMAUtERERERERB1PQEhERERERcTAFLREREREREQdT0BIREREREXEwBS0REREREREHU9ASERERERFxMAUtERERERERB1PQEhERERERcTAFLREREREREQf7H8lvUAvFUL6mAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nüî¨ Sample Breakdown (Interpretability Check):\nTrue RUL   | Pred RUL   | Low W    | Mid W    | High W   | Decision\n---------------------------------------------------------------------------\n0.15       | 0.19       | 0.17     | 0.48     | 0.35     | Mid\n0.24       | 0.19       | 0.16     | 0.48     | 0.36     | Mid\n0.17       | 0.19       | 0.16     | 0.48     | 0.36     | Mid\n0.19       | 0.19       | 0.16     | 0.48     | 0.36     | Mid\n0.24       | 0.19       | 0.16     | 0.48     | 0.36     | Mid\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:22:51.445342Z","iopub.execute_input":"2026-02-03T21:22:51.445656Z","iopub.status.idle":"2026-02-03T21:22:51.595605Z","shell.execute_reply.started":"2026-02-03T21:22:51.445630Z","shell.execute_reply":"2026-02-03T21:22:51.594888Z"}},"outputs":[{"name":"stdout","text":"best_lstm_model-window-100_model_pinn_data_all.pth\nbest_lstm_model-window-100_model_pinn_data_high.pth\nbest_lstm_model-window-100_model_pinn_data_low.pth\nbest_lstm_model-window-100_model_pinn_data_mid.pth\nbest_moe_model_window_100.pth\nhistory-from-3-window_99_model_pinn.png\nhistory-full-log-window_99_model_pinn.png\nhistory-full-window_99_model_pinn.png\nlast_model_window_100_model_pinn_data_all.pth\nlast_model_window_100_model_pinn_data_high.pth\nlast_model_window_100_model_pinn_data_low.pth\nlast_model_window_100_model_pinn_data_mid.pth\ntraining_metrics__window_99_model_pinn.npz\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:42.410124Z","iopub.execute_input":"2026-02-03T21:07:42.410446Z","iopub.status.idle":"2026-02-03T21:07:42.414518Z","shell.execute_reply.started":"2026-02-03T21:07:42.410414Z","shell.execute_reply":"2026-02-03T21:07:42.413887Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# for X_batch, _, y_rul_batch in test_loader:\n#     pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T21:07:42.415572Z","iopub.execute_input":"2026-02-03T21:07:42.415918Z","iopub.status.idle":"2026-02-03T21:07:42.426125Z","shell.execute_reply.started":"2026-02-03T21:07:42.415882Z","shell.execute_reply":"2026-02-03T21:07:42.425504Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}