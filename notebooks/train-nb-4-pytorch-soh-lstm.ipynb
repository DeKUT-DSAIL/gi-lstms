{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12908579,"sourceType":"datasetVersion","datasetId":5945289},{"sourceId":287936110,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook covers three model implementations:\n\n1. A physics based model based on the Gompertz function\n2. To train an LSTM model (data-driven-model) to predict a sequence (e.g., battery capacity over time)\n3. To train an LSTM model to predict a sequence (e.g., battery capacity over time) while ensuring it respects physical behavior modeled by the Gompertz function.\n\n\n## FIXES \n\n### Fix 1 : Added Sanity Check for val loss and val rmse\n\nExplanation:\nIssue\tFix\tWhy\ntorch.Tensor()\t‚Üí torch.tensor()\tThe lowercase version is the recommended constructor for creating a single-value tensor.\nMissing closing parenthesis\tAdded\tFixes syntax error.\navg_val_loss type\tEnsure it‚Äôs a scalar (float or int)\tIf it‚Äôs already a tensor, remove the outer torch.tensor() call.\n\n### Fix 2: Use log loss in calculation of metric charts\n### Fix 3: Update reproducibility\n### Fix 4: time GPU run","metadata":{"_uuid":"16c6b475-8271-444d-85dd-c0c90ba51a4e","_cell_guid":"5fbc7d50-bc9d-4e73-aba7-d7ef5fd55ae5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\nprint(\"# A: Import Libraries and set reproducibility\")\n# !git clone https://github.com/Yuri-Njathi/battery-lstm-ML.git\n# import sys\n# sys.path.append(\"battery-lstm-ML/\")\n\n\nimport torch\nimport numpy as np\nimport random\nimport os\nimport pandas as pd\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom typing import Callable, Optional\n\ndef set_seed(seed=42):\n    \"\"\"\n    Set all relevant random seeds to ensure full reproducibility.\n    \"\"\"\n    # 1. Set basic seeds\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if multiple GPUs\n    \n    # 2. Force deterministic behavior in cudnn\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False  # turn off auto-tuning\n    \n    # 3. Optional: make dataloaders deterministic\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'  # deterministic cublas (for CUDA >= 10.2)\n\n    print(f\"‚úÖ Reproducibility environment set with seed = {seed}\")\n\n# Call this once at the start, pull from assess\nset_seed(42)\n\n'''\nset mode i.e.\n0 == physics based\n1 == lstm (data driven) #SoH only \n2 == lstm (physics constrained)\n'''\nmode = 1\n\nif mode == 1:\n    model_columns = ['SoH']#,'rul','Cycle number']\nif mode == 2:\n    model_columns = ['SoH','Cycle number','k','a','b']\n\n\n\nprint(\"# B: Setup variables and functions\")\n# # Set variables\n# WINDOW_SIZE = 35\nmodel_type = ['lstm','seq2seq-lstm'][0]\n\ncutoff_soh = 0.70\n# Set Computing Environment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nINPUT_SIZE = len(model_columns)\nOUTPUT_SIZE =len(model_columns)#1 #controls how many values the lstm outputs\nnum_epochs = 80 #60\nbatch_size = 5120 #32\nnormalize_soh = False\nif normalize_soh:\n    soh_normalization_constant = 115.0 #115.0 may be better as it allows bounding between 0 and 1\n    cutoff_soh = cutoff_soh/soh_normalization_constant #set cutoff soh wrt to normalizer\nelse:\n    soh_normalization_constant = 1.0\n    cutoff_soh = cutoff_soh/soh_normalization_constant #set cutoff soh wrt to normalizer\n\nprint(\"Cutoff SoH : \",cutoff_soh)\n\ndef df_to_X_y_tensor(df, window_size=5,output_size=5):\n    '''\n    Converts a time series into (X, y) tensors for LSTM training.\n    \n    X shape: (num_samples, window_size, 1)\n    y shape: (num_samples, 1)\n    '''\n    if isinstance(df, (pd.DataFrame, pd.Series)):\n        df_as_np = df.to_numpy()\n    else:\n        df_as_np = df  # Assume already numpy\n\n    X, y = [], []\n    for i in range(len(df_as_np) - window_size):\n        X.append([[val] for val in df_as_np[i:i+window_size]])\n        #y.append([df_as_np[i + window_size:i + window_size+output_size]])\n        y.append([[val] for val in df_as_np[i + window_size:i + window_size+1]]) #next cycle\n    X,y = np.array(X),np.array(y)\n    X_tensor = torch.tensor(X, dtype=torch.float32)#.squeeze()\n    y_tensor = torch.tensor(y, dtype=torch.float32)#.squeeze()\n    return X_tensor, y_tensor\n\ndef get_x_y_lists(paths):\n    X_list,y_list = [],[]\n    for path in paths:\n        print(path)\n        df = pd.read_csv(path)\n        df['Cycle number'] = df['Cycle number']/10000\n        df['rul'] = df['rul']\n        #normalize SoH\n        df['SoH'] =  df['SoH']/soh_normalization_constant\n        df.index = df['Cycle number']\n        SoH = df[model_columns]\n        X, y = df_to_X_y_tensor(SoH, window_size=WINDOW_SIZE,output_size=OUTPUT_SIZE)\n        X_list.append(X)\n        y_list.append(y)\n    return X_list,y_list\n\ndef give_paths_get_loaders(paths,data_type,shuffle=False):\n    X_list, y_list = get_x_y_lists(paths)\n\n    if INPUT_SIZE == 1:\n        # Concatenate all X and y\n        X_1,y_1 = torch.cat(X_list, dim=0).squeeze(-1),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE)\n    else:\n        X_1,y_1 = torch.cat(X_list, dim=0), torch.cat(y_list, dim=0)#.view(-1, OUTPUT_SIZE)\n    \n    print(f\"X_{data_type} , y_{data_type} shapes : \",X_1.shape, y_1.shape)\n    \n    #DataLoader\n    print(\"load : \")\n    loader = DataLoader(TensorDataset(X_1, y_1), batch_size=32, shuffle=shuffle)\n    print(f\"{data_type}loader lengths : \",loader.__len__())\n    return loader,X_1,y_1\n\n\n\nprint(\"## üß† Model Architecture\")\n# class LSTMModel(nn.Module):\n#     def __init__(self, input_size, hidden_size, mid_size, output_size):\n#         super().__init__()\n#         self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n#         self.relu = nn.ReLU()\n#         self.fc1 = nn.Linear(hidden_size, mid_size)\n#         self.fc2 = nn.Linear(mid_size, output_size)\n\n#     def forward(self, x):\n#         out, _ = self.lstm(x)\n#         last_out = out[:, -1, :]\n#         mid = self.relu(self.fc1(last_out))\n#         return self.fc2(mid)\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size=5, hidden_size=64, output_size=5):\n        super(LSTMModel, self).__init__()\n        \n        # LSTM: input_size=5 match your features\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        \n        # Linear Layer: Maps hidden_size (64) -> output_size (5)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # x shape: [32, 10, 5]\n        \n        # Run LSTM\n        # lstm_out shape: [32, 10, 64]\n        lstm_out, _ = self.lstm(x)\n        \n        # Take the last time step only\n        last_time_step = lstm_out[:, -1, :] \n        # last_time_step shape: [32, 64]\n        \n        # Project to 5 output features\n        prediction = self.fc(last_time_step)\n        # prediction shape: [32, 5]\n        \n        return prediction\nclass Seq2SeqLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, mid_size, output_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(hidden_size, mid_size)\n        self.fc2 = nn.Linear(mid_size, output_size)\n\n    def forward(self, x):\n        # out: [batch_size, seq_len, hidden_size]\n        out, _ = self.lstm(x)\n\n        # Apply the linear layers to each timestep\n        mid = self.relu(self.fc1(out))   # shape: [batch, seq_len, mid_size]\n        out_seq = self.fc2(mid)          # shape: [batch, seq_len, output_size]\n\n        return out_seq\n\n\n    #on initial tensorflow experiments I used 1,64,1,1 for those values.\n\n### TEST ON SEQUENTIAL MODEL ###\n# model(torch.Tensor([[86.4707],[86.4150],[86.3590],[86.3035],[86.2506],[86.2512],[86.1954],[86.1403],[86.1427],[86.0904],[86.0373],[85.9772],[85.9743],[85.9198],[85.8654],[85.8090],[85.8077],[85.7524],[85.6986],[85.6407],[85.5883],[85.5882],[85.6112],[85.4756],[85.4753],[85.4187],[85.3639],[85.3086],[85.3098],[85.3628],[85.1723],[85.1430],[85.1444],[85.0896],[85.0364]]))","metadata":{"_uuid":"d905f68b-0092-4d8e-980e-bd9d93d36ec8","_cell_guid":"bc0a48fc-f795-4b2c-a672-5942fa073d9a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-22T14:59:04.712867Z","iopub.execute_input":"2026-01-22T14:59:04.713228Z","iopub.status.idle":"2026-01-22T14:59:08.817176Z","shell.execute_reply.started":"2026-01-22T14:59:04.713195Z","shell.execute_reply":"2026-01-22T14:59:08.816372Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ‚ÄúNormalization‚Äù ‚â† ‚Äúscaling to [0,1]‚Äù.\n\n#### It simply means rescaling values to a stable, comparable numerical range.","metadata":{"_uuid":"5a5cf86c-22c4-4f30-a2ea-27e58384e19a","_cell_guid":"bfed866a-86b9-4ad2-9f29-45f1fac22912","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# C: Setup Train, Val and Test Loaders\n# 0. Data\n\nEach `X_train` is of shape `(num_samples, window_size)`\n\nEach `y_train` is of shape `(num_samples,)` (usually next value prediction)","metadata":{"_uuid":"e5c11278-599b-4fcf-a3a3-4b5cffc4aab6","_cell_guid":"ae57eb62-c436-4826-96f2-cc55c36376ee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"##  üß∞ Convert to Tensors for LSTM\nLSTM expects input shape: (batch_size, sequence_length, num_features)\n\nLet‚Äôs reshape the data and convert it:","metadata":{"_uuid":"e55a3991-26a1-4a5a-9aa1-a4095a055a67","_cell_guid":"e87496f0-d330-454c-82f9-86f0f16e746d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T14:59:08.818312Z","iopub.execute_input":"2026-01-22T14:59:08.818812Z","iopub.status.idle":"2026-01-22T14:59:08.822505Z","shell.execute_reply.started":"2026-01-22T14:59:08.818786Z","shell.execute_reply":"2026-01-22T14:59:08.821629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"WINDOW_SIZES = [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100] #[i for i in range (5,100,5)]\nprint(\"WINDOW SIZES TO TEST : \",WINDOW_SIZES,len(WINDOW_SIZES))\n\n\n# Make list of CSV paths\nmain_files_path = '/kaggle/input/generate-hust-data-gompertz-k-a-b/'\n#/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\ncsv_files = os.listdir(main_files_path)\ncsv_files = [f for f in csv_files if re.match(r'^\\d', f) and f.endswith('-hust_gompertz_params.csv')]\n\n#BatteryML like train-val-test split\ncsv_files = [f.removesuffix('-hust_gompertz_params.csv') for f in csv_files]\nprint(csv_files)\n\ntrain_ids = [\n    '1-3',  '1-4',  '1-5',  '1-6',  '1-7',  '1-8',  '2-2',  '2-3',\n    '2-4',  '2-6',  '2-7',  '2-8',  '3-2',  '3-3',  '3-4',  '3-5',\n    '3-6',  '3-7',  '3-8',  '4-1',  '4-2',  '4-3',  '4-4',  '4-6',\n    '4-7',  '4-8',  '5-1',  '5-2',  '5-4',  '5-5',  '5-6',  '5-7',\n    '6-3',  '6-4',  '6-5',  '7-1',  '7-2',  '7-3',  '7-4',  '7-7',\n    '7-8',  '8-2',  '8-3',  '8-4',  '8-7',  '9-1',  '9-2',  '9-3',\n    '9-5',  '9-7',  '9-8',  '10-2', '10-3', '10-5', '10-8']\n\ntest_ids = [f for f in csv_files if f not in train_ids]\n\nprint(test_ids,len(test_ids))\n\n#csv_paths = [os.path.join(main_files_path, file) for file in csv_files]\n#separate according to train, val and test\ntrain_paths = [os.path.join(main_files_path, file+'-hust_gompertz_params.csv') for file in train_ids]\n\ntesting_paths = [os.path.join(main_files_path, file+'-hust_gompertz_params.csv') for file in test_ids]\n\nval_paths = testing_paths[:int(len(testing_paths)*0.5)]\ntest_paths = testing_paths[int(len(testing_paths)*0.5):]\n\nprint(len(train_paths), len(val_paths), len(test_paths))","metadata":{"_uuid":"764511b2-155c-4f27-9abb-b9020665168e","_cell_guid":"da093279-8ac7-4672-b25f-7a87931a0801","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-22T14:59:08.824032Z","iopub.execute_input":"2026-01-22T14:59:08.824280Z","iopub.status.idle":"2026-01-22T14:59:08.849812Z","shell.execute_reply.started":"2026-01-22T14:59:08.824247Z","shell.execute_reply":"2026-01-22T14:59:08.848937Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for WINDOW_SIZE in WINDOW_SIZES:\n    #OUTPUT_SIZE = WINDOW_SIZE\n    print(\"## üß† Model\")\n    if model_type == 'lstm':\n        model = LSTMModel(input_size=INPUT_SIZE, output_size=OUTPUT_SIZE).to(device) # values for multioutput model\n        last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n        print(\"Last model window : \",last_model_path)\n    if model_type == 'seq2seq-lstm':\n        model = Seq2SeqLSTM(INPUT_SIZE,64,32,OUTPUT_SIZE).to(device) # values from previously working tensorflow model # 1, 64, 8, 1 \n        last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n        print(\"Last model window : \",last_model_path)\n    \n    #### PREPARE ALL DATA ####\n    ### DONT RUN AS LOOP\n    def give_paths_get_loaders(paths,data_type,shuffle=False):\n        X_list, y_list = get_x_y_lists(paths)\n    \n        if INPUT_SIZE == 1:\n            # Concatenate all X and y\n            X_1,y_1 = torch.cat(X_list, dim=0).squeeze(-1),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE)\n        else:\n            X_1,y_1 = torch.cat(X_list, dim=0).squeeze(2),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE)\n        \n        print(f\"X_{data_type} , y_{data_type} shapes : \",X_1.shape, y_1.shape)\n        \n        #DataLoader\n        print(\"load : \")\n        loader = DataLoader(TensorDataset(X_1, y_1), batch_size=32, shuffle=shuffle)\n        print(f\"{data_type}loader lengths : \",loader.__len__())\n        return loader,X_1,y_1\n    \n    data_use = {\n        0:[\"train\"],1:[\"val\"],2:[\"test\"]\n    }\n    train_loader,X_train,y_train = give_paths_get_loaders(train_paths,data_use[0],shuffle=True)\n    val_loader,X_val,y_val= give_paths_get_loaders(val_paths,data_use[1])\n    test_loader,X_test,y_test = give_paths_get_loaders(test_paths,data_use[2])\n    \n    print('''##\n    ### üìà Gompertz Function (Physics Law)\n    \n    * `x`: Time (or cycle number)\n    \n    * `k`: Max value (e.g., max capacity)\n    \n    * `a`, `b`: Shape parameters''')\n    \n    def gompertz_func(x, k, a, b):\n        return k * torch.exp(-a * torch.exp(-b * x))\n    \n    print(\"## üß† Loss Functions\\n\")\n    \n    print('''## ‚öôÔ∏è 1. Data-Informed Loss Function\n    a data loss (what the LSTM learns from data)\n    \n    * Mean Squared Error for Training\n    * RMSE for autoregressive approximation of compound error\n    \n    ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n    You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n    \n    * `alpha`: controls how strongly physics is enforced.''')\n        \n    def pinn_loss(prediction, target, x, k, a, b, alpha=1.0):\n        data_loss = F.mse_loss(prediction, target)\n        physics_pred = gompertz_func(x, k, a, b) #this needs to be refined !!!\n        physics_loss = F.mse_loss(prediction, physics_pred)\n        return data_loss + alpha * physics_loss, data_loss.item(), physics_loss.item()\n    \n    def data_loss_func(prediction, target, x, k, a, b, alpha=1.0):\n        data_loss = F.mse_loss(prediction, target)\n        return data_loss, data_loss.item() , None\n    \n    def physics_loss(prediction, target, x, k, a, b, alpha=1.0):\n        #data_loss = F.mse_loss(prediction, target)\n        physics_pred = gompertz_func(x, k, a, b) #this needs to be refined !!!\n        physics_loss = F.mse_loss(prediction, physics_pred)\n        return alpha * physics_loss, physics_loss.item(), None\n    \n    epoch = 0\n    avg_train_loss = 0.0\n    avg_val_loss = 0.0\n    data_loss = 0.0\n    phys_loss = None  # Only set if you're using physics loss\n    \n    if mode == 0:\n        criterion = physics_loss #??\n        best_model_path = f'best_pysics_model-window-{WINDOW_SIZE}.pth' #??\n        loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n    if mode == 1:\n        criterion = data_loss_func\n        best_model_path = f'best_lstm_model-window-{WINDOW_SIZE}.pth'\n        loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f}\"\n    if mode == 2:\n        # phys_loss = 0.0 # Only set if you're using physics loss\n        # criterion = pinn_loss\n        # best_model_path = f'best_pinn_lstm_model-window-{WINDOW_SIZE}.pth'\n        # loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n        criterion = data_loss_func\n        best_model_path = f'best_lstm_model-window-{WINDOW_SIZE}.pth'\n        loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f}\"\n    \n    print(\"## üõ†Ô∏è Parameter Strategy\")\n    \n    k = nn.Parameter(torch.tensor(1.0))\n    a = nn.Parameter(torch.tensor(0.1))\n    b = nn.Parameter(torch.tensor(0.1))\n    # Include in optimizer\n    optimizer = torch.optim.Adam(list(model.parameters()) + [k, a, b], lr=1e-3)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=20)\n    \n    print(\"## üîÅ Training Loop\")\n    def compute_rmse(pred, target):\n        return torch.sqrt(F.mse_loss(pred, target))\n        \n    train_losses = []\n    val_losses = []\n    val_rmses = []\n    data_losses = []\n    phys_losses = []\n    \n    best_val_loss = float('inf')\n    best_epoch = 0\n    \n    # Provide as a train function\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        total_data_loss = 0\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            #Set computing environment\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            #Predict\n            y_pred = model(X_batch)\n    \n            # Create x_seq for physics loss\n            x_seq = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_batch.size(0), 1).to(X_batch.device)\n    \n            loss, data_loss, phys_loss = criterion(y_pred, y_batch, x_seq, k, a, b, alpha=0.2)\n            loss.backward()\n            optimizer.step()\n            total_data_loss += data_loss#.item() #TRY\n            total_loss += loss.item()\n        \n        # Adjust learning rate\n        scheduler.step()\n        avg_data_loss = total_data_loss / len(train_loader)\n        avg_train_loss = total_loss / len(train_loader)\n    \n        # ---- Validation Pass ----\n        model.eval()\n        val_loss_total = 0.0\n        val_rmse = 0.0\n        with torch.no_grad():\n            for X_val_batch, y_val_batch in val_loader:\n                #Set computing environment\n                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n                #Predict\n                y_val_pred = model(X_val_batch)\n    \n                #ADD A METRIC\n                val_rmse += compute_rmse(y_val_pred, y_val_batch)\n                \n                x_seq_val = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_val_batch.size(0), 1).to(X_val_batch.device)\n                val_loss, _, _ = criterion(y_val_pred, y_val_batch, x_seq_val, k, a, b, alpha=0.2)\n    \n                val_loss_total += val_loss.item()\n    \n        avg_val_loss = val_loss_total / len(val_loader)\n        avg_val_rmse = val_rmse / len(val_loader)\n        # Save model if validation improves\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            best_epoch = epoch\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"‚úÖ Saved best model at epoch {epoch+1} (Val Loss = {best_val_loss:.8f})\")\n        if epoch+1 == num_epochs:\n            torch.save(model.state_dict(),last_model_path)\n            print(f\"‚úÖ Saved last model at epoch {epoch+1} \")\n    \n    \n        if mode == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f}\")\n        if mode == 1:\n            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f} | ‚àö(Val Loss) = {torch.sqrt(torch.tensor(avg_val_loss)):.8f}\")\n        if mode == 2:\n            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f} | ‚àö(Val Loss) = {torch.sqrt(torch.tensor(avg_val_loss)):.8f}\")\n            #print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f}\")\n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        val_rmses.append(avg_val_rmse.item())  # assuming avg_val_rmse is a tensor\n        data_losses.append(avg_data_loss)   # assuming this is the last batch's data loss\n        phys_losses.append(phys_loss)   # assuming this is the last batch's physics loss\n    \n    model.load_state_dict(torch.load(best_model_path))\n    \n    np.savez(f\"training_metrics__window_{WINDOW_SIZE}_model_{model_type}.npz\",\n             train_losses=train_losses,\n             val_losses=val_losses,\n             val_rmses=val_rmses,\n             data_losses=data_losses,\n             phys_losses=phys_losses)\n    print(\"Plot losses after training 3:\")\n    plt.figure(figsize=(12, 6))\n    plt.plot(train_losses[3:], label=\"Train Loss\")\n    plt.plot(val_losses[3:], label=\"Val Loss\")\n    plt.plot(val_rmses[3:], label=\"Val RMSE\")\n    #plt.plot(data_losses[3:], label=\"Data Loss\")\n    plt.plot(phys_losses[3:], label=\"Physics Loss\")\n    x_line = best_epoch \n    plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n    plt.text(x_line + 0.05, best_val_loss, 'Saved Model', rotation=45, color='red')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Training & Validation Metrics\")\n    plt.grid(True)\n    plt.savefig(fname = f\"history-from-3-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n    plt.show()\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Val Loss\")\n    plt.plot(val_rmses, label=\"Val RMSE\")\n    #plt.plot(data_losses, label=\"Data Loss\")\n    plt.plot(phys_losses, label=\"Physics Loss\")\n    plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n    plt.text(x_line + 0.05, best_val_loss, 'Saved Model', rotation=45, color='red')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Training & Validation Metrics\")\n    plt.grid(True)\n    plt.savefig(fname = f\"history-full-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n    plt.show()\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Val Loss\")\n    plt.plot(val_rmses, label=\"Val RMSE\")\n    #plt.plot(data_losses, label=\"Data Loss\")\n    #plt.plot(phys_losses, label=\"Physics Loss\")\n    plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n    plt.text(x_line, best_val_loss*10, 'Saved Model', rotation=30, color='red')\n    plt.yscale('log')  # visualize on log scale\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Training & Validation Metrics\")\n    plt.grid(True)\n    plt.savefig(fname = f\"history-full-log-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T14:59:08.851019Z","iopub.execute_input":"2026-01-22T14:59:08.851303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model(X_batch).shape","metadata":{"_uuid":"ab16f4e0-1acc-499b-8d8e-43e88dc79030","_cell_guid":"35554e79-62f0-483e-95f8-aab488cb1af6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_batch.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}