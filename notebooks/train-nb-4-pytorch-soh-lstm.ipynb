{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12908579,"sourceType":"datasetVersion","datasetId":5945289},{"sourceId":287936110,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook covers three model implementations:\n\n1. A physics based model based on the Gompertz function\n2. To train an LSTM model (data-driven-model) to predict a sequence (e.g., battery capacity over time)\n3. To train an LSTM model to predict a sequence (e.g., battery capacity over time) while ensuring it respects physical behavior modeled by the Gompertz function.\n\n\n## FIXES \n\n### Fix 1 : Added Sanity Check for val loss and val rmse\n\nExplanation:\nIssue\tFix\tWhy\ntorch.Tensor()\t‚Üí torch.tensor()\tThe lowercase version is the recommended constructor for creating a single-value tensor.\nMissing closing parenthesis\tAdded\tFixes syntax error.\navg_val_loss type\tEnsure it‚Äôs a scalar (float or int)\tIf it‚Äôs already a tensor, remove the outer torch.tensor() call.\n\n### Fix 2: Use log loss in calculation of metric charts\n### Fix 3: Update reproducibility\n### Fix 4: time GPU run","metadata":{"_uuid":"16c6b475-8271-444d-85dd-c0c90ba51a4e","_cell_guid":"5fbc7d50-bc9d-4e73-aba7-d7ef5fd55ae5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\nprint(\"# A: Import Libraries and set reproducibility\")\n# !git clone https://github.com/Yuri-Njathi/battery-lstm-ML.git\n# import sys\n# sys.path.append(\"battery-lstm-ML/\")\n\n\nimport torch\nimport numpy as np\nimport random\nimport os\nimport pandas as pd\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom typing import Callable, Optional\n\ndef set_seed(seed=42):\n    \"\"\"\n    Set all relevant random seeds to ensure full reproducibility.\n    \"\"\"\n    # 1. Set basic seeds\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if multiple GPUs\n    \n    # 2. Force deterministic behavior in cudnn\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False  # turn off auto-tuning\n    \n    # 3. Optional: make dataloaders deterministic\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'  # deterministic cublas (for CUDA >= 10.2)\n\n    print(f\"‚úÖ Reproducibility environment set with seed = {seed}\")\n\n# Call this once at the start, pull from assess\nset_seed(42)\n\n'''\nset mode i.e.\n0 == physics based\n1 == lstm (data driven) #SoH only \n2 == lstm (physics constrained)\n'''\nmode = 1\n\nif mode == 1:\n    model_columns = ['SoH']#,'rul','Cycle number']\nif mode == 2:\n    model_columns = ['SoH','Cycle number','k','a','b']\n\n\n\nprint(\"# B: Setup variables and functions\")\n# # Set variables\n# WINDOW_SIZE = 35\nmodel_type = ['lstm','seq2seq-lstm'][0]\n\ncutoff_soh = 0.70\n# Set Computing Environment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nINPUT_SIZE = len(model_columns)\nOUTPUT_SIZE =len(model_columns)#1 #controls how many values the lstm outputs\nnum_epochs = 80 #60\nbatch_size = 5120 #32\nnormalize_soh = False\nif normalize_soh:\n    soh_normalization_constant = 115.0 #115.0 may be better as it allows bounding between 0 and 1\n    cutoff_soh = cutoff_soh/soh_normalization_constant #set cutoff soh wrt to normalizer\nelse:\n    soh_normalization_constant = 1.0\n    cutoff_soh = cutoff_soh/soh_normalization_constant #set cutoff soh wrt to normalizer\n\nprint(\"Cutoff SoH : \",cutoff_soh)\n\ndef df_to_X_y_tensor(df, window_size=5,output_size=5):\n    '''\n    Converts a time series into (X, y) tensors for LSTM training.\n    \n    X shape: (num_samples, window_size, 1)\n    y shape: (num_samples, 1)\n    '''\n    if isinstance(df, (pd.DataFrame, pd.Series)):\n        df_as_np = df.to_numpy()\n    else:\n        df_as_np = df  # Assume already numpy\n\n    X, y = [], []\n    for i in range(len(df_as_np) - window_size):\n        X.append([[val] for val in df_as_np[i:i+window_size]])\n        #y.append([df_as_np[i + window_size:i + window_size+output_size]])\n        y.append([[val] for val in df_as_np[i + window_size:i + window_size+1]]) #next cycle\n    X,y = np.array(X),np.array(y)\n    X_tensor = torch.tensor(X, dtype=torch.float32)#.squeeze()\n    y_tensor = torch.tensor(y, dtype=torch.float32)#.squeeze()\n    return X_tensor, y_tensor\n\ndef get_x_y_lists(paths):\n    X_list,y_list = [],[]\n    for path in paths:\n        print(path)\n        df = pd.read_csv(path)\n        df['Cycle number'] = df['Cycle number']/10000\n        df['rul'] = df['rul']\n        #normalize SoH\n        df['SoH'] =  df['SoH']/soh_normalization_constant\n        df.index = df['Cycle number']\n        SoH = df[model_columns]\n        X, y = df_to_X_y_tensor(SoH, window_size=WINDOW_SIZE,output_size=OUTPUT_SIZE)\n        X_list.append(X)\n        y_list.append(y)\n    return X_list,y_list\n\ndef give_paths_get_loaders(paths,data_type,shuffle=False):\n    X_list, y_list = get_x_y_lists(paths)\n\n    if INPUT_SIZE == 1:\n        # Concatenate all X and y\n        X_1,y_1 = torch.cat(X_list, dim=0).squeeze(-1),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE)\n    else:\n        X_1,y_1 = torch.cat(X_list, dim=0), torch.cat(y_list, dim=0)#.view(-1, OUTPUT_SIZE)\n    \n    print(f\"X_{data_type} , y_{data_type} shapes : \",X_1.shape, y_1.shape)\n    \n    #DataLoader\n    print(\"load : \")\n    loader = DataLoader(TensorDataset(X_1, y_1), batch_size=32, shuffle=shuffle)\n    print(f\"{data_type}loader lengths : \",loader.__len__())\n    return loader,X_1,y_1\n\n\n\nprint(\"## üß† Model Architecture\")\n# class LSTMModel(nn.Module):\n#     def __init__(self, input_size, hidden_size, mid_size, output_size):\n#         super().__init__()\n#         self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n#         self.relu = nn.ReLU()\n#         self.fc1 = nn.Linear(hidden_size, mid_size)\n#         self.fc2 = nn.Linear(mid_size, output_size)\n\n#     def forward(self, x):\n#         out, _ = self.lstm(x)\n#         last_out = out[:, -1, :]\n#         mid = self.relu(self.fc1(last_out))\n#         return self.fc2(mid)\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size=5, hidden_size=64, output_size=5):\n        super(LSTMModel, self).__init__()\n        \n        # LSTM: input_size=5 match your features\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        \n        # Linear Layer: Maps hidden_size (64) -> output_size (5)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # x shape: [32, 10, 5]\n        \n        # Run LSTM\n        # lstm_out shape: [32, 10, 64]\n        lstm_out, _ = self.lstm(x)\n        \n        # Take the last time step only\n        last_time_step = lstm_out[:, -1, :] \n        # last_time_step shape: [32, 64]\n        \n        # Project to 5 output features\n        prediction = self.fc(last_time_step)\n        # prediction shape: [32, 5]\n        \n        return prediction\nclass Seq2SeqLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, mid_size, output_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(hidden_size, mid_size)\n        self.fc2 = nn.Linear(mid_size, output_size)\n\n    def forward(self, x):\n        # out: [batch_size, seq_len, hidden_size]\n        out, _ = self.lstm(x)\n\n        # Apply the linear layers to each timestep\n        mid = self.relu(self.fc1(out))   # shape: [batch, seq_len, mid_size]\n        out_seq = self.fc2(mid)          # shape: [batch, seq_len, output_size]\n\n        return out_seq\n\n\n    #on initial tensorflow experiments I used 1,64,1,1 for those values.\n\n### TEST ON SEQUENTIAL MODEL ###\n# model(torch.Tensor([[86.4707],[86.4150],[86.3590],[86.3035],[86.2506],[86.2512],[86.1954],[86.1403],[86.1427],[86.0904],[86.0373],[85.9772],[85.9743],[85.9198],[85.8654],[85.8090],[85.8077],[85.7524],[85.6986],[85.6407],[85.5883],[85.5882],[85.6112],[85.4756],[85.4753],[85.4187],[85.3639],[85.3086],[85.3098],[85.3628],[85.1723],[85.1430],[85.1444],[85.0896],[85.0364]]))","metadata":{"_uuid":"d905f68b-0092-4d8e-980e-bd9d93d36ec8","_cell_guid":"bc0a48fc-f795-4b2c-a672-5942fa073d9a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-28T13:00:10.253626Z","iopub.execute_input":"2026-01-28T13:00:10.253958Z","iopub.status.idle":"2026-01-28T13:00:14.255129Z","shell.execute_reply.started":"2026-01-28T13:00:10.253933Z","shell.execute_reply":"2026-01-28T13:00:14.254330Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"# A: Import Libraries and set reproducibility\n‚úÖ Reproducibility environment set with seed = 42\n# B: Setup variables and functions\nCutoff SoH :  0.7\n## üß† Model Architecture\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"#### ‚ÄúNormalization‚Äù ‚â† ‚Äúscaling to [0,1]‚Äù.\n\n#### It simply means rescaling values to a stable, comparable numerical range.","metadata":{"_uuid":"5a5cf86c-22c4-4f30-a2ea-27e58384e19a","_cell_guid":"bfed866a-86b9-4ad2-9f29-45f1fac22912","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# C: Setup Train, Val and Test Loaders\n# 0. Data\n\nEach `X_train` is of shape `(num_samples, window_size)`\n\nEach `y_train` is of shape `(num_samples,)` (usually next value prediction)","metadata":{"_uuid":"e5c11278-599b-4fcf-a3a3-4b5cffc4aab6","_cell_guid":"ae57eb62-c436-4826-96f2-cc55c36376ee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"##  üß∞ Convert to Tensors for LSTM\nLSTM expects input shape: (batch_size, sequence_length, num_features)\n\nLet‚Äôs reshape the data and convert it:","metadata":{"_uuid":"e55a3991-26a1-4a5a-9aa1-a4095a055a67","_cell_guid":"e87496f0-d330-454c-82f9-86f0f16e746d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:00:14.256108Z","iopub.execute_input":"2026-01-28T13:00:14.256545Z","iopub.status.idle":"2026-01-28T13:00:14.260691Z","shell.execute_reply.started":"2026-01-28T13:00:14.256520Z","shell.execute_reply":"2026-01-28T13:00:14.259986Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"WINDOW_SIZES = [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100,500,1000] #[i for i in range (5,100,5)]\nprint(\"WINDOW SIZES TO TEST : \",WINDOW_SIZES,len(WINDOW_SIZES))\n\n\n# Make list of CSV paths\nmain_files_path = '/kaggle/input/generate-hust-data-gompertz-k-a-b/'\n#/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\ncsv_files = os.listdir(main_files_path)\ncsv_files = [f for f in csv_files if re.match(r'^\\d', f) and f.endswith('-hust_gompertz_params.csv')]\n\n#BatteryML like train-val-test split\ncsv_files = [f.removesuffix('-hust_gompertz_params.csv') for f in csv_files]\nprint(csv_files)\n\ntrain_ids = [\n    '1-3',  '1-4',  '1-5',  '1-6',  '1-7',  '1-8',  '2-2',  '2-3',\n    '2-4',  '2-6',  '2-7',  '2-8',  '3-2',  '3-3',  '3-4',  '3-5',\n    '3-6',  '3-7',  '3-8',  '4-1',  '4-2',  '4-3',  '4-4',  '4-6',\n    '4-7',  '4-8',  '5-1',  '5-2',  '5-4',  '5-5',  '5-6',  '5-7',\n    '6-3',  '6-4',  '6-5',  '7-1',  '7-2',  '7-3',  '7-4',  '7-7',\n    '7-8',  '8-2',  '8-3',  '8-4',  '8-7',  '9-1',  '9-2',  '9-3',\n    '9-5',  '9-7',  '9-8',  '10-2', '10-3', '10-5', '10-8']\n\ntest_ids = [f for f in csv_files if f not in train_ids]\n\nprint(test_ids,len(test_ids))\n\n#csv_paths = [os.path.join(main_files_path, file) for file in csv_files]\n#separate according to train, val and test\ntrain_paths = [os.path.join(main_files_path, file+'-hust_gompertz_params.csv') for file in train_ids]\n\ntesting_paths = [os.path.join(main_files_path, file+'-hust_gompertz_params.csv') for file in test_ids]\n\nval_paths = testing_paths[:int(len(testing_paths)*0.5)]\ntest_paths = testing_paths[int(len(testing_paths)*0.5):]\n\nprint(len(train_paths), len(val_paths), len(test_paths))","metadata":{"_uuid":"764511b2-155c-4f27-9abb-b9020665168e","_cell_guid":"da093279-8ac7-4672-b25f-7a87931a0801","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-28T13:00:14.262304Z","iopub.execute_input":"2026-01-28T13:00:14.262567Z","iopub.status.idle":"2026-01-28T13:00:14.293667Z","shell.execute_reply.started":"2026-01-28T13:00:14.262548Z","shell.execute_reply":"2026-01-28T13:00:14.292735Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"WINDOW SIZES TO TEST :  [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100, 500, 1000] 16\n['6-6', '8-7', '8-6', '9-1', '10-1', '6-8', '8-8', '10-7', '3-5', '5-1', '5-5', '7-1', '2-7', '10-6', '4-7', '7-7', '7-6', '4-5', '9-2', '10-4', '3-1', '9-7', '8-1', '10-8', '8-4', '4-6', '4-4', '3-8', '5-4', '9-6', '10-5', '7-8', '5-2', '9-8', '1-2', '5-6', '10-2', '2-6', '6-1', '2-4', '1-4', '4-1', '1-6', '6-2', '8-5', '5-7', '1-5', '1-8', '5-3', '6-5', '9-5', '4-8', '7-2', '2-5', '7-3', '9-3', '9-4', '8-2', '10-3', '6-3', '3-2', '7-5', '3-7', '2-3', '1-3', '8-3', '2-8', '7-4', '4-2', '6-4', '1-1', '3-3', '4-3', '3-4', '2-2', '1-7', '3-6']\n['6-6', '8-6', '10-1', '6-8', '8-8', '10-7', '10-6', '7-6', '4-5', '10-4', '3-1', '8-1', '9-6', '1-2', '6-1', '6-2', '8-5', '5-3', '2-5', '9-4', '7-5', '1-1'] 22\n55 11 11\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"for WINDOW_SIZE in WINDOW_SIZES:\n    #OUTPUT_SIZE = WINDOW_SIZE\n    print(\"## üß† Model\")\n    if model_type == 'lstm':\n        model = LSTMModel(input_size=INPUT_SIZE, output_size=OUTPUT_SIZE).to(device) # values for multioutput model\n        last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n        print(\"Last model window : \",last_model_path)\n    if model_type == 'seq2seq-lstm':\n        model = Seq2SeqLSTM(INPUT_SIZE,64,32,OUTPUT_SIZE).to(device) # values from previously working tensorflow model # 1, 64, 8, 1 \n        last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n        print(\"Last model window : \",last_model_path)\n    \n    #### PREPARE ALL DATA ####\n    ### DONT RUN AS LOOP\n    def give_paths_get_loaders(paths,data_type,shuffle=False):\n        X_list, y_list = get_x_y_lists(paths)\n    \n        if INPUT_SIZE == 1:\n            # Concatenate all X and y\n            X_1,y_1 = torch.cat(X_list, dim=0).squeeze(-1),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE)\n        else:\n            X_1,y_1 = torch.cat(X_list, dim=0).squeeze(2),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE)\n        \n        print(f\"X_{data_type} , y_{data_type} shapes : \",X_1.shape, y_1.shape)\n        \n        #DataLoader\n        print(\"load : \")\n        loader = DataLoader(TensorDataset(X_1, y_1), batch_size=32, shuffle=shuffle)\n        print(f\"{data_type}loader lengths : \",loader.__len__())\n        return loader,X_1,y_1\n    \n    data_use = {\n        0:[\"train\"],1:[\"val\"],2:[\"test\"]\n    }\n    train_loader,X_train,y_train = give_paths_get_loaders(train_paths,data_use[0],shuffle=True)\n    val_loader,X_val,y_val= give_paths_get_loaders(val_paths,data_use[1])\n    test_loader,X_test,y_test = give_paths_get_loaders(test_paths,data_use[2])\n    \n    print('''##\n    ### üìà Gompertz Function (Physics Law)\n    \n    * `x`: Time (or cycle number)\n    \n    * `k`: Max value (e.g., max capacity)\n    \n    * `a`, `b`: Shape parameters''')\n    \n    def gompertz_func(x, k, a, b):\n        return k * torch.exp(-a * torch.exp(-b * x))\n    \n    print(\"## üß† Loss Functions\\n\")\n    \n    print('''## ‚öôÔ∏è 1. Data-Informed Loss Function\n    a data loss (what the LSTM learns from data)\n    \n    * Mean Squared Error for Training\n    * RMSE for autoregressive approximation of compound error\n    \n    ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n    You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n    \n    * `alpha`: controls how strongly physics is enforced.''')\n        \n    def pinn_loss(prediction, target, x, k, a, b, alpha=1.0):\n        data_loss = F.mse_loss(prediction, target)\n        physics_pred = gompertz_func(x, k, a, b) #this needs to be refined !!!\n        physics_loss = F.mse_loss(prediction, physics_pred)\n        return data_loss + alpha * physics_loss, data_loss.item(), physics_loss.item()\n    \n    def data_loss_func(prediction, target, x, k, a, b, alpha=1.0):\n        data_loss = F.mse_loss(prediction, target)\n        return data_loss, data_loss.item() , None\n    \n    def physics_loss(prediction, target, x, k, a, b, alpha=1.0):\n        #data_loss = F.mse_loss(prediction, target)\n        physics_pred = gompertz_func(x, k, a, b) #this needs to be refined !!!\n        physics_loss = F.mse_loss(prediction, physics_pred)\n        return alpha * physics_loss, physics_loss.item(), None\n    \n    epoch = 0\n    avg_train_loss = 0.0\n    avg_val_loss = 0.0\n    data_loss = 0.0\n    phys_loss = None  # Only set if you're using physics loss\n    \n    if mode == 0:\n        criterion = physics_loss #??\n        best_model_path = f'best_pysics_model-window-{WINDOW_SIZE}.pth' #??\n        loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n    if mode == 1:\n        criterion = data_loss_func\n        best_model_path = f'best_lstm_model-window-{WINDOW_SIZE}.pth'\n        loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f}\"\n    if mode == 2:\n        # phys_loss = 0.0 # Only set if you're using physics loss\n        # criterion = pinn_loss\n        # best_model_path = f'best_pinn_lstm_model-window-{WINDOW_SIZE}.pth'\n        # loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n        criterion = data_loss_func\n        best_model_path = f'best_lstm_model-window-{WINDOW_SIZE}.pth'\n        loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f}\"\n    \n    print(\"## üõ†Ô∏è Parameter Strategy\")\n    \n    k = nn.Parameter(torch.tensor(1.0))\n    a = nn.Parameter(torch.tensor(0.1))\n    b = nn.Parameter(torch.tensor(0.1))\n    # Include in optimizer\n    optimizer = torch.optim.Adam(list(model.parameters()) + [k, a, b], lr=1e-3)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=20)\n    \n    print(\"## üîÅ Training Loop\")\n    def compute_rmse(pred, target):\n        return torch.sqrt(F.mse_loss(pred, target))\n        \n    train_losses = []\n    val_losses = []\n    val_rmses = []\n    data_losses = []\n    phys_losses = []\n    \n    best_val_loss = float('inf')\n    best_epoch = 0\n    \n    # Provide as a train function\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        total_data_loss = 0\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            #Set computing environment\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            #Predict\n            y_pred = model(X_batch)\n    \n            # Create x_seq for physics loss\n            x_seq = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_batch.size(0), 1).to(X_batch.device)\n    \n            loss, data_loss, phys_loss = criterion(y_pred, y_batch, x_seq, k, a, b, alpha=0.2)\n            loss.backward()\n            optimizer.step()\n            total_data_loss += data_loss#.item() #TRY\n            total_loss += loss.item()\n        \n        # Adjust learning rate\n        scheduler.step()\n        avg_data_loss = total_data_loss / len(train_loader)\n        avg_train_loss = total_loss / len(train_loader)\n    \n        # ---- Validation Pass ----\n        model.eval()\n        val_loss_total = 0.0\n        val_rmse = 0.0\n        with torch.no_grad():\n            for X_val_batch, y_val_batch in val_loader:\n                #Set computing environment\n                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n                #Predict\n                y_val_pred = model(X_val_batch)\n    \n                #ADD A METRIC\n                val_rmse += compute_rmse(y_val_pred, y_val_batch)\n                \n                x_seq_val = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_val_batch.size(0), 1).to(X_val_batch.device)\n                val_loss, _, _ = criterion(y_val_pred, y_val_batch, x_seq_val, k, a, b, alpha=0.2)\n    \n                val_loss_total += val_loss.item()\n    \n        avg_val_loss = val_loss_total / len(val_loader)\n        avg_val_rmse = val_rmse / len(val_loader)\n        # Save model if validation improves\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            best_epoch = epoch\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"‚úÖ Saved best model at epoch {epoch+1} (Val Loss = {best_val_loss:.8f})\")\n        if epoch+1 == num_epochs:\n            torch.save(model.state_dict(),last_model_path)\n            print(f\"‚úÖ Saved last model at epoch {epoch+1} \")\n    \n    \n        if mode == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f}\")\n        if mode == 1:\n            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f} | ‚àö(Val Loss) = {torch.sqrt(torch.tensor(avg_val_loss)):.8f}\")\n        if mode == 2:\n            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f} | ‚àö(Val Loss) = {torch.sqrt(torch.tensor(avg_val_loss)):.8f}\")\n            #print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f}\")\n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        val_rmses.append(avg_val_rmse.item())  # assuming avg_val_rmse is a tensor\n        data_losses.append(avg_data_loss)   # assuming this is the last batch's data loss\n        phys_losses.append(phys_loss)   # assuming this is the last batch's physics loss\n    \n    model.load_state_dict(torch.load(best_model_path))\n    \n    np.savez(f\"training_metrics__window_{WINDOW_SIZE}_model_{model_type}.npz\",\n             train_losses=train_losses,\n             val_losses=val_losses,\n             val_rmses=val_rmses,\n             data_losses=data_losses,\n             phys_losses=phys_losses)\n    print(\"Plot losses after training 3:\")\n    plt.figure(figsize=(12, 6))\n    plt.plot(train_losses[3:], label=\"Train Loss\")\n    plt.plot(val_losses[3:], label=\"Val Loss\")\n    plt.plot(val_rmses[3:], label=\"Val RMSE\")\n    #plt.plot(data_losses[3:], label=\"Data Loss\")\n    plt.plot(phys_losses[3:], label=\"Physics Loss\")\n    x_line = best_epoch \n    plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n    plt.text(x_line + 0.05, best_val_loss, 'Saved Model', rotation=45, color='red')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Training & Validation Metrics\")\n    plt.grid(True)\n    plt.savefig(fname = f\"history-from-3-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n    plt.show()\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Val Loss\")\n    plt.plot(val_rmses, label=\"Val RMSE\")\n    #plt.plot(data_losses, label=\"Data Loss\")\n    plt.plot(phys_losses, label=\"Physics Loss\")\n    plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n    plt.text(x_line + 0.05, best_val_loss, 'Saved Model', rotation=45, color='red')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Training & Validation Metrics\")\n    plt.grid(True)\n    plt.savefig(fname = f\"history-full-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n    plt.show()\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Val Loss\")\n    plt.plot(val_rmses, label=\"Val RMSE\")\n    #plt.plot(data_losses, label=\"Data Loss\")\n    #plt.plot(phys_losses, label=\"Physics Loss\")\n    plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n    plt.text(x_line, best_val_loss*10, 'Saved Model', rotation=30, color='red')\n    plt.yscale('log')  # visualize on log scale\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Training & Validation Metrics\")\n    plt.grid(True)\n    plt.savefig(fname = f\"history-full-log-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:00:14.294934Z","iopub.execute_input":"2026-01-28T13:00:14.295275Z","iopub.status.idle":"2026-01-28T13:07:12.294673Z","shell.execute_reply.started":"2026-01-28T13:00:14.295242Z","shell.execute_reply":"2026-01-28T13:07:12.293383Z"}},"outputs":[{"name":"stdout","text":"## üß† Model\nLast model window :  last_model_window_5_model_lstm.pth\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-8-hust_gompertz_params.csv\nX_['train'] , y_['train'] shapes :  torch.Size([104364, 5, 1]) torch.Size([104364, 1])\nload : \n['train']loader lengths :  3262\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-7-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/10-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\nX_['val'] , y_['val'] shapes :  torch.Size([21406, 5, 1]) torch.Size([21406, 1])\nload : \n['val']loader lengths :  669\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-6-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-1-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/6-2-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/8-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/5-3-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\nX_['test'] , y_['test'] shapes :  torch.Size([19967, 5, 1]) torch.Size([19967, 1])\nload : \n['test']loader lengths :  624\n##\n    ### üìà Gompertz Function (Physics Law)\n    \n    * `x`: Time (or cycle number)\n    \n    * `k`: Max value (e.g., max capacity)\n    \n    * `a`, `b`: Shape parameters\n## üß† Loss Functions\n\n## ‚öôÔ∏è 1. Data-Informed Loss Function\n    a data loss (what the LSTM learns from data)\n    \n    * Mean Squared Error for Training\n    * RMSE for autoregressive approximation of compound error\n    \n    ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n    You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n    \n    * `alpha`: controls how strongly physics is enforced.\n## üõ†Ô∏è Parameter Strategy\n## üîÅ Training Loop\n‚úÖ Saved best model at epoch 1 (Val Loss = 0.00010479)\nEpoch 1/80 | Train Loss=0.00629522 | Val Loss=0.00010479 | Data=0.00629522 | Val RMSE: 0.00843981 | ‚àö(Val Loss) = 0.01023665\n‚úÖ Saved best model at epoch 2 (Val Loss = 0.00008240)\nEpoch 2/80 | Train Loss=0.00325933 | Val Loss=0.00008240 | Data=0.00325933 | Val RMSE: 0.00787653 | ‚àö(Val Loss) = 0.00907757\n‚úÖ Saved best model at epoch 3 (Val Loss = 0.00000242)\nEpoch 3/80 | Train Loss=0.00318349 | Val Loss=0.00000242 | Data=0.00318349 | Val RMSE: 0.00132466 | ‚àö(Val Loss) = 0.00155684\nEpoch 4/80 | Train Loss=0.00319591 | Val Loss=0.00000305 | Data=0.00319591 | Val RMSE: 0.00151379 | ‚àö(Val Loss) = 0.00174626\nEpoch 5/80 | Train Loss=0.00317259 | Val Loss=0.00000283 | Data=0.00317259 | Val RMSE: 0.00138382 | ‚àö(Val Loss) = 0.00168237\nEpoch 6/80 | Train Loss=0.00316370 | Val Loss=0.00000261 | Data=0.00316370 | Val RMSE: 0.00131150 | ‚àö(Val Loss) = 0.00161456\n‚úÖ Saved best model at epoch 7 (Val Loss = 0.00000183)\nEpoch 7/80 | Train Loss=0.00316357 | Val Loss=0.00000183 | Data=0.00316357 | Val RMSE: 0.00110231 | ‚àö(Val Loss) = 0.00135335\n‚úÖ Saved best model at epoch 8 (Val Loss = 0.00000129)\nEpoch 8/80 | Train Loss=0.00313100 | Val Loss=0.00000129 | Data=0.00313100 | Val RMSE: 0.00091277 | ‚àö(Val Loss) = 0.00113521\nEpoch 9/80 | Train Loss=0.00314699 | Val Loss=0.00002875 | Data=0.00314699 | Val RMSE: 0.00466403 | ‚àö(Val Loss) = 0.00536154\n‚úÖ Saved best model at epoch 10 (Val Loss = 0.00000115)\nEpoch 10/80 | Train Loss=0.00314133 | Val Loss=0.00000115 | Data=0.00314133 | Val RMSE: 0.00091502 | ‚àö(Val Loss) = 0.00107287\n‚úÖ Saved best model at epoch 11 (Val Loss = 0.00000078)\nEpoch 11/80 | Train Loss=0.00313297 | Val Loss=0.00000078 | Data=0.00313297 | Val RMSE: 0.00075985 | ‚àö(Val Loss) = 0.00088267\n‚úÖ Saved best model at epoch 12 (Val Loss = 0.00000068)\nEpoch 12/80 | Train Loss=0.00313310 | Val Loss=0.00000068 | Data=0.00313310 | Val RMSE: 0.00073850 | ‚àö(Val Loss) = 0.00082677\nEpoch 13/80 | Train Loss=0.00313163 | Val Loss=0.00000114 | Data=0.00313163 | Val RMSE: 0.00091761 | ‚àö(Val Loss) = 0.00106902\n‚úÖ Saved best model at epoch 14 (Val Loss = 0.00000030)\nEpoch 14/80 | Train Loss=0.00311461 | Val Loss=0.00000030 | Data=0.00311461 | Val RMSE: 0.00047407 | ‚àö(Val Loss) = 0.00054619\nEpoch 15/80 | Train Loss=0.00312394 | Val Loss=0.00000116 | Data=0.00312394 | Val RMSE: 0.00093655 | ‚àö(Val Loss) = 0.00107773\nEpoch 16/80 | Train Loss=0.00311342 | Val Loss=0.00000060 | Data=0.00311342 | Val RMSE: 0.00068048 | ‚àö(Val Loss) = 0.00077206\n‚úÖ Saved best model at epoch 17 (Val Loss = 0.00000029)\nEpoch 17/80 | Train Loss=0.00311516 | Val Loss=0.00000029 | Data=0.00311516 | Val RMSE: 0.00049435 | ‚àö(Val Loss) = 0.00054188\nEpoch 18/80 | Train Loss=0.00311643 | Val Loss=0.00000078 | Data=0.00311643 | Val RMSE: 0.00075363 | ‚àö(Val Loss) = 0.00088432\nEpoch 19/80 | Train Loss=0.00310319 | Val Loss=0.00000036 | Data=0.00310319 | Val RMSE: 0.00052700 | ‚àö(Val Loss) = 0.00059943\nEpoch 20/80 | Train Loss=0.00311430 | Val Loss=0.00000059 | Data=0.00311430 | Val RMSE: 0.00066733 | ‚àö(Val Loss) = 0.00076784\nEpoch 21/80 | Train Loss=0.00305449 | Val Loss=0.00000050 | Data=0.00305449 | Val RMSE: 0.00062319 | ‚àö(Val Loss) = 0.00070359\nEpoch 22/80 | Train Loss=0.00305432 | Val Loss=0.00000039 | Data=0.00305432 | Val RMSE: 0.00053970 | ‚àö(Val Loss) = 0.00062845\nEpoch 23/80 | Train Loss=0.00305372 | Val Loss=0.00000032 | Data=0.00305372 | Val RMSE: 0.00049437 | ‚àö(Val Loss) = 0.00056148\n‚úÖ Saved best model at epoch 24 (Val Loss = 0.00000028)\nEpoch 24/80 | Train Loss=0.00305431 | Val Loss=0.00000028 | Data=0.00305431 | Val RMSE: 0.00046400 | ‚àö(Val Loss) = 0.00052616\n‚úÖ Saved best model at epoch 25 (Val Loss = 0.00000026)\nEpoch 25/80 | Train Loss=0.00305419 | Val Loss=0.00000026 | Data=0.00305419 | Val RMSE: 0.00045337 | ‚àö(Val Loss) = 0.00050972\n‚úÖ Saved best model at epoch 26 (Val Loss = 0.00000025)\nEpoch 26/80 | Train Loss=0.00305416 | Val Loss=0.00000025 | Data=0.00305416 | Val RMSE: 0.00044800 | ‚àö(Val Loss) = 0.00050291\n‚úÖ Saved best model at epoch 27 (Val Loss = 0.00000025)\nEpoch 27/80 | Train Loss=0.00305419 | Val Loss=0.00000025 | Data=0.00305419 | Val RMSE: 0.00044342 | ‚àö(Val Loss) = 0.00050067\n‚úÖ Saved best model at epoch 28 (Val Loss = 0.00000025)\nEpoch 28/80 | Train Loss=0.00305428 | Val Loss=0.00000025 | Data=0.00305428 | Val RMSE: 0.00044274 | ‚àö(Val Loss) = 0.00049743\nEpoch 29/80 | Train Loss=0.00305339 | Val Loss=0.00000025 | Data=0.00305339 | Val RMSE: 0.00043806 | ‚àö(Val Loss) = 0.00049880\nEpoch 30/80 | Train Loss=0.00305425 | Val Loss=0.00000025 | Data=0.00305425 | Val RMSE: 0.00044036 | ‚àö(Val Loss) = 0.00049744\nEpoch 31/80 | Train Loss=0.00305422 | Val Loss=0.00000078 | Data=0.00305422 | Val RMSE: 0.00079359 | ‚àö(Val Loss) = 0.00088080\n‚úÖ Saved best model at epoch 32 (Val Loss = 0.00000024)\nEpoch 32/80 | Train Loss=0.00305350 | Val Loss=0.00000024 | Data=0.00305350 | Val RMSE: 0.00043838 | ‚àö(Val Loss) = 0.00049308\n‚úÖ Saved best model at epoch 33 (Val Loss = 0.00000024)\nEpoch 33/80 | Train Loss=0.00305421 | Val Loss=0.00000024 | Data=0.00305421 | Val RMSE: 0.00044163 | ‚àö(Val Loss) = 0.00049278\n‚úÖ Saved best model at epoch 34 (Val Loss = 0.00000024)\nEpoch 34/80 | Train Loss=0.00305411 | Val Loss=0.00000024 | Data=0.00305411 | Val RMSE: 0.00043498 | ‚àö(Val Loss) = 0.00049142\nEpoch 35/80 | Train Loss=0.00305414 | Val Loss=0.00000024 | Data=0.00305414 | Val RMSE: 0.00043380 | ‚àö(Val Loss) = 0.00049290\nEpoch 36/80 | Train Loss=0.00305416 | Val Loss=0.00000024 | Data=0.00305416 | Val RMSE: 0.00043335 | ‚àö(Val Loss) = 0.00049171\nEpoch 37/80 | Train Loss=0.00305412 | Val Loss=0.00000194 | Data=0.00305412 | Val RMSE: 0.00131324 | ‚àö(Val Loss) = 0.00139187\nEpoch 38/80 | Train Loss=0.00305391 | Val Loss=0.00000024 | Data=0.00305391 | Val RMSE: 0.00043252 | ‚àö(Val Loss) = 0.00049196\nEpoch 39/80 | Train Loss=0.00305420 | Val Loss=0.00000025 | Data=0.00305420 | Val RMSE: 0.00045250 | ‚àö(Val Loss) = 0.00049978\nEpoch 40/80 | Train Loss=0.00305416 | Val Loss=0.00000043 | Data=0.00305416 | Val RMSE: 0.00060965 | ‚àö(Val Loss) = 0.00065821\nEpoch 41/80 | Train Loss=0.00304857 | Val Loss=0.00000028 | Data=0.00304857 | Val RMSE: 0.00048567 | ‚àö(Val Loss) = 0.00053294\nEpoch 42/80 | Train Loss=0.00304853 | Val Loss=0.00000026 | Data=0.00304853 | Val RMSE: 0.00045765 | ‚àö(Val Loss) = 0.00050678\n‚úÖ Saved best model at epoch 43 (Val Loss = 0.00000024)\nEpoch 43/80 | Train Loss=0.00304852 | Val Loss=0.00000024 | Data=0.00304852 | Val RMSE: 0.00043926 | ‚àö(Val Loss) = 0.00049094\n‚úÖ Saved best model at epoch 44 (Val Loss = 0.00000024)\nEpoch 44/80 | Train Loss=0.00304854 | Val Loss=0.00000024 | Data=0.00304854 | Val RMSE: 0.00043808 | ‚àö(Val Loss) = 0.00049067\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-cc3e23a2b03e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphys_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mtotal_data_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata_loss\u001b[0m\u001b[0;31m#.item() #TRY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"model(X_batch).shape","metadata":{"_uuid":"ab16f4e0-1acc-499b-8d8e-43e88dc79030","_cell_guid":"35554e79-62f0-483e-95f8-aab488cb1af6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-28T13:07:12.297418Z","iopub.status.idle":"2026-01-28T13:07:12.297700Z","shell.execute_reply":"2026-01-28T13:07:12.297595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_batch.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:07:12.298874Z","iopub.status.idle":"2026-01-28T13:07:12.299198Z","shell.execute_reply":"2026-01-28T13:07:12.299091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:07:12.300132Z","iopub.status.idle":"2026-01-28T13:07:12.300531Z","shell.execute_reply":"2026-01-28T13:07:12.300355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}